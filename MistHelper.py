#!/usr/bin/env python3
"""
MistHelper - Comprehensive Juniper Mist API Data Export Tool
A powerful utility for extracting and analyzing data from Juniper Mist cloud environments.
"""

# ============================================================================
# GLOBAL DEPENDENCY MANAGEMENT AND IMPORT SYSTEM
# ============================================================================
import sys
import time
import socket
import argparse
import getpass
import logging
import logging.handlers
import os
import re
import ipaddress
import multiprocessing
import csv
import subprocess
import traceback
import platform
from math import cos, sin, pi
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed, wait, FIRST_COMPLETED
from threading import Lock
from typing import Tuple, Optional, List, Dict, Any, Union

# ============================================================================
# EARLY LOGGING SETUP
# ============================================================================
# Configure logging IMMEDIATELY after imports to prevent Python from creating
# a default handler that writes script.log to the root directory.
# This configuration will be enhanced later by GlobalImportManager._setup_logging()
# with additional handlers and formatting, but this ensures all early logging
# calls go to the correct location.
_early_log_dir = "data"
os.makedirs(_early_log_dir, exist_ok=True)
_early_log_path = os.path.join(_early_log_dir, "script.log")

# Get log levels from environment (same as GlobalImportManager._setup_logging)
_early_console_level = int(os.environ.get('CONSOLE_LOG_LEVEL', logging.INFO))
_early_file_level = int(os.environ.get('LOGGING_LOG_LEVEL', logging.INFO))

# Create handlers with appropriate levels
_early_console_handler = logging.StreamHandler()
_early_console_handler.setLevel(_early_console_level)
_early_file_handler = logging.FileHandler(_early_log_path)
_early_file_handler.setLevel(_early_file_level)

logging.basicConfig(
    level=logging.DEBUG,  # Root logger captures all, handlers filter
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[_early_file_handler, _early_console_handler],
    force=True
)

# Debug mode detection helper
def is_debug_mode():
    """Check if debug mode is enabled via command line arguments."""
    return '--debug' in sys.argv or '-d' in sys.argv

# Performance monitoring helper for detecting infinite loops
class PerformanceMonitor:
    """Simple performance monitoring to detect hangs and infinite loops."""
    
    def __init__(self, name, max_iterations=10000, log_interval=5.0):
        self.name = name
        self.start_time = time.time()
        self.last_log_time = self.start_time
        self.iteration_count = 0
        self.max_iterations = max_iterations
        self.log_interval = log_interval
        
    def check_iteration(self):
        """Call this on each loop iteration to monitor for hangs."""
        self.iteration_count += 1
        current_time = time.time()
        
        # Log performance periodically
        if is_debug_mode() and (current_time - self.last_log_time) >= self.log_interval:
            elapsed = current_time - self.start_time
            print(f"[PERF] {self.name}: {self.iteration_count} iterations in {elapsed:.1f}s")
            self.last_log_time = current_time
            
        # Circuit breaker for infinite loops
        if self.iteration_count > self.max_iterations:
            error_msg = f"CIRCUIT BREAKER: {self.name} exceeded {self.max_iterations} iterations!"
            print(f"[EMERGENCY] {error_msg}")
            logging.error(error_msg)
            raise RuntimeError(error_msg)
            
    def finish(self):
        """Call when loop completes normally."""
        elapsed = time.time() - self.start_time
        if is_debug_mode():
            print(f"[PERF] {self.name} completed: {self.iteration_count} iterations in {elapsed:.1f}s")

# ============================================================================
# EARLY DEPENDENCY AUTO-INSTALLER
# ============================================================================
# This section attempts to auto-install critical dependencies BEFORE any imports
# that might fail. This enables running the script directly without pre-setup.

# Package name to import name mapping for special cases
PACKAGE_IMPORT_MAP = {
    'websocket-client': 'websocket',
    'python-dotenv': 'dotenv',
    'usaddress-scourgify': 'scourgify',
    'pillow': 'PIL',
    'beautifulsoup4': 'bs4',
    'pyyaml': 'yaml',
    'python-dateutil': 'dateutil',
    'msgpack-python': 'msgpack',
}

def _parse_requirements_file(filepath='requirements.txt'):
    """
    Parse requirements.txt and return list of package specifications.
    
    SECURITY: Only reads from requirements.txt - no arbitrary file access.
    Skips commented lines, empty lines, and development dependencies.
    
    Returns:
        List of (package_name, package_spec) tuples
    """
    packages = []
    try:
        with open(filepath, 'r', encoding='utf-8') as requirements_file:
            for line in requirements_file:
                line = line.strip()
                
                # Skip empty lines and comments
                if not line or line.startswith('#'):
                    continue
                
                # Skip commented-out dev dependencies
                if line.startswith('# pytest') or line.startswith('# coverage'):
                    continue
                
                # Strip inline comments (e.g., "package>=1.0  # comment" -> "package>=1.0")
                if '#' in line:
                    line = line.split('#')[0].strip()
                
                # Extract package name from spec (e.g., "requests>=2.28.0" -> "requests")
                package_spec = line
                package_name = re.split(r'[><=!]', package_spec)[0].strip()
                
                packages.append((package_name, package_spec))
                
        logging.debug(f"Parsed {len(packages)} packages from {filepath}")
        return packages
    except FileNotFoundError:
        logging.warning(f"Requirements file not found: {filepath}")
        return []
    except Exception as parse_error:
        logging.warning(f"Error parsing requirements file: {parse_error}")
        return []

def _early_dependency_check():
    """
    Check and auto-install critical dependencies before they're imported.
    
    WORKFLOW:
    1. Check for missing dependencies
    2. Check if UV is installed
    3. If UV missing -> install UV with pip
    4. Verify UV is now available
    5. Use UV to install/update packages (with per-package pip fallback)
    
    SECURITY: Only installs from requirements.txt - no arbitrary package execution.
    This runs before main import logic to enable direct script execution.
    """
    # Check if auto-install is disabled
    if os.getenv("DISABLE_AUTO_INSTALL", "false").lower() == "true":
        logging.debug("Early dependency auto-install disabled via DISABLE_AUTO_INSTALL")
        return
    
    # Parse requirements.txt for all dependencies
    all_packages = _parse_requirements_file()
    if not all_packages:
        logging.warning("No packages found in requirements.txt - skipping dependency check")
        return
    
    # Quick check: try importing each package
    missing_packages = []
    for package_name, package_spec in all_packages:
        # Handle package name vs import name differences
        import_name = PACKAGE_IMPORT_MAP.get(package_name, package_name)
        
        try:
            __import__(import_name)
        except ImportError:
            missing_packages.append((package_name, package_spec))
            logging.info(f"Missing dependency detected: {package_name}")
    
    if not missing_packages:
        logging.debug(f"All {len(all_packages)} dependencies from requirements.txt present")
        return
    
    logging.info(f"Attempting to auto-install {len(missing_packages)} missing dependencies...")
    
    # Step 1: Check if UV is installed
    use_uv = False
    try:
        uv_result = subprocess.run(['uv', '--version'], 
                                  capture_output=True, text=True, timeout=5)
        use_uv = uv_result.returncode == 0
        if use_uv:
            logging.info(f"UV package manager detected: {uv_result.stdout.strip()}")
    except (FileNotFoundError, subprocess.SubprocessError):
        logging.info("UV package manager not found")
    
    # Step 2: If UV not installed, try to install it with pip
    if not use_uv:
        logging.info("Attempting to install UV package manager with pip...")
        try:
            install_result = subprocess.run(
                [sys.executable, '-m', 'pip', 'install', 'uv'],
                capture_output=True, text=True, timeout=30
            )
            if install_result.returncode == 0:
                logging.info("UV package manager installed successfully")
                # Step 3: Verify UV is now available
                try:
                    verify_result = subprocess.run(['uv', '--version'],
                                                  capture_output=True, text=True, timeout=5)
                    use_uv = verify_result.returncode == 0
                    if use_uv:
                        logging.info(f"UV verified: {verify_result.stdout.strip()}")
                except (FileNotFoundError, subprocess.SubprocessError):
                    logging.warning("UV installation succeeded but uv command not found in PATH")
                    use_uv = False
            else:
                logging.warning(f"Failed to install UV with pip: {install_result.stderr.strip()}")
        except Exception as install_error:
            logging.warning(f"Could not install UV: {install_error}")
    
    # Log installation strategy
    if use_uv:
        logging.info("Using UV for package installations (pip fallback per package if needed)")
    else:
        logging.info("Using pip for package installations (UV unavailable)")
    
    # Step 4: Install/update missing packages
    success_count = 0
    failure_count = 0
    
    for package_name, package_spec in missing_packages:
        installed = False
        
        # Try UV first if available
        if use_uv:
            try:
                cmd = ['uv', 'pip', 'install', '--python', sys.executable, package_spec]
                logging.info(f"Installing {package_spec} with UV...")
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
                
                if result.returncode == 0:
                    logging.info(f"Successfully installed {package_spec} with UV")
                    success_count += 1
                    installed = True
                else:
                    # UV failed - log and try pip fallback
                    logging.warning(f"UV installation failed for {package_spec}: {result.stderr.strip()}")
                    logging.info(f"Retrying {package_spec} with pip fallback...")
            except Exception as uv_error:
                logging.warning(f"UV installation error for {package_spec}: {uv_error}")
                logging.info(f"Retrying {package_spec} with pip fallback...")
        
        # Try pip if UV not available or UV failed
        if not installed:
            try:
                cmd = [sys.executable, '-m', 'pip', 'install', package_spec]
                logging.info(f"Installing {package_spec} with pip...")
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
                
                if result.returncode == 0:
                    logging.info(f"Successfully installed {package_spec} with pip")
                    success_count += 1
                    installed = True
                else:
                    logging.error(f"Pip installation failed for {package_spec}: {result.stderr.strip()}")
                    failure_count += 1
            except Exception as pip_error:
                logging.error(f"Could not install {package_spec} with pip: {pip_error}")
                failure_count += 1
    
    logging.info(f"Early dependency check completed: {success_count} installed, {failure_count} failed")

# Run early dependency check (will be skipped if DISABLE_AUTO_INSTALL=true)
_early_dependency_check()

# Additional standard library imports
from pathlib import Path
import json
import sqlite3
import datetime
from datetime import timezone, timedelta
import threading
import concurrent.futures
import ast
import math
import shutil
import glob
import difflib
import unicodedata
from collections import defaultdict
import inspect

# Third-party imports for static analysis (with fallbacks)
try:
    from prettytable import PrettyTable
except ImportError:
    PrettyTable = None

try:
    import numpy as np
except ImportError:
    np = None

try:
    import websocket
except ImportError:
    websocket = None

try:
    from difflib import SequenceMatcher
except ImportError:
    SequenceMatcher = None

# Import mistapi later through GlobalImportManager for better dependency management
mistapi = None

# tqdm will be properly imported by GlobalImportManager
# This fallback will be overridden by the real tqdm import
def tqdm(iterable, *args, **kwargs):
    """Fallback tqdm function - will be replaced by real tqdm after import initialization."""
    return iterable

try:
    import requests
except ImportError:
    requests = None

try:
    import urllib3
except ImportError:
    urllib3 = None

try:
    import pyte
except ImportError:
    pyte = None

try:
    import paramiko
    from paramiko import SSHClient, AutoAddPolicy
except ImportError:
    paramiko = None
    SSHClient = None
    AutoAddPolicy = None

# Optional imports with fallbacks
try:
    from scourgify import normalize_address_record
except ImportError:
    normalize_address_record = None

try:
    from rapidfuzz import fuzz
except ImportError:
    fuzz = None

# Keyboard listener functionality has been removed for simplicity
listen_keyboard = None
stop_listening = None

def listen_keyboard(*args, **kwargs):
    """Keyboard listener has been removed - this is a no-op fallback."""
    logging.info("Keyboard listener functionality has been removed")
    return None

def stop_listening():
    """No-op fallback for removed keyboard listener functionality."""
    pass

# ============================================================================
# CENTRALIZED PAGINATION DEFAULTS
# ============================================================================
# Several legacy code paths relied on the mistapi client's implicit default page
# size (commonly 100). That caused excessive paging (e.g., 10x HTTP calls for
# 1000-item datasets). We unify a single configurable default via environment
# variable MIST_PAGE_LIMIT (clamped to 1..1000). All new/updated listOrgSites /
# getOrgInventory calls should pass limit=DEFAULT_API_PAGE_LIMIT or use the
# helper wrappers below to ensure consistency and simpler tuning.
try:
    _raw_page_limit_env = os.environ.get("MIST_PAGE_LIMIT", "1000").strip()
    _parsed_limit = int(_raw_page_limit_env)
except Exception:
    _parsed_limit = 1000

DEFAULT_API_PAGE_LIMIT = max(1, min(_parsed_limit, 1000))
if _parsed_limit != DEFAULT_API_PAGE_LIMIT:
    logging.warning(
        f"MIST_PAGE_LIMIT value {_parsed_limit} adjusted to {DEFAULT_API_PAGE_LIMIT} (valid range 1..1000)"
    )

logging.debug(f"API Page Size Configuration Active: DEFAULT_API_PAGE_LIMIT={DEFAULT_API_PAGE_LIMIT}")

def fetch_all_sites_with_limit(org_id):
    """Fetch all sites with unified pagination.

    SECURITY: Read-only; no sensitive data logged.
    """
    resp = mistapi.api.v1.orgs.sites.listOrgSites(apisession, org_id, limit=DEFAULT_API_PAGE_LIMIT)
    return mistapi.get_all(response=resp, mist_session=apisession)

def fetch_all_inventory_with_limit(org_id):
    """Fetch full org inventory with unified pagination.

    SECURITY: Read-only; no secrets in inventory object fields.
    """
    resp = mistapi.api.v1.orgs.inventory.getOrgInventory(apisession, org_id, limit=DEFAULT_API_PAGE_LIMIT)
    return mistapi.get_all(response=resp, mist_session=apisession)

# Early dotenv import for configuration loading
try:
    from dotenv import load_dotenv
    DOTENV_AVAILABLE = True
    load_dotenv()
except ImportError:
    DOTENV_AVAILABLE = False
    # Manual .env loading fallback when python-dotenv is not available
    def load_dotenv():
        """Fallback .env loader when python-dotenv package is not installed."""
        try:
            with open(".env", "r") as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, value = line.split('=', 1)
                        os.environ[key.strip()] = value.strip()
        except FileNotFoundError:
            logging.debug("No .env file found")
        except Exception as e:
            logging.debug(f"Error loading .env file: {e}")
    load_dotenv()

class GlobalImportManager:
    """
    Centralized import and dependency management system for MistHelper.
    
    This class handles:
    - UV-based package installation and upgrades
    - Centralized import management
    - Dependency verification and auto-installation
    - Graceful handling of optional dependencies
    - Performance optimization through early imports
    """
    
    def __init__(self):
        """Initialize the import manager with configuration from environment variables."""
        # Configuration from .env file
        # For local development: UV enabled, auto-upgrades enabled
        # For containers: These are overridden by environment variables
        self.auto_upgrade_uv = os.getenv("AUTO_UPGRADE_UV", "true").lower() == "true"  # Default to true for local UV usage
        self.auto_upgrade_dependencies = os.getenv("AUTO_UPGRADE_DEPENDENCIES", "true").lower() == "true"  # Default to true for local development
        self.upgrade_check_timeout = int(os.getenv("UPGRADE_CHECK_TIMEOUT", "30"))  # Shorter timeout
        self.csv_freshness_minutes = int(os.getenv("CSV_FRESHNESS_MINUTES", "15"))
        # Only check for UV updates once per day by default
        self.uv_update_check_hours = int(os.getenv("UV_UPDATE_CHECK_HOURS", "24"))
        # Option to completely disable UV checking (useful for containers)
        self.disable_uv_check = os.getenv("DISABLE_UV_CHECK", "false").lower() == "true"
        # Option to completely disable auto-installation (useful for containers)
        self.disable_auto_install = os.getenv("DISABLE_AUTO_INSTALL", "false").lower() == "true"
        
        # Dependency tracking
        self.required_packages = {}
        self.optional_packages = {}
        self.failed_imports = []
        self.installed_packages = []
        
        # Import storage for global access
        self.imports = {}
        
        # UV availability cache to avoid repeated checks
        self._uv_available = None
        self._uv_checked = False
        self._last_uv_update_check = None  # Track when we last checked for UV updates
        
        # Import name mappings for cases where package name != import name
        self.import_name_mappings = {
            'websocket-client': 'websocket',      # websocket-client package provides websocket module
            'python-dotenv': 'dotenv',            # python-dotenv package provides dotenv module
            'usaddress-scourgify': 'scourgify',   # usaddress-scourgify package provides scourgify module
            'pillow': 'PIL',                      # Pillow package provides PIL module
            'beautifulsoup4': 'bs4',              # beautifulsoup4 package provides bs4 module
            'pyyaml': 'yaml',                     # PyYAML package provides yaml module
            'python-dateutil': 'dateutil',       # python-dateutil package provides dateutil module
            'msgpack-python': 'msgpack',          # msgpack-python package provides msgpack module
        }
        
        # Special import handlers for complex cases
        self.special_import_handlers = {
            'concurrent.futures': self._import_concurrent_futures,
            'datetime': self._import_datetime,
            'tqdm': self._import_tqdm,
        }
        
        # Initialize logging early
        self._setup_logging()
        
        # Detect virtual environment for better package management
        self._detect_virtual_environment()
        
        # Define all required and optional packages
        self._define_package_requirements()
        
    def _detect_virtual_environment(self):
        """Detect if we're running in a virtual environment and log info."""
        self.in_venv = hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix)
        
        if self.in_venv:
            venv_path = getattr(sys, 'prefix', 'unknown')
            logging.info(f"Running in virtual environment: {venv_path}")
            logging.info(f"Python executable: {sys.executable}")
        else:
            logging.info("Running in system Python environment")
            logging.info(f"Python executable: {sys.executable}")
        
    def _setup_logging(self):
        """Setup basic logging configuration with environment-specific levels."""
        # Get console and file log levels from environment (default to INFO if not set)
        console_log_level = int(os.environ.get('CONSOLE_LOG_LEVEL', logging.INFO))
        file_log_level = int(os.environ.get('LOGGING_LOG_LEVEL', logging.INFO))
        
        # Create console handler with environment-specified level
        console_handler = logging.StreamHandler()
        console_handler.setLevel(console_log_level)
        console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        console_handler.setFormatter(console_formatter)
        
        # Create file handler with environment-specified level
        # Use data directory for log files to ensure write permissions in container
        log_file_path = os.path.join("data", "script.log")
        os.makedirs("data", exist_ok=True)  # Ensure data directory exists
        file_handler = logging.FileHandler(log_file_path)
        file_handler.setLevel(file_log_level)
        file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(file_formatter)
        
        # Configure root logger to capture all messages, handlers will filter
        logging.basicConfig(
            level=logging.DEBUG,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[file_handler, console_handler],
            force=True  # Override any existing configuration
        )
        
    def _define_package_requirements(self):
        """Define all required and optional package dependencies."""
        # Required packages (core functionality)
        self.required_packages = {
            # Core API and networking
            'mistapi': 'mistapi>=0.3.0',
            'requests': 'requests>=2.28.0',
            'websocket-client': 'websocket-client>=1.4.0',
            
            # CLI and user interface
            'prettytable': 'prettytable>=3.5.0',
            'tqdm': 'tqdm>=4.64.0',
            
            # Data processing
            'numpy': 'numpy>=1.24.0',
            'python-dotenv': 'python-dotenv>=1.0.0',
            
            # SSH and direct device connections  
            'paramiko': 'paramiko>=2.9.0',  # More compatible version for SSH
            
            # Standard library modules (no installation needed)
            'argparse': None,  # Built-in
            'csv': None,       # Built-in
            'json': None,      # Built-in
            'sqlite3': None,   # Built-in
            'time': None,      # Built-in
            'datetime': None,  # Built-in
            'threading': None, # Built-in
            'concurrent.futures': None,  # Built-in
            'inspect': None,   # Built-in
            'http.client': None,  # Built-in
            're': None,        # Built-in
            'difflib': None,   # Built-in
            'unicodedata': None,  # Built-in
            'collections': None,  # Built-in
            'ast': None,       # Built-in
            'math': None,      # Built-in
            'shutil': None,    # Built-in
            'glob': None,      # Built-in
            'traceback': None, # Built-in
            'collections': None,  # Built-in
            'glob': None,      # Built-in
            'traceback': None, # Built-in
        }
        
        # Optional packages (enhanced functionality)
        optional_packages_raw = {
            'sshkeyboard': 'sshkeyboard>=2.3.0',
            'pyte': 'pyte>=0.8.0',
            'usaddress-scourgify': 'usaddress-scourgify>=0.6.0',
            'rapidfuzz': 'rapidfuzz>=3.8.0',
            'urllib3': 'urllib3>=1.26.0',
            'plotly': 'plotly>=5.14.0',
            'dash': 'dash>=2.9.0',
            'kaleido': 'kaleido>=0.2.1',
            'matplotlib': 'matplotlib>=3.5.0',
        }
        # Filter out None values (platform-incompatible packages)
        self.optional_packages = {k: v for k, v in optional_packages_raw.items() if v is not None}
        
    def check_uv_installation(self) -> bool:
        """Check if UV package manager is installed and accessible (cached)."""
        # If UV checking is disabled, return False immediately
        if self.disable_uv_check:
            return False
            
        # Return cached result if already checked
        if self._uv_checked:
            return self._uv_available
            
        try:
            result = subprocess.run(['uv', '--version'], 
                                  capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                logging.info(f"UV package manager found: {result.stdout.strip()}")
                self._uv_available = True
            else:
                logging.warning("UV package manager not found or not working properly")
                self._uv_available = False
        except (subprocess.TimeoutExpired, FileNotFoundError, subprocess.SubprocessError) as e:
            logging.warning(f"UV package manager check failed: {e}")
            self._uv_available = False
            
        # Cache the result
        self._uv_checked = True
        return self._uv_available
            
    def install_uv(self) -> bool:
        """Install UV package manager if not present."""
        if not self.auto_upgrade_uv:
            logging.info("Auto-upgrade of UV is disabled in configuration")
            return False
            
        logging.info("Attempting to install UV package manager...")
        try:
            # Try installing UV using pip as fallback
            result = subprocess.run([sys.executable, '-m', 'pip', 'install', 'uv'], 
                                  capture_output=True, text=True, timeout=self.upgrade_check_timeout)
            if result.returncode == 0:
                logging.info("UV package manager installed successfully via pip")
                return True
            else:
                logging.error(f"Failed to install UV via pip: {result.stderr}")
                return False
        except (subprocess.TimeoutExpired, subprocess.SubprocessError) as e:
            logging.error(f"Failed to install UV package manager: {e}")
            return False
            
    def upgrade_uv(self) -> bool:
        """Upgrade UV package manager to latest version (only if needed)."""
        if not self.auto_upgrade_uv:
            return True
        
        # Check if we've recently checked for updates
        now = time.time()
        if self._last_uv_update_check:
            hours_since_last_check = (now - self._last_uv_update_check) / 3600
            if hours_since_last_check < self.uv_update_check_hours:
                logging.debug(f"UV update check skipped (last check {hours_since_last_check:.1f} hours ago, threshold: {self.uv_update_check_hours} hours)")
                return True
            
        try:
            logging.info("Checking for UV package manager updates...")
            # First try UV self-update (for standalone installations)
            result = subprocess.run(['uv', 'self', 'update'], 
                                  capture_output=True, text=True, timeout=self.upgrade_check_timeout)
            
            # Update the last check time regardless of result
            self._last_uv_update_check = now
            
            if result.returncode == 0:
                logging.info("UV package manager updated successfully")
                return True
            else:
                # If self-update fails, try pip upgrade (for pip-installed UV)
                if "Self-update is only available for uv binaries installed via the standalone installation scripts" in result.stderr:
                    logging.info("UV was installed via pip, attempting pip upgrade...")
                    pip_result = subprocess.run([sys.executable, '-m', 'pip', 'install', '--upgrade', 'uv'], 
                                              capture_output=True, text=True, timeout=self.upgrade_check_timeout)
                    if pip_result.returncode == 0:
                        logging.info("UV package manager updated successfully via pip")
                        return True
                    else:
                        logging.warning(f"Failed to upgrade UV via pip: {pip_result.stderr}")
                        return True  # Non-critical failure
                else:
                    logging.warning(f"UV self-update returned non-zero: {result.stderr}")
                    return True  # Non-critical failure
        except (subprocess.TimeoutExpired, subprocess.SubprocessError) as e:
            # Still update the last check time to avoid repeated failures
            self._last_uv_update_check = now
            logging.warning(f"UV self-update failed: {e}")
            return True  # Non-critical failure
            
    def install_package_with_uv(self, package_spec: str) -> bool:
        """Install a package using UV package manager with fast resolution and virtual environment awareness."""
        try:
            logging.debug(f"Installing package with UV: {package_spec}")
            
            # Check if we're in a virtual environment and prefer venv's UV if available
            uv_cmd = 'uv'
            if hasattr(self, 'in_venv') and self.in_venv:
                # Try to use UV from the virtual environment first
                venv_uv = os.path.join(os.path.dirname(sys.executable), 'uv.exe')
                if os.path.exists(venv_uv):
                    uv_cmd = venv_uv
                    logging.debug(f"Using venv UV: {venv_uv}")
                
                # Use UV with the current Python environment
                cmd = [uv_cmd, 'pip', 'install', '--python', sys.executable, '--no-build-isolation', package_spec]
            else:
                # Use UV with default behavior
                cmd = [uv_cmd, 'pip', 'install', '--no-build-isolation', package_spec]
                
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=self.upgrade_check_timeout)
            if result.returncode == 0:
                logging.info(f"Successfully installed {package_spec} with UV")
                return True
            else:
                # Try without --no-build-isolation if it failed
                logging.debug(f"UV install failed with --no-build-isolation, retrying without it")
                if hasattr(self, 'in_venv') and self.in_venv:
                    cmd = [uv_cmd, 'pip', 'install', '--python', sys.executable, package_spec]
                else:
                    cmd = [uv_cmd, 'pip', 'install', package_spec]
                    
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=self.upgrade_check_timeout)
                if result.returncode == 0:
                    logging.info(f"Successfully installed {package_spec} with UV (fallback)")
                    return True
                else:
                    logging.warning(f"UV install failed for {package_spec}: {result.stderr}")
                    return False
        except (subprocess.TimeoutExpired, subprocess.SubprocessError) as e:
            logging.warning(f"Failed to install {package_spec} with UV: {e}")
            return False
            
    def install_package_with_pip(self, package_spec: str) -> bool:
        """Install a package using pip as fallback with virtual environment awareness."""
        try:
            logging.info(f"Installing package with pip: {package_spec}")
            # Always use the current Python executable to ensure installation in the right environment
            result = subprocess.run([sys.executable, '-m', 'pip', 'install', package_spec], 
                                  capture_output=True, text=True, timeout=self.upgrade_check_timeout)
            if result.returncode == 0:
                logging.info(f"Successfully installed {package_spec} with pip")
                return True
            else:
                logging.error(f"Failed to install {package_spec} with pip: {result.stderr}")
                return False
        except (subprocess.TimeoutExpired, subprocess.SubprocessError) as e:
            logging.error(f"Failed to install {package_spec} with pip: {e}")
            return False
            
    def should_check_uv_update(self) -> bool:
        """Check if we should check for UV updates based on time since last check."""
        if not self.auto_upgrade_uv:
            return False
            
        if self._last_uv_update_check is None:
            return True
            
        time_since_check = time.time() - self._last_uv_update_check
        hours_since_check = time_since_check / 3600
        return hours_since_check >= self.uv_update_check_hours
    
    def check_uv_needs_update(self) -> bool:
        """Check if UV actually needs an update by comparing versions."""
        try:
            # Get current UV version
            result = subprocess.run(['uv', '--version'], 
                                  capture_output=True, text=True, timeout=5)
            if result.returncode != 0:
                return False
                
            # For now, we'll assume UV is up to date since checking remote version is complex
            # In a production environment, you might want to implement version comparison
            logging.debug("UV version check complete - assuming current version is adequate")
            return False
            
        except (subprocess.TimeoutExpired, subprocess.SubprocessError):
            return False
        """Check if a package is already installed."""
        try:
            result = subprocess.run([sys.executable, '-m', 'pip', 'show', package_name], 
                                  capture_output=True, text=True, timeout=10)
            return result.returncode == 0
        except (subprocess.TimeoutExpired, subprocess.SubprocessError):
            return False
    
    def upgrade_all_dependencies(self) -> bool:
        """Install missing dependencies and upgrade existing ones."""
        if not self.auto_upgrade_dependencies:
            logging.info("Auto-upgrade of dependencies is disabled in configuration")
            return True
            
        # Collect package specs, filtering out built-in modules
        packages_to_process = []
        for pkg_name, pkg_spec in {**self.required_packages, **self.optional_packages}.items():
            if pkg_spec is not None:  # Skip built-in modules
                packages_to_process.append((pkg_name, pkg_spec))
                
        if not packages_to_process:
            logging.info("No packages to process")
            return True
        
        logging.info(f"Processing {len(packages_to_process)} packages...")
        
        # Process packages individually for better error handling
        uv_available = self.check_uv_installation()
        if uv_available:
            logging.info("Using UV package manager for installations")
        else:
            logging.info("Using pip for package installations (UV not available)")
            
        success_count = 0
        
        for pkg_name, pkg_spec in packages_to_process:
            # Extract base package name from spec (e.g., "requests>=2.28.0" -> "requests")
            base_name = pkg_spec.split('>=')[0].split('==')[0].split('<')[0].split('>')[0].strip()
            
            try:
                # Try UV first if available
                if uv_available:
                    if self.install_package_with_uv(pkg_spec):
                        success_count += 1
                        continue
                
                # Fallback to pip
                if self.install_package_with_pip(pkg_spec):
                    success_count += 1
                else:
                    logging.warning(f"Failed to install/upgrade {pkg_spec}")
                    
            except Exception as e:
                logging.warning(f"Error processing package {pkg_spec}: {e}")
        
        logging.info(f"Successfully processed {success_count}/{len(packages_to_process)} packages")
        return success_count > 0
    
    def _import_concurrent_futures(self):
        """Special handler for concurrent.futures import."""
        from concurrent.futures import ThreadPoolExecutor, as_completed
        return type('ConcurrentFutures', (), {
            'ThreadPoolExecutor': ThreadPoolExecutor,
            'as_completed': as_completed
        })()
    
    def _import_datetime(self):
        """Special handler for datetime import."""
        # Import the actual datetime module for module-level access
        import datetime as dt_module
        from datetime import datetime, timezone, timedelta
        
        # The code expects 'datetime' to refer to the datetime class, not the module
        # But we also need module-level access. Create a special object that behaves like both.
        class DateTimeHandler:
            def __init__(self):
                # Make this object callable like datetime class
                self.now = datetime.now
                self.fromtimestamp = datetime.fromtimestamp
                self.fromisoformat = datetime.fromisoformat
                self.strptime = datetime.strptime
                self.utcnow = datetime.utcnow
                # Add module attributes
                self.datetime = datetime
                self.timezone = timezone
                self.timedelta = timedelta
                # Add module for UTC access
                self.timezone = timezone
                
            def __call__(self, *args, **kwargs):
                # Allow calling like datetime()
                return datetime(*args, **kwargs)
                
        return DateTimeHandler()
    
    def _import_tqdm(self):
        """Special handler for tqdm import to ensure proper functionality."""
        try:
            from tqdm import tqdm
            logging.debug("Successfully imported tqdm from package")
            return tqdm
        except ImportError:
            logging.warning("tqdm package not available, using fallback")
            # Return the fallback function if tqdm is not available
            def tqdm_fallback(iterable, *args, **kwargs):
                """Fallback when tqdm package is not available."""
                desc = kwargs.get('desc', 'Processing')
                unit = kwargs.get('unit', 'item')
                if hasattr(iterable, '__len__'):
                    total = len(iterable)
                    logging.info(f"{desc}: {total} {unit}s to process")
                else:
                    logging.info(f"{desc}: processing {unit}s...")
                return iterable
            return tqdm_fallback
    
    def _check_and_upgrade_package(self, module_name: str, package_spec: str) -> bool:
        """Check if a package needs upgrading and upgrade it if necessary."""
        if not package_spec:
            return True  # Built-in modules don't need upgrading
            
        try:
            # Extract package name from spec
            package_name = package_spec.split('>=')[0].split('==')[0].split('<')[0].split('>')[0].strip()
            
            # Check current version
            result = subprocess.run([sys.executable, '-m', 'pip', 'show', package_name], 
                                  capture_output=True, text=True, timeout=10)
            
            if result.returncode != 0:
                logging.debug(f"Package {package_name} not found, skipping upgrade check")
                return True
                
            # Parse current version from pip show output
            current_version = None
            for line in result.stdout.split('\n'):
                if line.startswith('Version:'):
                    current_version = line.split(':', 1)[1].strip()
                    break
                    
            if current_version:
                logging.debug(f"Current version of {package_name}: {current_version}")
                
                # Try to upgrade the package
                logging.info(f"  Checking for updates to {package_name}...")
                
                # Use UV if available, otherwise pip
                if self.check_uv_installation():
                    # Check if we're in a virtual environment and prefer venv's UV if available
                    uv_cmd = 'uv'
                    if hasattr(self, 'in_venv') and self.in_venv:
                        # Try to use UV from the virtual environment first
                        venv_uv = os.path.join(os.path.dirname(sys.executable), 'uv.exe')
                        if os.path.exists(venv_uv):
                            uv_cmd = venv_uv
                        upgrade_cmd = [uv_cmd, 'pip', 'install', '--python', sys.executable, '--upgrade', package_spec]
                    else:
                        upgrade_cmd = [uv_cmd, 'pip', 'install', '--upgrade', package_spec]
                else:
                    upgrade_cmd = [sys.executable, '-m', 'pip', 'install', '--upgrade', package_spec]
                
                upgrade_result = subprocess.run(upgrade_cmd, capture_output=True, text=True, timeout=self.upgrade_check_timeout)
                
                if upgrade_result.returncode == 0:
                    # Check if version actually changed
                    new_result = subprocess.run([sys.executable, '-m', 'pip', 'show', package_name], 
                                              capture_output=True, text=True, timeout=10)
                    
                    new_version = None
                    for line in new_result.stdout.split('\n'):
                        if line.startswith('Version:'):
                            new_version = line.split(':', 1)[1].strip()
                            break
                    
                    if new_version and new_version != current_version:
                        logging.info(f"  [OK] {package_name}: Upgraded from {current_version} to {new_version}")
                        return True
                    else:
                        logging.debug(f"  [OK] {package_name}: Already up to date ({current_version})")
                        return True
                else:
                    logging.debug(f"  [WARN] {package_name}: Upgrade check failed: {upgrade_result.stderr}")
                    return True  # Non-critical failure
            
            return True
            
        except Exception as e:
            logging.debug(f"Error checking/upgrading {module_name}: {e}")
            return True  # Non-critical failure
    
    def _get_actual_import_name(self, module_name: str) -> str:
        """Get the actual import name for a given module name, handling mappings."""
        return self.import_name_mappings.get(module_name, module_name)
            
    def import_module_safely(self, module_name: str, package_spec: Optional[str] = None, 
                           required: bool = True, skip_deps: bool = False, skip_upgrade: bool = True) -> Optional[Any]:
        """
        Safely import a module with automatic installation and upgrade checking if needed.
        
        Args:
            module_name: Name of the module to import
            package_spec: Package specification for installation (e.g., 'requests>=2.28.0')
            required: Whether this is a required dependency
            skip_deps: Whether to skip dependency checking and installation
            skip_upgrade: Whether to skip upgrade checking (default True for faster imports)
            
        Returns:
            The imported module or None if import failed
        """
        try:
            # Check if we have a special handler for this module
            if module_name in self.special_import_handlers:
                module = self.special_import_handlers[module_name]()
            else:
                # Use mapping to get actual import name
                actual_import_name = self._get_actual_import_name(module_name)
                module = __import__(actual_import_name)
                
            self.imports[module_name] = module
            logging.debug(f"Successfully imported {module_name}")
            
            # Check if we should upgrade existing packages (only if auto-upgrade is enabled and not skipping)
            if package_spec and self.auto_upgrade_dependencies and not skip_deps and not skip_upgrade:
                self._check_and_upgrade_package(module_name, package_spec)
            
            return module
            
        except ImportError as e:
            logging.warning(f"Failed to import {module_name}: {e}")
            
            # Attempt to install if package spec is provided and auto-installation is enabled
            if package_spec and self.auto_upgrade_dependencies and not skip_deps and not self.disable_auto_install:
                logging.info(f"Attempting to install missing dependency: {package_spec}")
                
                # Try installing the package
                installed = False
                
                # Try UV first if available (using cached check)
                if self.check_uv_installation():
                    logging.debug(f"Trying UV installation for {package_spec}")
                    installed = self.install_package_with_uv(package_spec)
                
                # Fallback to pip if UV failed or not available
                if not installed:
                    logging.debug(f"Trying pip installation for {package_spec}")
                    installed = self.install_package_with_pip(package_spec)
                    
                if installed:
                    # Clear import caches to allow fresh import
                    import importlib
                    importlib.invalidate_caches()
                    
                    # Remove any cached failed imports
                    actual_import_name = self._get_actual_import_name(module_name)
                    modules_to_clear = [actual_import_name, module_name]
                    for mod_name in modules_to_clear:
                        if mod_name in sys.modules:
                            del sys.modules[mod_name]
                            logging.debug(f"Cleared cached module: {mod_name}")
                    
                    # Wait a moment for installation to complete
                    time.sleep(0.5)
                    
                    # Retry import after installation
                    try:
                        if module_name in self.special_import_handlers:
                            module = self.special_import_handlers[module_name]()
                        else:
                            actual_import_name = self._get_actual_import_name(module_name)
                            module = __import__(actual_import_name)
                            
                        self.imports[module_name] = module
                        self.installed_packages.append(package_spec)
                        logging.info(f"Successfully imported {module_name} after installation")
                        return module
                        
                    except ImportError as retry_e:
                        logging.error(f"Import still failed after installation for {module_name}: {retry_e}")
                        # For optional packages, this is not critical
                        if not required:
                            logging.info(f"Optional package {module_name} installation succeeded but import failed - likely needs system restart or different Python session")
                else:
                    logging.error(f"Failed to install {package_spec}")
                        
            # Handle failure
            if required:
                self.failed_imports.append(module_name)
                logging.error(f"Required dependency {module_name} could not be imported or installed")
            else:
                logging.warning(f"Optional dependency {module_name} not available")
                
            return None
            
    def _import_packages_concurrently(self, packages_dict, required=True, skip_deps=False, max_workers=4):
        """
        Import packages concurrently for faster dependency resolution.
        
        Args:
            packages_dict: Dictionary of package_name: package_spec
            required: Whether these are required (True) or optional (False) packages
            skip_deps: Whether to skip dependency installation
            max_workers: Maximum number of concurrent workers
        """
        import concurrent.futures
        import threading
        
        # Thread-safe logging
        log_lock = threading.Lock()
        
        def import_single_package(package_info):
            module_name, package_spec = package_info
            package_type = "required" if required else "optional"
            
            with log_lock:
                logging.info(f"  Checking {package_type} dependency: {module_name} ({package_spec or 'built-in'})")
            
            result = self.import_module_safely(module_name, package_spec, required=required, skip_deps=skip_deps, skip_upgrade=True)
            
            with log_lock:
                if result:
                    logging.info(f"  [OK] {module_name}: Available")
                else:
                    if required:
                        logging.error(f"  [FAIL] {module_name}: Failed to import")
                    else:
                        logging.warning(f"  [WARN] {module_name}: Not available")
            
            return module_name, result
        
        # Split packages into built-in and external for better processing
        builtin_packages = {k: v for k, v in packages_dict.items() if v is None}
        external_packages = {k: v for k, v in packages_dict.items() if v is not None}
        
        # Process built-in packages first (fast, no network needed)
        for module_name, package_spec in builtin_packages.items():
            import_single_package((module_name, package_spec))
        
        # Process external packages concurrently
        if external_packages:
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_package = {
                    executor.submit(import_single_package, item): item 
                    for item in external_packages.items()
                }
                
                for future in concurrent.futures.as_completed(future_to_package):
                    package_info = future_to_package[future]
                    try:
                        module_name, result = future.result()
                    except Exception as exc:
                        with log_lock:
                            logging.error(f"Package {package_info[0]} import generated an exception: {exc}")
            
    def initialize_all_imports(self, skip_deps: bool = False) -> bool:
        """
        Initialize all imports and dependencies upfront.
        
        Args:
            skip_deps: Skip dependency checking and installation
            
        Returns:
            Tuple of (success: bool, global_assignments: dict)
        """
        # Check if already initialized to avoid duplicate work
        if hasattr(self, '_initialization_complete'):
            logging.debug("Import initialization already completed, returning cached results")
            return self._initialization_success, self._cached_global_assignments
            
        start_time = time.time()
        logging.info("Initializing global import management system...")
        
        if skip_deps:
            logging.info("Dependency checking and installation skipped (--skip-deps flag)")
        else:
            # Only check UV if auto-upgrade is enabled
            if self.auto_upgrade_uv:
                if not self.check_uv_installation():
                    self.install_uv()
                else:
                    self.upgrade_uv()
        
        # Import all required packages (this will install missing ones automatically)
        logging.info("Importing required dependencies...")
        
        # Use concurrent processing for faster dependency resolution
        if not skip_deps and len(self.required_packages) > 3:
            self._import_packages_concurrently(self.required_packages, required=True, skip_deps=skip_deps)
        else:
            # Sequential for smaller loads or when skipping deps
            for module_name, package_spec in self.required_packages.items():
                logging.info(f"  Checking required dependency: {module_name} ({package_spec or 'built-in'})")
                result = self.import_module_safely(module_name, package_spec, required=True, skip_deps=skip_deps, skip_upgrade=True)
                if result:
                    logging.info(f"  [OK] {module_name}: Available")
                else:
                    logging.error(f"  [FAIL] {module_name}: Failed to import")
            
        # Import optional packages
        logging.info("Importing optional dependencies...")
        if not skip_deps and len(self.optional_packages) > 3:
            self._import_packages_concurrently(self.optional_packages, required=False, skip_deps=skip_deps)
        else:
            # Sequential for smaller loads or when skipping deps
            for module_name, package_spec in self.optional_packages.items():
                logging.info(f"  Checking optional dependency: {module_name} ({package_spec or 'built-in'})")
                result = self.import_module_safely(module_name, package_spec, required=False, skip_deps=skip_deps, skip_upgrade=True)
                if result:
                    logging.info(f"  [OK] {module_name}: Available")
                else:
                    logging.warning(f"  [WARN] {module_name}: Not available")
            
        # Special imports for commonly used components
        self._import_special_modules()
        
        # Report results
        elapsed_time = time.time() - start_time
        total_required = len(self.required_packages)
        failed_required = len([p for p in self.failed_imports if p in self.required_packages])
        successful_required = total_required - failed_required
        optional_imported = len([p for p in self.imports.keys() if p in self.optional_packages])
        
        logging.info(f"Import initialization completed in {elapsed_time:.2f} seconds")
        logging.info(f"Required dependencies: {successful_required}/{total_required} successful")
        logging.info(f"Optional dependencies: {optional_imported}/{len(self.optional_packages)} available")
        
        if self.installed_packages:
            logging.info(f"Newly installed packages: {', '.join(self.installed_packages)}")
            
        if self.failed_imports:
            logging.error(f"Failed imports: {', '.join(self.failed_imports)}")
            
        # Make imported modules available globally
        global_assignments = self._get_global_assignments()
        
        # Cache results to avoid duplicate initialization
        success = len(self.failed_imports) == 0
        self._initialization_complete = True
        self._initialization_success = success
        self._cached_global_assignments = global_assignments
        
        # Return success status and global assignments
        return success, global_assignments
        
    def _get_global_assignments(self):
        """Get dictionary of global variable assignments for imported modules."""
        global_vars = {}
        
        # Add all imported modules to globals
        for module_name, module_obj in self.imports.items():
            # Add to global namespace
            global_vars[module_name] = module_obj
            
            # Handle special cases for commonly used attributes
            if module_name == 'datetime':
                global_vars['timezone'] = getattr(module_obj, 'timezone', None)
                global_vars['timedelta'] = getattr(module_obj, 'timedelta', None)
            elif module_name == 'concurrent.futures':
                global_vars['ThreadPoolExecutor'] = getattr(module_obj, 'ThreadPoolExecutor', None)
                global_vars['as_completed'] = getattr(module_obj, 'as_completed', None)
                global_vars['concurrent'] = module_obj  # For concurrent.futures references
            elif module_name == 'prettytable':
                global_vars['PrettyTable'] = getattr(module_obj, 'PrettyTable', None)
            elif module_name == 'numpy':
                global_vars['np'] = module_obj
            elif module_name == 'tqdm':
                # tqdm is used directly throughout the script
                global_vars['tqdm'] = module_obj
            elif module_name == 'collections':
                global_vars['defaultdict'] = getattr(module_obj, 'defaultdict', None)
            elif module_name == 'difflib':
                global_vars['SequenceMatcher'] = getattr(module_obj, 'SequenceMatcher', None)
            elif module_name == 'usaddress-scourgify':
                # Handle optional package - need to import the normalize function
                if module_obj:
                    try:
                        normalize_func = getattr(module_obj, 'normalize_address_record', None)
                        if normalize_func:
                            global_vars['normalize_address_record'] = normalize_func
                        else:
                            # Try importing directly from scourgify
                            from scourgify import normalize_address_record
                            global_vars['normalize_address_record'] = normalize_address_record
                    except (ImportError, AttributeError):
                        logging.debug("Could not import normalize_address_record from scourgify, using fallback")
            elif module_name == 'rapidfuzz':
                # Handle optional package - need to import the fuzz submodule
                if module_obj:
                    try:
                        fuzz_module = getattr(module_obj, 'fuzz', None)
                        if fuzz_module:
                            global_vars['fuzz'] = fuzz_module
                        else:
                            # Try importing fuzz directly from rapidfuzz
                            from rapidfuzz import fuzz
                            global_vars['fuzz'] = fuzz
                    except (ImportError, AttributeError):
                        logging.debug("Could not import fuzz from rapidfuzz, using fallback")
            # Note: pynput handling removed for simplicity
            elif module_name == 'mistapi':
                # Handle mistapi module - make it globally available
                if module_obj:
                    global_vars['mistapi'] = module_obj
                    logging.debug("Added mistapi to global namespace")
            elif module_name == 'paramiko':
                # Handle SSH client functionality
                if module_obj:
                    global_vars['paramiko'] = module_obj
                    logging.debug("Added paramiko to global namespace")
            # Note: pexpect handling removed for simplicity
            elif module_name == 'redexpect':
                # Handle cross-platform SSH automation
                if module_obj:
                    global_vars['redexpect'] = module_obj
                    logging.debug("Added redexpect to global namespace")
                        
        # Handle fallbacks for missing optional modules
        self._add_fallbacks_to_globals(global_vars)
        
        return global_vars
        
    def _make_modules_global(self):
        """Make all successfully imported modules available in the global namespace."""
        import builtins
        
        # Add all imported modules to globals
        for module_name, module_obj in self.imports.items():
            # Add to global namespace
            globals()[module_name] = module_obj
            
            # Handle special cases for commonly used attributes
            if module_name == 'datetime':
                globals()['timezone'] = getattr(module_obj, 'timezone', None)
                globals()['timedelta'] = getattr(module_obj, 'timedelta', None)
            elif module_name == 'concurrent.futures':
                globals()['ThreadPoolExecutor'] = getattr(module_obj, 'ThreadPoolExecutor', None)
                globals()['as_completed'] = getattr(module_obj, 'as_completed', None)
                globals()['concurrent'] = module_obj  # For concurrent.futures references
            elif module_name == 'prettytable':
                globals()['PrettyTable'] = getattr(module_obj, 'PrettyTable', None)
            elif module_name == 'numpy':
                globals()['np'] = module_obj
            elif module_name == 'tqdm':
                # tqdm is used directly throughout the script
                globals()['tqdm'] = module_obj
            elif module_name == 'collections':
                globals()['defaultdict'] = getattr(module_obj, 'defaultdict', None)
            elif module_name == 'difflib':
                globals()['SequenceMatcher'] = getattr(module_obj, 'SequenceMatcher', None)
            elif module_name == 'usaddress-scourgify':
                # Handle optional package - need to import the normalize function
                if module_obj:
                    try:
                        normalize_func = getattr(module_obj, 'normalize_address_record', None)
                        if normalize_func:
                            globals()['normalize_address_record'] = normalize_func
                        else:
                            # Try importing directly from scourgify
                            from scourgify import normalize_address_record
                            globals()['normalize_address_record'] = normalize_address_record
                    except (ImportError, AttributeError):
                        logging.debug("Could not import normalize_address_record from scourgify, using fallback")
            elif module_name == 'rapidfuzz':
                # Handle optional package - need to import the fuzz submodule
                if module_obj:
                    try:
                        fuzz_module = getattr(module_obj, 'fuzz', None)
                        if fuzz_module:
                            globals()['fuzz'] = fuzz_module
                        else:
                            # Try importing fuzz directly from rapidfuzz
                            from rapidfuzz import fuzz
                            globals()['fuzz'] = fuzz
                    except (ImportError, AttributeError):
                        logging.debug("Could not import fuzz from rapidfuzz, using fallback")
            # Note: pynput handling removed for simplicity
                        
        # Handle fallbacks for missing optional modules
        self._setup_fallbacks()
                        
        logging.debug("Successfully made imported modules available globally")
        
    def _add_fallbacks_to_globals(self, global_vars):
        """Add fallbacks for optional modules that failed to import."""
        # If scourgify not available, provide a fallback
        if 'normalize_address_record' not in global_vars or global_vars['normalize_address_record'] is None:
            def normalize_address_record_fallback(address_string):
                """Fallback function when scourgify is not available."""
                logging.debug("Using fallback address normalization (scourgify not available)")
                return {
                    'address_line_1': address_string,
                    'city': '',
                    'state': '', 
                    'zip': '',
                    'country': ''
                }
            global_vars['normalize_address_record'] = normalize_address_record_fallback
            
        # If rapidfuzz not available, provide fallback
        if 'fuzz' not in global_vars or global_vars['fuzz'] is None:
            class FuzzFallback:
                """Fallback class when rapidfuzz is not available."""
                @staticmethod
                def token_sort_ratio(str1, str2):
                    """Fallback using difflib SequenceMatcher."""
                    if global_vars.get('difflib'):
                        return int(global_vars['difflib'].SequenceMatcher(None, str1, str2).ratio() * 100)
                    return 0
            global_vars['fuzz'] = FuzzFallback()
            
        # Keyboard listener functionality has been removed for simplicity
        # No fallback needed since the feature is no longer supported
        
        # Add SSH connection fallbacks
        if 'paramiko' not in global_vars or global_vars['paramiko'] is None:
            class SSHFallback:
                """Fallback class when paramiko is not available."""
                @staticmethod
                def SSHClient():
                    raise ImportError("SSH functionality requires 'paramiko' package. Install with: pip install paramiko")
            global_vars['paramiko'] = SSHFallback()
            
        # pexpect functionality has been removed for simplicity
        # SSH automation should use paramiko directly
        
        if 'redexpect' not in global_vars or global_vars['redexpect'] is None:
            class RedexpectFallback:
                """Fallback class when redexpect is not available."""
                @staticmethod
                def spawn(*args, **kwargs):
                    raise ImportError("Cross-platform SSH automation requires 'redexpect' package. Install with: pip install redexpect")
            global_vars['redexpect'] = RedexpectFallback()
        
    def _import_special_modules(self):
        """Import special modules with custom handling."""
        # Import mistapi with its sub-modules only if base mistapi is available
        if 'mistapi' in self.imports:
            try:
                mistapi = self.imports['mistapi']
                # Import commonly used mistapi modules without forcing sub-module structure
                try:
                    # These imports may not be available in all mistapi versions
                    # Import them dynamically to avoid hard dependencies on specific structure
                    globals()['mistapi'] = mistapi
                    # Also ensure it's in the module's global namespace
                    import sys
                    sys.modules[__name__].mistapi = mistapi
                    logging.debug("Successfully imported mistapi main module")
                    # Verify the api module is accessible
                    if hasattr(mistapi, 'api') and hasattr(mistapi.api, 'v1'):
                        logging.debug("mistapi.api.v1 module structure confirmed")
                    else:
                        logging.warning("mistapi.api.v1 structure not found - this may cause API call failures")
                except Exception as sub_e:
                    logging.debug(f"Note: mistapi sub-modules handled dynamically: {sub_e}")
            except Exception as e:
                logging.warning(f"Error accessing mistapi: {e}")
        else:
            logging.debug("mistapi not imported, skipping sub-module imports")
            
        # Import websocket if available
        if 'websocket-client' in self.imports:
            logging.debug("websocket-client available for WebSocket operations")
        else:
            logging.debug("websocket-client not available - WebSocket operations will be disabled")
            
    def get_import(self, module_name: str) -> Optional[Any]:
        """Get an imported module by name."""
        return self.imports.get(module_name)
        
    def is_available(self, module_name: str) -> bool:
        """Check if a module is available."""
        return module_name in self.imports
        
    def get_configuration(self) -> Dict[str, Any]:
        """Get current configuration values."""
        return {
            'auto_upgrade_uv': self.auto_upgrade_uv,
            'auto_upgrade_dependencies': self.auto_upgrade_dependencies,
            'upgrade_check_timeout': self.upgrade_check_timeout,
            'csv_freshness_minutes': self.csv_freshness_minutes,
            'uv_update_check_hours': self.uv_update_check_hours,
        }

# ============================================================================
# GLOBAL CONSTANTS
# ============================================================================

# File paths for configuration and data
# SECURITY / SAFETY: Place tuning data inside the data/ directory to avoid
# permission issues when running as non-root inside a container with read-only
# application root. The file is small and safe to persist across runs.
def _get_tuning_data_file_path() -> str:
    """Return full path to tuning data JSON stored in data/ directory.

    Ensures the directory exists. Separated for future extension (e.g.,
    namespacing by org or mode) without scattering path logic.
    """
    data_dir = os.path.join(os.getcwd(), "data")
    try:
        os.makedirs(data_dir, exist_ok=True)
    except Exception as e:
        # If directory creation fails, fall back to current working directory;
        # logging deferred until logger configured.
        return os.path.join(os.getcwd(), "tuning_data.json")
    return os.path.join(data_dir, "tuning_data.json")

tuning_data_file = _get_tuning_data_file_path()

# API usage tracking cache
_api_usage_cache = {
    "timestamp": 0,
    "used": 0,
    "limit": 5000,
    "last_updated": 0,
    "perceived_requests": 0,
    "initialized": False 
}

# ============================================================================
# GLOBAL IMPORT MANAGER INITIALIZATION
# ============================================================================

# Create global import manager instance
import_manager = GlobalImportManager()

# Initialize imports immediately (unless deferred by CLI flags)
# Test mode and skip-deps both defer initialization to main() for better control
_initialize_imports_now = True

# Check for test mode or skip-deps from command line
if '--test' in sys.argv or '--testinteractive' in sys.argv or '--skip-deps' in sys.argv:
    _initialize_imports_now = False
    if ('--test' in sys.argv or '--testinteractive' in sys.argv) and '--skip-deps' not in sys.argv:
        logging.info("Deferring import initialization for test mode (dependencies will still be checked)")
    elif '--skip-deps' in sys.argv:
        logging.info("Deferring import initialization due to --skip-deps flag")
    else:
        logging.info("Deferring import initialization due to CLI flags")

if _initialize_imports_now:
    # Initialize all imports upfront for faster runtime performance
    success, global_assignments = import_manager.initialize_all_imports()
    
    # Apply global assignments to module namespace
    if global_assignments:
        for var_name, var_value in global_assignments.items():
            globals()[var_name] = var_value
            # Special handling for tqdm to ensure it overrides the fallback
            if var_name == 'tqdm' and var_value is not None:
                logging.info(f"Successfully imported real tqdm: {type(var_value)}")
        logging.debug(f"Applied {len(global_assignments)} global variable assignments")
        
        # Verify tqdm was properly imported
        if 'tqdm' in global_assignments:
            logging.info(f"tqdm is available in global namespace: {type(globals().get('tqdm'))}")
        else:
            logging.warning("tqdm was not found in global assignments - progress bars will not be functional")
    
    if not success:
        logging.warning("Some required imports failed - functionality may be limited")
else:
    # Deferred initialization - will be done in main()
    success, global_assignments = False, {}

# ============================================================================
# TEST MODE GLOBALS & DYNAMIC LOOKBACK HELPER
# ============================================================================
# Central flag for test mode (available early so helper functions outside main can use it)
IS_TEST_MODE = '--test' in sys.argv or '--testinteractive' in sys.argv

def get_dynamic_lookback_hours(default_hours: int = 24, test_hours: int = 1) -> int:
    """Return lookback hours adjusted for test mode.

    In normal operation we retain the full 24 hour (or caller provided) window.
    When the global --test flag is present, we shrink the lookback to 1 hour to:
      - Minimize API payload sizes / speed up systematic tests
      - Still exercise recent-data code paths
    The value is intentionally conservative (1h) to avoid missing fresh events while
    keeping runtime low. If a caller passes a different default_hours (e.g., 12),
    that value will be honored outside test mode.

    Parameters
    ----------
    default_hours : int
        Standard lookback window (typically 24).
    test_hours : int
        Reduced lookback for test mode (default 1 hour).

    Returns
    -------
    int
        Hours to use for lookback calculations.
    """
    try:
        if IS_TEST_MODE:
            # Boundaries & safety: never return less than 1 hour
            if test_hours < 1:
                return 1
            return test_hours
        if default_hours < 1:
            return 1
        return default_hours
    except Exception as e:
        logging.debug(f"get_dynamic_lookback_hours fallback due to error: {e}")
        return test_hours if IS_TEST_MODE else default_hours

def log_dynamic_lookback(context: str, hours: int):
    """Helper to produce a consistent log line when dynamic lookback applies."""
    if IS_TEST_MODE:
        logging.info(f"[TEST MODE] Using reduced lookback window of {hours}h for {context} (normally 24h)")
    else:
        logging.debug(f"Using standard lookback window of {hours}h for {context}")

# ============================================================================
# IMPORT STATUS AND HELPER FUNCTIONS
# ============================================================================

def get_import_status():
    """Get status of all imports for debugging."""
    return {
        'required_packages': import_manager.required_packages,
        'optional_packages': import_manager.optional_packages,
        'failed_imports': import_manager.failed_imports,
        'successful_imports': list(import_manager.imports.keys()),
        'installed_packages': import_manager.installed_packages
    }

def ensure_tqdm_available():
    """Ensure tqdm is available and properly imported."""
    global tqdm
    
    # Check if tqdm is properly imported (not our fallback)
    if hasattr(tqdm, '__module__') and tqdm.__module__ == 'tqdm':
        logging.debug("tqdm is properly imported and available")
        return True
    
    # Try to get tqdm from the import manager
    tqdm_from_manager = import_manager.get_import('tqdm')
    if tqdm_from_manager:
        tqdm = tqdm_from_manager
        logging.info("Retrieved tqdm from import manager")
        return True
    
    # Try importing tqdm directly
    try:
        from tqdm import tqdm as real_tqdm
        tqdm = real_tqdm
        logging.info("Successfully imported tqdm directly")
        return True
    except ImportError:
        logging.warning("tqdm package is not available - progress bars will be disabled")
        return False

def clean_unicode_for_logging(message):
    """Clean Unicode characters from log messages to prevent encoding errors on Windows."""
    if isinstance(message, str):
        # Replace common Unicode characters with ASCII equivalents
        replacements = {
            'OK': '[OK]',
            'FAIL': '[FAIL]',
            'WARN': '[WARN]',
            '*': '*',
            '-': '-',
            '-': '-',
            '-': '-',
            ''': "'",
            ''': "'",
            '"': '"',
            '"': '"',
        }
        for unicode_char, ascii_replacement in replacements.items():
            message = message.replace(unicode_char, ascii_replacement)
        
        # Remove any remaining non-ASCII characters
        message = message.encode('ascii', 'replace').decode('ascii')
    return message

def safe_print(message):
    """Print message with Unicode characters cleaned for Windows compatibility."""
    print(clean_unicode_for_logging(str(message)))

def safe_input(prompt, default_value="", allow_empty=True, context="unknown"):
    """
    Safely handle user input with proper EOF and KeyboardInterrupt handling.
    
    Args:
        prompt: The prompt message to display
        default_value: Value to return if user provides empty input or EOF
        allow_empty: Whether to allow empty input (only applies when default_value is not set)
        context: Context description for logging
    
    Returns:
        str: User input or default_value on EOF/empty input
        None: On KeyboardInterrupt
    """
    try:
        user_input = input(prompt).strip()
        
        # If user provided empty input and we have a default value, use it
        if not user_input and default_value:
            logging.debug(f"Empty input for {context}, using default: '{default_value}'")
            return default_value
        
        # If user provided empty input, no default, but empty is allowed
        if not user_input and allow_empty:
            return user_input
        
        # If user provided empty input, no default, and empty not allowed
        if not user_input and not allow_empty:
            logging.warning(f"Empty input not allowed for {context}, returning None")
            return None
        
        # User provided non-empty input
        return user_input
        
    except EOFError:
        # Handle EOF condition (Ctrl+D, broken pipe, SSH disconnection)
        print(f"\n[EOF] Input stream closed during {context}. Using default value: '{default_value}'")
        logging.info(f"EOF encountered on input during {context} - returning default: '{default_value}'")
        return default_value
    except KeyboardInterrupt:
        # Handle Ctrl+C
        print(f"\n[INTERRUPT] User interrupted {context}. Canceling...")
        logging.info(f"KeyboardInterrupt encountered during {context}")
        return None

# ============================================================================
# CONFIGURATION VARIABLES
# ============================================================================

# Configuration variables from .env (with defaults) - now managed by import manager
config = import_manager.get_configuration()
CSV_FRESHNESS_MINUTES = config['csv_freshness_minutes']
AUTO_UPGRADE_UV = config['auto_upgrade_uv']
AUTO_UPGRADE_DEPENDENCIES = config['auto_upgrade_dependencies']
UPGRADE_CHECK_TIMEOUT = config['upgrade_check_timeout']

# Fast Mode Configuration from .env
FAST_MODE_MAX_RETRIES = int(os.getenv("FAST_MODE_MAX_RETRIES", "3"))
FAST_MODE_RETRY_DELAY = float(os.getenv("FAST_MODE_RETRY_DELAY", "0.5"))

org_id=None

# Additional Fast Mode Configuration from .env (continuing from earlier definitions)
FAST_MODE_BACKOFF_MULTIPLIER = float(os.getenv("FAST_MODE_BACKOFF_MULTIPLIER", "1.5"))
FAST_MODE_DEVICES_PER_THREAD = int(os.getenv("FAST_MODE_DEVICES_PER_THREAD", "10"))
FAST_MODE_RETRY_THREADS = int(os.getenv("FAST_MODE_RETRY_THREADS", "4"))
FAST_MODE_RETRY_MAX_RETRIES = int(os.getenv("FAST_MODE_RETRY_MAX_RETRIES", "2"))
FAST_MODE_SEQUENTIAL_MAX_RETRIES = int(os.getenv("FAST_MODE_SEQUENTIAL_MAX_RETRIES", "1"))
FAST_MODE_FALLBACK_THREADS = int(os.getenv("FAST_MODE_FALLBACK_THREADS", "8"))
FAST_MODE_MAX_CONCURRENT_CONNECTIONS = int(os.getenv("FAST_MODE_MAX_CONCURRENT_CONNECTIONS", "8"))
FAST_MODE_USE_CONNECTION_AWARE_THREADING = os.getenv("FAST_MODE_USE_CONNECTION_AWARE_THREADING", "true").lower() == "true"

# Global configuration for output format (CSV or SQLite)
# Default to CSV for general use, can be overridden by CLI flag
OUTPUT_FORMAT = "csv"  # Valid values: "csv", "sqlite"
DATABASE_PATH = os.path.join("data", "mist_data.db")  # Path to hybrid SQLite database with natural primary keys

# ============================================================================
# GLOBAL SESSION INITIALIZATION
# ============================================================================

# Initialize Mist API session (will be set up after authentication)
apisession = None

def initialize_mist_session():
    """Initialize the Mist API session with authentication.

    Strategy:
      1. Try APISession with env_file (legacy behavior).
      2. If that fails, normalize token(s) from MIST_APITOKEN / MIST_API_TOKEN and try APISession with each via 'apitoken='.
      3. Fallback to mistapi.Session() if available.
      4. If all fail, return False (do NOT create placeholder that lacks required methods).

    SECURITY: Tokens are only logged in redacted preview at DEBUG level.
    """
    global apisession
    if apisession:
        return True

    host = os.getenv('MIST_HOST', 'api.mist.com')
    raw_token_env = os.getenv('MIST_APITOKEN') or os.getenv('MIST_API_TOKEN')
    if raw_token_env:
        tokens = [t.strip() for t in re.split(r'[\n,]+', raw_token_env) if t.strip()]
    else:
        tokens = []
    if tokens:
        redacted_preview = ','.join([(t[:4] + '...' + t[-4:]) if len(t) >= 8 else '***' for t in tokens])
        logging.debug(f"Token(s) discovered for initialization (redacted): {redacted_preview}")
    else:
        logging.debug("No tokens discovered in environment; will rely on env_file or mistapi.Session fallback")

    # Helper function to test if a token is currently rate-limited
    def is_token_rate_limited(token: str, test_host: str) -> bool:
        """Test if a token is currently rate-limited by calling /api/v1/self"""
        try:
            import requests
            url = f"https://{test_host}/api/v1/self"
            headers = {"Authorization": f"Token {token}"}
            response = requests.get(url, headers=headers, timeout=5)
            if response.status_code == 429:
                logging.debug(f"Token {token[:4]}...{token[-4:]} is rate-limited (HTTP 429)")
                return True
            elif response.status_code == 200:
                logging.debug(f"Token {token[:4]}...{token[-4:]} is available (HTTP 200)")
                return False
            else:
                logging.warning(f"Token {token[:4]}...{token[-4:]} returned unexpected status {response.status_code}")
                return True  # Treat as unavailable
        except Exception as test_err:
            logging.warning(f"Failed to test token {token[:4]}...{token[-4:]}: {test_err}")
            return True  # Treat as unavailable on error

    # Dynamically interrogate APISession signature to avoid wrong parameter names
    apisession_cls = getattr(mistapi, 'APISession', None) if mistapi else None
    tried_variants = []
    if apisession_cls:
        try:
            sig_params = list(inspect.signature(apisession_cls).parameters.keys())
            logging.debug(f"mistapi.APISession accepted parameters: {sig_params}")
        except Exception:
            sig_params = []
    else:
        sig_params = []

    # Candidate constructors to attempt (ordered)
    attempts = []
    if apisession_cls:
        # 1. Direct tokens with potential parameter names - try this FIRST
        # IMPORTANT: mistapi expects a comma-separated string of ALL tokens, not individual tokens
        # This allows mistapi to rotate through tokens when hitting rate limits (HTTP 429)
        token_param_names = [n for n in ['apitoken', 'api_token', 'token'] if n in sig_params]
        if tokens and token_param_names:
            # Join all tokens into a comma-separated string as mistapi expects
            all_tokens_str = ','.join(tokens)
            for pname in token_param_names:
                base_kwargs = {pname: all_tokens_str}
                if 'host' in sig_params:
                    base_kwargs['host'] = host
                attempts.append(base_kwargs)
        # 2. env_file only if supported AND we don't already have tokens from environment
        #    (env_file will read the same tokens again, causing duplicate validation failures)
        if 'env_file' in sig_params and not tokens:
            attempts.append({'env_file': '.env'})
        # 3. Host only (unauthenticated) if allowed (rare but safe to record)
        if 'host' in sig_params and not tokens:
            attempts.append({'host': host})

    # Execute attempts
    successful_method = None
    rate_limit_detected = False
    for i, kwargs in enumerate(attempts, start=1):
        try:
            tried_variants.append(kwargs)
            apisession = apisession_cls(**kwargs)
            successful_method = kwargs
            logging.info(f"Mist API session initialized with mistapi.APISession using kwargs={list(kwargs.keys())}")
            break
        except Exception as e:
            error_msg = str(e)
            logging.warning(f"APISession attempt {i}/{len(attempts)} failed kwargs={kwargs}: {e}")
            
            # Detect rate limiting during token validation
            if "'NoneType' object is not iterable" in error_msg:
                rate_limit_detected = True
                logging.warning("Detected possible rate limiting during token validation - tokens may be throttled")
            
            # Always log traceback for API session failures to aid debugging
            try:
                import traceback
                tb_details = ''.join(traceback.format_exception(type(e), e, e.__traceback__))
                for line in tb_details.rstrip().splitlines():
                    logging.info(f"  TRACE: {line}")
            except Exception as trace_err:
                logging.warning(f"Failed to log traceback: {trace_err}")
            apisession = None
    
    # If rate-limited with multiple tokens, try each token individually until one works
    if not apisession and rate_limit_detected and tokens and len(tokens) > 1:
        logging.warning(f"Multi-token initialization failed due to rate limiting - testing {len(tokens)} tokens individually")
        
        # Pre-filter tokens to find those not currently rate-limited
        available_tokens = []
        for token_index, individual_token in enumerate(tokens, start=1):
            if not is_token_rate_limited(individual_token, host):
                available_tokens.append(individual_token)
                logging.info(f"Token {token_index}/{len(tokens)} ({individual_token[:4]}...{individual_token[-4:]}) is available")
            else:
                logging.warning(f"Token {token_index}/{len(tokens)} ({individual_token[:4]}...{individual_token[-4:]}) is rate-limited - skipping")
        
        if not available_tokens:
            logging.error(f"All {len(tokens)} tokens are currently rate-limited - cannot initialize API session")
        else:
            logging.info(f"Found {len(available_tokens)} available token(s) out of {len(tokens)} total")
            
            # Use all available tokens (comma-separated) for rotation capability
            available_tokens_str = ','.join(available_tokens)
            
            # CRITICAL: Temporarily clear MIST_APITOKEN from environment to prevent mistapi 
            # from reading all tokens (including rate-limited ones) during _load_env() call
            original_mist_token = os.environ.get('MIST_APITOKEN')
            try:
                if 'MIST_APITOKEN' in os.environ:
                    del os.environ['MIST_APITOKEN']
                    logging.debug("Temporarily cleared MIST_APITOKEN from environment for filtered token initialization")
                
                try:
                    filtered_kwargs = {}
                    if 'apitoken' in sig_params:
                        filtered_kwargs['apitoken'] = available_tokens_str
                    if 'host' in sig_params:
                        filtered_kwargs['host'] = host
                    
                    logging.info(f"Initializing with {len(available_tokens)} available token(s)")
                    apisession = apisession_cls(**filtered_kwargs)
                    successful_method = filtered_kwargs
                    logging.info(f"SUCCESS: API session initialized with {len(available_tokens)} available token(s)")
                    tried_variants.append(filtered_kwargs)
                except Exception as filtered_err:
                    logging.error(f"Failed to initialize with filtered tokens: {filtered_err}")
                    apisession = None
            finally:
                # Restore original MIST_APITOKEN to environment
                if original_mist_token:
                    os.environ['MIST_APITOKEN'] = original_mist_token
                    logging.debug("Restored MIST_APITOKEN to environment")

    # Fallback to mistapi.Session if APISession failed
    if not apisession and mistapi and hasattr(mistapi, 'Session'):
        try:
            apisession = mistapi.Session()
            successful_method = {'fallback': 'mistapi.Session'}
            logging.info("Mist API session initialized with mistapi.Session fallback")
        except Exception as e:
            logging.error(f"mistapi.Session fallback failed: {e}")
            apisession = None

    if not apisession:
        logging.error("All Mist API session initialization attempts failed. Variants tried:")
        for variant in tried_variants:
            logging.error(f"  - {variant}")
        return False

    # Validate that required request method exists (mist_get is used by code)
    if not hasattr(apisession, 'mist_get'):
        # Some versions expose 'get' instead; we can wrap it for compatibility
        if hasattr(apisession, 'get') and callable(getattr(apisession, 'get')):
            def _mist_get_wrapper(*args, **kwargs):  # pragma: no cover (simple adapter)
                return apisession.get(*args, **kwargs)
            setattr(apisession, 'mist_get', _mist_get_wrapper)
            logging.info("Added mist_get wrapper around underlying get() method for compatibility")
        else:
            logging.error("Initialized session lacks 'mist_get' or 'get' methods required for API calls")
            return False

    # Enhanced token validation - only warn if no authentication method was used
    token_attr = next((a for a in ("apitoken", "api_token", "token") if hasattr(apisession, a)), None)
    has_readable_token = token_attr and getattr(apisession, token_attr)
    used_env_file = successful_method and 'env_file' in successful_method
    used_direct_token = successful_method and any(param in successful_method for param in ['apitoken', 'api_token', 'token'])
    used_fallback_session = successful_method and 'fallback' in successful_method
    
    # Only warn if no authentication method appears to be configured
    if not (has_readable_token or used_env_file or used_direct_token or used_fallback_session):
        logging.warning("Session established but no authentication method detected; API calls may fail if authentication required")
        logging.warning("To fix this: 1) Copy documentation/sample.env to .env, 2) Set MIST_APITOKEN to your Mist API token")
        logging.warning("Get your API token from: https://manage.mist.com/admin/apitoken")
    elif used_env_file:
        logging.debug("Session initialized using env_file - authentication configured via .env file")
    elif used_direct_token:
        logging.debug("Session initialized using direct token parameter - authentication configured")
    elif has_readable_token:
        logging.debug("Session has readable token attribute - authentication appears configured")

    return True

# ============================================================================
# ENDPOINT PRIMARY KEY STRATEGY CONFIGURATION
# ============================================================================
# This configuration determines how each API endpoint's data should be stored in SQLite
# with proper primary keys to eliminate artificial api_id fields and enable efficient queries
ENDPOINT_PRIMARY_KEY_STRATEGIES = {
    # Type 1: Natural primary key using API id field (for entity APIs)
    # These APIs return objects with stable UUID identifiers that make perfect primary keys
    'getOrgInventory': {
        'type': 'natural_pk',
        'primary_key': ['id'],
        'indexes': ['org_id', 'site_id', 'mac', 'serial', 'model', 'type'],
        'unique_constraints': [],
        'description': 'Organization device inventory with stable UUID identifiers'
    },
    'listOrgSites': {
        'type': 'natural_pk', 
        'primary_key': ['id'],
        'indexes': ['org_id', 'name', 'country_code', 'address'],
        'unique_constraints': [],
        'description': 'Organization sites with stable UUID identifiers'
    },
    'listSiteDevices': {
        'type': 'natural_pk',
        'primary_key': ['id'], 
        'indexes': ['site_id', 'mac', 'serial', 'model', 'type', 'name'],
        'unique_constraints': [],
        'description': 'Site devices with stable UUID identifiers'
    },
    'getOrgDevices': {
        'type': 'natural_pk',
        'primary_key': ['id'],
        'indexes': ['org_id', 'site_id', 'mac', 'serial', 'model', 'type'],
        'unique_constraints': [],
        'description': 'Organization devices with stable UUID identifiers'
    },
    
    # Template and configuration entities
    'listOrgGatewayTemplates': {
        'type': 'natural_pk',
        'primary_key': ['id'],
        'indexes': ['org_id', 'name', 'type'],
        'unique_constraints': [],
        'description': 'Gateway templates with stable UUID identifiers'
    },
    'listOrgNetworkTemplates': {
        'type': 'natural_pk',
        'primary_key': ['id'],
        'indexes': ['org_id', 'name'],
        'unique_constraints': [],
        'description': 'Network templates with stable UUID identifiers'
    },
    'listOrgRfTemplates': {
        'type': 'natural_pk',
        'primary_key': ['id'],
        'indexes': ['org_id', 'name', 'band'],
        'unique_constraints': [],
        'description': 'RF templates with stable UUID identifiers'
    },
    'listOrgSiteTemplates': {
        'type': 'natural_pk',
        'primary_key': ['id'],
        'indexes': ['org_id', 'name'],
        'unique_constraints': [],
        'description': 'Site templates with stable UUID identifiers'
    },
    'listOrgAptemplates': {
        'type': 'natural_pk',
        'primary_key': ['id'],
        'indexes': ['org_id', 'name'],
        'unique_constraints': [],
        'description': 'AP templates with stable UUID identifiers'
    },
    'listOrgSecPolicies': {
        'type': 'natural_pk',
        'primary_key': ['id'],
        'indexes': ['org_id', 'name'],
        'unique_constraints': [],
        'description': 'Security policies with stable UUID identifiers'
    },
    'listOrgPsks': {
        'type': 'natural_pk',
        'primary_key': ['id'],
        'indexes': ['org_id', 'name', 'ssid'],
        'unique_constraints': [],
        'description': 'Pre-shared keys with stable UUID identifiers'
    },
    'listOrgWebhooks': {
        'type': 'natural_pk',
        'primary_key': ['id'],
        'indexes': ['org_id', 'name', 'type'],
        'unique_constraints': [],
        'description': 'Webhooks with stable UUID identifiers'
    },
    
    # Type 2: Composite primary key for event and log APIs
    # These APIs return time-series data that requires composite keys for uniqueness
    'searchOrgAlarms': {
        'type': 'composite_pk',
        'primary_key': ['id', 'org_id', 'timestamp'],
        'indexes': ['org_id', 'timestamp', 'severity', 'type', 'site_id'],
        'unique_constraints': [],
        'description': 'Organization alarms with composite key for time-series data'
    },
    'searchOrgDeviceEvents': {
        'type': 'composite_pk',
        'primary_key': ['id', 'device_id', 'timestamp'],
        'indexes': ['device_id', 'timestamp', 'type', 'org_id', 'site_id'],
        'unique_constraints': [],
        'description': 'Device events with composite key for uniqueness'
    },
    'searchOrgClientEvents': {
        'type': 'composite_pk',
        'primary_key': ['id', 'site_id', 'timestamp'],
        'indexes': ['site_id', 'timestamp', 'type', 'client_mac', 'device_id'],
        'unique_constraints': [],
        'description': 'Client events with composite key for uniqueness'
    },
    'searchOrgSystemEvents': {
        'type': 'composite_pk',
        'primary_key': ['id', 'org_id', 'timestamp'],
        'indexes': ['org_id', 'timestamp', 'type'],
        'unique_constraints': [],
        'description': 'System events with composite key for uniqueness'
    },
    
    # Type 3: Composite key for statistics and metrics APIs
    # These APIs return aggregated data that benefits from composite keys
    'listOrgDevicesStats': {
        'type': 'composite_pk',
        'primary_key': ['device_id', 'timestamp'],
        'indexes': ['device_id', 'timestamp', 'org_id', 'site_id', 'type'],
        'unique_constraints': [],
        'description': 'Organization device statistics with composite key for metrics'
    },
    'listSiteDevicesStats': {
        'type': 'composite_pk',
        'primary_key': ['device_id', 'timestamp'],
        'indexes': ['device_id', 'timestamp', 'site_id', 'type'],
        'unique_constraints': [],
        'description': 'Site device statistics with composite key for metrics'
    },
    'listSiteWirelessClientsStats': {
        'type': 'composite_pk',
        'primary_key': ['client_mac', 'timestamp'],
        'indexes': ['client_mac', 'timestamp', 'site_id', 'device_id'],
        'unique_constraints': [],
        'description': 'Site wireless client statistics with composite key for metrics'
    },
    'searchOrgSwOrGwPorts': {
        'type': 'composite_pk',
        'primary_key': ['device_id', 'port_id', 'timestamp'],
        'indexes': ['device_id', 'port_id', 'timestamp', 'org_id'],
        'unique_constraints': [],
        'description': 'Switch/gateway port statistics with composite key'
    },
    'searchSiteSwOrGwPorts': {
        'type': 'composite_pk',
        'primary_key': ['device_id', 'port_id', 'timestamp'],
        'indexes': ['device_id', 'port_id', 'timestamp', 'site_id'],
        'unique_constraints': [],
        'description': 'Site switch/gateway port statistics with composite key'
    },
    'searchOrgPeerPathStats': {
        'type': 'composite_pk',
        'primary_key': ['from_device', 'to_device', 'timestamp'],
        'indexes': ['from_device', 'to_device', 'timestamp', 'org_id'],
        'unique_constraints': [],
        'description': 'Peer path statistics with composite key'
    },
    
    # Map-related endpoints
    'listSiteMaps': {
        'type': 'natural_pk',
        'primary_key': ['id'],
        'indexes': ['site_id', 'name', 'type', 'created_time', 'modified_time'],
        'unique_constraints': [],
        'description': 'Site maps with stable UUID identifiers'
    },
    'getSiteMap': {
        'type': 'natural_pk',
        'primary_key': ['id'],
        'indexes': ['site_id', 'name', 'type'],
        'unique_constraints': [],
        'description': 'Individual site map with stable UUID identifier'
    },
    
    # Type 4: Client search APIs (special handling for large datasets)
    'searchOrgWirelessClients': {
        'type': 'composite_pk',
        'primary_key': ['mac', 'timestamp'],
        'indexes': ['mac', 'timestamp', 'site_id', 'device_id', 'ssid'],
        'unique_constraints': [],
        'description': 'Wireless client data with composite key for time-series'
    },
    'searchOrgWiredClients': {
        'type': 'composite_pk',
        'primary_key': ['mac', 'timestamp'],
        'indexes': ['mac', 'timestamp', 'site_id', 'device_id', 'port_id'],
        'unique_constraints': [],
        'description': 'Wired client data with composite key for time-series'
    },
    
    # Type 5: License and summary APIs (often aggregated data)
    'getOrgLicensesSummary': {
        'type': 'auto_increment_with_unique',
        'primary_key': ['misthelper_internal_id'],
        'indexes': ['org_id', 'sku', 'type'],
        'unique_constraints': [],
        'description': 'License summary data (aggregated, no stable primary key)'
    },
    
    # Default fallback strategy for unclassified endpoints
    # Uses auto-increment with unique constraint on API id field if present
    'default': {
        'type': 'auto_increment_with_unique',
        'primary_key': ['misthelper_internal_id'],
        'indexes': [],  # Will be determined at runtime based on available fields
        'unique_constraints': [],  # Will be applied if 'id' field exists in data
        'description': 'Fallback strategy with auto-increment primary key and unique constraint on API id'
    }
}

def check_and_generate_csv(file_name, generate_function, freshness_minutes=None):
    """
    Checks if a CSV file exists and is fresh (modified within the last `freshness_minutes`).
    If not, it runs the `generate_function` to regenerate the file.
    freshness_minutes is now settable via the .env file as CSV_FRESHNESS_MINUTES.
    """
    logging.debug(f"ENTRY: check_and_generate_csv(file_name={file_name}, generate_function={generate_function.__name__}, freshness_minutes={freshness_minutes})")
    
    if freshness_minutes is None:
        freshness_minutes = CSV_FRESHNESS_MINUTES
        
    # Get the full path to the CSV file in the data directory
    full_file_path = get_csv_file_path(file_name)
    
    # Check if the file already exists
    if os.path.exists(full_file_path):
        try:
            # Get the last modified time of the file
            file_mtime = datetime.fromtimestamp(os.path.getmtime(full_file_path))
            logging.debug(f"File I/O: Successfully read modification time for {full_file_path}: {file_mtime}")
            
            # Check if the file is still fresh
            if datetime.now() - file_mtime < timedelta(minutes=freshness_minutes):
                # Log that the cached file is being used
                logging.info(f"! Using cached {file_name} (fresh)")
                logging.debug(f"EXIT: check_and_generate_csv - using cached file")
                return True
            else:
                # Log that the file is stale and will be regenerated
                logging.info(f"* {file_name} is older than {freshness_minutes} minutes. Regenerating...")
        except OSError as e:
            logging.error(f"File I/O: Failed to read modification time for {full_file_path}: {e}")
            logging.info(f"* {file_name} exists but cannot read metadata. Regenerating...")
    else:
        # Log that the file does not exist and will be generated
        logging.info(f"* {file_name} not found. Generating...")

    # Call the function to generate the file
    logging.info(f"* Running {generate_function.__name__} to generate {file_name}...")
    try:
        generate_function()
        logging.info(f"! {file_name} generated or refreshed.")
        logging.debug(f"EXIT: check_and_generate_csv - file generated successfully")
        return True
    except Exception as e:
        logging.error(f"Failed to generate {file_name} using {generate_function.__name__}: {e}")
        logging.debug(f"EXIT: check_and_generate_csv - generation failed")
        return False

def prepare_data_and_write_csv(data, filename, sort_key=None):
    """
    Flattens, sanitizes, optionally sorts, and writes data to a CSV file.
    """
    # Flatten nested dictionaries and lists
    data = flatten_nested_fields_in_list(data)
    
    # Escape multiline strings for CSV compatibility
    data = escape_multiline_strings_for_csv(data)
    
    # Sort data by the specified key if provided
    if sort_key:
        data = sorted(data, key=lambda x: x.get(sort_key, ""))
    
    # Write the processed data to a CSV file
    DataExporter.save_data_to_output(data, filename)

def display_dict_list_as_pretty_table(data, fields=None, sortby=None):
    """
    Displays a PrettyTable from a list of dictionaries.
    """
    # Return early if there's no data to display
    if not data:
        return

    # Use provided fields or extract all unique keys
    fields = fields or get_all_unique_dict_keys(data)

    # Initialize the PrettyTable with field names
    table = PrettyTable()
    table.field_names = fields

    # Set the sort column if it's valid
    if sortby and sortby in fields:
        table.sortby = sortby

    # Add each row of data to the table
    for item in data:
        row = [item.get(field, "") for field in fields]
        table.add_row(row)

    # Log the table as a string (debug mode only)
    logging.debug("\n" + table.get_string())

def interactive_fetch_device_data_to_csv(fetch_function, filename, description, device_type="all", site_id=None, device_id=None):
    """
    Fetches data for a specific device (by site_id/device_id if provided, else prompts user),
    writes the result to a CSV file, and displays it as a PrettyTable.
    """
    # Use provided site_id or prompt user
    if not site_id:
        site_id = prompt_select_site_id_from_csv()
        if not site_id:
            return

    # Use provided device_id or prompt user
    if not device_id:
        device_id = prompt_select_device_id_from_inventory(site_id, device_type=device_type)
        if not device_id:
            return

    # Log the action being performed
    logging.info(f"{description} for device ID: {device_id}")

    # Fetch data using the provided function
    stats = fetch_function(apisession, site_id, device_id).data

    # Flatten and sanitize the data
    stats = flatten_nested_fields_in_list([stats])
    stats = escape_multiline_strings_for_csv(stats)

    # Write the data to a CSV file
    DataExporter.save_data_to_output(stats, filename)

    # Display the data in a table
    display_dict_list_as_pretty_table(stats)


# ============================================================================
# WEBSOCKET MANAGEMENT CLASS
# ============================================================================

class WebSocketManager:
    """
    WebSocket Manager for Mist API real-time communications.
    
    This class handles WebSocket connections to the Mist API following the 
    documented patterns: subscribe first, issue POST command, await results.
    
    SECURITY: WebSocket connections use authenticated sessions with proper
    credential handling and session-based command demultiplexing.
    """
    
    def __init__(self, mist_session, mist_host=None):
        """
        Initialize WebSocket manager with Mist session.
        
        Args:
            mist_session: Authenticated Mist API session
            mist_host: Mist API host (if None, will get from session)
        """
        self.mist_session = mist_session
        self.mist_host = mist_host or getattr(mist_session, "host", None) or os.getenv("MIST_HOST", "api.mist.com")
        
        # Convert API host to WebSocket host
        websocket_host = self.mist_host.replace("api.", "api-ws.")
        self.websocket_url = f"wss://{websocket_host}/api-ws/v1/stream"
        self.websocket_connection = None
        self.logger = logging.getLogger(__name__)
        self.connected = False
        self.subscribed_channels = set()
        self.confirmed_subscriptions = set()  # Track confirmed subscriptions
        
        # Results storage for command outputs
        self.command_results = {}
        self.results_lock = threading.Lock()
        
    def connect(self):
        """
        Establish WebSocket connection with proper authentication.
        
        Returns:
            bool: True if connection successful, False otherwise
        """
        try:
            # Prepare authentication headers using session token
            mist_apitoken = getattr(self.mist_session, "apitoken", None) or os.getenv("MIST_APITOKEN")
            if not mist_apitoken:
                self.logger.error("No API token found in session or environment")
                return False
                
            auth_header = f"Authorization: Token {mist_apitoken}"
            headers = [auth_header]
            
            # Create WebSocket connection
            self.websocket_connection = websocket.WebSocketApp(
                self.websocket_url,
                header=headers,
                on_message=self._on_message,
                on_error=self._on_error,
                on_close=self._on_close,
                on_open=self._on_open
            )
            
            # Start connection in background thread
            self.websocket_thread = threading.Thread(
                target=self.websocket_connection.run_forever,
                daemon=True
            )
            self.websocket_thread.start()
            
            # Wait for connection to establish
            timeout_counter = 0
            while not self.connected and timeout_counter < 10:
                time.sleep(0.5)
                timeout_counter += 1
                
            if self.connected:
                self.logger.info("WebSocket connection established successfully")
                return True
            else:
                self.logger.error("WebSocket connection timeout")
                return False
                
        except Exception as connection_error:
            self.logger.error(f"WebSocket connection failed: {connection_error}")
            return False
    
    def subscribe_to_channel(self, channel_path):
        """
        Subscribe to a WebSocket channel for receiving command outputs.
        
        Args:
            channel_path (str): Channel path (e.g., "/sites/{site_id}/devices/{device_id}/cmd")
            
        Returns:
            bool: True if subscription successful, False otherwise
        """
        if not self.connected:
            self.logger.error("Cannot subscribe: WebSocket not connected")
            return False
            
        try:
            subscription_message = {
                "subscribe": channel_path
            }
            
            self.websocket_connection.send(json.dumps(subscription_message))
            self.subscribed_channels.add(channel_path)
            self.logger.debug(f"Subscribed to channel: {channel_path}")
            return True
            
        except Exception as subscription_error:
            self.logger.error(f"Channel subscription failed: {subscription_error}")
            return False
    
    def wait_for_subscription_confirmation(self, channel_path, timeout_seconds=10):
        """
        Wait for WebSocket subscription confirmation for a specific channel.
        
        Args:
            channel_path (str): Channel path to wait for confirmation
            timeout_seconds (int): Maximum time to wait for confirmation
            
        Returns:
            bool: True if confirmation received, False if timeout
        """
        import time
        start_time = time.time()
        
        debug_mode = getattr(self, 'debug_mode', False) or os.getenv('DEBUG', '').lower() in ['true', '1', 'yes']
        
        if debug_mode:
            self.logger.debug(f"Waiting for subscription confirmation for: {channel_path}")
            print(f"[DEBUG] Waiting for subscription confirmation for: {channel_path}")
        
        while time.time() - start_time < timeout_seconds:
            # Check if subscription is confirmed
            if channel_path in self.confirmed_subscriptions:
                if debug_mode:
                    self.logger.debug(f"Subscription confirmed for: {channel_path}")
                    print(f"[DEBUG] Subscription confirmed for: {channel_path}")
                return True
            
            time.sleep(0.1)  # Small sleep to avoid busy waiting
        
        # Timeout reached
        if debug_mode:
            self.logger.debug(f"Timeout waiting for subscription confirmation: {channel_path}")
            print(f"[DEBUG] Timeout waiting for subscription confirmation: {channel_path}")
        self.logger.warning(f"Timeout waiting for subscription confirmation: {channel_path}")
        return False
    
    def wait_for_command_result(self, session_id, timeout_seconds=30, activity_timeout_seconds=None):
        """
        Wait for command result with specific session ID.
        
        For commands like ping that produce multiple output segments,
        this will collect all results until the command completes.
        
        Args:
            session_id (str): Session ID from command POST response
            timeout_seconds (int): Maximum time to wait for result
            activity_timeout_seconds (int): Seconds to wait after last message before considering complete
            
        Returns:
            dict: Complete command result data or None if timeout
        """
        import time
        debug_mode = is_debug_mode()
        start_time = time.time()
        last_activity = time.time()
        last_message_count = 0  # Track number of messages to detect new activity
        
        # Reset MAC table completion cache for new sessions
        if hasattr(self, '_mac_expected_entries'):
            delattr(self, '_mac_expected_entries')
        
        # Use custom activity timeout if provided, otherwise default to 2 seconds
        activity_timeout = activity_timeout_seconds if activity_timeout_seconds is not None else 2
        check_count = 0
        last_debug_time = start_time
        performance_log_interval = 5.0  # Log performance every 5 seconds
        
        # Create performance monitor to detect infinite loops
        perf_monitor = PerformanceMonitor(f"wait_for_command_result({session_id[:8]}...)", 
                                        max_iterations=10000, log_interval=5.0)
        
        if debug_mode:
            self.logger.debug(f"Waiting for session {session_id} (timeout: {timeout_seconds}s)")
            self.logger.debug(f"Current time: {time.time()}")
            self.logger.debug(f"Activity timeout: {activity_timeout}s)")
            print(f"[DEBUG] Waiting for session {session_id} (timeout: {timeout_seconds}s)")
            print(f"[DEBUG] Current time: {time.time()}")
            print(f"[DEBUG] Activity timeout: {activity_timeout}s)")
        
        while time.time() - start_time < timeout_seconds:
            # Monitor for infinite loops
            perf_monitor.check_iteration()
            
            current_time = time.time()
            check_count += 1
            
            # Performance logging every 5 seconds in debug mode
            if debug_mode and (current_time - last_debug_time) >= performance_log_interval:
                elapsed = current_time - start_time
                self.logger.debug(f"Check #{check_count} at {elapsed:.1f}s - Still waiting for session {session_id}")
                self.logger.debug(f"Last activity: {current_time - last_activity:.1f}s ago")
                print(f"[PERF] Check #{check_count} at {elapsed:.1f}s - Still waiting for session {session_id}")
                print(f"[PERF] Last activity: {current_time - last_activity:.1f}s ago")
                with self.results_lock:
                    available_sessions = list(self.command_results.keys())
                    if session_id in self.command_results:
                        msg_count = len(self.command_results[session_id])
                        self.logger.debug(f"Found {msg_count} messages for our session")
                        print(f"[PERF] Found {msg_count} messages for our session")
                    else:
                        self.logger.debug(f"Our session not in results yet. Available: {available_sessions}")
                        print(f"[PERF] Our session not in results yet. Available: {available_sessions}")
                last_debug_time = current_time
            
            with self.results_lock:
                if session_id in self.command_results:
                    collected_output = self.command_results[session_id]
                    current_message_count = len(collected_output)
                    
                    # Update last_activity only when NEW messages arrive
                    if current_message_count > last_message_count:
                        last_activity = time.time()
                        last_message_count = current_message_count
                        if debug_mode:
                            self.logger.debug(f"New activity detected: {current_message_count} messages (+{current_message_count - (last_message_count - (current_message_count - last_message_count))}) ")
                    
                    if collected_output:
                        # Check ALL messages for completion indicators, not just the latest
                        all_raw_content = ""
                        for result in collected_output:
                            all_raw_content += result.get("raw", "")
                        
                        latest_result = collected_output[-1]
                        latest_raw = latest_result.get("raw", "")
                        
                        if debug_mode and check_count % 50 == 1:  # Debug every 50 checks (roughly 5 seconds)
                            self.logger.debug(f"Check #{check_count}, found {len(collected_output)} messages")
                            self.logger.debug(f"Latest raw (first 100 chars): {repr(latest_raw[:100])}")
                            self.logger.debug(f"Total content length: {len(all_raw_content)} chars")
                            print(f"[DEBUG] Check #{check_count}, found {len(collected_output)} messages")
                            print(f"[DEBUG] Latest raw (first 100 chars): {repr(latest_raw[:100])}")
                            print(f"[DEBUG] Total content length: {len(all_raw_content)} chars")
                            # For service ping debugging, show more content
                            if len(all_raw_content) > 0:
                                self.logger.debug(f"Service ping content sample: {repr(all_raw_content[:300])}")
                                print(f"[DEBUG] Service ping content sample: {repr(all_raw_content[:300])}")
                                if "bytes from" in all_raw_content.lower():
                                    self.logger.debug("Service ping: Found 'bytes from' pattern")
                                    print(f"[DEBUG] Service ping: Found 'bytes from' pattern")
                                if "seq=" in all_raw_content.lower():
                                    self.logger.debug("Service ping: Found 'seq=' pattern")
                                    print(f"[DEBUG] Service ping: Found 'seq=' pattern")
                                if "time=" in all_raw_content.lower():
                                    self.logger.debug("Service ping: Found 'time=' pattern")
                                    print(f"[DEBUG] Service ping: Found 'time=' pattern")
                        
                        # Check if ANY of the collected content looks like a final command summary
                        # Ping completion indicators
                        ping_indicators = ["round-trip min/avg/max", "round-trip min/avg/max/stddev", "rtt min/avg/max"]
                        # Service ping completion indicators - SSR service ping specific patterns
                        service_ping_indicators = [
                            "service ping completed",    # Generic service ping completion
                            "service-ping",              # Service ping command reference
                            "packet transmitted",        # "10 packets transmitted"
                            "packets transmitted",       # Alternative format
                            "received",                  # "10 received" - common in ping summaries
                            "packet loss",              # Alternative packet loss indicator
                            "transmission failure",      # Service ping specific failure
                            "service path",             # Service path reference
                            "tenant context",           # Tenant context completion
                            "service route",            # Service routing completion
                        ]
                        # ARP completion indicators - specific patterns from actual ARP output
                        arp_indicators = [
                            "total mac entries",     # "Total 31 MAC Entries."
                            "total flows:",          # "Total Flows:151"
                            "mac-flow hi-water",     # "Mac-Flow Hi-Water:2865"
                            "arp table",             # Generic ARP table reference
                            "no arp entries",        # Empty ARP table
                            "arp cache"              # ARP cache reference
                        ]
                        # Gateway-specific completion indicators (SSR gateways often have shorter output)
                        gateway_indicators = [
                            "connected routes",      # Gateway routing table
                            "total entries",         # Gateway route totals
                            "kernel routes",         # SSR kernel routing
                            "bgp routes",           # BGP routing information
                            "static routes",        # Static route information
                            "route table"           # Generic route table reference
                        ]
                        # Switch-specific completion indicators
                        switch_indicators = [
                            "learning table",        # Switch MAC learning
                            "fdb entries",          # Forwarding database
                            "vlan information",     # VLAN details
                            "port statistics",      # Port stats
                            "interface status",     # Interface information
                            "ethernet switching table",  # MAC table header
                            "entries, 40 learned",       # MAC table summary (specific count varies)
                            "entries,",             # Generic MAC table entry count
                            "learned"               # MAC learning completion
                        ]
                        # General completion indicators
                        general_indicators = ["command completed", "operation complete", "finished"]
                        
                        all_indicators = ping_indicators + service_ping_indicators + arp_indicators + gateway_indicators + switch_indicators + general_indicators
                        found_indicator = None
                        
                        if debug_mode and check_count % 100 == 1:  # Debug indicator checking every 100 checks (roughly 10 seconds)
                            self.logger.debug(f"Checking {len(all_indicators)} completion indicators")
                            self.logger.debug(f"Content sample for indicator check: {repr(all_raw_content.lower()[:150])}")
                            print(f"[DEBUG] Checking {len(all_indicators)} completion indicators")
                            print(f"[DEBUG] Content sample for indicator check: {repr(all_raw_content.lower()[:150])}")
                        
                        for indicator in all_indicators:
                            # Skip generic indicators for MAC table commands to allow proper completion detection
                            if "ethernet switching table" in all_raw_content.lower() and indicator in ["ethernet switching table", "entries,", "learned"]:
                                continue  # Let MAC table specific logic handle this
                            if indicator in all_raw_content.lower():
                                found_indicator = indicator
                                if debug_mode:
                                    self.logger.debug(f"FOUND completion indicator: '{indicator}'")
                                    print(f"[DEBUG] FOUND completion indicator: '{indicator}'")
                                break
                        
                        # Alternative completion: look for "packet loss" followed by "round-trip" pattern (ping specific)
                        if not found_indicator and "packet loss" in all_raw_content.lower():
                            # Check if we have the complete statistics block
                            lines = all_raw_content.lower().split('\n')
                            for line in lines:
                                if "packet loss" in line and ("round-trip" in all_raw_content.lower() or "rtt" in all_raw_content.lower()):
                                    found_indicator = "complete statistics block"
                                    if debug_mode:
                                        self.logger.debug("FOUND ping statistics completion pattern")
                                        self.logger.debug(f"Packet loss line: {repr(line[:100])}")
                                        print(f"[DEBUG] FOUND ping statistics completion pattern")
                                        print(f"[DEBUG] Packet loss line: {repr(line[:100])}")
                                    break
                        
                        # Service ping specific completion: look for individual ping responses with timing
                        if not found_indicator and len(collected_output) >= 3:  # Service ping typically has multiple responses
                            # Look for service ping patterns - individual responses with seq/ttl/time
                            service_ping_pattern_count = 0
                            if "seq=" in all_raw_content.lower() and ("ttl=" in all_raw_content.lower() or "time=" in all_raw_content.lower()):
                                service_ping_pattern_count += 1
                            if "bytes from" in all_raw_content.lower():
                                service_ping_pattern_count += 1
                            
                            if debug_mode and check_count % 200 == 1:  # Debug service ping patterns
                                self.logger.debug(f"Service ping pattern analysis: found {service_ping_pattern_count} service ping indicators")
                                print(f"[DEBUG] Service ping pattern analysis: found {service_ping_pattern_count} service ping indicators")
                                if "seq=" in all_raw_content.lower():
                                    self.logger.debug("Found seq= pattern in service ping output")
                                    print(f"[DEBUG] Found seq= pattern in service ping output")
                                if "bytes from" in all_raw_content.lower():
                                    self.logger.debug("Found 'bytes from' pattern in service ping output")
                                    print(f"[DEBUG] Found 'bytes from' pattern in service ping output")
                            
                            # If we see service ping patterns and have been collecting for reasonable time
                            if service_ping_pattern_count >= 2:
                                # For service ping, if we have multiple ping responses and some idle time, consider complete
                                if time.time() - last_activity > 3:  # Wait 3 seconds after last response for service ping
                                    found_indicator = "service ping pattern detected"
                                    if debug_mode:
                                        self.logger.debug(f"FOUND service ping completion: {service_ping_pattern_count} patterns detected")
                                        self.logger.debug(f"Service ping idle time: {time.time() - last_activity:.1f}s")
                                        print(f"[DEBUG] FOUND service ping completion: {service_ping_pattern_count} patterns detected")
                                        print(f"[DEBUG] Service ping idle time: {time.time() - last_activity:.1f}s")
                        
                        # Alternative service ping completion: check for count-based completion
                        if not found_indicator and len(collected_output) >= 5:  # Reasonable number of responses
                            # Count individual ping responses in format: "64 bytes from X.X.X.X: seq=N ttl=N time=N ms"
                            ping_response_count = all_raw_content.lower().count("bytes from")
                            if ping_response_count >= 5 and time.time() - last_activity > 2:  # Have responses and idle time
                                found_indicator = f"count-based completion ({ping_response_count} responses)"
                                if debug_mode:
                                    self.logger.debug(f"FOUND count-based service ping completion: {ping_response_count} responses")
                                    self.logger.debug(f"Idle time since last response: {time.time() - last_activity:.1f}s")
                                    print(f"[DEBUG] FOUND count-based service ping completion: {ping_response_count} responses")
                                    print(f"[DEBUG] Idle time since last response: {time.time() - last_activity:.1f}s")
                        
                        # MAC table completion: detect when table is complete and device stops sending
                        if not found_indicator and ("ethernet switching table" in all_raw_content.lower() or "thernet switching table" in all_raw_content.lower()):
                            # Search for "Ethernet switching table : XXX entries" pattern in reassembled buffer
                            import re
                            # Look for pattern like "Ethernet switching table : 44 entries" (handles chunking)
                            table_pattern = r'ethernet switching table\s*:\s*(\d+)\s+entries'
                            match = re.search(table_pattern, all_raw_content.lower())
                            
                            if match:
                                entry_count = int(match.group(1))
                                
                                # First check: if we're getting the same message content repeatedly (completion signal)
                                if len(collected_output) >= 5:
                                    # Get the last 5 message contents
                                    last_messages = [msg.get('raw', '') for msg in collected_output[-5:]]
                                    # If all 5 are identical (and not empty), the command has finished
                                    if len(set(last_messages)) == 1 and last_messages[0].strip():
                                        found_indicator = f"mac table completion (detected {len(last_messages)} repeated identical messages)"
                                        if debug_mode:
                                            self.logger.debug(f"FOUND MAC table completion: {len(last_messages)} repeated identical messages detected")
                                            self.logger.debug(f"Repeated message: {repr(last_messages[0][:100])}")
                                            print(f"[DEBUG] FOUND MAC table completion: {len(last_messages)} repeated identical messages detected")
                                            print(f"[DEBUG] Repeated message: {repr(last_messages[0][:100])}")
                                    else:
                                        if debug_mode and check_count % 50 == 1:
                                            # Show how many unique messages in the last 5
                                            unique_count = len(set(last_messages))
                                            print(f"[DEBUG] MAC table: found {entry_count} entries, last 5 messages have {unique_count} unique contents")
                                
                                # Second check: if device has been idle for 3+ seconds and we have substantial MAC entries
                                if not found_indicator and len(collected_output) >= 10 and entry_count >= 10:
                                    idle_time = time.time() - last_activity
                                    if idle_time >= 3.0:  # Device has been quiet for 3 seconds
                                        found_indicator = f"mac table completion (idle timeout: {entry_count} entries, {idle_time:.1f}s idle)"
                                        if debug_mode:
                                            self.logger.debug(f"FOUND MAC table completion via idle timeout: {entry_count} entries, {idle_time:.1f}s idle")
                                            print(f"[DEBUG] FOUND MAC table completion via idle timeout: {entry_count} entries, {idle_time:.1f}s idle")
                                
                                if not found_indicator and debug_mode and check_count % 50 == 1:
                                    idle_time = time.time() - last_activity
                                    print(f"[DEBUG] MAC table: found {entry_count} entries, idle for {idle_time:.1f}s")
                            else:
                                if debug_mode and check_count % 50 == 1:
                                    print(f"[DEBUG] MAC table: checking for completion pattern in {len(all_raw_content)} chars")
                        
                        # ARP-specific completion: check for structured ARP output patterns
                        if not found_indicator and len(collected_output) >= 2:
                            # Look for ARP table structure patterns
                            arp_patterns = ["ip address", "hw address", "interface", "incomplete", "permanent"]
                            arp_pattern_count = sum(1 for pattern in arp_patterns if pattern in all_raw_content.lower())
                            
                            if debug_mode and check_count % 200 == 1:  # Debug ARP patterns less frequently
                                print(f"[DEBUG] ARP pattern analysis: found {arp_pattern_count}/{len(arp_patterns)} patterns")
                                found_patterns = [p for p in arp_patterns if p in all_raw_content.lower()]
                                print(f"[DEBUG] Found ARP patterns: {found_patterns}")
                            
                            # If we see multiple ARP patterns, this might be a complete ARP table
                            if arp_pattern_count >= 2:
                                # Check if we've been collecting for at least 1 second (ARP commands are usually fast)
                                if time.time() - last_activity > 1:
                                    found_indicator = "arp table structure detected"
                                    if debug_mode:
                                        print(f"[DEBUG] FOUND ARP table completion: {arp_pattern_count} patterns detected")
                        
                        if found_indicator:
                            # This appears to be the final ping summary
                            if debug_mode:
                                self.logger.debug(f"Found completion indicator '{found_indicator}' in combined content")
                                self.logger.debug(f"Completing after {check_count} checks")
                                self.logger.debug(f"Total collected messages: {len(collected_output)}")
                                self.logger.debug(f"Total content length: {len(all_raw_content)} characters")
                                self.logger.debug(f"Raw content sample (first 200 chars): {repr(all_raw_content[:200])}")
                                self.logger.debug(f"Raw content sample (last 200 chars): {repr(all_raw_content[-200:])}")
                                print(f"[DEBUG] Found completion indicator '{found_indicator}' in combined content")
                                print(f"[DEBUG] Completing after {check_count} checks")
                                print(f"[DEBUG] Total collected messages: {len(collected_output)}")
                                print(f"[DEBUG] Total content length: {len(all_raw_content)} characters")
                                print(f"[DEBUG] Raw content sample (first 200 chars): {repr(all_raw_content[:200])}")
                                print(f"[DEBUG] Raw content sample (last 200 chars): {repr(all_raw_content[-200:])}")
                            final_results = self.command_results.pop(session_id)
                            break
                else:
                    if debug_mode and check_count % 50 == 1:  # Debug every 5 seconds
                        self.logger.debug(f"Check #{check_count}, no results yet for session {session_id}")
                        self.logger.debug(f"Available sessions: {list(self.command_results.keys())}")
                        print(f"[DEBUG] Check #{check_count}, no results yet for session {session_id}")
                        print(f"[DEBUG] Available sessions: {list(self.command_results.keys())}")
            
            # Emergency circuit breaker - if we're doing too many checks, something is wrong
            if check_count > 10000:  # At 0.1s per check, this is ~16 minutes
                if debug_mode:
                    self.logger.error(f"Circuit breaker triggered at {check_count} checks!")
                    self.logger.error("This indicates a possible infinite loop or system hang")
                    print(f"[EMERGENCY] Circuit breaker triggered at {check_count} checks!")
                    print(f"[EMERGENCY] This indicates a possible infinite loop or system hang")
                self.logger.error(f"Emergency circuit breaker: {check_count} checks exceeded for session {session_id}")
                with self.results_lock:
                    final_results = self.command_results.pop(session_id, [])
                return final_results if final_results else None
            
            # Check for activity timeout (no new messages)
            collected_count = 0
            with self.results_lock:
                if session_id in self.command_results:
                    collected_count = len(self.command_results[session_id])
            
            if collected_count > 0 and (time.time() - last_activity > activity_timeout):
                if debug_mode:
                    self.logger.debug(f"Activity timeout reached ({activity_timeout}s), completing with {collected_count} messages")
                    print(f"[DEBUG] Activity timeout reached ({activity_timeout}s), completing with {collected_count} messages")
                self.logger.info(f"No new data for {activity_timeout}s, assuming command complete")
                with self.results_lock:
                    if session_id in self.command_results:
                        final_results = self.command_results.pop(session_id)
                        break
                
            # Critical: Ensure we don't create a busy wait loop
            time.sleep(0.1)  # Check every 100ms - DO NOT REMOVE THIS SLEEP
        else:
            # Timeout occurred
            if debug_mode:
                self.logger.debug(f"Timeout occurred after {timeout_seconds}s, {check_count} checks")
                print(f"[DEBUG] Timeout occurred after {timeout_seconds}s, {check_count} checks")
            with self.results_lock:
                final_results = self.command_results.pop(session_id, [])
            
            if not final_results:
                if debug_mode:
                    print(f"[DEBUG] No results collected for session {session_id}")
                self.logger.warning(f"Timeout waiting for command result: {session_id}")
                perf_monitor.finish()  # Mark performance monitoring as complete
                return None
        
        # Combine all collected output
        perf_monitor.finish()  # Mark performance monitoring as complete
        
        if final_results:
            if debug_mode:
                self.logger.debug(f"Combining {len(final_results)} result segments")
                self.logger.debug(f"Total wait time: {time.time() - start_time:.2f} seconds")
                self.logger.debug(f"Total checks performed: {check_count}")
                print(f"[DEBUG] Combining {len(final_results)} result segments")
                print(f"[DEBUG] Total wait time: {time.time() - start_time:.2f} seconds")
                print(f"[DEBUG] Total checks performed: {check_count}")
            
            combined_raw = ""
            combined_other = {}
            
            for index, result in enumerate(final_results):
                raw_content = result.get("raw", "")
                if raw_content:
                    combined_raw += raw_content
                    if debug_mode and len(final_results) > 5:  # Only show details for complex results
                        print(f"[DEBUG] Segment {index+1}: {len(raw_content)} chars")
                    
                # Collect any other fields
                for key, value in result.items():
                    if key not in ["raw", "session"]:
                        if key in combined_other:
                            combined_other[key] = str(combined_other[key]) + str(value)
                        else:
                            combined_other[key] = value
            
            # Return combined result
            final_result = {"raw": combined_raw, "session": session_id}
            final_result.update(combined_other)
            
            if debug_mode:
                print(f"[DEBUG] Final combined result length: {len(combined_raw)} characters")
                print(f"[DEBUG] Final result fields: {list(final_result.keys())}")
                print(f"[DEBUG] First 150 chars of final result: {repr(combined_raw[:150])}")
                print(f"[DEBUG] Last 150 chars of final result: {repr(combined_raw[-150:])}")
                if len(combined_raw) == 0:
                    print(f"[DEBUG] WARNING: Final result is empty - this may indicate an issue")
                print(f"[DEBUG] Session {session_id} result collection complete")
                print(f"[DEBUG] " + "="*60)
            
            self.logger.info(f"Command completed with {len(final_results)} message segments")
            return final_result
        
        perf_monitor.finish()  # Mark performance monitoring as complete  
        return None
    
    def _on_open(self, websocket_connection):
        """WebSocket connection opened callback."""
        self.connected = True
        self.logger.debug("WebSocket connection opened")
    
    def _on_message(self, websocket_connection, message):
        """
        WebSocket message received callback.
        
        Processes incoming messages following the documented Mist API format:
        {
            "event": "data", 
            "channel": "/sites/{site_id}/devices/{device_id}/cmd", 
            "data": { 
                "session": "session_id", 
                "raw": "64 bytes from 23.211.0.110: seq=8 ttl=58 time=12.323 ms\n"
            } 
        }
        """
        debug_mode = is_debug_mode()
        
        try:
            if debug_mode:
                print(f"[DEBUG] Raw WebSocket message received: {repr(message)} (type: {type(message)})")
            self.logger.debug(f"Raw WebSocket message received: {repr(message)} (type: {type(message)})")
            
            # Parse JSON message - handle string messages first
            message_data = None
            if isinstance(message, str):
                try:
                    message_data = json.loads(message)
                    if debug_mode:
                        print(f"[DEBUG] Successfully parsed JSON message: {message_data}")
                    self.logger.debug(f"Successfully parsed JSON message: {message_data}")
                except json.JSONDecodeError as json_error:
                    if debug_mode:
                        print(f"[DEBUG] Failed to parse JSON message: {json_error}")
                        print(f"[DEBUG] Raw message content: {repr(message)}")
                    self.logger.warning(f"Failed to parse JSON message: {json_error}")
                    self.logger.debug(f"Raw message content: {repr(message)}")
                    return
            elif isinstance(message, dict):
                message_data = message
                if debug_mode:
                    print(f"[DEBUG] Received dict message: {message_data}")
                self.logger.debug(f"Received dict message: {message_data}")
            else:
                if debug_mode:
                    print(f"[DEBUG] Unexpected message type: {type(message)}, content: {repr(message)}")
                self.logger.warning(f"Unexpected message type: {type(message)}, content: {repr(message)}")
                return
            
            # Ensure we have a valid message_data dict before proceeding
            if not isinstance(message_data, dict):
                if debug_mode:
                    print(f"[DEBUG] Message data is not a dict after parsing: {type(message_data)}")
                self.logger.error(f"Message data is not a dict after parsing: {type(message_data)}")
                return
            
            # Enhanced packet content logging for debug mode
            if debug_mode:
                print(f"[PACKET] WebSocket packet details:")
                print(f"[PACKET]   Event: {message_data.get('event', 'unknown')}")
                print(f"[PACKET]   Channel: {message_data.get('channel', 'unknown')}")
                if 'data' in message_data:
                    data_content = message_data['data']
                    print(f"[PACKET]   Data type: {type(data_content)}")
                    if isinstance(data_content, dict):
                        print(f"[PACKET]   Data keys: {list(data_content.keys())}")
                        if 'session' in data_content:
                            session_id = data_content['session']
                            print(f"[PACKET]   Session ID: {session_id}")
                        if 'raw' in data_content:
                            raw_content = data_content['raw']
                            print(f"[PACKET]   Raw content length: {len(str(raw_content))} chars")
                            print(f"[PACKET]   Raw content: {repr(raw_content)}")
                    else:
                        print(f"[PACKET]   Data content: {repr(data_content)}")
                else:
                    print(f"[PACKET]   No data field in message")
            
            # Handle subscription confirmation
            if message_data.get("event") == "channel_subscribed":
                channel = message_data.get("channel")
                if debug_mode:
                    print(f"[DEBUG] Channel subscription confirmed: {channel}")
                self.logger.info(f"Channel subscription confirmed: {channel}")
                # Track confirmed subscription
                if channel:
                    self.confirmed_subscriptions.add(channel)
                return
            
            # Handle command data following documented format
            if message_data.get("event") == "data":
                channel = message_data.get("channel", "")
                data_payload = message_data.get("data", {})
                
                if debug_mode:
                    print(f"[DEBUG] Processing data event from channel: {channel}")
                    print(f"[DEBUG] Data payload type: {type(data_payload)}")
                    print(f"[DEBUG] Data payload content: {repr(data_payload)}")
                
                # Check if data_payload is actually a nested message structure
                if isinstance(data_payload, str):
                    try:
                        # Sometimes the data field contains a JSON string
                        data_payload = json.loads(data_payload)
                        if debug_mode:
                            print(f"[DEBUG] Parsed nested JSON in data field: {data_payload}")
                        self.logger.debug(f"Parsed nested JSON in data field: {data_payload}")
                    except json.JSONDecodeError:
                        if debug_mode:
                            print(f"[DEBUG] Data field is string but not JSON: {data_payload}")
                        self.logger.warning(f"Data field is string but not JSON: {data_payload}")
                        return
                
                # Check if we have another nested event structure
                if isinstance(data_payload, dict) and data_payload.get("event") == "data":
                    # This is a nested structure, extract the actual data
                    actual_data = data_payload.get("data", {})
                    if debug_mode:
                        print(f"[DEBUG] Found nested event structure, extracting actual data: {actual_data}")
                    self.logger.debug(f"Found nested event structure, extracting actual data: {actual_data}")
                    data_payload = actual_data
                
                session_id = data_payload.get("session") if isinstance(data_payload, dict) else None
                
                if debug_mode:
                    print(f"[DEBUG] Processing data event - channel: {channel}, session: {session_id}")
                    print(f"[DEBUG] Final data payload: {data_payload}")
                    if session_id:
                        print(f"[DEBUG] Session ID extracted: {session_id}")
                    else:
                        print(f"[DEBUG] No session ID found in data payload")
                
                self.logger.debug(f"Processing data event - channel: {channel}, session: {session_id}")
                self.logger.debug(f"Final data payload: {data_payload}")
                
                if session_id:
                    # Store each message for streaming commands like ping
                    with self.results_lock:
                        # Initialize a list for this session if it doesn't exist
                        if session_id not in self.command_results:
                            self.command_results[session_id] = []
                            if debug_mode:
                                print(f"[DEBUG] Initialized new result list for session: {session_id}")
                        # Append this message to the list
                        self.command_results[session_id].append(data_payload)
                        
                        if debug_mode:
                            current_count = len(self.command_results[session_id])
                            print(f"[DEBUG] Stored message #{current_count} for session {session_id}")
                            if 'raw' in data_payload:
                                raw_data = data_payload['raw']
                                print(f"[DEBUG] Raw data in stored message: {repr(raw_data)}")
                            print(f"[DEBUG] Complete stored message: {data_payload}")
                        
                        self.logger.debug(f"Stored command result for session {session_id}: {data_payload}")
                    self.logger.debug(f"Command result segment received for session: {session_id}")
                    self.logger.debug(f"Total segments for session: {len(self.command_results[session_id])}")
                else:
                    self.logger.warning(f"Received data event without session ID. Full message: {message_data}")
                    self.logger.warning(f"Data payload: {data_payload}")
            else:
                self.logger.debug(f"Unhandled message event type: {message_data.get('event')}")
                
        except Exception as message_error:
            self.logger.error(f"Error processing WebSocket message: {message_error}")
            self.logger.debug(f"Exception details:", exc_info=True)
            self.logger.debug(f"Problematic message: {repr(message)}")
            self.logger.debug(f"Message type: {type(message)}")
    
    def _on_error(self, websocket_connection, error):
        """WebSocket error callback."""
        self.logger.error(f"WebSocket error: {error}")
    
    def _on_close(self, websocket_connection, close_status_code, close_message):
        """WebSocket connection closed callback."""
        self.connected = False
        self.logger.info("WebSocket connection closed")
    
    def disconnect(self):
        """Close WebSocket connection and cleanup resources."""
        if self.websocket_connection:
            self.websocket_connection.close()
        self.connected = False
        self.subscribed_channels.clear()
        
        with self.results_lock:
            self.command_results.clear()


class PacketCaptureManager:
    """
    Comprehensive packet capture management for Juniper Mist environments.
    
    This class handles both organization-level and site-level packet captures with support
    for multiple capture types:
    - Client captures (wireless/wired)
    - Gateway captures (wired/wireless)
    - Scan captures (wireless radiotap)
    - MxEdge captures (org-level only)
    
    All captures stream output via WebSocket for real-time monitoring.
    
    SECURITY:
        - Validates all user inputs (MAC addresses, channels, durations)
        - Enforces API constraints (max duration, packet counts)
        - Requires explicit confirmation for capture initiation
        - Logs all operations with full audit trail
    
    ARCHITECTURE:
        - Leverages existing WebSocketManager for streaming
        - Follows NASA/JPL defensive programming patterns
        - Class-based design eliminates wrapper functions
    """
    
    def __init__(self, mist_session, org_id=None):
        """
        Initialize packet capture manager.
        
        Args:
            mist_session: Active Mist API session
            org_id (str, optional): Organization ID for operations
        """
        self.mist_session = mist_session
        self.org_id = org_id or get_cached_or_prompted_org_id()
        self.websocket_manager = None
        logging.debug(f"PacketCaptureManager initialized for org_id: {self.org_id}")
    
    @staticmethod
    def validate_mac_address(mac_address: str) -> bool:
        """
        Validate MAC address format.
        
        Args:
            mac_address (str): MAC address to validate
            
        Returns:
            bool: True if valid, False otherwise
            
        SECURITY: Prevents injection of malformed MAC addresses into API calls
        """
        if not mac_address:
            return False
        
        # Support common MAC formats: aa:bb:cc:dd:ee:ff, aa-bb-cc-dd-ee-ff, aabbccddeeff
        mac_pattern = re.compile(r'^([0-9A-Fa-f]{2}[:-]){5}([0-9A-Fa-f]{2})$|^[0-9A-Fa-f]{12}$')
        return bool(mac_pattern.match(mac_address))
    
    @staticmethod
    def normalize_mac_address(mac_address: str) -> str:
        """
        Normalize MAC address to colon-separated format.
        
        Args:
            mac_address (str): MAC address in any common format
            
        Returns:
            str: Normalized MAC address (aa:bb:cc:dd:ee:ff)
        """
        # Remove all separators
        mac_clean = re.sub(r'[:-]', '', mac_address.lower())
        # Insert colons every 2 characters
        return ':'.join(mac_clean[i:i+2] for i in range(0, 12, 2))
    
    def _get_tcpdump_expression_selection(self):
        """
        Prompt user for tcpdump expression with comprehensive examples.
        Based on Daniel Miessler's tcpdump tutorial (danielmiessler.com/blog/tcpdump)
        
        Returns:
            str: tcpdump expression or empty string to skip
        """
        print("\n" + "=" * 80)
        print(" PACKET FILTER SELECTION (tcpdump expression)")
        print("=" * 80)
        print("\n--- BASIC FILTERS ---")
        print("  1.  All traffic (no filter)")
        print("  2.  HTTPS only (port 443)")
        print("  3.  HTTP/HTTPS (port 80 or 443)")
        print("  4.  DNS (port 53)")
        print("  5.  SSH (port 22)")
        print("  6.  FTP (port 21)")
        print("  7.  SMTP Email (port 25)")
        print("  8.  ICMP/ping")
        print("  9.  ARP")
        
        print("\n--- PROTOCOL FILTERS ---")
        print("  10. TCP only")
        print("  11. UDP only")
        print("  12. Not ICMP (exclude ping)")
        
        print("\n--- DIRECTION FILTERS ---")
        print("  13. Outbound to port 443")
        print("  14. Inbound from port 80")
        
        print("\n--- COMBINED FILTERS ---")
        print("  15. HTTP or HTTPS or DNS (port 80 or 443 or 53)")
        print("  16. All except SSH (not port 22)")
        print("  17. TCP SYN packets (connection attempts)")
        print("  18. TCP SYN-ACK packets (connection replies)")
        print("  19. TCP RST packets (connection resets)")
        print("  20. TCP FIN packets (connection close)")
        
        print("\n--- ADVANCED FILTERS ---")
        print("  21. Non-standard ports (>1024)")
        print("  22. All except ARP and DNS")
        print("  23. TCP traffic on non-standard ports")
        print("  24. Broadcast traffic")
        print("  25. Multicast traffic")
        print("  26. IPv6 only")
        print("  27. VLAN tagged traffic")
        
        print("\n--- APPLICATION PROTOCOLS ---")
        print("  28. SMB/CIFS file sharing (port 445)")
        print("  29. RDP Remote Desktop (port 3389)")
        print("  30. NTP time sync (port 123)")
        print("  31. SNMP monitoring (port 161)")
        print("  32. Syslog (port 514)")
        print("  33. DHCP (port 67 or 68)")
        print("  34. LDAP directory (port 389)")
        print("  35. MySQL database (port 3306)")
        
        print("\n--- SECURITY & TROUBLESHOOTING ---")
        print("  36. Port scans (SYN without ACK)")
        print("  37. Fragmented packets")
        print("  38. Large packets (>1500 bytes)")
        print("  39. Retransmissions (duplicate SEQ)")
        print("  40. Custom expression")
        
        print("=" * 80)
        
        choice = safe_input("\nEnter choice (default 1 - all traffic): ", default_value="1", context="tcpdump_filter")
        
        expressions = {
            # Basic filters
            "1": "",  # No filter
            "2": "port 443",
            "3": "port 80 or port 443",
            "4": "port 53",
            "5": "port 22",
            "6": "port 21",
            "7": "port 25",
            "8": "icmp",
            "9": "arp",
            
            # Protocol filters
            "10": "tcp",
            "11": "udp",
            "12": "not icmp",
            
            # Direction filters
            "13": "dst port 443",
            "14": "src port 80",
            
            # Combined filters
            "15": "port 80 or port 443 or port 53",
            "16": "not port 22",
            "17": "tcp[tcpflags] & tcp-syn != 0",
            "18": "tcp[tcpflags] = 0x12",
            "19": "tcp[tcpflags] & tcp-rst != 0",
            "20": "tcp[tcpflags] & tcp-fin != 0",
            
            # Advanced filters
            "21": "tcp[0:2] > 1024 or udp[0:2] > 1024",
            "22": "not arp and not port 53",
            "23": "tcp and port > 1024",
            "24": "ether broadcast",
            "25": "ether multicast",
            "26": "ip6",
            "27": "vlan",
            
            # Application protocols
            "28": "port 445",
            "29": "port 3389",
            "30": "port 123",
            "31": "port 161",
            "32": "port 514",
            "33": "port 67 or port 68",
            "34": "port 389",
            "35": "port 3306",
            
            # Security & troubleshooting
            "36": "tcp[tcpflags] & (tcp-syn) != 0 and tcp[tcpflags] & (tcp-ack) = 0",
            "37": "ip[6:2] & 0x1fff != 0",
            "38": "greater 1500",
            "39": "tcp[tcpflags] & (tcp-syn|tcp-fin|tcp-rst|tcp-push|tcp-ack|tcp-urg) = 0"
        }
        
        if choice in expressions:
            expr = expressions[choice]
            if expr:
                print(f"\n! Filter applied: {expr}")
            else:
                print("\n! Filter: None (capturing all traffic)")
            return expr
        elif choice == "40":
            print("\nEnter custom tcpdump expression:")
            print("  Examples: 'host 192.168.1.1', 'net 10.0.0.0/8', 'port 8080'")
            custom_expr = safe_input("Expression: ", context="tcpdump_custom", allow_empty=True)
            if custom_expr:
                print(f"\n! Filter applied: {custom_expr}")
                return custom_expr
            else:
                print("\n! No filter applied")
                return ""
        else:
            print(f"\n! Invalid choice, using no filter")
            return ""
    
    def _get_capture_format_selection(self):
        """
        Prompt user for capture format selection.
        
        NOTE: API documentation shows switches/gateways only support "stream" format,
        but testing confirms "pcap" format works and generates downloadable files.
        We offer both options and let the API reject if unsupported.
        
        Returns:
            str: Selected format - 'pcap' or 'stream'
        """
        print("\nCapture format:")
        print("  1. PCAP file - downloadable (default, recommended)")
        print("  2. Stream to Mist Cloud (WebSocket real-time)")
        format_choice = safe_input("Enter choice (default 1): ", default_value="1", context="format")
        return "pcap" if format_choice == "1" else "stream"
    
    def start_site_packet_capture(self):
        """
        Interactive menu for starting site-level packet captures.
        
        Presents user with capture type options and guides through configuration.
        """
        logging.info("ENTRY: PacketCaptureManager.start_site_packet_capture()")
        
        print("\n" + "=" * 80)
        print(" SITE PACKET CAPTURE MANAGER")
        print("=" * 80)
        print("\nSelect capture type:")
        print("  1. Client Capture (Wireless) - Captures ongoing traffic from connected clients")
        print("  2. Client Capture (Wired) - Captures wired client traffic")
        print("  3. Gateway Capture - Captures WAN/LAN gateway port traffic")
        print("  4. Switch Capture - Captures switch port traffic")
        print("  5. New Association Capture - Captures NEW connection attempts (auth/assoc handshakes)")
        print("  6. Scan Radio Capture - Captures raw 802.11 frames on specific channel")
        print("  0. Cancel")
        print("=" * 80)
        
        choice = safe_input("\nEnter your choice: ", context="site_capture_menu")
        
        if choice == "1":
            self._start_site_client_capture_wireless()
        elif choice == "2":
            self._start_site_client_capture_wired()
        elif choice == "3":
            self._start_site_gateway_capture()
        elif choice == "4":
            self._start_site_switch_capture()
        elif choice == "5":
            self._start_site_new_association_capture()
        elif choice == "6":
            self._start_site_scan_capture()
        elif choice == "0":
            print("\n! Cancelled by user")
            return
        else:
            print("\n! Invalid choice")
            return
    
    def _start_site_client_capture_wireless(self):
        """Start wireless client packet capture at site level."""
        logging.info("Starting site wireless client capture")
        
        # Get site selection
        site_id = prompt_and_log_site_selection()
        if not site_id:
            return
        
        # Get capture parameters
        print("\n" + "-" * 80)
        print(" WIRELESS CLIENT CAPTURE CONFIGURATION")
        print("-" * 80)
        print("\nThis capture type monitors ongoing traffic from ALREADY CONNECTED wireless clients.")
        print("Note: To capture new connection attempts (auth/assoc handshakes), use New Association Capture instead.")
        
        # Client MAC selection
        print("\nClient selection:")
        print("  1. Select from connected clients")
        print("  2. Manually enter MAC address")
        client_choice = safe_input("Enter choice (default 1): ", default_value="1", context="client_select")
        
        client_mac = None
        if client_choice == "1":
            client_mac = prompt_select_client_mac_from_site(site_id)
            if not client_mac:
                print("\n! No client selected")
                return
        else:
            client_mac = safe_input("\nEnter client MAC address: ", context="client_mac")
        
        if not self.validate_mac_address(client_mac):
            print(f"\n! Invalid MAC address format: {client_mac}")
            return
        client_mac = self.normalize_mac_address(client_mac)
        
        # Optional AP MAC filter
        print("\nOptional: Filter by specific AP")
        print("  1. Select AP from list")
        print("  2. Enter MAC manually")
        print("  3. Skip (capture from any AP)")
        ap_choice = safe_input("Enter choice (default 3): ", default_value="3", context="ap_filter")
        
        ap_mac = None
        if ap_choice == "1":
            ap_mac = prompt_select_ap_mac_from_site(site_id)
            if ap_mac:
                ap_mac = self.normalize_mac_address(ap_mac)
        elif ap_choice == "2":
            ap_mac = safe_input("Enter AP MAC address: ", context="ap_mac")
            if not self.validate_mac_address(ap_mac):
                print(f"\n! Invalid AP MAC address format: {ap_mac}")
                return
            ap_mac = self.normalize_mac_address(ap_mac)
        
        # Duration (Mist API enforces minimum 60 seconds for all captures)
        duration_str = safe_input("Enter capture duration in seconds (default 60, max 86400): ", 
                                 default_value="60", context="duration")
        try:
            duration = int(duration_str)
            if duration < 60 or duration > 86400:
                print(f"\n! Duration must be between 60 and 86400 seconds")
                print(f"  (Mist API requires minimum 60 seconds for all packet captures)")
                return
        except ValueError:
            print(f"\n! Invalid duration: {duration_str}")
            return
        
        # Number of packets
        num_packets_str = safe_input("Enter number of packets (default 1024, max 10000, 0 for unlimited): ", 
                                    default_value="1024", context="num_packets")
        try:
            num_packets = int(num_packets_str)
            if num_packets < 0 or num_packets > 10000:
                print(f"\n! Number of packets must be between 0 and 10000")
                return
        except ValueError:
            print(f"\n! Invalid number of packets: {num_packets_str}")
            return
        
        # Max packet length
        max_pkt_len_str = safe_input("Enter max packet length in bytes (default 1300, max 2048): ", 
                                    default_value="1300", context="max_pkt_len")
        try:
            max_pkt_len = int(max_pkt_len_str)
            if max_pkt_len < 64 or max_pkt_len > 2048:
                print(f"\n! Max packet length must be between 64 and 2048 bytes")
                return
        except ValueError:
            print(f"\n! Invalid max packet length: {max_pkt_len_str}")
            return
        
        # Multicast option
        includes_mcast_input = safe_input("Include multicast traffic? (y/n, default n): ", 
                                         default_value="n", context="includes_mcast")
        includes_mcast = includes_mcast_input.lower() == 'y'
        
        # Tcpdump filter selection
        tcpdump_expr = self._get_tcpdump_expression_selection()
        
        # Format selection
        capture_format = self._get_capture_format_selection()
        
        # Loop mode option
        print("\nLoop Mode:")
        print("  Automatically start a new capture when the current one completes")
        print("  Downloads happen in background while next capture runs")
        loop_mode = safe_input("Enable continuous loop mode? (y/n, default n): ", 
                              default_value="n", context="loop_mode")
        enable_loop = loop_mode.lower() == 'y'
        
        # Build request payload
        payload = {
            "type": "client",
            "client_mac": client_mac,
            "duration": duration,
            "num_packets": num_packets,
            "max_pkt_len": max_pkt_len,
            "includes_mcast": includes_mcast,
            "format": capture_format
        }
        
        if ap_mac:
            payload["ap_mac"] = ap_mac
        
        # Add tcpdump filter if specified
        if tcpdump_expr:
            payload["tcpdump_expression"] = tcpdump_expr
        
        # Display configuration and confirm
        print("\n" + "=" * 80)
        print(" CAPTURE CONFIGURATION SUMMARY")
        print("=" * 80)
        print(f"  Capture Type: Wireless Client")
        print(f"  Client MAC: {client_mac}")
        if ap_mac:
            print(f"  AP MAC Filter: {ap_mac}")
        if tcpdump_expr:
            print(f"  Packet Filter: {tcpdump_expr}")
        else:
            print(f"  Packet Filter: None (all traffic)")
        print(f"  Duration: {duration} seconds")
        print(f"  Packets: {num_packets} ({'unlimited' if num_packets == 0 else 'max'})")
        print(f"  Max Packet Length: {max_pkt_len} bytes")
        print(f"  Include Multicast: {'Yes' if includes_mcast else 'No'}")
        print(f"  Format: {capture_format}")
        print(f"  Loop Mode: {'ENABLED (continuous until Ctrl+C)' if enable_loop else 'Disabled (single capture)'}")
        print("=" * 80)
        
        # Prompt user to proceed (Enter to continue, Ctrl+C to cancel)
        safe_input("\nPress Enter to start capture (Ctrl+C to cancel): ", context="confirmation", allow_empty=True)
        
        # Start capture via API
        if enable_loop:
            self._execute_site_capture_loop(site_id, payload)
        else:
            self._execute_site_capture(site_id, payload)
    
    def _start_site_client_capture_wired(self):
        """Start wired client packet capture at site level."""
        logging.info("Starting site wired client capture")
        
        # Get site selection
        site_id = prompt_and_log_site_selection()
        if not site_id:
            return
        
        print("\n" + "-" * 80)
        print(" WIRED CLIENT CAPTURE CONFIGURATION")
        print("-" * 80)
        
        # Client MAC selection
        print("\nClient selection:")
        print("  1. Select from connected clients")
        print("  2. Manually enter MAC address")
        client_choice = safe_input("Enter choice (default 1): ", default_value="1", context="client_select")
        
        client_mac = None
        if client_choice == "1":
            client_mac = prompt_select_client_mac_from_site(site_id)
            if not client_mac:
                print("\n! No client selected")
                return
        else:
            client_mac = safe_input("\nEnter client MAC address: ", context="client_mac")
        
        if not self.validate_mac_address(client_mac):
            print(f"\n! Invalid MAC address format: {client_mac}")
            return
        client_mac = self.normalize_mac_address(client_mac)
        
        # Duration (Mist API enforces minimum 60 seconds for all captures)
        duration_str = safe_input("Enter capture duration in seconds (default 60, max 86400): ", 
                                 default_value="60", context="duration")
        try:
            duration = int(duration_str)
            if duration < 60 or duration > 86400:
                print(f"\n! Duration must be between 60 and 86400 seconds")
                print(f"  (Mist API requires minimum 60 seconds for all packet captures)")
                return
        except ValueError:
            print(f"\n! Invalid duration: {duration_str}")
            return
        
        num_packets_str = safe_input("Enter number of packets (default 1024, max 10000, 0 for unlimited): ", 
                                    default_value="1024", context="num_packets")
        try:
            num_packets = int(num_packets_str)
            if num_packets < 0 or num_packets > 10000:
                print(f"\n! Number of packets must be between 0 and 10000")
                return
        except ValueError:
            print(f"\n! Invalid number of packets: {num_packets_str}")
            return
        
        # Multicast option
        includes_mcast_input = safe_input("Include multicast traffic? (y/n, default n): ", 
                                         default_value="n", context="includes_mcast")
        includes_mcast = includes_mcast_input.lower() == 'y'
        
        # Tcpdump filter selection
        tcpdump_expr = self._get_tcpdump_expression_selection()
        
        # Format selection
        capture_format = self._get_capture_format_selection()
        
        # Loop mode option
        print("\nLoop Mode:")
        print("  Automatically start a new capture when the current one completes")
        print("  Downloads happen in background while next capture runs")
        loop_mode = safe_input("Enable continuous loop mode? (y/n, default n): ", 
                              default_value="n", context="loop_mode")
        enable_loop = loop_mode.lower() == 'y'
        
        # Build payload
        payload = {
            "type": "client",
            "client_mac": client_mac,
            "duration": duration,
            "num_packets": num_packets,
            "includes_mcast": includes_mcast,
            "format": capture_format
        }
        
        # Add tcpdump filter if specified
        if tcpdump_expr:
            payload["tcpdump_expression"] = tcpdump_expr
        
        # Display and confirm
        print("\n" + "=" * 80)
        print(" CAPTURE CONFIGURATION SUMMARY")
        print("=" * 80)
        print(f"  Capture Type: Wired Client")
        print(f"  Client MAC: {client_mac}")
        if tcpdump_expr:
            print(f"  Packet Filter: {tcpdump_expr}")
        else:
            print(f"  Packet Filter: None (all traffic)")
        print(f"  Duration: {duration} seconds")
        print(f"  Packets: {num_packets} ({'unlimited' if num_packets == 0 else 'max'})")
        print(f"  Include Multicast: {'Yes' if includes_mcast else 'No'}")
        print(f"  Loop Mode: {'ENABLED (continuous until Ctrl+C)' if enable_loop else 'Disabled (single capture)'}")
        print("=" * 80)
        
        # Prompt user to proceed (Enter to continue, Ctrl+C to cancel)
        safe_input("\nPress Enter to start capture (Ctrl+C to cancel): ", context="confirmation", allow_empty=True)
        
        if enable_loop:
            self._execute_site_capture_loop(site_id, payload)
        else:
            self._execute_site_capture(site_id, payload)
    
    def _start_site_gateway_capture(self):
        """Start gateway packet capture at site level."""
        logging.info("Starting site gateway capture")
        
        site_id = prompt_and_log_site_selection()
        if not site_id:
            return
        
        print("\n" + "-" * 80)
        print(" GATEWAY CAPTURE CONFIGURATION")
        print("-" * 80)
        
        # Gateway selection - interactive list
        logging.debug("Prompting for gateway selection from site inventory")
        gateway_mac = prompt_select_gateway_mac_from_site(site_id)
        if not gateway_mac:
            logging.warning("No gateway selected or gateway selection failed - aborting capture")
            return
        
        # Normalize MAC address (already validated by selection function)
        gateway_mac = self.normalize_mac_address(gateway_mac)
        logging.debug(f"Selected and normalized gateway MAC: {gateway_mac}")
        
        # Port selection - now using interactive port selector with status information
        logging.debug("Prompting for port selection from gateway")
        port_list, available_ports = prompt_select_ports_from_device(site_id, gateway_mac, device_type="gateway", return_available=True)
        
        if port_list is None:
            logging.warning("Port selection failed or cancelled - aborting capture")
            return
        
        # If port_list is empty (all ports), populate with all available port names
        if not port_list and available_ports:
            port_list = [port_name for port_name, _ in available_ports]
            logging.debug(f"User selected all ports - expanded to: {port_list}")
        else:
            logging.debug(f"User selected specific ports: {port_list}")
        
        # Duration (Mist API enforces minimum 60 seconds for all captures)
        duration_str = safe_input("Enter capture duration in seconds (default 60, max 86400): ", 
                                 default_value="60", context="duration")
        try:
            duration = int(duration_str)
            if duration < 60 or duration > 86400:
                print(f"\n! Duration must be between 60 and 86400 seconds")
                print(f"  (Mist API requires minimum 60 seconds for all packet captures)")
                return
        except ValueError:
            print(f"\n! Invalid duration: {duration_str}")
            return
        
        num_packets_str = safe_input("Enter number of packets (default 1024, max 10000): ", 
                                    default_value="1024", context="num_packets")
        try:
            num_packets = int(num_packets_str)
            if num_packets < 0 or num_packets > 10000:
                print(f"\n! Number of packets must be between 0 and 10000")
                return
        except ValueError:
            print(f"\n! Invalid number of packets: {num_packets_str}")
            return
        
        # Packet filter selection (applies to all selected ports)
        tcpdump_expr = self._get_tcpdump_expression_selection()
        
        # Format selection
        capture_format = self._get_capture_format_selection()
        
        # Loop mode option
        print("\nLoop Mode:")
        print("  Automatically start a new capture when the current one completes")
        print("  Downloads happen in background while next capture runs")
        loop_mode = safe_input("Enable continuous loop mode? (y/n, default n): ", 
                              default_value="n", context="loop_mode")
        enable_loop = loop_mode.lower() == 'y'
        
        # Build payload - CORRECT structure per API spec
        payload = {
            "type": "gateway",
            "duration": duration,
            "num_packets": num_packets,
            "max_pkt_len": 1500,  # API example uses 1500 for gateways
            "format": capture_format
        }
        
        # Build gateways structure with actual port names
        gateways_config = {}
        ports_config = {}
        
        # Always list actual port names (never empty dict)
        for port in port_list:
            ports_config[port] = {}
            if tcpdump_expr:
                ports_config[port]["tcpdump_expression"] = tcpdump_expr
        
        gateways_config[gateway_mac] = {"ports": ports_config}
        payload["gateways"] = gateways_config
        
        # Display and confirm
        print("\n" + "=" * 80)
        print(" CAPTURE CONFIGURATION SUMMARY")
        print("=" * 80)
        print(f"  Capture Type: Gateway")
        print(f"  Gateway MAC: {gateway_mac}")
        if port_list:
            print(f"  Ports: {', '.join(port_list)}")
        else:
            print(f"  Ports: All ports")
        print(f"  Duration: {duration} seconds")
        print(f"  Packets: {num_packets}")
        print(f"  Max Packet Length: 1500 bytes")
        if tcpdump_expr:
            print(f"  Filter: {tcpdump_expr}")
        print(f"  Loop Mode: {'ENABLED (continuous until Ctrl+C)' if enable_loop else 'Disabled (single capture)'}")
        print("=" * 80)
        
        # Prompt user to proceed (Enter to continue, Ctrl+C to cancel)
        safe_input("\nPress Enter to start capture (Ctrl+C to cancel): ", context="confirmation", allow_empty=True)
        
        if enable_loop:
            self._execute_site_capture_loop(site_id, payload)
        else:
            self._execute_site_capture(site_id, payload)
    
    def _start_site_switch_capture(self):
        """Start switch packet capture at site level."""
        logging.info("Starting site switch capture")
        
        site_id = prompt_and_log_site_selection()
        if not site_id:
            return
        
        print("\n" + "-" * 80)
        print(" SWITCH CAPTURE CONFIGURATION")
        print("-" * 80)
        
        # Switch selection - interactive list
        logging.debug("Prompting for switch selection from site inventory")
        switch_mac = prompt_select_switch_mac_from_site(site_id)
        if not switch_mac:
            logging.warning("No switch selected or switch selection failed - aborting capture")
            return
        
        # Normalize MAC address (already validated by selection function)
        switch_mac = self.normalize_mac_address(switch_mac)
        logging.debug(f"Selected and normalized switch MAC: {switch_mac}")
        
        # Port selection - now using interactive port selector with status information
        logging.debug("Prompting for port selection from switch")
        port_list, available_ports = prompt_select_ports_from_device(site_id, switch_mac, device_type="switch", return_available=True)
        
        if port_list is None:
            logging.warning("Port selection failed or cancelled - aborting capture")
            return
        
        # If port_list is empty (all ports), populate with all available port names
        if not port_list and available_ports:
            port_list = [port_name for port_name, _ in available_ports]
            logging.debug(f"User selected all ports - expanded to: {port_list}")
        else:
            logging.debug(f"User selected specific ports: {port_list}")
        
        # Duration (Mist API enforces minimum 60 seconds for all captures)
        duration_str = safe_input("Enter capture duration in seconds (default 60, max 86400): ", 
                                 default_value="60", context="duration")
        try:
            duration = int(duration_str)
            if duration < 60 or duration > 86400:
                print(f"\n! Duration must be between 60 and 86400 seconds")
                print(f"  (Mist API requires minimum 60 seconds for all packet captures)")
                return
        except ValueError:
            print(f"\n! Invalid duration: {duration_str}")
            return
        
        num_packets_str = safe_input("Enter number of packets (default 1024, max 10000): ", 
                                    default_value="1024", context="num_packets")
        try:
            num_packets = int(num_packets_str)
            if num_packets < 0 or num_packets > 10000:
                print(f"\n! Number of packets must be between 0 and 10000")
                return
        except ValueError:
            print(f"\n! Invalid number of packets: {num_packets_str}")
            return
        
        # Packet filter selection (applies to all selected ports)
        tcpdump_expr = self._get_tcpdump_expression_selection()
        
        # Format selection
        capture_format = self._get_capture_format_selection()
        
        # Loop mode option
        print("\nLoop Mode:")
        print("  Automatically start a new capture when the current one completes")
        print("  Downloads happen in background while next capture runs")
        loop_mode = safe_input("Enable continuous loop mode? (y/n, default n): ", 
                              default_value="n", context="loop_mode")
        enable_loop = loop_mode.lower() == 'y'
        
        # Build payload
        payload = {
            "type": "switch",
            "duration": duration,
            "num_packets": num_packets,
            "max_pkt_len": 1500,  # API example uses 1500 for switches
            "format": capture_format
        }
        
        # Build switches structure with actual port names
        switches_config = {}
        ports_config = {}
        
        # Always list actual port names (never empty dict)
        for port in port_list:
            ports_config[port] = {}
            if tcpdump_expr:
                ports_config[port]["tcpdump_expression"] = tcpdump_expr
        
        switches_config[switch_mac] = {"ports": ports_config}
        payload["switches"] = switches_config
        
        # Display and confirm
        print("\n" + "=" * 80)
        print(" CAPTURE CONFIGURATION SUMMARY")
        print("=" * 80)
        print(f"  Capture Type: Switch")
        print(f"  Switch MAC: {switch_mac}")
        if port_list:
            print(f"  Ports: {', '.join(port_list)}")
        else:
            print(f"  Ports: All ports")
        print(f"  Duration: {duration} seconds")
        print(f"  Packets: {num_packets}")
        print(f"  Max Packet Length: 1500 bytes")
        if tcpdump_expr:
            print(f"  Filter: {tcpdump_expr}")
        print(f"  Loop Mode: {'ENABLED (continuous until Ctrl+C)' if enable_loop else 'Disabled (single capture)'}")
        print("=" * 80)
        
        # Prompt user to proceed (Enter to continue, Ctrl+C to cancel)
        safe_input("\nPress Enter to start capture (Ctrl+C to cancel): ", context="confirmation", allow_empty=True)
        
        if enable_loop:
            self._execute_site_capture_loop(site_id, payload)
        else:
            self._execute_site_capture(site_id, payload)
    
    def _start_site_new_association_capture(self):
        """Start new association packet capture at site level."""
        logging.info("Starting site new association capture")
        
        site_id = prompt_and_log_site_selection()
        if not site_id:
            return
        
        print("\n" + "-" * 80)
        print(" NEW ASSOCIATION CAPTURE CONFIGURATION")
        print("-" * 80)
        print("\nThis capture type monitors NEW client connection attempts (802.11 auth/assoc handshakes).")
        print("Note: To capture ongoing traffic from already-connected clients, use Client Capture (Wireless) instead.")
        
        # Optional SSID filter
        ssid = safe_input("\nEnter SSID to monitor (optional, press Enter for all): ", 
                         context="ssid", allow_empty=True)
        
        # Duration (Mist API enforces minimum 60 seconds for new_assoc captures)
        duration_str = safe_input("Enter capture duration in seconds (default 60, max 86400): ", 
                                 default_value="60", context="duration")
        try:
            duration = int(duration_str)
            if duration < 60 or duration > 86400:
                print(f"\n! Duration must be between 60 and 86400 seconds")
                print(f"  (Mist API requires minimum 60 seconds for new association captures)")
                return
        except ValueError:
            print(f"\n! Invalid duration: {duration_str}")
            return
        
        # Format selection
        capture_format = self._get_capture_format_selection()
        
        # Loop mode option
        print("\nLoop Mode:")
        print("  Automatically start a new capture when the current one completes")
        print("  Downloads happen in background while next capture runs")
        loop_mode = safe_input("Enable continuous loop mode? (y/n, default n): ", 
                              default_value="n", context="loop_mode")
        enable_loop = loop_mode.lower() == 'y'
        
        # Build payload
        payload = {
            "type": "new_assoc",
            "duration": duration,
            "format": capture_format
        }
        
        if ssid:
            payload["ssid"] = ssid
        
        # Display and confirm
        print("\n" + "=" * 80)
        print(" CAPTURE CONFIGURATION SUMMARY")
        print("=" * 80)
        print(f"  Capture Type: New Association")
        if ssid:
            print(f"  SSID Filter: {ssid}")
        else:
            print(f"  SSID Filter: All SSIDs")
        print(f"  Duration: {duration} seconds")
        print(f"  Loop Mode: {'ENABLED (continuous until Ctrl+C)' if enable_loop else 'Disabled (single capture)'}")
        print("=" * 80)
        
        # Prompt user to proceed (Enter to continue, Ctrl+C to cancel)
        safe_input("\nPress Enter to start capture (Ctrl+C to cancel): ", context="confirmation", allow_empty=True)
        
        if enable_loop:
            self._execute_site_capture_loop(site_id, payload)
        else:
            self._execute_site_capture(site_id, payload)
    
    def _start_site_scan_capture(self):
        """Start scan radio packet capture at site level."""
        logging.info("Starting site scan capture")
        
        site_id = prompt_and_log_site_selection()
        logging.debug(f"Site selection returned: {site_id}")
        if not site_id:
            logging.warning("No site_id returned from selection - aborting capture")
            return
        
        logging.debug(f"Proceeding with scan capture configuration for site: {site_id}")
        print("\n" + "-" * 80)
        print(" SCAN RADIO CAPTURE CONFIGURATION")
        print("-" * 80)
        
        # AP Selection - interactive list
        logging.debug("Prompting for AP selection from site inventory")
        ap_mac = prompt_select_ap_mac_from_site(site_id)
        if not ap_mac:
            logging.warning("No AP selected or AP selection failed - aborting capture")
            return
        
        # Check if user selected all APs
        if ap_mac == 'ALL_APS':
            logging.info("User selected all APs - launching multi-AP captures")
            self._start_site_scan_capture_all_aps(site_id)
            return
        
        # Normalize MAC address (already validated by selection function)
        ap_mac = self.normalize_mac_address(ap_mac)
        logging.debug(f"Selected and normalized AP MAC: {ap_mac}")
        
        # Band selection
        logging.debug("Prompting for band selection")
        print("\nSelect band:")
        print("  1. 2.4 GHz")
        print("  2. 5 GHz (default)")
        print("  3. 6 GHz")
        band_choice = safe_input("Enter choice [1-3] (default 2): ", default_value="2", context="band")
        
        # Support both menu numbers (1,2,3) and actual band values (24, 5, 6)
        band_map = {
            "1": "24", "2": "5", "3": "6",    # Menu choices
            "24": "24", "5": "5", "6": "6"     # Direct band values
        }
        band = band_map.get(band_choice, "5")
        logging.debug(f"Band selected: {band} (choice: {band_choice})")
        
        # Channel
        logging.debug("Prompting for channel")
        if band == "24":
            channel_str = safe_input("Enter channel (1-11, default 1): ", default_value="1", context="channel")
        elif band == "5":
            channel_str = safe_input("Enter channel (36, 40, 44, 48, 52, 56, 60, 64, 100, 104, 108, 112, 116, 120, 124, 128, 132, 136, 140, 144, default 36): ", 
                                    default_value="36", context="channel")
        else:  # band == "6"
            channel_str = safe_input("Enter channel (1-233, default 1): ", default_value="1", context="channel")
        
        try:
            channel = int(channel_str)
            logging.debug(f"Channel selected: {channel}")
        except ValueError:
            print(f"\n! Invalid channel: {channel_str}")
            logging.error(f"Invalid channel value: {channel_str}")
            return
        
        # Bandwidth
        logging.debug("Prompting for bandwidth")
        print("\nSelect bandwidth:")
        print("  1. 20 MHz")
        print("  2. 40 MHz")
        if band in ["5", "6"]:
            print("  3. 80 MHz")
        if band == "6":
            print("  4. 160 MHz")
        bw_choice = safe_input("Enter choice (default 1): ", default_value="1", context="bandwidth")
        bw_map = {"1": "20", "2": "40", "3": "80", "4": "160"}
        bandwidth = bw_map.get(bw_choice, "20")
        logging.debug(f"Bandwidth selected: {bandwidth} MHz (choice: {bw_choice})")
        
        # Validate bandwidth for band
        if band == "24" and bandwidth not in ["20", "40"]:
            print(f"\n! Invalid bandwidth {bandwidth} for 2.4 GHz band")
            logging.error(f"Invalid bandwidth {bandwidth} for 2.4 GHz band")
            return
        
        # Duration
        logging.debug("Prompting for duration")
        duration_str = safe_input("Enter capture duration in seconds (default 60, min 60, max 86400): ", 
                                 default_value="60", context="duration")
        try:
            duration = int(duration_str)
            if duration < 60 or duration > 86400:
                print(f"\n! Duration must be between 60 and 86400 seconds (API requirement)")
                logging.error(f"Duration out of range: {duration}")
                return
            logging.debug(f"Duration set: {duration} seconds")
        except ValueError:
            print(f"\n! Invalid duration: {duration_str}")
            logging.error(f"Invalid duration value: {duration_str}")
            return
        
        # Number of packets
        logging.debug("Prompting for packet count")
        num_packets_str = safe_input("Enter number of packets (default 1024, max 10000): ", 
                                    default_value="1024", context="num_packets")
        try:
            num_packets = int(num_packets_str)
            if num_packets < 0 or num_packets > 10000:
                print(f"\n! Number of packets must be between 0 and 10000")
                logging.error(f"Packet count out of range: {num_packets}")
                return
            logging.debug(f"Packet count set: {num_packets}")
        except ValueError:
            print(f"\n! Invalid number of packets: {num_packets_str}")
            logging.error(f"Invalid packet count value: {num_packets_str}")
            return
        
        # Format selection
        capture_format = self._get_capture_format_selection()
        
        # Loop mode option
        print("\nLoop Mode:")
        print("  Automatically start a new capture when the current one completes")
        print("  Downloads happen in background while next capture runs")
        loop_mode = safe_input("Enable continuous loop mode? (y/n, default n): ", 
                              default_value="n", context="loop_mode")
        enable_loop = loop_mode.lower() == 'y'
        
        # Build payload
        logging.debug("Building capture payload")
        payload = {
            "type": "scan",
            "ap_mac": ap_mac,
            "band": band,
            "channel": channel,
            "bandwidth": bandwidth,
            "duration": duration,
            "num_packets": num_packets,
            "format": capture_format,
            "max_pkt_len": 1300
        }
        logging.debug(f"Payload constructed: {payload}")
        
        # Display and confirm
        print("\n" + "=" * 80)
        print(" CAPTURE CONFIGURATION SUMMARY")
        print("=" * 80)
        print(f"  Capture Type: Scan Radio")
        print(f"  AP MAC: {ap_mac}")
        print(f"  Band: {band} GHz")
        print(f"  Channel: {channel}")
        print(f"  Bandwidth: {bandwidth} MHz")
        print(f"  Duration: {duration} seconds")
        print(f"  Packets: {num_packets}")
        print(f"  Loop Mode: {'ENABLED (continuous until Ctrl+C)' if enable_loop else 'Disabled (single capture)'}")
        print("=" * 80)
        
        # Prompt user to proceed (Enter to continue, Ctrl+C to cancel)
        logging.debug("Waiting for user confirmation")
        safe_input("\nPress Enter to start capture (Ctrl+C to cancel): ", context="confirmation", allow_empty=True)
        
        # Check for existing captures on this AP
        print(f"\n> Checking for existing captures on AP {ap_mac}...")
        try:
            response = mistapi.api.v1.sites.pcaps.listSitePacketCaptures(
                self.mist_session,
                site_id
            )
            
            if response.status_code == 200:
                existing_captures = response.data or []
                ap_has_capture = any(
                    cap.get('ap_mac', '').replace(':', '').replace('-', '').lower() == ap_mac.replace(':', '').replace('-', '').lower()
                    for cap in existing_captures
                )
                
                if ap_has_capture:
                    print(f"\n! WARNING: This AP already has a capture in progress or recently completed")
                    print(f"  Mist only allows one capture per AP at a time")
                    print(f"  The new capture may fail with 'Recording already in progress'")
                    
                    proceed = safe_input("\nContinue anyway? (y/n, default n): ", 
                                       default_value="n", context="capture_conflict_confirmation").lower()
                    if proceed != 'y':
                        print("\n* Capture cancelled by user")
                        logging.info("User cancelled capture due to existing capture on AP")
                        return
        except Exception as error:
            logging.warning(f"Failed to check for existing captures: {error}")
            # Continue anyway - this is just a courtesy check
        
        logging.info("User confirmed - executing site capture")
        if enable_loop:
            self._execute_site_capture_loop(site_id, payload)
        else:
            self._execute_site_capture(site_id, payload)
    
    def _start_site_scan_capture_all_aps(self, site_id: str):
        """
        Start scan radio packet captures for ALL APs at a site simultaneously.
        
        Args:
            site_id (str): Site UUID
        """
        logging.info(f"Starting multi-AP scan capture for site: {site_id}")
        
        # Get all AP MACs from site
        ap_macs = get_all_ap_macs_from_site(site_id)
        if not ap_macs:
            print("\n! No APs found at site")
            return
        
        print(f"\n* Found {len(ap_macs)} APs at site")
        
        # Check for existing captures at this site
        print(f"  Checking for existing captures...")
        try:
            response = mistapi.api.v1.sites.pcaps.listSitePacketCaptures(
                self.mist_session,
                site_id
            )
            
            if response.status_code == 200:
                existing_captures = response.data or []
                # Silently log existing captures but don't warn user
                if existing_captures:
                    logging.debug(f"{len(existing_captures)} capture(s) already in progress or recently completed")
        except Exception as check_error:
            logging.debug(f"Could not check for existing captures: {check_error}")
        
        print(f"  Preparing to launch {len(ap_macs)} simultaneous captures...")
        
        # Get common capture parameters for all APs
        print("\n" + "-" * 80)
        print(" SCAN RADIO CAPTURE CONFIGURATION (All APs)")
        print("-" * 80)
        
        # Band selection
        print("\nSelect band:")
        print("  1. 2.4 GHz")
        print("  2. 5 GHz (default)")
        print("  3. 6 GHz")
        band_choice = safe_input("Enter choice [1-3] (default 2): ", default_value="2", context="band")
        
        band_map = {"1": "24", "2": "5", "3": "6", "24": "24", "5": "5", "6": "6"}
        band = band_map.get(band_choice, "5")
        
        # Channel
        if band == "24":
            channel_str = safe_input("Enter channel (1-11, default 1): ", default_value="1", context="channel")
        elif band == "5":
            channel_str = safe_input("Enter channel (36-144, default 36): ", default_value="36", context="channel")
        else:  # band == "6"
            channel_str = safe_input("Enter channel (1-233, default 1): ", default_value="1", context="channel")
        
        try:
            channel = int(channel_str)
        except ValueError:
            print(f"\n! Invalid channel: {channel_str}")
            return
        
        # Bandwidth
        print("\nSelect bandwidth:")
        print("  1. 20 MHz")
        print("  2. 40 MHz")
        if band in ["5", "6"]:
            print("  3. 80 MHz")
        if band == "6":
            print("  4. 160 MHz")
        bw_choice = safe_input("Enter choice (default 1): ", default_value="1", context="bandwidth")
        bw_map = {"1": "20", "2": "40", "3": "80", "4": "160"}
        bandwidth = bw_map.get(bw_choice, "20")
        
        # Duration
        duration_str = safe_input("Enter capture duration in seconds (default 60, min 60, max 86400): ", 
                                 default_value="60", context="duration")
        try:
            duration = int(duration_str)
            if duration < 60 or duration > 86400:
                print(f"\n! Duration must be between 60 and 86400 seconds (API requirement)")
                return
        except ValueError:
            print(f"\n! Invalid duration: {duration_str}")
            return
        
        # Number of packets
        num_packets_str = safe_input("Enter number of packets (default 1024, max 10000): ", 
                                    default_value="1024", context="num_packets")
        try:
            num_packets = int(num_packets_str)
            if num_packets < 0 or num_packets > 10000:
                print(f"\n! Number of packets must be between 0 and 10000")
                return
        except ValueError:
            print(f"\n! Invalid number of packets: {num_packets_str}")
            return
        
        # Format selection
        capture_format = self._get_capture_format_selection()
        
        # Display configuration summary
        print("\n" + "=" * 80)
        print(" MULTI-AP CAPTURE CONFIGURATION SUMMARY")
        print("=" * 80)
        print(f"  Capture Type: Scan Radio (All APs)")
        print(f"  Number of APs: {len(ap_macs)}")
        print(f"  Band: {band} GHz")
        print(f"  Channel: {channel}")
        print(f"  Bandwidth: {bandwidth} MHz")
        print(f"  Duration: {duration} seconds")
        print(f"  Packets: {num_packets}")
        print(f"  Format: {capture_format}")
        print("=" * 80)
        
        confirmation = safe_input(f"\nPress Enter to start capture for {len(ap_macs)} APs (Ctrl+C to cancel): ", 
                                 context="confirmation", allow_empty=True)
        
        # Build single payload with aps dictionary for all APs
        print(f"\n> Launching multi-AP capture for {len(ap_macs)} APs with single API call...")
        
        # Build the aps dictionary - each AP uses the same parent configuration
        aps_dict = {}
        for ap_mac in ap_macs:
            normalized_mac = self.normalize_mac_address(ap_mac)
            # Per-AP config inherits from parent, so we can leave empty or specify overrides
            aps_dict[normalized_mac] = {
                "band": band,
                "channel": str(channel),
                "width": str(bandwidth)
            }
        
        # Build single payload with parent config + aps dictionary
        payload = {
            "type": "scan",
            "band": band,
            "channel": channel,
            "bandwidth": bandwidth,
            "duration": duration,
            "num_packets": num_packets,
            "format": capture_format,
            "max_pkt_len": 1300,
            "aps": aps_dict
        }
        
        logging.debug(f"Multi-AP payload constructed for {len(ap_macs)} APs")
        
        try:
            response = mistapi.api.v1.sites.pcaps.startSitePacketCapture(
                self.mist_session,
                site_id,
                payload
            )
            
            if response.status_code == 200:
                result = response.data
                capture_id = result.get('id', 'unknown')
                ap_count = result.get('ap_count', len(ap_macs))
                
                print(f"\n* Multi-AP capture started successfully!")
                print(f"  Capture ID: {capture_id}")
                print(f"  AP Count: {ap_count}")
                print(f"  Format: {capture_format}")
                print(f"  Duration: {duration} seconds")
                print(f"  Expires: {result.get('expiry', 'unknown')}")
                
                logging.info(f"Multi-AP capture started: capture_id={capture_id}, ap_count={ap_count}")
                
                # Export capture details
                self._export_capture_info_to_csv(result, 'site', site_id)
                
                # Handle based on format
                if capture_format == 'pcap':
                    print(f"\n> Waiting for PCAP file to be ready...")
                    print(f"  This may take a few moments after capture completes.")
                    self._wait_and_download_pcap(site_id, capture_id, duration)
                elif capture_format == 'stream':
                    print(f"\n> Stream format selected - subscribe to WebSocket for real-time data")
                    self._subscribe_to_site_capture_stream(site_id, capture_id)
                
            else:
                error_details = response.data if hasattr(response, 'data') else 'Unknown error'
                
                # Check for common errors
                if response.status_code == 400 and isinstance(error_details, dict):
                    detail = error_details.get('detail', '')
                    if 'Recording already in progress' in detail:
                        print(f"\n! Capture(s) already in progress on one or more APs")
                        print(f"  Mist only allows one capture per AP at a time")
                        print(f"  Wait for existing captures to complete or check Mist portal to stop them")
                    else:
                        print(f"\n! Failed to start capture: {response.status_code}")
                        print(f"  Error details: {error_details}")
                else:
                    print(f"\n! Failed to start capture: {response.status_code}")
                    print(f"  Error details: {error_details}")
                
                logging.error(f"Multi-AP capture failed: {response.status_code} - {error_details}")
                
        except Exception as error:
            print(f"\n! Error starting multi-AP capture: {error}")
            logging.error(f"Exception launching multi-AP capture: {error}", exc_info=True)
        
        logging.info(f"Multi-AP scan capture function completed")
    
    def _execute_site_capture(self, site_id: str, payload: dict):
        """
        Execute site-level packet capture via API.
        
        Args:
            site_id (str): Site UUID
            payload (dict): Capture configuration payload
        """
        try:
            print(f"\n> Starting packet capture for site {site_id}...")
            logging.info(f"Initiating site capture with payload: {payload}")
            
            # Call Mist API to start capture
            response = mistapi.api.v1.sites.pcaps.startSitePacketCapture(
                self.mist_session,
                site_id,
                payload
            )
            
            if response.status_code == 200:
                result = response.data
                capture_id = result.get('id', 'unknown')
                capture_format = result.get('format', 'unknown')
                print(f"\n* Capture started successfully!")
                print(f"  Capture ID: {capture_id}")
                print(f"  Format: {capture_format}")
                print(f"  Duration: {result.get('duration', 0)} seconds")
                print(f"  Expires: {result.get('expiry', 'unknown')}")
                
                logging.info(f"Site capture started: capture_id={capture_id}, format={capture_format}")
                
                # Handle based on format
                if capture_format == 'pcap':
                    # PCAP file format - wait for file and download
                    print(f"\n> Waiting for PCAP file to be ready...")
                    print(f"  This may take a few moments after capture completes.")
                    self._wait_and_download_pcap(site_id, capture_id, result.get('duration', 600))
                elif capture_format == 'stream':
                    # Stream format - subscribe to WebSocket
                    self._subscribe_to_site_capture_stream(site_id, capture_id)
                
                # Export capture details to CSV
                self._export_capture_info_to_csv(result, 'site', site_id)
                
            else:
                error_details = response.data if hasattr(response, 'data') else 'No error details available'
                
                # Check for specific "Recording already in progress" error
                if response.status_code == 400 and isinstance(error_details, dict):
                    if 'Recording already in progress' in error_details.get('detail', ''):
                        print(f"\n! Capture already in progress on this AP")
                        print(f"  Only one capture per AP is allowed at a time")
                        print(f"  Wait for the existing capture to complete or check the Mist portal to stop it")
                        logging.error(f"Capture conflict: Recording already in progress on AP")
                        return
                
                print(f"\n! Failed to start capture: {response.status_code}")
                print(f"  Error details: {error_details}")
                logging.error(f"Capture failed: {response.status_code} - {error_details}")
                
        except Exception as error:
            print(f"\n! Error starting capture: {error}")
            logging.error(f"Exception in _execute_site_capture: {error}", exc_info=True)
    
    def _execute_site_capture_loop(self, site_id: str, payload: dict):
        """
        Execute site-level packet captures in continuous loop mode.
        
        New Strategy:
        1. Check API for completed PCAPs (last 24 hours)
        2. Download any we don't already have
        3. Start new capture if minimum time has elapsed
        4. Repeat
        
        Args:
            site_id (str): Site UUID
            payload (dict): Capture configuration payload (reused for each iteration)
        """
        import time
        import os
        from datetime import datetime, timedelta
        
        iteration = 0
        last_capture_time = None
        min_capture_interval = payload.get('duration', 60)  # Minimum time between captures
        download_folder = os.path.join(os.getcwd(), 'data')
        
        print(f"\n{'=' * 80}")
        print(f" CONTINUOUS CAPTURE MODE ACTIVE")
        print(f"{'=' * 80}")
        print(f"  Press Ctrl+C to stop and exit gracefully")
        print(f"  Capture duration: {payload.get('duration', 60)} seconds")
        print(f"  Strategy: Download existing PCAPs, then start new captures")
        print(f"{'=' * 80}\n")
        
        try:
            while True:
                iteration += 1
                loop_start_time = time.time()
                
                print(f"\n{'=' * 60}")
                print(f"Loop Iteration #{iteration}")
                print(f"{'=' * 60}")
                
                # Step 1: Get list of all PCAPs from last 24 hours
                print(f"\n[Step 1/3] Checking for completed PCAPs in last 24 hours...")
                logging.info(f"Loop iteration {iteration}: Fetching PCAP list from API")
                
                try:
                    pcaps_response = mistapi.api.v1.sites.pcaps.listSitePacketCaptures(
                        self.mist_session,
                        site_id,
                        duration='1d',  # Last 24 hours
                        limit=100
                    )
                    
                    if pcaps_response.status_code == 200:
                        pcaps_data = pcaps_response.data
                        
                        # Handle pagination structure
                        if isinstance(pcaps_data, dict) and 'results' in pcaps_data:
                            pcap_list = pcaps_data.get('results', [])
                        else:
                            pcap_list = pcaps_data if isinstance(pcaps_data, list) else []
                        
                        # Filter for completed PCAPs with download URLs
                        completed_pcaps = [
                            pcap for pcap in pcap_list 
                            if pcap.get('pcap_url') and pcap.get('format') == 'pcap'
                        ]
                        
                        print(f"  Found {len(completed_pcaps)} completed PCAP(s) with download URLs")
                        logging.info(f"Loop iteration {iteration}: Found {len(completed_pcaps)} completed PCAPs")
                        
                        # Step 2: Download any PCAPs we don't have yet
                        if completed_pcaps:
                            print(f"\n[Step 2/3] Checking for new PCAPs to download...")
                            downloads_this_round = 0
                            
                            for pcap in completed_pcaps:
                                capture_id = pcap.get('id')
                                pcap_url = pcap.get('pcap_url')
                                
                                # Check if we already have this file
                                expected_filename = f"PacketCapture_{capture_id}.pcap"
                                local_path = os.path.join(download_folder, expected_filename)
                                
                                if os.path.exists(local_path):
                                    logging.debug(f"  Skipping {capture_id} - already downloaded")
                                    continue
                                
                                # Download this PCAP
                                print(f"\n  --> Downloading PCAP: {capture_id}")
                                try:
                                    download_response = requests.get(pcap_url, stream=True)
                                    
                                    if download_response.status_code == 200:
                                        with open(local_path, 'wb') as pcap_file:
                                            for chunk in download_response.iter_content(chunk_size=8192):
                                                pcap_file.write(chunk)
                                        
                                        file_size_mb = os.path.getsize(local_path) / (1024 * 1024)
                                        print(f"      Downloaded: {expected_filename} ({file_size_mb:.2f} MB)")
                                        logging.info(f"Downloaded PCAP {capture_id}: {file_size_mb:.2f} MB")
                                        downloads_this_round += 1
                                    else:
                                        print(f"      Failed to download: HTTP {download_response.status_code}")
                                        logging.error(f"Download failed for {capture_id}: {download_response.status_code}")
                                        
                                except Exception as download_error:
                                    print(f"      Error downloading: {download_error}")
                                    logging.error(f"Download exception for {capture_id}: {download_error}", exc_info=True)
                            
                            if downloads_this_round > 0:
                                print(f"\n  Downloaded {downloads_this_round} new PCAP file(s) this round")
                            else:
                                print(f"\n  No new PCAPs to download (all already exist locally)")
                        else:
                            print(f"\n[Step 2/3] No completed PCAPs available for download")
                    
                    else:
                        print(f"  Warning: Could not fetch PCAP list (HTTP {pcaps_response.status_code})")
                        logging.warning(f"Failed to list PCAPs: {pcaps_response.status_code}")
                        
                except Exception as list_error:
                    print(f"  Error fetching PCAP list: {list_error}")
                    logging.error(f"Exception listing PCAPs: {list_error}", exc_info=True)
                
                # Step 3: Determine if we should start a new capture
                print(f"\n[Step 3/3] Checking if ready to start new capture...")
                
                should_capture = False
                wait_time = 0
                
                if last_capture_time is None:
                    should_capture = True
                    print(f"  First capture of this session - starting now")
                else:
                    elapsed = time.time() - last_capture_time
                    if elapsed >= min_capture_interval:
                        should_capture = True
                        print(f"  {elapsed:.0f}s elapsed since last capture (>= {min_capture_interval}s) - ready")
                    else:
                        wait_time = min_capture_interval - elapsed
                        print(f"  Only {elapsed:.0f}s elapsed - waiting {wait_time:.0f}s more...")
                
                if should_capture:
                    print(f"\n  Starting new packet capture...")
                    logging.info(f"Loop iteration {iteration}: Starting new capture with payload: {payload}")
                    
                    try:
                        response = mistapi.api.v1.sites.pcaps.startSitePacketCapture(
                            self.mist_session,
                            site_id,
                            payload
                        )
                        
                        if response.status_code == 200:
                            result = response.data
                            capture_id = result.get('id', 'unknown')
                            duration = result.get('duration', 600)
                            
                            print(f"  Capture started successfully!")
                            print(f"    Capture ID: {capture_id}")
                            print(f"    Duration: {duration} seconds")
                            
                            logging.info(f"Loop iteration {iteration}: Capture started - ID={capture_id}")
                            last_capture_time = time.time()
                            
                            # Export capture metadata
                            self._export_capture_info_to_csv(result, 'site', site_id)
                            
                        else:
                            error_details = response.data if hasattr(response, 'data') else 'No error details'
                            print(f"  Failed to start capture: HTTP {response.status_code}")
                            print(f"    Error: {error_details}")
                            logging.error(f"Loop iteration {iteration} capture failed: {response.status_code} - {error_details}")
                            
                            # Check for conflict
                            if response.status_code == 400 and isinstance(error_details, dict):
                                if 'Recording already in progress' in error_details.get('detail', ''):
                                    print(f"    Capture conflict detected - will retry next loop")
                                    
                    except Exception as capture_error:
                        print(f"  Error starting capture: {capture_error}")
                        logging.error(f"Exception starting capture: {capture_error}", exc_info=True)
                
                # Calculate sleep time for next iteration
                loop_duration = time.time() - loop_start_time
                
                if wait_time > 0:
                    sleep_time = wait_time
                elif loop_duration < 30:
                    # If loop was very fast, wait at least 30 seconds before next check
                    sleep_time = 30 - loop_duration
                else:
                    sleep_time = 10
                
                print(f"\n{'=' * 60}")
                print(f"Loop iteration #{iteration} complete")
                print(f"Waiting {sleep_time:.0f} seconds before next check...")
                print(f"{'=' * 60}\n")
                
                time.sleep(sleep_time)
                        
        except KeyboardInterrupt:
            print(f"\n\n{'=' * 80}")
            print(f" LOOP MODE INTERRUPTED BY USER")
            print(f"{'=' * 80}")
            print(f"  Completed {iteration} loop iteration(s)")
            print(f"  All available PCAPs have been downloaded")
            print(f"  Exiting gracefully...")
            logging.info(f"Capture loop stopped by user after {iteration} iterations")
        
        except Exception as loop_error:
            print(f"\n! Unexpected error in capture loop: {loop_error}")
            logging.error(f"Exception in capture loop: {loop_error}", exc_info=True)
    
    def _wait_for_capture_completion(self, site_id: str, capture_id: str, expected_duration: int) -> bool:
        """
        Poll for capture completion status (separate from PCAP download availability).
        Returns as soon as capture completes, does not wait for PCAP file URL.
        
        Args:
            site_id (str): Site UUID
            capture_id (str): Capture session ID
            expected_duration (int): Expected capture duration in seconds
            
        Returns:
            bool: True if capture confirmed complete, False if timeout/error
        """
        import time
        
        # Poll more frequently for completion detection
        poll_interval = 3  # Check every 3 seconds
        max_wait = expected_duration + 30  # Duration + 30 second buffer
        max_polls = max_wait // poll_interval
        
        start_time = time.time()
        
        for poll_attempt in range(1, max_polls + 1):
            try:
                elapsed = int(time.time() - start_time)
                
                response = mistapi.api.v1.sites.pcaps.listSitePacketCaptures(
                    self.mist_session,
                    site_id
                )
                
                if response.status_code == 200:
                    raw_data = response.data
                    
                    # Extract results list
                    if isinstance(raw_data, dict) and 'results' in raw_data:
                        captures = raw_data['results']
                    elif isinstance(raw_data, list):
                        captures = raw_data
                    else:
                        logging.warning(f"Completion check: Unexpected data structure")
                        time.sleep(poll_interval)
                        continue
                    
                    # Find our capture
                    for capture in captures:
                        if not isinstance(capture, dict):
                            continue
                        
                        if capture.get('id') == capture_id:
                            # Check if capture is complete
                            # Capture is complete when:
                            # 1. It has been running for at least the expected duration
                            # 2. The 'enabled' field is False (capture stopped)
                            enabled = capture.get('enabled', True)
                            timestamp = capture.get('timestamp', 0)
                            
                            # Check if enough time has passed
                            time_running = time.time() - timestamp if timestamp else elapsed
                            
                            if not enabled:
                                # Capture explicitly stopped
                                logging.debug(f"Capture {capture_id} completed (enabled=False)")
                                return True
                            elif time_running >= expected_duration:
                                # Capture has run for expected duration
                                logging.debug(f"Capture {capture_id} completed (duration reached)")
                                return True
                            else:
                                # Still running
                                remaining = int(expected_duration - time_running)
                                if poll_attempt % 5 == 0:  # Log every 15 seconds
                                    print(f"  ...capture in progress (~{remaining}s remaining)", end='\r')
                                logging.debug(f"Capture {capture_id} still running ({remaining}s remaining)")
                    
                    # Capture not found - might be very new or very old
                    if elapsed < 10:
                        # Give it time to appear in API
                        logging.debug(f"Capture {capture_id} not found yet (elapsed={elapsed}s)")
                    else:
                        logging.warning(f"Capture {capture_id} not found in list (elapsed={elapsed}s)")
                
                time.sleep(poll_interval)
                
            except Exception as poll_error:
                logging.error(f"Completion poll error: {poll_error}", exc_info=True)
                time.sleep(poll_interval)
        
        # Timeout reached
        logging.warning(f"Capture {capture_id} completion check timed out after {max_wait}s")
        return False
    
    def start_org_packet_capture(self):
        """
        Interactive menu for starting org-level packet captures (MxEdge only).
        
        NOTE: Organization-level captures are for Mist Edges only.
        Site-level Mist Edges should use site captures (option 9).
        """
        logging.info("ENTRY: PacketCaptureManager.start_org_packet_capture()")
        
        print("\n" + "=" * 80)
        print(" ORGANIZATION PACKET CAPTURE MANAGER")
        print("=" * 80)
        print("\n! NOTE: Org-level captures are for organization-level Mist Edges ONLY")
        print("  For site-level Mist Edges, use Site Packet Capture (option 9)")
        print("\n" + "=" * 80)
        
        # Fetch list of MxEdges
        print("\n  Fetching available MxEdges...")
        try:
            response = mistapi.api.v1.orgs.mxedges.listOrgMxEdges(
                self.mist_session,
                self.org_id,
                limit=1000
            )
            mxedges = mistapi.get_all(response=response, mist_session=self.mist_session)
            
            if not mxedges:
                print("\n! No MxEdges found for this organization")
                logging.warning("Menu #10: No MxEdges found")
                return
                
        except Exception as error:
            print(f"\n! Error fetching MxEdges: {error}")
            logging.error(f"Menu #10: Failed to fetch MxEdges: {error}")
            return
        
        # Fetch stats to get status information
        print(f"  Fetching MxEdge status information...")
        mxedge_stats_map = {}
        try:
            stats_response = mistapi.api.v1.orgs.stats.listOrgMxEdgesStats(
                self.mist_session,
                self.org_id,
                limit=1000
            )
            stats_data = mistapi.get_all(response=stats_response, mist_session=self.mist_session)
            
            if stats_data:
                for stat in stats_data:
                    mxedge_id = stat.get("id")
                    if mxedge_id:
                        mxedge_stats_map[mxedge_id] = stat
        except Exception as error:
            logging.warning(f"Menu #10: Failed to fetch MxEdge stats: {error}")
            # Continue without status information
        
        # Display indexed list of MxEdges with detailed status
        print(f"\n  Available MxEdges ({len(mxedges)} found):")
        print("=" * 120)
        
        index_to_mxedge = {}
        for index, mxedge in enumerate(mxedges):
            mxedge_name = mxedge.get("name", "Unnamed MxEdge")
            mxedge_id = mxedge.get("id", "No ID")
            model = mxedge.get("model", "Unknown")
            
            # Get detailed stats for this MxEdge
            stat = mxedge_stats_map.get(mxedge_id, {})
            status = stat.get("status", "unknown")
            uptime = stat.get("uptime", 0)
            service_stat = stat.get("service_stat", {})
            
            # Format uptime
            if uptime > 0:
                uptime_days = uptime // 86400
                uptime_hours = (uptime % 86400) // 3600
                uptime_str = f"{uptime_days}d {uptime_hours}h"
            else:
                uptime_str = "N/A"
            
            # Get service states
            mxagent_stat = service_stat.get("mxagent", {})
            tunterm_stat = service_stat.get("tunterm", {})
            
            mxagent_state = mxagent_stat.get("running_state", "Unknown")
            tunterm_state = tunterm_stat.get("running_state", "Unknown")
            
            # Show online/offline status
            if status == "connected":
                status_marker = "ONLINE"
            elif status == "disconnected":
                status_marker = "OFFLINE"
            else:
                status_marker = status.upper()
            
            print(f"  [{index}] {mxedge_name:30} | Model: {model:10} | Status: {status_marker:8} | Uptime: {uptime_str:10}")
            print(f"       mxagent: {mxagent_state:15} | tunterm: {tunterm_state:15}")
            index_to_mxedge[index] = mxedge
        
        # Get user selection (API limitation: only 1 MxEdge allowed for org-level captures)
        print()
        print("  ! API Limitation: Only 1 MxEdge can be captured at a time for organization-level captures")
        try:
            selection_input = safe_input(
                f"Select MxEdge index [0-{len(mxedges)-1}]: ",
                context="mxedge_selection"
            ).strip()
        except (EOFError, KeyboardInterrupt):
            print("\n! Operation cancelled")
            logging.info("Menu #10: User cancelled MxEdge selection")
            return
        
        # Parse selection (single MxEdge only)
        selected_mxedges = []
        try:
            idx = int(selection_input)
            if idx in index_to_mxedge:
                selected_mxedges.append(index_to_mxedge[idx])
            else:
                print(f"\n! Invalid index {idx}. Please select from 0-{len(mxedges)-1}")
                logging.warning(f"Menu #10: Invalid MxEdge index: {idx}")
                return
        except ValueError:
            print(f"\n! Invalid input format. Please enter a single numeric index.")
            logging.warning(f"Menu #10: Invalid selection input: {selection_input}")
            return
        
        if not selected_mxedges:
            print(f"\n! No valid MxEdge selected")
            logging.warning("Menu #10: No valid MxEdge selected")
            return
        
        print(f"\n  Selected MxEdge:")
        for mxedge in selected_mxedges:
            print(f"    -> {mxedge.get('name', 'Unnamed')} (ID: {mxedge.get('id')})")
        
        # Fetch and display interface status for selected MxEdges with indexed selection
        print(f"\n  Fetching interface status for selected MxEdge(s)...")
        mxedge_interfaces = {}
        all_ports_by_mxedge = {}
        
        for mxedge in selected_mxedges:
            mxedge_id = mxedge.get("id")
            mxedge_name = mxedge.get("name", "Unnamed MxEdge")
            
            try:
                stats_response = mistapi.api.v1.orgs.stats.getOrgMxEdgeStats(
                    self.mist_session,
                    self.org_id,
                    mxedge_id
                )
                
                if stats_response.status_code == 200:
                    stats_data = stats_response.data if hasattr(stats_response, 'data') else {}
                    port_stat = stats_data.get('port_stat', {})
                    
                    if port_stat:
                        mxedge_interfaces[mxedge_id] = {
                            'name': mxedge_name,
                            'ports': port_stat
                        }
                        
                        # Build indexed port list for this MxEdge
                        port_list = []
                        print(f"\n  {mxedge_name} - Available Interfaces:")
                        print(f"  {'-' * 70}")
                        for port_index, (port_name, port_info) in enumerate(sorted(port_stat.items())):
                            status = "UP" if port_info.get('up', False) else "DOWN"
                            speed = port_info.get('speed', 0)
                            speed_str = f"{speed}Mbps" if speed else "N/A"
                            mac = port_info.get('mac', 'N/A')
                            print(f"    [{port_index}] {port_name:10} Status: {status:5} Speed: {speed_str:10} MAC: {mac}")
                            port_list.append(port_name)
                        
                        all_ports_by_mxedge[mxedge_id] = {
                            'name': mxedge_name,
                            'ports': port_list
                        }
                    else:
                        print(f"\n  {mxedge_name} - No interface stats available")
                        mxedge_interfaces[mxedge_id] = {'name': mxedge_name, 'ports': {}}
                        all_ports_by_mxedge[mxedge_id] = {'name': mxedge_name, 'ports': []}
                else:
                    print(f"\n  {mxedge_name} - Failed to fetch stats (HTTP {stats_response.status_code})")
                    mxedge_interfaces[mxedge_id] = {'name': mxedge_name, 'ports': {}}
                    all_ports_by_mxedge[mxedge_id] = {'name': mxedge_name, 'ports': []}
                    
            except Exception as error:
                print(f"\n  {mxedge_name} - Error fetching stats: {error}")
                logging.error(f"Menu #10: Failed to fetch stats for {mxedge_name}: {error}")
                mxedge_interfaces[mxedge_id] = {'name': mxedge_name, 'ports': {}}
                all_ports_by_mxedge[mxedge_id] = {'name': mxedge_name, 'ports': []}
        
        # Port selection using indices (API limitation: only 1 port allowed)
        print(f"\n  Port Selection:")
        print(f"  ! API Limitation: Only 1 port can be captured at a time")
        
        selected_ports_by_mxedge = {}
        for mxedge_id, port_info in all_ports_by_mxedge.items():
            mxedge_name = port_info['name']
            port_list = port_info['ports']
            
            if not port_list:
                print(f"\n  {mxedge_name}: No ports available, skipping...")
                selected_ports_by_mxedge[mxedge_id] = []
                continue
            
            try:
                port_input = safe_input(
                    f"\n  {mxedge_name} - Select a single port index [0-{len(port_list)-1}]: ",
                    context=f"port_selection_{mxedge_id}"
                ).strip()
            except (EOFError, KeyboardInterrupt):
                print("\n! Operation cancelled")
                logging.info("Menu #10: User cancelled port selection")
                return
            
            if not port_input:
                print(f"\n! Port selection is required. Please select a port index.")
                logging.warning("Menu #10: No port selected")
                return
            else:
                # Parse single port index
                try:
                    idx = int(port_input)
                    if 0 <= idx < len(port_list):
                        selected_port = port_list[idx]
                        selected_ports_by_mxedge[mxedge_id] = [selected_port]
                        print(f"    -> Selected port: {selected_port}")
                    else:
                        print(f"\n! Invalid index {idx} (valid range: 0-{len(port_list)-1})")
                        logging.warning(f"Menu #10: Invalid port index: {idx}")
                        return
                except ValueError:
                    print(f"\n! Invalid input format. Please enter a single numeric index.")
                    logging.warning(f"Menu #10: Invalid port input: {port_input}")
                    return
        
        # Tcpdump filter selection
        tcpdump_expr = self._get_tcpdump_expression_selection()
        
        # Duration
        duration_str = safe_input("\nEnter capture duration in seconds (default 30, max 86400): ", 
                                 default_value="30", context="duration")
        try:
            duration = int(duration_str)
            if duration < 30 or duration > 86400:
                print(f"\n! Duration must be between 30 and 86400 seconds")
                return
        except ValueError:
            print(f"\n! Invalid duration: {duration_str}")
            return
        
        # Number of packets
        num_packets_str = safe_input("Enter number of packets (default 1024, max 10000, 0 for unlimited): ", 
                                    default_value="1024", context="num_packets")
        try:
            num_packets = int(num_packets_str)
            if num_packets < 0 or num_packets > 10000:
                print(f"\n! Number of packets must be between 0 and 10000")
                return
        except ValueError:
            print(f"\n! Invalid number of packets: {num_packets_str}")
            return
        
        # Max packet length
        max_pkt_len_str = safe_input("Enter max packet length in bytes (default 128, max 2048): ", 
                                    default_value="128", context="max_pkt_len")
        try:
            max_pkt_len = int(max_pkt_len_str)
            if max_pkt_len < 64 or max_pkt_len > 2048:
                print(f"\n! Max packet length must be between 64 and 2048 bytes")
                return
        except ValueError:
            print(f"\n! Invalid max packet length: {max_pkt_len_str}")
            return
        
        # Format selection (moved to end of prompts)
        print("\nCapture format:")
        print("  1. Stream to Mist Cloud (default)")
        print("  2. TZSP stream to remote host (Wireshark)")
        format_choice = safe_input("Enter choice (default 1): ", default_value="1", context="format")
        
        if format_choice == "2":
            # TZSP configuration
            tzsp_host = safe_input("Enter TZSP host (IP address or hostname): ", context="tzsp_host")
            if not tzsp_host:
                print("\n! TZSP host required")
                return
            
            tzsp_port_str = safe_input("Enter TZSP port (default 37008): ", 
                                      default_value="37008", context="tzsp_port")
            try:
                tzsp_port = int(tzsp_port_str)
                if tzsp_port < 1 or tzsp_port > 65535:
                    print(f"\n! Port must be between 1 and 65535")
                    return
            except ValueError:
                print(f"\n! Invalid port: {tzsp_port_str}")
                return
            
            capture_format = "tzsp"
        else:
            capture_format = "stream"
            tzsp_host = None
            tzsp_port = None
        
        # Build payload for multiple MxEdges with selected ports
        payload = {
            "type": "mxedge",
            "duration": duration,
            "num_packets": num_packets,
            "max_pkt_len": max_pkt_len,
            "format": capture_format,
            "mxedges": {}
        }
        
        # Add tcpdump filter if specified
        if tcpdump_expr:
            payload["tcpdump_expression"] = tcpdump_expr
        
        # Add each selected MxEdge to payload with their selected ports
        for mxedge_id, port_names in selected_ports_by_mxedge.items():
            payload["mxedges"][mxedge_id] = {}
            
            if port_names:
                # Build interfaces structure per API specification
                # API expects: "interfaces": { "port_name": {} }
                payload["mxedges"][mxedge_id]["interfaces"] = {}
                for port_name in port_names:
                    payload["mxedges"][mxedge_id]["interfaces"][port_name] = {}
        
        if capture_format == "tzsp":
            payload["tzsp_host"] = tzsp_host
            payload["tzsp_port"] = tzsp_port
        
        # Display configuration and confirm
        print("\n" + "=" * 80)
        print(" CAPTURE CONFIGURATION SUMMARY")
        print("=" * 80)
        print(f"  Capture Type: MxEdge (Organization Level)")
        print(f"  MxEdge: {selected_mxedges[0].get('name', 'Unnamed')} (ID: {selected_mxedges[0].get('id')})")
        
        mxedge_id = selected_mxedges[0].get('id')
        selected_ports = selected_ports_by_mxedge.get(mxedge_id, [])
        port_str = selected_ports[0] if selected_ports else 'None'
        print(f"  Port: {port_str}")
        
        if tcpdump_expr:
            print(f"  Packet Filter: {tcpdump_expr}")
        else:
            print(f"  Packet Filter: None (all traffic)")
        
        print(f"  Duration: {duration} seconds")
        print(f"  Packets: {num_packets} ({'unlimited' if num_packets == 0 else 'max'})")
        print(f"  Max Packet Length: {max_pkt_len} bytes")
        print(f"  Format: {capture_format}")
        if capture_format == "tzsp":
            print(f"  TZSP Host: {tzsp_host}:{tzsp_port}")
        print("=" * 80)
        
        # Prompt user to proceed (Enter to continue, Ctrl+C to cancel)
        safe_input("\nPress Enter to start capture (Ctrl+C to cancel): ", context="confirmation", allow_empty=True)
        
        # Execute org capture
        self._execute_org_capture(payload)
    
    def _execute_org_capture(self, payload: dict):
        """
        Execute org-level packet capture via API.
        
        Args:
            payload (dict): Capture configuration payload
        """
        try:
            print(f"\n> Starting organization packet capture...")
            logging.info(f"Initiating org capture with payload: {payload}")
            
            # Call Mist API to start capture
            response = mistapi.api.v1.orgs.pcaps.startOrgPacketCapture(
                self.mist_session,
                self.org_id,
                payload
            )
            
            if response.status_code == 200:
                result = response.data
                capture_id = result.get('id', 'unknown')
                print(f"\n* Capture started successfully!")
                print(f"  Capture ID: {capture_id}")
                print(f"  Format: {result.get('format', 'unknown')}")
                print(f"  Duration: {result.get('duration', 0)} seconds")
                print(f"  Expires: {result.get('expiry', 'unknown')}")
                
                logging.info(f"Org capture started: capture_id={capture_id}")
                
                # Handle based on format type
                capture_format = payload.get('format', 'pcap')
                
                if capture_format == 'pcap':
                    # Wait for PCAP file and download it
                    # Note: For org captures, we need the org ID instead of site_id
                    self._wait_and_download_pcap_org(self.org_id, capture_id, result.get('duration', 60))
                elif capture_format == 'stream':
                    # Subscribe to WebSocket for streaming results
                    self._subscribe_to_org_capture_stream(capture_id)
                
                # Export capture details to CSV
                self._export_capture_info_to_csv(result, 'org', self.org_id)
                
            else:
                print(f"\n! Failed to start capture: {response.status_code}")
                error_details = response.data if hasattr(response, 'data') else 'No error details available'
                print(f"  Error details: {error_details}")
                logging.error(f"Capture failed: {response.status_code} - {error_details}")
                
        except Exception as error:
            print(f"\n! Error starting capture: {error}")
            logging.error(f"Exception in _execute_org_capture: {error}", exc_info=True)
    
    def _subscribe_to_site_capture_stream(self, site_id: str, capture_id: str):
        """
        Subscribe to WebSocket stream for site capture results.
        
        Args:
            site_id (str): Site UUID
            capture_id (str): Capture session ID
        """
        try:
            print(f"\n> Subscribing to capture stream...")
            print(f"  Press Ctrl+C to stop monitoring")
            
            # Initialize WebSocket manager if needed
            if not self.websocket_manager:
                self.websocket_manager = WebSocketManager(self.mist_session)
            
            # Connect and subscribe
            if not self.websocket_manager.connected:
                self.websocket_manager.connect()
            
            channel = f"/sites/{site_id}/pcaps"
            self.websocket_manager.subscribe_to_channel(channel)
            
            # Wait for subscription confirmation
            confirmed = self.websocket_manager.wait_for_subscription_confirmation(channel, timeout_seconds=10)
            if confirmed:
                print(f"\n* Subscribed to capture stream")
                print(f"  Capture ID: {capture_id}")
                print(f"  Monitoring for packets...")
                print("-" * 80)
                
                # Monitor for results (simplified - full implementation would parse pcap data)
                packet_count = 0
                start_time = time.time()
                
                try:
                    while True:
                        # Check for messages
                        with self.websocket_manager.results_lock:
                            messages = list(self.websocket_manager.command_results.values())
                        
                        for msg in messages:
                            if msg.get('channel') == channel:
                                data = msg.get('data', {})
                                if data.get('capture_id') == capture_id:
                                    packet_count += 1
                                    if packet_count % 10 == 0:
                                        elapsed = time.time() - start_time
                                        print(f"  Received {packet_count} packets ({elapsed:.1f}s elapsed)")
                                    
                                    # Check for stop message
                                    if data.get('pcap_dict') is None:
                                        print(f"\n* Capture completed: {packet_count} packets received")
                                        return
                        
                        time.sleep(0.1)
                        
                except KeyboardInterrupt:
                    print(f"\n\n! Monitoring stopped by user")
                    print(f"  Total packets received: {packet_count}")
                    
            else:
                print(f"\n! Failed to subscribe to capture stream")
                
        except Exception as error:
            print(f"\n! Error subscribing to stream: {error}")
            logging.error(f"Exception in _subscribe_to_site_capture_stream: {error}", exc_info=True)
    
    def _subscribe_to_org_capture_stream(self, capture_id: str):
        """
        Subscribe to WebSocket stream for org capture results.
        
        Args:
            capture_id (str): Capture session ID
        """
        try:
            print(f"\n> Subscribing to capture stream...")
            print(f"  Press Ctrl+C to stop monitoring")
            
            # Similar to site capture stream but uses org channel
            if not self.websocket_manager:
                self.websocket_manager = WebSocketManager(self.mist_session)
            
            if not self.websocket_manager.connected:
                self.websocket_manager.connect()
            
            channel = f"/orgs/{self.org_id}/pcaps"
            self.websocket_manager.subscribe_to_channel(channel)
            
            confirmed = self.websocket_manager.wait_for_subscription_confirmation(channel, timeout_seconds=10)
            if confirmed:
                print(f"\n* Subscribed to capture stream")
                print(f"  Capture ID: {capture_id}")
                print(f"  Monitoring for packets...")
                print("-" * 80)
                
                packet_count = 0
                start_time = time.time()
                
                try:
                    while True:
                        with self.websocket_manager.results_lock:
                            messages = list(self.websocket_manager.command_results.values())
                        
                        for msg in messages:
                            if msg.get('channel') == channel:
                                data = msg.get('data', {})
                                if data.get('capture_id') == capture_id:
                                    packet_count += 1
                                    if packet_count % 10 == 0:
                                        elapsed = time.time() - start_time
                                        print(f"  Received {packet_count} packets ({elapsed:.1f}s elapsed)")
                                    
                                    if data.get('pcap_dict') is None:
                                        print(f"\n* Capture completed: {packet_count} packets received")
                                        return
                        
                        time.sleep(0.1)
                        
                except KeyboardInterrupt:
                    print(f"\n\n! Monitoring stopped by user")
                    print(f"  Total packets received: {packet_count}")
                    
            else:
                print(f"\n! Failed to subscribe to capture stream")
                
        except Exception as error:
            print(f"\n! Error subscribing to stream: {error}")
            logging.error(f"Exception in _subscribe_to_org_capture_stream: {error}", exc_info=True)
    
    def _wait_and_download_pcap(self, site_id: str, capture_id: str, duration: int):
        """
        Wait for PCAP capture to complete and download the file.
        
        When format='pcap', the Mist cloud saves the capture as a PCAP file
        and provides a download URL via the pcap_url field.
        
        Args:
            site_id (str): Site UUID
            capture_id (str): Capture session ID returned from API
            duration (int): Expected capture duration in seconds
        """
        import time
        import requests
        from pathlib import Path
        
        try:
            print(f"\n* Capture initiated (ID: {capture_id})")
            print(f"  Duration: {duration} seconds (plus processing time)")
            print(f"  Polling for PCAP file availability...")
            print(f"  Press Ctrl+C to cancel wait and check portal manually")
            
            # Poll for the PCAP file availability
            # Start polling immediately - the capture runs on the Mist cloud
            max_wait_time = duration + 120  # Capture duration + 2 minutes buffer
            poll_interval = 5  # Check every 5 seconds
            max_polls = max_wait_time // poll_interval
            pcap_url = None
            start_time = time.time()
            
            for poll_attempt in range(1, max_polls + 1):
                try:
                    elapsed = int(time.time() - start_time)
                    
                    # List captures for this site to find our capture_id
                    logging.debug(f"Poll attempt {poll_attempt}: Querying listSitePacketCaptures for site {site_id}")
                    response = mistapi.api.v1.sites.pcaps.listSitePacketCaptures(
                        self.mist_session,
                        site_id
                    )
                    
                    logging.debug(f"Poll attempt {poll_attempt}: Response status={response.status_code}")
                    
                    if response.status_code == 200:
                        raw_data = response.data
                        logging.debug(f"Poll attempt {poll_attempt}: Received raw data type: {type(raw_data)}")
                        
                        # Handle case where API returns dict with 'results' key
                        if isinstance(raw_data, dict) and 'results' in raw_data:
                            captures = raw_data['results']
                            logging.debug(f"Poll attempt {poll_attempt}: Extracted 'results' key containing {len(captures)} items")
                        elif isinstance(raw_data, list):
                            captures = raw_data
                            logging.debug(f"Poll attempt {poll_attempt}: Data is already a list with {len(captures)} items")
                        else:
                            logging.warning(f"Poll attempt {poll_attempt}: Unexpected data structure: {type(raw_data)}")
                            logging.warning(f"  Raw data: {raw_data}")
                            time.sleep(poll_interval)
                            continue
                        
                        # Log the captures list structure
                        if captures:
                            logging.debug(f"Poll attempt {poll_attempt}: Processing {len(captures)} captures")
                        
                        # Find our capture in the list
                        found_capture = False
                        for capture in captures:
                            # Handle case where capture might be a string or other type
                            if not isinstance(capture, dict):
                                logging.warning(f"Poll attempt {poll_attempt}: Capture is {type(capture)}, not dict: {capture}")
                                continue
                            
                            cap_id = capture.get('id')
                            if cap_id == capture_id:
                                found_capture = True
                                pcap_url = capture.get('pcap_url')
                                
                                # Log all relevant fields from the capture object
                                logging.debug(f"Poll attempt {poll_attempt}: Found our capture {capture_id}")
                                logging.debug(f"  - enabled: {capture.get('enabled')}")
                                logging.debug(f"  - format: {capture.get('format')}")
                                logging.debug(f"  - type: {capture.get('type')}")
                                logging.debug(f"  - ap_count: {capture.get('ap_count')}")
                                logging.debug(f"  - duration: {capture.get('duration')}")
                                logging.debug(f"  - expiry: {capture.get('expiry')}")
                                logging.debug(f"  - timestamp: {capture.get('timestamp')}")
                                logging.debug(f"  - pcap_url: {pcap_url if pcap_url else 'NOT SET YET'}")
                                
                                if pcap_url:
                                    print(f"\r* PCAP file ready for download (after {elapsed}s)                    ")
                                    logging.info(f"PCAP URL available after {elapsed}s: {pcap_url}")
                                    break
                                else:
                                    logging.debug(f"  - Capture found but pcap_url not yet available (still processing)")
                        
                        if not found_capture:
                            logging.debug(f"Poll attempt {poll_attempt}: Our capture {capture_id} not found in list of {len(captures)} captures")
                            if captures:
                                # Safely extract IDs, handling non-dict items
                                capture_ids = [c.get('id') if isinstance(c, dict) else str(c) for c in captures]
                                logging.debug(f"  Available capture IDs: {capture_ids}")
                        
                        if pcap_url:
                            break
                    else:
                        logging.warning(f"Poll attempt {poll_attempt}: API returned status {response.status_code}")
                        error_detail = response.data if hasattr(response, 'data') else 'No details'
                        logging.warning(f"  Error details: {error_detail}")
                    
                    # Continue waiting if not found yet
                    if poll_attempt < max_polls:
                        print(f"  Waiting for PCAP file... {elapsed}s elapsed (checking every {poll_interval}s)    ", end='\r')
                        time.sleep(poll_interval)
                    
                except Exception as poll_error:
                    logging.error(f"Poll attempt {poll_attempt} exception: {poll_error}", exc_info=True)
                    time.sleep(poll_interval)
            
            if not pcap_url:
                elapsed_total = int(time.time() - start_time)
                print(f"\r! PCAP file URL not available after waiting {elapsed_total} seconds                    ")
                print(f"  The capture may still be processing. Check the Mist portal for capture ID: {capture_id}")
                return
            
            # Download the PCAP file
            print(f"\n* Downloading PCAP file...")
            download_response = requests.get(pcap_url, timeout=300)
            
            if download_response.status_code == 200:
                # Save to data directory with sanitized filename
                output_dir = Path("data")
                output_dir.mkdir(exist_ok=True)
                
                output_filename = output_dir / f"PacketCapture_{capture_id}.pcap"
                
                with open(output_filename, 'wb') as pcap_file:
                    pcap_file.write(download_response.content)
                
                file_size_mb = len(download_response.content) / (1024 * 1024)
                print(f"\n* PCAP file downloaded successfully")
                print(f"  Location: {output_filename}")
                print(f"  Size: {file_size_mb:.2f} MB")
                print(f"\n  Open with Wireshark or other PCAP analysis tools")
                
                logging.info(f"PCAP file downloaded: {output_filename} ({file_size_mb:.2f} MB)")
                
            else:
                print(f"\n! Failed to download PCAP file")
                print(f"  HTTP Status: {download_response.status_code}")
                print(f"  You can try downloading manually from: {pcap_url}")
                logging.error(f"PCAP download failed: HTTP {download_response.status_code}")
        
        except KeyboardInterrupt:
            print(f"\n\n! Download cancelled by user")
            print(f"  Capture ID: {capture_id}")
            if pcap_url:
                print(f"  Download manually from: {pcap_url}")
        
        except Exception as error:
            print(f"\n! Error downloading PCAP file: {error}")
            logging.error(f"Exception in _wait_and_download_pcap: {error}", exc_info=True)
            if pcap_url:
                print(f"  Try downloading manually from: {pcap_url}")
    
    def _wait_and_download_pcap_org(self, org_id: str, capture_id: str, duration: int):
        """
        Wait for org-level PCAP capture to complete and download the file.
        
        When format='pcap', the Mist cloud saves the capture as a PCAP file
        and provides a download URL via the pcap_url field.
        
        Args:
            org_id (str): Organization UUID
            capture_id (str): Capture session ID returned from API
            duration (int): Expected capture duration in seconds
        """
        import time
        import requests
        from pathlib import Path
        
        try:
            print(f"\n* Capture initiated (ID: {capture_id})")
            print(f"  Duration: {duration} seconds (plus processing time)")
            print(f"  Polling for PCAP file availability...")
            print(f"  Press Ctrl+C to cancel wait and check portal manually")
            
            # Poll for the PCAP file availability
            # Start polling immediately - the capture runs on the Mist cloud
            max_wait_time = duration + 120  # Capture duration + 2 minutes buffer
            poll_interval = 5  # Check every 5 seconds
            max_polls = max_wait_time // poll_interval
            pcap_url = None
            start_time = time.time()
            
            for poll_attempt in range(1, max_polls + 1):
                try:
                    elapsed = int(time.time() - start_time)
                    
                    # List captures for this org to find our capture_id
                    logging.debug(f"Poll attempt {poll_attempt}: Querying listOrgPacketCaptures for org {org_id}")
                    response = mistapi.api.v1.orgs.pcaps.listOrgPacketCaptures(
                        self.mist_session,
                        org_id
                    )
                    
                    logging.debug(f"Poll attempt {poll_attempt}: Response status={response.status_code}")
                    
                    if response.status_code == 200:
                        raw_data = response.data
                        logging.debug(f"Poll attempt {poll_attempt}: Received raw data type: {type(raw_data)}")
                        
                        # Handle case where API returns dict with 'results' key
                        if isinstance(raw_data, dict) and 'results' in raw_data:
                            captures = raw_data['results']
                            logging.debug(f"Poll attempt {poll_attempt}: Extracted 'results' key containing {len(captures)} items")
                        elif isinstance(raw_data, list):
                            captures = raw_data
                            logging.debug(f"Poll attempt {poll_attempt}: Data is already a list with {len(captures)} items")
                        else:
                            logging.warning(f"Poll attempt {poll_attempt}: Unexpected data structure: {type(raw_data)}")
                            logging.warning(f"  Raw data: {raw_data}")
                            time.sleep(poll_interval)
                            continue
                        
                        # Log the captures list structure
                        if captures:
                            logging.debug(f"Poll attempt {poll_attempt}: Processing {len(captures)} captures")
                        
                        # Find our capture in the list
                        found_capture = False
                        for capture in captures:
                            # Handle case where capture might be a string or other type
                            if not isinstance(capture, dict):
                                logging.warning(f"Poll attempt {poll_attempt}: Capture is {type(capture)}, not dict: {capture}")
                                continue
                            
                            cap_id = capture.get('id')
                            if cap_id == capture_id:
                                found_capture = True
                                pcap_url = capture.get('pcap_url')
                                
                                # Log all relevant fields from the capture object
                                logging.debug(f"Poll attempt {poll_attempt}: Found our capture {capture_id}")
                                logging.debug(f"  - enabled: {capture.get('enabled')}")
                                logging.debug(f"  - format: {capture.get('format')}")
                                logging.debug(f"  - type: {capture.get('type')}")
                                logging.debug(f"  - duration: {capture.get('duration')}")
                                logging.debug(f"  - expiry: {capture.get('expiry')}")
                                logging.debug(f"  - timestamp: {capture.get('timestamp')}")
                                logging.debug(f"  - pcap_url: {pcap_url if pcap_url else 'NOT SET YET'}")
                                
                                if pcap_url:
                                    print(f"\r* PCAP file ready for download (after {elapsed}s)                    ")
                                    logging.info(f"PCAP URL available after {elapsed}s: {pcap_url}")
                                    break
                                else:
                                    logging.debug(f"  - Capture found but pcap_url not yet available (still processing)")
                        
                        if not found_capture:
                            logging.debug(f"Poll attempt {poll_attempt}: Our capture {capture_id} not found in list of {len(captures)} captures")
                            if captures:
                                # Safely extract IDs, handling non-dict items
                                capture_ids = [c.get('id') if isinstance(c, dict) else str(c) for c in captures]
                                logging.debug(f"  Available capture IDs: {capture_ids}")
                        
                        if pcap_url:
                            break
                    else:
                        logging.warning(f"Poll attempt {poll_attempt}: API returned status {response.status_code}")
                        error_detail = response.data if hasattr(response, 'data') else 'No details'
                        logging.warning(f"  Error details: {error_detail}")
                    
                    # Continue waiting if not found yet
                    if poll_attempt < max_polls:
                        print(f"  Waiting for PCAP file... {elapsed}s elapsed (checking every {poll_interval}s)    ", end='\r')
                        time.sleep(poll_interval)
                    
                except Exception as poll_error:
                    logging.error(f"Poll attempt {poll_attempt} exception: {poll_error}", exc_info=True)
                    time.sleep(poll_interval)
            
            if not pcap_url:
                elapsed_total = int(time.time() - start_time)
                print(f"\r! PCAP file URL not available after waiting {elapsed_total} seconds                    ")
                print(f"  The capture may still be processing. Check the Mist portal for capture ID: {capture_id}")
                return
            
            # Download the PCAP file
            print(f"\n* Downloading PCAP file...")
            download_response = requests.get(pcap_url, timeout=300)
            
            if download_response.status_code == 200:
                # Save to data directory with sanitized filename
                output_dir = Path("data")
                output_dir.mkdir(exist_ok=True)
                
                output_filename = output_dir / f"PacketCapture_org_{capture_id}.pcap"
                
                with open(output_filename, 'wb') as pcap_file:
                    pcap_file.write(download_response.content)
                
                file_size_mb = len(download_response.content) / (1024 * 1024)
                print(f"\n* PCAP file downloaded successfully")
                print(f"  Location: {output_filename}")
                print(f"  Size: {file_size_mb:.2f} MB")
                print(f"\n  Open with Wireshark or other PCAP analysis tools")
                
                logging.info(f"Org PCAP file downloaded: {output_filename} ({file_size_mb:.2f} MB)")
                
            else:
                print(f"\n! Failed to download PCAP file")
                print(f"  HTTP Status: {download_response.status_code}")
                print(f"  You can try downloading manually from: {pcap_url}")
                logging.error(f"Org PCAP download failed: HTTP {download_response.status_code}")
        
        except KeyboardInterrupt:
            print(f"\n\n! Download cancelled by user")
            print(f"  Capture ID: {capture_id}")
            if pcap_url:
                print(f"  Download manually from: {pcap_url}")
        
        except Exception as error:
            print(f"\n! Error downloading PCAP file: {error}")
            logging.error(f"Exception in _wait_and_download_pcap_org: {error}", exc_info=True)
            if pcap_url:
                print(f"  Try downloading manually from: {pcap_url}")
    
    def _export_capture_info_to_csv(self, capture_data: dict, scope: str, scope_id: str):
        """
        Export capture session information to CSV.
        
        Args:
            capture_data (dict): Capture response from API
            scope (str): 'site' or 'org'
            scope_id (str): Site or org UUID
        """
        try:
            filename = f"PacketCapture_{scope}_{capture_data.get('id', 'unknown')}.csv"
            
            # Add scope context
            export_data = {
                'scope': scope,
                'scope_id': scope_id,
                **capture_data
            }
            
            write_data_with_format_selection(
                [export_data],
                filename,
                api_function_name='startSitePacketCapture' if scope == 'site' else 'startOrgPacketCapture'
            )
            
            print(f"\n* Capture info exported to: {filename}")
            logging.info(f"Capture info exported to {filename}")
            
        except Exception as error:
            logging.error(f"Failed to export capture info: {error}", exc_info=True)


class SFPTransceiverDataProcessor:
    """Process and correlate SFP / transceiver data with site & device context.

    RATIONALE:
        This logic was previously a standalone function (`process_and_merge_csv_for_sfp_address`).
        It is only invoked by menu option 77 and has no tight coupling with most runtime state.
        Encapsulating it in a class improves hierarchy and opens the door for future extensions
        (e.g., JSON export, filtering, unit tests) without growing the monolithic global scope.

    SECURITY:
        Operates only on locally generated CSV artifacts inside the controlled `data/` directory.
        No external network or credential usage. Filenames are static and not user-injected.
    """

    OUTPUT_FILENAME = 'MergedTransceiverData.csv'

    @staticmethod
    def merge_transceiver_data():
        """Generate a merged transceiver CSV linking port optics to site + device context.

        Steps:
            1. Ensure prerequisite CSVs exist (generate if missing):
               - OrgDevicePortStats.csv
               - AllDevicesWithSiteInfo.csv
            2. Load device/site context keyed by MAC.
            3. Filter port stats to rows containing a non-empty transceiver model.
            4. Write merged result to `MergedTransceiverData.csv` via DataExporter.
        """
        logging.debug("ENTRY: SFPTransceiverDataProcessor.merge_transceiver_data()")

        org_port_stats_path = get_csv_file_path('OrgDevicePortStats.csv')
        devices_with_site_info_path = get_csv_file_path('AllDevicesWithSiteInfo.csv')

        # Generate prerequisites if absent (idempotent behavior matches prior function)
        if not os.path.exists(org_port_stats_path):
            print("* OrgDevicePortStats.csv not found. Generating it now...")
            logging.info("OrgDevicePortStats.csv missing; invoking export_device_port_stats_to_csv()")
            export_device_port_stats_to_csv()

        if not os.path.exists(devices_with_site_info_path):
            print("* AllDevicesWithSiteInfo.csv not found. Generating it now...")
            logging.info("AllDevicesWithSiteInfo.csv missing; invoking export_devices_with_site_info_to_csv()")
            export_devices_with_site_info_to_csv()

        try:
            # Load context keyed by MAC
            logging.debug(f"File I/O: Reading {devices_with_site_info_path}")
            with open(devices_with_site_info_path, mode='r', encoding='utf-8') as file:
                reader = csv.DictReader(file)
                site_info = {
                    row['mac']: {
                        'site_name': row.get('site_name', ''),
                        'site_address': row.get('site_address', ''),
                        'device_name': row.get('name', '')
                    } for row in reader
                }
            logging.info(f"Loaded {len(site_info)} device entries from {devices_with_site_info_path}")

            merged_data = []
            total_rows = 0
            candidate_rows = 0  # rows having a non-empty transceiver model (may or may not map to a known device MAC)
            matched_rows = 0    # rows contributing to merged output
            unique_devices_with_transceivers: set[str] = set()

            logging.debug(f"File I/O: Reading {org_port_stats_path}")
            with open(org_port_stats_path, mode='r', encoding='utf-8') as file:
                reader = csv.DictReader(file)
                for row in reader:
                    total_rows += 1
                    mac = row.get('mac')
                    transceiver_model = row.get('xcvr_model', '').strip()

                    if transceiver_model:
                        candidate_rows += 1

                    if mac in site_info and transceiver_model:
                        matched_rows += 1
                        unique_devices_with_transceivers.add(mac)
                        merged_data.append({
                            'site_name': site_info[mac]['site_name'],
                            'site_address': site_info[mac]['site_address'],
                            'device_name': site_info[mac]['device_name'],
                            'port_id': row.get('port_id', ''),
                            'transceiver_part_number': row.get('xcvr_part_number', ''),
                            'transceiver_model': transceiver_model,
                            'transceiver_serial_number': row.get('xcvr_serial', '')
                        })

            if matched_rows == 0:
                # Downgraded severity explanation lives here; DataExporter currently emits a WARNING when given 0 rows.
                logging.info(
                    "Processed port stats; no matching transceivers found. total_rows=%d candidate_rows=%d known_devices=%d. "
                    "This can be normal if the inventory currently has no optics populated.",
                    total_rows, candidate_rows, len(site_info)
                )
            else:
                logging.info(
                    "Processed port stats; %d ports with transceivers found (total_rows=%d candidate_rows=%d unique_devices=%d)",
                    matched_rows, total_rows, candidate_rows, len(unique_devices_with_transceivers)
                )

            DataExporter.save_data_to_output(merged_data, SFPTransceiverDataProcessor.OUTPUT_FILENAME)
            logging.info(f"Wrote {len(merged_data)} rows to {SFPTransceiverDataProcessor.OUTPUT_FILENAME}")
            print(f"! Merged data written to {SFPTransceiverDataProcessor.OUTPUT_FILENAME}")
            logging.debug("EXIT: SFPTransceiverDataProcessor.merge_transceiver_data - success")
        except FileNotFoundError as e:
            logging.error(f"File I/O: Required CSV file not found: {e}")
            logging.debug("EXIT: SFPTransceiverDataProcessor.merge_transceiver_data - file not found")
            raise
        except csv.Error as e:
            logging.error(f"File I/O: CSV processing error: {e}")
            logging.debug("EXIT: SFPTransceiverDataProcessor.merge_transceiver_data - CSV error")
            raise
        except Exception as e:
            logging.error(f"File I/O: Unexpected error during transceiver merge: {e}")
            logging.debug("EXIT: SFPTransceiverDataProcessor.merge_transceiver_data - unexpected error")
            raise


    # NOTE: Legacy function name `process_and_merge_csv_for_sfp_address` removed; menu now invokes class method directly.

def get_csv_file_path(filename: str) -> str:
    """
    Helper function to ensure consistent CSV file paths in the data directory.
    
    Args:
        filename (str): The CSV filename (with or without path)
    
    Returns:
        str: Full path to the CSV file in the data directory
    """
    # Ensure data directory exists
    data_dir = "data"
    os.makedirs(data_dir, exist_ok=True)
    
    # If filename already includes a path, use it as-is
    if os.path.dirname(filename):
        return filename
    
    # Otherwise, place it in the data directory
    return os.path.join(data_dir, filename)

def is_running_in_container() -> bool:
    """Determine if execution appears to be inside a container.

    Detection strategy is deliberately multi-factor and conservative. A positive
    result enables continuous interactive looping behavior. False negatives can
    cause the menu to exit after one operation (observed issue when attaching
    via SSH inside the container with a different runtime user name).

    Order of checks (first positive returns immediately):
      1. Explicit override environment variables:
         - MISTHELPER_FORCE_CONTAINER_LOOP
         - MISTHELPER_CONTAINER
         Any of: '1','true','yes','on' (case-insensitive)
      2. Standard /.dockerenv sentinel file
      3. Well-known container environment variables
      4. cgroup markers
      5. Runtime user name 'misthelper'
      6. /app path detection with sshd presence

    SECURITY: Only boolean enabling of loop behavior; no privileged actions.
    """
    try:
        true_values = {"1", "true", "yes", "on"}
        # Explicit operator override (most reliable and fastest)
        for explicit_var in ("MISTHELPER_FORCE_CONTAINER_LOOP", "MISTHELPER_CONTAINER"):
            value = os.environ.get(explicit_var, "").strip().lower()
            if value in true_values:
                logging.debug(f"Container detection: override via {explicit_var}={value}")
                return True

        # /.dockerenv sentinel
        if os.path.exists('/.dockerenv'):
            logging.debug("Container detection: /.dockerenv present")
            return True

        container_env_vars = [
            'CONTAINER',
            'DOCKER_CONTAINER',
            'PODMAN_CONTAINER',
            'KUBERNETES_SERVICE_HOST',
            'CONTAINERD_NAMESPACE'
        ]
        for env_var in container_env_vars:
            if os.environ.get(env_var):
                logging.debug(f"Container detection: environment variable {env_var} present")
                return True

        # cgroup heuristic
        try:
            with open('/proc/1/cgroup', 'r', encoding='utf-8', errors='ignore') as cgroup_file:
                cgroup_content = cgroup_file.read().lower()
                for indicator in ('docker', 'containerd', 'podman', 'lxc'):
                    if indicator in cgroup_content:
                        logging.debug(f"Container detection: cgroup indicator '{indicator}' found")
                        return True
        except (FileNotFoundError, PermissionError):
            # Not Linux or insufficient permissions; ignore silently
            pass

        # Runtime user name heuristic
        try:
            import pwd  # Unix only
            current_user_name = pwd.getpwuid(os.getuid()).pw_name
            if current_user_name == 'misthelper':
                logging.debug("Container detection: running as user 'misthelper'")
                return True
        except Exception:
            # Non-Unix or lookup failure; treat as non-container for this heuristic step
            pass

        # Heuristic: application installed in canonical container path /app and script present
        try:
            this_file_dir = os.path.abspath(os.path.dirname(__file__))
            if this_file_dir.startswith('/app') and os.path.exists('/app/MistHelper.py'):
                # Additional guard: presence of sshd in typical container location indicates container packaging
                if os.path.exists('/usr/sbin/sshd'):
                    logging.debug("Container detection: /app path with MistHelper.py and sshd present")
                    return True
        except Exception:
            pass
    except Exception as container_detection_error:
        logging.debug(f"Container detection failed with exception: {container_detection_error}")
    
    # If we reach here, no container indicators were found
    logging.debug("Container detection: no container indicators found - running in direct mode")
    return False

def validate_site_id(site_id: Optional[str], function_name: str = "unknown") -> bool:
    """
    Validates that site_id is not None or empty before making API calls.
    
    Args:
        site_id: The site ID to validate
        function_name: Name of the calling function for logging
    
    Returns:
        bool: True if valid, False otherwise
    
    Raises:
        ValueError: If site_id is None or empty
    """
    if site_id is None:
        error_msg = f"! site_id is None in {function_name}. Cannot make API call."
        logging.error(error_msg)
        raise ValueError(error_msg)
    
    if isinstance(site_id, str) and site_id.strip() == "":
        error_msg = f"! site_id is empty string in {function_name}. Cannot make API call."
        logging.error(error_msg)
        raise ValueError(error_msg)
    
    return True

def validate_device_id(device_id: Optional[str], function_name: str = "unknown") -> bool:
    """
    Validates that device_id is not None or empty before making API calls.
    
    Args:
        device_id: The device ID to validate
        function_name: Name of the calling function for logging
    
    Returns:
        bool: True if valid, False otherwise
    
    Raises:
        ValueError: If device_id is None or empty
    """
    if device_id is None:
        error_msg = f"! device_id is None in {function_name}. Cannot make API call."
        logging.error(error_msg)
        raise ValueError(error_msg)
    
    if isinstance(device_id, str) and device_id.strip() == "":
        error_msg = f"! device_id is empty string in {function_name}. Cannot make API call."
        logging.error(error_msg)
        raise ValueError(error_msg)
    
    return True

def create_missing_csv_template(filename: str, headers: Optional[List[str]] = None, sample_data: Optional[List[List[str]]] = None) -> str:
    """
    Creates a basic CSV file placeholder in the correct location.
    
    Args:
        filename (str): Name of the CSV file to create
        headers (list): List of header names (optional)
        sample_data (list): Optional list of sample data rows (optional)
    
    Returns:
        str: Full path to the created file
    """
    file_path = get_csv_file_path(filename)
    
    try:
        # Just create an empty file in the correct location
        with open(file_path, 'w', newline='', encoding='utf-8') as f:
            if headers:
                writer = csv.writer(f)
                writer.writerow(headers)
            # Don't write sample data - user will add their own content
        
        logging.info(f"Created template file: {file_path}")
        return file_path
    except Exception as e:
        logging.error(f"Failed to create template file {filename}: {e}")
        raise

def safe_api_call(api_function: Any, *args: Any, **kwargs: Any) -> Tuple[bool, Any, str]:
    """
    Safely calls an API function and handles common error conditions.
    
    Args:
        api_function: The API function to call
        *args: Arguments to pass to the API function
        **kwargs: Keyword arguments to pass to the API function
    
    Returns:
        tuple: (success: bool, data: any, error: str)
    """
    try:
        response = api_function(*args, **kwargs)
        
        if not hasattr(response, 'data'):
            return False, None, "Response has no data attribute"
        
        if response.data is None:
            return False, None, "Response data is None"
        
        return True, response.data, None
        
    except Exception as e:
        error_str = str(e)
        if "404" in error_str:
            return False, None, f"Endpoint not found (404): {error_str}"
        elif "403" in error_str:
            return False, None, f"Access denied (403): {error_str}"
        elif "429" in error_str:
            return False, None, f"Rate limited (429): {error_str}"
        else:
            return False, None, f"API error: {error_str}"

def get_csv_file_path(filename):
    """
    Helper function to ensure consistent CSV file paths in the data directory.
    
    Args:
        filename (str): The CSV filename (with or without path)
    
    Returns:
        str: Full path to the CSV file in the data directory
    """
    # Ensure data directory exists
    data_dir = "data"
    os.makedirs(data_dir, exist_ok=True)
    
    # If filename already includes a path, use it as-is
    if os.path.dirname(filename):
        return filename
    
    # Otherwise, place it in the data directory
    return os.path.join(data_dir, filename)

def get_cached_or_prompted_org_id() -> str:
    """
    Get organization ID from various sources in order of preference:
    1. Global variable
    2. Environment variable
    3. .env file
    4. Interactive prompt
    """
    global org_id
    # 1. Check global variable
    if org_id:
        logging.info(f"! Using org_id from global variable: {org_id}")
        return org_id
    # 2. Check environment variable (set by dotenv or OS)
    org_id_env = os.environ.get("org_id") or os.environ.get("ORG_ID")
    if org_id_env:
        org_id = org_id_env
        logging.info(f"! Loaded org_id from environment: {org_id}")
        return org_id
    # 3. Fallback: Try to load from .env manually (rarely needed)
    try:
        with open(".env", "r") as f:
            for line in f:
                if line.strip().startswith("org_id="):
                    org_id = line.strip().split("=", 1)[1].strip().strip('"')
        if org_id:
            logging.info(f"! Loaded org_id from .env: {org_id}")
            return org_id
    except FileNotFoundError:
        logging.warning("! .env file not found.")
    # 4. Prompt if still not set
    logging.info("* No org_id found in .env or CLI. Prompting user...")
    org_id_list = mistapi.cli.select_org(apisession)
    org_id = org_id_list[0]
    return org_id

def fetch_organization_services() -> List[Dict[str, Any]]:
    """
    Fetch all services defined at the organization level using the Mist API.
    
    Returns:
        list: List of service dictionaries with service definitions, or empty list if error
        
    SECURITY: Read-only operation fetching configuration data only.
    """
    try:
        org_id = get_cached_or_prompted_org_id()
        logging.info(f"Fetching organization services for org_id: {org_id}")
        
        # Call the Mist API to get organization services
        response = mistapi.api.v1.orgs.services.listOrgServices(apisession, org_id, limit=1000)
        
        if hasattr(response, 'data') and response.data:
            services_data = response.data
            logging.info(f"Successfully retrieved {len(services_data)} organization services")
            
            # Extract service names and types for easier display
            services_list = []
            for service in services_data:
                if isinstance(service, dict):
                    service_name = service.get('name', 'unnamed')
                    service_type = service.get('type', 'custom')
                    service_desc = service.get('description', '')
                    services_list.append({
                        'name': service_name,
                        'type': service_type,
                        'description': service_desc,
                        'full_config': service  # Keep full config for reference
                    })
                    
            return services_list
            
        else:
            logging.warning("No organization services found or response data is empty")
            return []
            
    except Exception as error:
        logging.error(f"Failed to fetch organization services: {error}")
        return []

def fetch_organization_tenants() -> List[str]:
    """
    Fetch all tenants defined in organization networks using the Mist API.
    
    Returns:
        list: List of tenant names found in organization networks, or empty list if error
        
    SECURITY: Read-only operation fetching configuration data only.
    """
    try:
        org_id = get_cached_or_prompted_org_id()
        logging.info(f"Fetching organization networks for tenant information from org_id: {org_id}")
        
        # Call the Mist API to get organization networks which contain tenant definitions
        response = mistapi.api.v1.orgs.networks.listOrgNetworks(apisession, org_id, limit=1000)
        
        if hasattr(response, 'data') and response.data:
            networks_data = response.data
            logging.info(f"Successfully retrieved {len(networks_data)} organization networks")
            
            # Extract tenant names from all networks
            tenant_names = set()  # Use set to avoid duplicates
            for network in networks_data:
                if isinstance(network, dict):
                    # The network name itself is a tenant for service ping
                    network_name = network.get('name')
                    if network_name and isinstance(network_name, str):
                        tenant_names.add(network_name)
                        logging.debug(f"Found network tenant '{network_name}'")
                    
                    # Also check for any explicit tenants within the network
                    if 'tenants' in network:
                        tenants_dict = network.get('tenants', {})
                        if isinstance(tenants_dict, dict):
                            # Each key in the tenants dict is also a tenant name
                            for tenant_name in tenants_dict.keys():
                                if tenant_name and isinstance(tenant_name, str):
                                    tenant_names.add(tenant_name)
                                    logging.debug(f"Found explicit tenant '{tenant_name}' in network '{network.get('name', 'unnamed')}'")
            
            tenant_list = sorted(list(tenant_names))  # Convert to sorted list
            logging.info(f"Found {len(tenant_list)} unique tenants across organization networks: {tenant_list}")
            return tenant_list
            
        else:
            logging.warning("No organization networks found or response data is empty")
            return []
            
    except Exception as error:
        logging.error(f"Error fetching organization tenants from networks: {error}")
        return []

def fetch_site_tenants(site_id: str) -> List[str]:
    """
    Fetch all tenants defined in site-level derived networks using the Mist API.
    
    Args:
        site_id (str): The site ID to fetch tenants for
    
    Returns:
        list: List of tenant names found in site derived networks, or empty list if error
        
    SECURITY: Read-only operation fetching configuration data only.
    """
    try:
        logging.info(f"Fetching site derived networks for tenant information from site_id: {site_id}")
        
        # Call the Mist API to get site derived networks which contain tenant definitions
        response = mistapi.api.v1.sites.networks.listSiteNetworksDerived(apisession, site_id)
        
        if hasattr(response, 'data') and response.data:
            networks_data = response.data
            logging.info(f"Successfully retrieved {len(networks_data)} site derived networks")
            
            # Extract tenant names from all networks
            tenant_names = set()  # Use set to avoid duplicates
            for network in networks_data:
                if isinstance(network, dict):
                    # The network name itself is a tenant for service ping
                    network_name = network.get('name')
                    if network_name and isinstance(network_name, str):
                        tenant_names.add(network_name)
                        logging.debug(f"Found site network tenant '{network_name}'")
                    
                    # Also check for any explicit tenants within the network
                    if 'tenants' in network:
                        tenants_dict = network.get('tenants', {})
                        if isinstance(tenants_dict, dict):
                            # Each key in the tenants dict is also a tenant name
                            for tenant_name in tenants_dict.keys():
                                if tenant_name and isinstance(tenant_name, str):
                                    tenant_names.add(tenant_name)
                                    logging.debug(f"Found explicit tenant '{tenant_name}' in site network '{network.get('name', 'unnamed')}'")
            
            tenant_list = sorted(list(tenant_names))  # Convert to sorted list
            logging.info(f"Found {len(tenant_list)} unique tenants across site derived networks: {tenant_list}")
            return tenant_list
            
        else:
            logging.warning("No site derived networks found or response data is empty")
            return []
            
    except Exception as error:
        logging.error(f"Error fetching site tenants from derived networks: {error}")
        return []

def fetch_service_policy_tenants(site_id=None):
    """
    Fetch all tenants defined in organization and site service policies using the Mist API.
    
    Args:
        site_id (str, optional): The site ID to fetch site-specific policies. If None, only org policies are fetched.
    
    Returns:
        list: List of tenant names found in service policies, or empty list if error
        
    SECURITY: Read-only operation fetching configuration data only.
    """
    try:
        tenant_names = set()  # Use set to avoid duplicates
        
        # Fetch organization service policies
        org_id = get_cached_or_prompted_org_id()
        logging.info(f"Fetching organization service policies for tenant information from org_id: {org_id}")
        
        try:
            response = mistapi.api.v1.orgs.servicepolicies.listOrgServicePolicies(apisession, org_id, limit=1000)
            
            if hasattr(response, 'data') and response.data:
                policies_data = response.data
                logging.info(f"Successfully retrieved {len(policies_data)} organization service policies")
                
                # Extract tenant names from service policies
                for policy in policies_data:
                    if isinstance(policy, dict):
                        # Check for tenants array in policy (this is where the tenant names are)
                        tenants_list = policy.get('tenants', [])
                        if isinstance(tenants_list, list):
                            for tenant_name in tenants_list:
                                if tenant_name and isinstance(tenant_name, str):
                                    tenant_names.add(tenant_name)
                                    logging.debug(f"Found tenant '{tenant_name}' in org service policy '{policy.get('name', 'unnamed')}'")
                        
                        # Also check legacy single tenant field for compatibility
                        tenant_name = policy.get('tenant', '')
                        if tenant_name and isinstance(tenant_name, str):
                            tenant_names.add(tenant_name)
                            logging.debug(f"Found single tenant '{tenant_name}' in org service policy '{policy.get('name', 'unnamed')}'")
                        
                        # Check for tenants in policy services (if any)
                        services = policy.get('services', [])
                        if isinstance(services, list):
                            for service in services:
                                if isinstance(service, dict):
                                    service_tenant = service.get('tenant', '')
                                    if service_tenant and isinstance(service_tenant, str):
                                        tenant_names.add(service_tenant)
                                        logging.debug(f"Found tenant '{service_tenant}' in org service policy service")
            
        except Exception as org_error:
            logging.warning(f"Could not fetch organization service policies: {org_error}")
        
        # Fetch site service policies if site_id provided
        if site_id:
            logging.info(f"Fetching site service policies for tenant information from site_id: {site_id}")
            
            try:
                response = mistapi.api.v1.sites.servicepolicies.listSiteServicePoliciesDerived(apisession, site_id)
                
                if hasattr(response, 'data') and response.data:
                    policies_data = response.data
                    logging.info(f"Successfully retrieved {len(policies_data)} site service policies")
                    
                    # Extract tenant names from site service policies
                    for policy in policies_data:
                        if isinstance(policy, dict):
                            # Check for tenants array in policy (this is where the tenant names are)
                            tenants_list = policy.get('tenants', [])
                            if isinstance(tenants_list, list):
                                for tenant_name in tenants_list:
                                    if tenant_name and isinstance(tenant_name, str):
                                        tenant_names.add(tenant_name)
                                        logging.debug(f"Found tenant '{tenant_name}' in site service policy '{policy.get('name', 'unnamed')}'")
                            
                            # Also check legacy single tenant field for compatibility
                            tenant_name = policy.get('tenant', '')
                            if tenant_name and isinstance(tenant_name, str):
                                tenant_names.add(tenant_name)
                                logging.debug(f"Found single tenant '{tenant_name}' in site service policy '{policy.get('name', 'unnamed')}'")
                            
                            # Check for tenants in policy services (if any)
                            services = policy.get('services', [])
                            if isinstance(services, list):
                                for service in services:
                                    if isinstance(service, dict):
                                        service_tenant = service.get('tenant', '')
                                        if service_tenant and isinstance(service_tenant, str):
                                            tenant_names.add(service_tenant)
                                            logging.debug(f"Found tenant '{service_tenant}' in site service policy service")
                
            except Exception as site_error:
                logging.warning(f"Could not fetch site service policies: {site_error}")
        
        tenant_list = sorted(list(tenant_names))  # Convert to sorted list
        logging.info(f"Found {len(tenant_list)} unique tenants across service policies: {tenant_list}")
        return tenant_list
        
    except Exception as error:
        logging.error(f"Error fetching tenants from service policies: {error}")
        return []

def fetch_gateway_template_tenants(site_id=None):
    """
    Fetch all tenants defined in organization and site gateway templates using the Mist API.
    
    Args:
        site_id (str, optional): The site ID to fetch site-specific templates. If None, only org templates are fetched.
    
    Returns:
        list: List of tenant names found in gateway templates, or empty list if error
        
    SECURITY: Read-only operation fetching configuration data only.
    """
    try:
        tenant_names = set()  # Use set to avoid duplicates
        
        # Fetch organization gateway templates
        org_id = get_cached_or_prompted_org_id()
        logging.info(f"Fetching organization gateway templates for tenant information from org_id: {org_id}")
        
        try:
            response = mistapi.api.v1.orgs.gatewaytemplates.listOrgGatewayTemplates(apisession, org_id, limit=1000)
            
            if hasattr(response, 'data') and response.data:
                templates_data = response.data
                logging.info(f"Successfully retrieved {len(templates_data)} organization gateway templates")
                
                # Extract tenant names from gateway templates
                for template in templates_data:
                    if isinstance(template, dict):
                        # Check router configuration in gateway template
                        router_config = template.get('router', {})
                        if isinstance(router_config, dict):
                            # Check for tenants in router config
                            tenants_config = router_config.get('tenants', [])
                            if isinstance(tenants_config, list):
                                for tenant_item in tenants_config:
                                    if isinstance(tenant_item, dict):
                                        tenant_name = tenant_item.get('name', '')
                                        if tenant_name and isinstance(tenant_name, str):
                                            tenant_names.add(tenant_name)
                                            logging.debug(f"Found tenant '{tenant_name}' in org gateway template '{template.get('name', 'unnamed')}'")
                            
                            # Also check router.tenant_profiles which might contain tenant definitions
                            tenant_profiles = router_config.get('tenant_profiles', {})
                            if isinstance(tenant_profiles, dict):
                                for tenant_name in tenant_profiles.keys():
                                    if tenant_name and isinstance(tenant_name, str):
                                        tenant_names.add(tenant_name)
                                        logging.debug(f"Found tenant profile '{tenant_name}' in org gateway template")
                        
                        # Check networks configuration which might have tenant mappings
                        networks_config = template.get('networks', [])
                        if isinstance(networks_config, list):
                            for network in networks_config:
                                if isinstance(network, dict) and 'tenants' in network:
                                    tenants_dict = network.get('tenants', {})
                                    if isinstance(tenants_dict, dict):
                                        for tenant_name in tenants_dict.keys():
                                            if tenant_name and isinstance(tenant_name, str):
                                                tenant_names.add(tenant_name)
                                                logging.debug(f"Found tenant '{tenant_name}' in org gateway template network")
            
        except Exception as org_error:
            logging.warning(f"Could not fetch organization gateway templates: {org_error}")
        
        # Fetch site gateway templates if site_id provided
        if site_id:
            logging.info(f"Fetching site gateway templates for tenant information from site_id: {site_id}")
            
            try:
                response = mistapi.api.v1.sites.gatewaytemplates.listSiteGatewayTemplatesDerived(apisession, site_id)
                
                if hasattr(response, 'data') and response.data:
                    templates_data = response.data
                    logging.info(f"Successfully retrieved {len(templates_data)} site gateway templates")
                    
                    # Extract tenant names from site gateway templates
                    for template in templates_data:
                        if isinstance(template, dict):
                            # Check router configuration in gateway template
                            router_config = template.get('router', {})
                            if isinstance(router_config, dict):
                                # Check for tenants in router config
                                tenants_config = router_config.get('tenants', [])
                                if isinstance(tenants_config, list):
                                    for tenant_item in tenants_config:
                                        if isinstance(tenant_item, dict):
                                            tenant_name = tenant_item.get('name', '')
                                            if tenant_name and isinstance(tenant_name, str):
                                                tenant_names.add(tenant_name)
                                                logging.debug(f"Found tenant '{tenant_name}' in site gateway template '{template.get('name', 'unnamed')}'")
                                
                                # Also check router.tenant_profiles
                                tenant_profiles = router_config.get('tenant_profiles', {})
                                if isinstance(tenant_profiles, dict):
                                    for tenant_name in tenant_profiles.keys():
                                        if tenant_name and isinstance(tenant_name, str):
                                            tenant_names.add(tenant_name)
                                            logging.debug(f"Found tenant profile '{tenant_name}' in site gateway template")
                            
                            # Check networks configuration
                            networks_config = template.get('networks', [])
                            if isinstance(networks_config, list):
                                for network in networks_config:
                                    if isinstance(network, dict) and 'tenants' in network:
                                        tenants_dict = network.get('tenants', {})
                                        if isinstance(tenants_dict, dict):
                                            for tenant_name in tenants_dict.keys():
                                                if tenant_name and isinstance(tenant_name, str):
                                                    tenant_names.add(tenant_name)
                                                    logging.debug(f"Found tenant '{tenant_name}' in site gateway template network")
                
            except Exception as site_error:
                logging.warning(f"Could not fetch site gateway templates: {site_error}")
        
        tenant_list = sorted(list(tenant_names))  # Convert to sorted list
        logging.info(f"Found {len(tenant_list)} unique tenants across gateway templates: {tenant_list}")
        return tenant_list
        
    except Exception as error:
        logging.error(f"Error fetching tenants from gateway templates: {error}")
        return []

def flatten_dict_recursively(d: Dict[str, Any], parent_key: str = '', sep: str = '_') -> Dict[str, Any]:
    """
    Recursively flattens a nested dictionary, joining keys with `sep`.
    Lists of dicts are flattened with indexed keys.
    Non-dict lists are joined as comma-separated strings.
    All keys are converted to strings for CSV/JSON compatibility.
    """
    items = []
    for k, v in d.items():
        k_str = str(k)
        new_key = f"{parent_key}{sep}{k_str}" if parent_key else k_str
        # If the value is a dictionary, recurse
        if isinstance(v, dict):
            items.extend(flatten_dict_recursively(v, new_key, sep=sep).items())
        # If the value is a list
        elif isinstance(v, list):
            if all(isinstance(i, dict) for i in v):
                # If all items are dicts, flatten each with an index
                for idx, item in enumerate(v):
                    items.extend(flatten_dict_recursively(item, f"{new_key}{sep}{idx}", sep=sep).items())
            else:
                # Otherwise, join list items as a comma-separated string
                items.append((new_key, ','.join(map(str, v))))
        else:
            # Base case: not a dict or list, just add the value
            items.append((new_key, v))
    # Uncomment the next line to enable debug logging of the flattening process
    # logging.debug(f"Flattened dict at key '{parent_key}': {dict(items)}")
    return dict(items)

def flatten_nested_fields_in_list(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Flattens all nested fields in a list of dictionaries.
    - Attempts to parse stringified dicts/lists.
    - Recursively flattens nested dicts and lists of dicts.
    - Joins non-dict lists as comma-separated strings.
    """
    flattened = []
    for entry in data:
        # Skip entries that are not dictionaries (defensive programming)
        if not isinstance(entry, dict):
            logging.debug(f"Skipping non-dictionary entry: {type(entry).__name__} - {entry}")
            continue
            
        new_entry = {}
        for key, value in entry.items():
            # Try to parse stringified dicts/lists
            if isinstance(value, str) and (value.startswith("{") or value.startswith("[")):
                try:
                    value = ast.literal_eval(value)
                    logging.debug(f"Parsed stringified value for key '{key}': {value}")
                except Exception:
                    try:
                        value = json.loads(value)
                        logging.debug(f"JSON loaded value for key '{key}': {value}")
                    except Exception:
                        # Leave as string if parsing fails
                        logging.debug(f"Failed to parse value for key '{key}', leaving as string.")

            # Flatten if it's a dict or list of dicts
            if isinstance(value, dict):
                # Recursively flatten nested dict
                flat = flatten_dict_recursively(value, parent_key=key)
                new_entry.update(flat)
                logging.debug(f"Flattened dict for key '{key}': {flat}")
            elif isinstance(value, list):
                if all(isinstance(i, dict) for i in value):
                    # Flatten each dict in the list with an index
                    for idx, item in enumerate(value):
                        flat = flatten_dict_recursively(item, parent_key=f"{key}_{idx}")
                        new_entry.update(flat)
                        logging.debug(f"Flattened dict in list for key '{key}_{idx}': {flat}")
                else:
                    # Join non-dict lists as comma-separated strings
                    new_entry[key] = ','.join(map(str, value))
                    logging.debug(f"Joined list for key '{key}': {new_entry[key]}")
            else:
                # Base case: not a dict or list, just add the value
                new_entry[key] = value
        flattened.append(new_entry)
    return flattened

def format_marvis_data_for_csv(api_response_data, analysis_type="generic"):
    """
    Optimized formatter for Marvis API responses to create readable CSV files.
    
    Args:
        api_response_data: Raw API response data from Marvis troubleshoot calls
        analysis_type: Type of analysis ("client", "device", "network", "sites")
    
    Returns:
        List of dictionaries optimized for CSV readability
    """
    try:
        # Handle different response structures
        if not api_response_data:
            logging.warning("Empty Marvis API response received")
            return []
        
        # Ensure we have a list to work with
        if not isinstance(api_response_data, list):
            data_list = [api_response_data]
        else:
            data_list = api_response_data
        
        formatted_data = []
        
        for item in data_list:
            if not isinstance(item, dict):
                logging.warning(f"Unexpected data type in Marvis response: {type(item)}")
                continue
            
            # Handle organization sites SLE data specially for readability
            if analysis_type == "sites" and "results" in item and isinstance(item["results"], list):
                logging.info(f"Processing organization sites SLE data with {len(item['results'])} sites")
                
                # Create one row per site instead of flattening all sites into one massive row
                for idx, site_data in enumerate(item["results"]):
                    site_row = {}
                    
                    # Add metadata from parent response
                    for meta_key in ["start", "end", "limit", "page", "total"]:
                        if meta_key in item:
                            site_row[meta_key] = item[meta_key]
                    
                    # Add site index for reference
                    site_row["site_index"] = idx
                    
                    # Add site data with clean column names
                    if isinstance(site_data, dict):
                        for key, value in site_data.items():
                            # Use clean column names instead of results_X_key format
                            clean_key = key.replace("-", "_")  # Replace hyphens for CSV compatibility
                            site_row[clean_key] = value
                    
                    formatted_data.append(site_row)
                
                logging.info(f"Converted {len(item['results'])} sites into {len(formatted_data)} readable rows")
                
            else:
                # Handle single troubleshoot results (client, device, network)
                formatted_row = {}
                
                # Add top-level metadata
                for key, value in item.items():
                    if key == "results" and isinstance(value, list):
                        # Handle results array - flatten each result with cleaner naming
                        for idx, result in enumerate(value):
                            if isinstance(result, dict):
                                for result_key, result_value in result.items():
                                    # Use clean column names: result_0_category instead of results_0_category
                                    clean_key = f"result_{idx}_{result_key.replace('-', '_')}"
                                    formatted_row[clean_key] = result_value
                            else:
                                formatted_row[f"result_{idx}"] = str(result)
                    elif isinstance(value, dict):
                        # Flatten nested dicts with clean naming
                        for nested_key, nested_value in value.items():
                            clean_key = f"{key}_{nested_key}".replace("-", "_")
                            formatted_row[clean_key] = nested_value
                    elif isinstance(value, list):
                        # Join lists as comma-separated values
                        formatted_row[key] = ",".join(map(str, value))
                    else:
                        # Direct assignment for simple values
                        formatted_row[key] = value
                
                if formatted_row:  # Only add if we have data
                    formatted_data.append(formatted_row)
        
        # Apply final CSV-friendly formatting
        formatted_data = escape_multiline_strings_for_csv(formatted_data)
        
        logging.info(f"Marvis data formatting complete: {len(formatted_data)} rows for {analysis_type} analysis")
        return formatted_data
        
    except Exception as e:
        logging.error(f"Error formatting Marvis data for CSV: {e}")
        # Fall back to old method if new formatting fails
        logging.info("Falling back to legacy flattening method")
        fallback_data = [api_response_data] if not isinstance(api_response_data, list) else api_response_data
        fallback_data = flatten_nested_fields_in_list(fallback_data)
        fallback_data = escape_multiline_strings_for_csv(fallback_data)
        return fallback_data

def convert_list_values_to_csv_strings(data):
    """
    Converts all list, tuple, or set values in a list of dictionaries to comma-separated strings.
    Adds debug logging for each conversion.
    """
    for entry in data:
        for key, value in entry.items():
            if isinstance(value, (list, tuple, set)):
                # Log the conversion for debugging
                logging.debug(f"Converting list/tuple/set at key '{key}' to string: {value}")
                entry[key] = ','.join(map(str, value))
    return data

def get_all_unique_dict_keys(data):
    """
    Returns a sorted list of all unique keys present in a list of dictionaries.
    Useful for determining CSV fieldnames or PrettyTable columns.
    """
    fields = set()
    for entry in data:
        # Add all keys from each dictionary to the set
        fields.update(entry.keys())
    # Log the discovered unique keys for debugging
    logging.debug(f"Discovered unique keys: {fields}")
    # Convert all keys to strings for sorting and CSV compatibility
    return sorted(str(f) for f in fields)

def escape_multiline_strings_for_csv(data):
    """
    Escapes multiline strings in a list of dictionaries for CSV compatibility.
    - Joins list values as comma-separated strings.
    - Replaces newline characters in strings with '\\n' and removes carriage returns.
    """
    for entry in data:
        for key, value in entry.items():
            if isinstance(value, list):
                # Convert list to comma-separated string for CSV compatibility
                logging.debug(f"Converting list at key '{key}' to string: {value}")
                entry[key] = ','.join(map(str, value))
            elif isinstance(value, str):
                # Replace newlines and carriage returns in strings
                if '\n' in value or '\r' in value:
                    logging.debug(f"Escaping newlines in string at key '{key}': {repr(value)}")
                entry[key] = value.replace('\n', '\\n').replace('\r', '')
    return data

def write_dict_list_to_csv(data: List[Dict[str, Any]], csv_file: str) -> None:
    """
    Writes a list of dictionaries to a CSV file.
    - Escapes multiline strings for CSV compatibility.
    - Determines all unique fields for the CSV header.
    - Writes each row, filling missing fields with empty strings.
    - Uses data directory for container persistence.
    """
    logging.debug(f"ENTRY: write_dict_list_to_csv(data_rows={len(data) if data else 0}, csv_file={csv_file})")
    
    if not data:
        logging.warning(f"No data provided to write to {csv_file}")
        logging.debug(f"EXIT: write_dict_list_to_csv - no data to write")
        return
        
    # Ensure data directory exists and construct proper file path
    data_dir = "data"
    os.makedirs(data_dir, exist_ok=True)
    
    # If csv_file doesn't already include a path, place it in the data directory
    if not os.path.dirname(csv_file):
        csv_file_path = os.path.join(data_dir, csv_file)
    else:
        csv_file_path = csv_file
        
    logging.debug(f"Preparing to write {len(data)} rows to {csv_file_path}...")
    data = escape_multiline_strings_for_csv(data)
    fields = get_all_unique_dict_keys(data)
    logging.debug(f"CSV fields determined: {fields}")

    try:
        logging.debug(f"File I/O: Attempting to open {csv_file_path} for writing")
        with open(csv_file_path, 'w', newline='', encoding='utf-8') as file:
            writer = csv.DictWriter(file, fieldnames=fields)
            writer.writeheader()
            logging.debug(f"File I/O: Successfully wrote CSV header to {csv_file_path}")
            
            for idx, row in enumerate(data):
                writer.writerow({field: row.get(field, "") for field in fields})
                if idx < 3:  # Log the first few rows for debugging
                    logging.debug(f"Row {idx} written: {row}")
                    
        logging.info(f"File I/O: Successfully wrote {len(data)} rows to {csv_file_path}")
        logging.debug(f"EXIT: write_dict_list_to_csv - success")
        
    except PermissionError as e:
        logging.error(f"File I/O: Permission denied when writing to {csv_file_path}: {e}")
        print(f"! Cannot write to {csv_file_path}. Is it open in another program?")
        logging.debug(f"EXIT: write_dict_list_to_csv - permission error")
        raise
    except OSError as e:
        logging.error(f"File I/O: OS error when writing to {csv_file_path}: {e}")
        logging.debug(f"EXIT: write_dict_list_to_csv - OS error")
        raise
    except Exception as e:
        logging.error(f"File I/O: Unexpected error when writing to {csv_file}: {e}")
        logging.debug(f"EXIT: write_dict_list_to_csv - unexpected error")
        raise


def determine_api_function_name_from_context() -> str:
    """
    Attempts to determine the API function name from the current call stack.
    This helps identify which endpoint strategy to use for table schema.
    
    Returns:
        str: The API function name if found, else 'unknown'
    """
    
    # Look through the call stack for known API function patterns
    frame = inspect.currentframe()
    try:
        while frame:
            function_name = frame.f_code.co_name
            # Check if this looks like an API function name
            if any(pattern in function_name for pattern in [
                'getOrg', 'listOrg', 'searchOrg', 'getSite', 'listSite', 'searchSite'
            ]):
                logging.debug(f"Detected API function name from stack: {function_name}")
                return function_name
            frame = frame.f_back
    except Exception as e:
        logging.debug(f"Error determining API function name: {e}")
    finally:
        del frame
    
    return 'unknown'

def get_endpoint_strategy(api_function_name: str, data_fields: List[str]) -> Dict[str, Any]:
    """
    Determines the appropriate database schema strategy for an API endpoint.
    
    Args:
        api_function_name (str): Name of the API function being called
        data_fields (list): List of field names in the data
    
    Returns:
        dict: Strategy configuration including primary key, indexes, etc.
    """
    # First check if we have a specific strategy for this endpoint
    if api_function_name in ENDPOINT_PRIMARY_KEY_STRATEGIES:
        strategy = ENDPOINT_PRIMARY_KEY_STRATEGIES[api_function_name].copy()
        logging.debug(f"Using configured strategy for {api_function_name}: {strategy['type']}")
        return strategy
    
    # If no specific strategy, use intelligent defaults based on data structure
    strategy = ENDPOINT_PRIMARY_KEY_STRATEGIES['default'].copy()
    
    # Enhance default strategy based on available fields
    if 'id' in data_fields:
        # If data has an 'id' field, use it as unique constraint
        strategy['unique_constraints'] = ['id']
        strategy['indexes'] = ['id']
        logging.debug(f"Enhanced default strategy for {api_function_name}: adding unique constraint on 'id'")
    
    # Add common indexes for frequently queried fields
    common_index_fields = ['org_id', 'site_id', 'device_id', 'timestamp', 'mac', 'serial']
    for field in common_index_fields:
        if field in data_fields and field not in strategy['indexes']:
            strategy['indexes'].append(field)
    
    logging.debug(f"Using enhanced default strategy for {api_function_name}: {strategy}")
    return strategy

def build_create_table_sql(table_name: str, fields: List[str], strategy: Dict[str, Any]) -> str:
    """
    Builds the CREATE TABLE SQL statement based on the endpoint strategy.
    
    Args:
        table_name (str): Name of the table to create
        fields (list): List of field names from the data
        strategy (dict): Strategy configuration for this endpoint
    
    Returns:
        str: Complete CREATE TABLE SQL statement
    """
    timestamp = datetime.now(timezone.utc).isoformat()
    
    # Sanitize table name
    safe_table_name = re.sub(r'[^a-zA-Z0-9_]', '_', table_name)
    if not safe_table_name or safe_table_name[0].isdigit():
        safe_table_name = f"table_{safe_table_name}"
    
    # Start building SQL
    if strategy['type'] == 'natural_pk':
        # Use API id field(s) as primary key
        pk_fields = strategy['primary_key']
        sql_parts = [f"CREATE TABLE IF NOT EXISTS {safe_table_name} ("]
        
        # Add all fields, with primary key fields getting special treatment
        field_definitions = []
        for field in fields:
            safe_field = re.sub(r'[^a-zA-Z0-9_]', '_', str(field))
            if field in pk_fields:
                field_definitions.append(f"{safe_field} TEXT NOT NULL")
            else:
                field_definitions.append(f"{safe_field} TEXT")
        
        # Add metadata fields
        field_definitions.append("misthelper_created_time TEXT DEFAULT CURRENT_TIMESTAMP")
        field_definitions.append("misthelper_updated_time TEXT DEFAULT CURRENT_TIMESTAMP")
        
        sql_parts.append(", ".join(field_definitions))
        
        # Add primary key constraint
        pk_constraint = f"PRIMARY KEY ({', '.join(pk_fields)})"
        sql_parts.append(f", {pk_constraint}")
        
        sql_parts.append(")")
        create_sql = "".join(sql_parts)
        
    elif strategy['type'] == 'composite_pk':
        # Use composite primary key
        pk_fields = strategy['primary_key']
        sql_parts = [f"CREATE TABLE IF NOT EXISTS {safe_table_name} ("]
        
        field_definitions = []
        for field in fields:
            safe_field = re.sub(r'[^a-zA-Z0-9_]', '_', str(field))
            if field in pk_fields:
                field_definitions.append(f"{safe_field} TEXT NOT NULL")
            else:
                field_definitions.append(f"{safe_field} TEXT")
        
        # Add metadata fields
        field_definitions.append("misthelper_created_time TEXT DEFAULT CURRENT_TIMESTAMP")
        field_definitions.append("misthelper_updated_time TEXT DEFAULT CURRENT_TIMESTAMP")
        
        sql_parts.append(", ".join(field_definitions))
        
        # Add composite primary key constraint
        available_pk_fields = [f for f in pk_fields if f in fields]
        if available_pk_fields:
            pk_constraint = f"PRIMARY KEY ({', '.join(available_pk_fields)})"
            sql_parts.append(f", {pk_constraint}")
        
        sql_parts.append(")")
        create_sql = "".join(sql_parts)
        
    else:  # auto_increment_with_unique
        # Use auto-increment primary key with unique constraints
        sql_parts = [f"CREATE TABLE IF NOT EXISTS {safe_table_name} ("]
        
        field_definitions = ["misthelper_internal_id INTEGER PRIMARY KEY AUTOINCREMENT"]
        
        for field in fields:
            safe_field = re.sub(r'[^a-zA-Z0-9_]', '_', str(field))
            field_definitions.append(f"{safe_field} TEXT")
        
        # Add metadata fields
        field_definitions.append("misthelper_created_time TEXT DEFAULT CURRENT_TIMESTAMP")
        field_definitions.append("misthelper_updated_time TEXT DEFAULT CURRENT_TIMESTAMP")
        
        sql_parts.append(", ".join(field_definitions))
        
        # Add unique constraints if specified
        unique_fields = [f for f in strategy['unique_constraints'] if f in fields]
        if unique_fields:
            for field in unique_fields:
                safe_field = re.sub(r'[^a-zA-Z0-9_]', '_', str(field))
                sql_parts.append(f", UNIQUE({safe_field})")
        
        sql_parts.append(")")
        create_sql = "".join(sql_parts)
    
    logging.debug(f"Generated CREATE TABLE SQL for {safe_table_name}: {create_sql[:100]}...")
    return create_sql

def build_indexes_sql(table_name: str, fields: List[str], strategy: Dict[str, Any]) -> List[str]:
    """
    Builds CREATE INDEX SQL statements for the specified strategy.
    
    Args:
        table_name (str): Name of the table
        fields (list): Available fields in the data
        strategy (dict): Strategy configuration
    
    Returns:
        list: List of CREATE INDEX SQL statements
    """
    safe_table_name = re.sub(r'[^a-zA-Z0-9_]', '_', table_name)
    if not safe_table_name or safe_table_name[0].isdigit():
        safe_table_name = f"table_{safe_table_name}"
    
    index_sqls = []
    
    # Create indexes for fields specified in strategy
    for field in strategy.get('indexes', []):
        if field in fields:
            safe_field = re.sub(r'[^a-zA-Z0-9_]', '_', str(field))
            index_name = f"idx_{safe_table_name}_{safe_field}"
            index_sql = f"CREATE INDEX IF NOT EXISTS {index_name} ON {safe_table_name} ({safe_field})"
            index_sqls.append(index_sql)
    
    return index_sqls

def write_dict_list_to_sqlite_database_inside_container(data: List[Dict[str, Any]], table_name: str, api_function_name: Optional[str] = None) -> bool:
    """
    Writes a list of dictionaries to a SQLite database table using hybrid primary key strategies.
    This new implementation eliminates artificial api_id fields and uses proper business keys.
    Follows NASA/JPL coding standards with comprehensive logging and error handling.
    
    Args:
        data (list): List of dictionaries containing the data to write
        table_name (str): Name of the database table to write to
        api_function_name (str, optional): Name of the API function for strategy selection
    
    Returns:
        bool: True if successful, False otherwise
    """
    # Entry logging with enhanced input validation
    timestamp = datetime.now(timezone.utc).isoformat()
    logging.debug(f"ENTRY: write_dict_list_to_sqlite_database_inside_container(data_rows={len(data) if data else 0}, table_name={table_name}, api_function_name={api_function_name}) at {timestamp}")
    
    # Input validation - Check if data is provided and is a list
    if not data:
        logging.warning(f"No data provided to write to table {table_name} at {timestamp}")
        logging.debug(f"EXIT: write_dict_list_to_sqlite_database_inside_container - no data to write")
        return False
        
    if not isinstance(data, list):
        logging.error(f"Invalid data type: expected list, got {type(data)} at {timestamp}")
        logging.debug(f"EXIT: write_dict_list_to_sqlite_database_inside_container - invalid data type")
        return False
        
    # Input validation - Check table name
    if not table_name or not isinstance(table_name, str):
        logging.error(f"Invalid table name: {table_name} at {timestamp}")
        logging.debug(f"EXIT: write_dict_list_to_sqlite_database_inside_container - invalid table name")
        return False
    
    # Determine API function name if not provided
    if not api_function_name:
        api_function_name = determine_api_function_name_from_context()
        
    logging.debug(f"Processing {len(data)} rows for table {table_name} using API function {api_function_name} at {timestamp}")
    
    # Ensure database directory exists
    db_dir = os.path.dirname(DATABASE_PATH)
    if db_dir and not os.path.exists(db_dir):
        try:
            os.makedirs(db_dir, exist_ok=True)
            logging.info(f"Created database directory: {db_dir} at {timestamp}")
        except OSError as e:
            logging.error(f"Failed to create database directory {db_dir}: {e} at {timestamp}")
            logging.debug(f"EXIT: write_dict_list_to_sqlite_database_inside_container - directory creation failed")
            return False
    
    # Process data to handle formatting for database storage
    try:
        processed_data = escape_multiline_strings_for_csv(data)
        logging.debug(f"Successfully processed data for SQLite compatibility at {timestamp}")
    except Exception as e:
        logging.error(f"Failed to process data: {e} at {timestamp}")
        logging.debug(f"EXIT: write_dict_list_to_sqlite_database_inside_container - data processing failed")
        return False
    
    # Get all unique fields and determine strategy
    try:
        fields = get_all_unique_dict_keys(processed_data)
        if not fields:
            logging.error(f"No fields found in data for table {table_name} at {timestamp}")
            logging.debug(f"EXIT: write_dict_list_to_sqlite_database_inside_container - no fields")
            return False
        
        strategy = get_endpoint_strategy(api_function_name, fields)
        logging.info(f"Using hybrid SQLite strategy '{strategy['type']}' for table {table_name}: {strategy['description']}")
        logging.debug(f"Database fields determined: {fields} at {timestamp}")
        logging.debug(f"Endpoint {api_function_name} mapped to {strategy['type']} strategy - eliminates need for artificial api_id fields")
        
    except Exception as e:
        logging.error(f"Failed to determine fields and strategy: {e} at {timestamp}")
        logging.debug(f"EXIT: write_dict_list_to_sqlite_database_inside_container - field determination failed")
        return False
    
    # Database operations with comprehensive error handling
    connection = None
    try:
        # Connect to SQLite database
        logging.debug(f"Attempting to connect to database: {DATABASE_PATH} at {timestamp}")
        connection = sqlite3.connect(DATABASE_PATH)
        cursor = connection.cursor()
        logging.info(f"Successfully connected to database: {DATABASE_PATH} at {timestamp}")
        
        # Create table with strategy-appropriate schema
        create_table_sql = build_create_table_sql(table_name, fields, strategy)
        cursor.execute(create_table_sql)
        logging.debug(f"Table {table_name} created/verified with hybrid {strategy['type']} schema - using natural business keys from API")
        
        # Create indexes for performance
        index_sqls = build_indexes_sql(table_name, fields, strategy)
        for index_sql in index_sqls:
            cursor.execute(index_sql)
        if index_sqls:
            logging.debug(f"Created {len(index_sqls)} performance indexes for table {table_name} with {strategy['type']} strategy")
        
        # Determine insert strategy based on schema type
        if strategy['type'] in ['natural_pk', 'composite_pk']:
            # Use REPLACE for natural and composite keys to handle updates gracefully
            insert_mode = "INSERT OR REPLACE"
            logging.debug(f"Using REPLACE mode for {strategy['type']} strategy - enables efficient upsert operations with natural keys")
        else:
            # Clear table for auto-increment strategy (fallback for unclassified endpoints)
            cursor.execute(f"DELETE FROM {re.sub(r'[^a-zA-Z0-9_]', '_', table_name)}")
            insert_mode = "INSERT"
            logging.debug(f"Cleared existing data and using INSERT mode for auto-increment fallback strategy")
        
        # Prepare field mapping
        safe_fields = []
        for field in fields:
            safe_field = re.sub(r'[^a-zA-Z0-9_]', '_', str(field))
            safe_fields.append(safe_field)
        
        # Add metadata fields
        safe_fields.extend(["misthelper_created_time", "misthelper_updated_time"])
        
        # Insert data rows
        current_time = datetime.now(timezone.utc).isoformat()
        successful_inserts = 0
        
        for idx, row in enumerate(processed_data):
            try:
                # Prepare values for insertion
                values = []
                for field in fields:
                    value = row.get(field, "")
                    # Convert value to string for TEXT storage
                    if value is None:
                        value = ""
                    else:
                        value = str(value)
                    values.append(value)
                
                # Add metadata values
                values.extend([current_time, current_time])
                
                # Create parameterized query for safety
                placeholders = ", ".join(["?"] * len(values))
                safe_table_name = re.sub(r'[^a-zA-Z0-9_]', '_', table_name)
                if not safe_table_name or safe_table_name[0].isdigit():
                    safe_table_name = f"table_{safe_table_name}"
                
                insert_sql = f"{insert_mode} INTO {safe_table_name} ({', '.join(safe_fields)}) VALUES ({placeholders})"
                
                cursor.execute(insert_sql, values)
                successful_inserts += 1
                
                # Log first few rows for debugging
                if idx < 3:
                    logging.debug(f"Row {idx} inserted into {table_name} using {insert_mode} at {timestamp}")
                    
            except Exception as e:
                logging.error(f"Failed to insert row {idx} into {table_name}: {e} at {timestamp}")
                # Continue with other rows rather than failing completely
                continue
        
        # Commit transaction
        connection.commit()
        logging.info(f"Successfully wrote {successful_inserts}/{len(processed_data)} rows to table {table_name} in database {DATABASE_PATH} using {strategy['type']} strategy at {timestamp}")
        
        # Verify data was written
        safe_table_name = re.sub(r'[^a-zA-Z0-9_]', '_', table_name)
        if not safe_table_name or safe_table_name[0].isdigit():
            safe_table_name = f"table_{safe_table_name}"
        cursor.execute(f"SELECT COUNT(*) FROM {safe_table_name}")
        row_count = cursor.fetchone()[0]
        logging.info(f"Database verification: {row_count} rows confirmed in table {table_name} at {timestamp}")
        
        logging.debug(f"EXIT: write_dict_list_to_sqlite_database_inside_container - success")
        return True
        
    except sqlite3.Error as e:
        logging.error(f"SQLite error when writing to {table_name}: {e} at {timestamp}")
        if connection:
            try:
                connection.rollback()
                logging.debug(f"Transaction rolled back for table {table_name} at {timestamp}")
            except Exception as rollback_error:
                logging.error(f"Failed to rollback transaction: {rollback_error} at {timestamp}")
        logging.debug(f"EXIT: write_dict_list_to_sqlite_database_inside_container - SQLite error")
        return False
        
    except Exception as e:
        logging.error(f"Unexpected error when writing to table {table_name}: {e} at {timestamp}")
        if connection:
            try:
                connection.rollback()
                logging.debug(f"Transaction rolled back for table {table_name} at {timestamp}")
            except Exception as rollback_error:
                logging.error(f"Failed to rollback transaction: {rollback_error} at {timestamp}")
        logging.debug(f"EXIT: write_dict_list_to_sqlite_database_inside_container - unexpected error")
        return False
        
    finally:
        # Always close database connection (safety-critical: resource cleanup)
        if connection:
            try:
                connection.close()
                logging.debug(f"Database connection closed for table {table_name} at {timestamp}")
            except Exception as e:
                logging.error(f"Failed to close database connection: {e} at {timestamp}")


def write_data_with_format_selection(data: List[Dict[str, Any]], filename_or_table: str, format_override: Optional[str] = None, api_function_name: Optional[str] = None) -> bool:
    """
    Writes data to either CSV or SQLite database based on global OUTPUT_FORMAT or override.
    Follows NASA/JPL coding standards with comprehensive logging.
    
    Args:
        data (list): List of dictionaries containing the data to write
        filename_or_table (str): CSV filename or database table name
        format_override (str): Optional override for output format ("csv" or "sqlite")
        api_function_name (str, optional): Name of the API function for SQLite strategy selection
    
    Returns:
        bool: True if successful, False otherwise
    """
    timestamp = datetime.now(timezone.utc).isoformat()
    logging.debug(f"ENTRY: write_data_with_format_selection(data_rows={len(data) if data else 0}, filename_or_table={filename_or_table}, format_override={format_override}, api_function_name={api_function_name}) at {timestamp}")
    
    # Determine output format (format_override takes precedence over global setting)
    output_format = format_override if format_override else OUTPUT_FORMAT
    
    # Input validation
    if not data:
        logging.warning(f"No data provided for output to {filename_or_table} at {timestamp}")
        logging.debug(f"EXIT: write_data_with_format_selection - no data")
        return False
        
    if output_format not in ["csv", "sqlite"]:
        logging.error(f"Invalid output format: {output_format}. Must be 'csv' or 'sqlite' at {timestamp}")
        logging.debug(f"EXIT: write_data_with_format_selection - invalid format")
        return False
    
    try:
        if output_format == "csv":
            # Ensure CSV files have .csv extension
            csv_filename = filename_or_table if filename_or_table.endswith('.csv') else f"{filename_or_table}.csv"
            logging.info(f"Writing {len(data)} rows to CSV file: {csv_filename} at {timestamp}")
            write_dict_list_to_csv(data, csv_filename)
            logging.debug(f"EXIT: write_data_with_format_selection - CSV success")
            return True
        else:  # sqlite
            # Convert filename to table name (remove .csv extension if present)
            table_name = filename_or_table
            if table_name.endswith('.csv'):
                table_name = table_name[:-4]  # Remove .csv extension
            
            logging.info(f"Writing {len(data)} rows to SQLite table: {table_name} using API function {api_function_name} at {timestamp}")
            result = write_dict_list_to_sqlite_database_inside_container(data, table_name, api_function_name=api_function_name)
            logging.debug(f"EXIT: write_data_with_format_selection - SQLite {'success' if result else 'failed'}")
            return result
            
    except Exception as e:
        logging.error(f"Failed to write data to {filename_or_table} in {output_format} format: {e} at {timestamp}")
        logging.debug(f"EXIT: write_data_with_format_selection - exception")
        return False


class DataExporter:
    """
    Handles data export operations for CSV and SQLite output formats.
    Centralizes all data saving logic that was previously scattered across functions.
    Uses static methods to avoid unnecessary object instantiation.
    """
    
    @staticmethod
    def save_data_to_output(data, filename, api_function_name=None):
        """
        Save data to the specified format (CSV or SQLite).
        This replaces the save_data_to_output function with identical signature.
        
        Args:
            data (list): List of dictionaries containing the data to write
            filename (str): CSV filename or database table name  
            api_function_name (str, optional): Name of the API function for SQLite strategy selection
            
        Returns:
            bool: True if successful, False otherwise
        """
        return write_data_with_format_selection(data, filename, api_function_name=api_function_name)
    
    @staticmethod
    def export_with_processing(data, filename, sort_key=None, api_function_name=None):
        """
        Export data with standard processing (flatten, escape, sort).
        Common pattern used throughout the codebase.
        
        Args:
            data (list): Raw data from API
            filename (str): Output filename
            sort_key (str, optional): Key to sort by
            api_function_name (str, optional): API function name for strategy
            
        Returns:
            int: Number of records processed
        """
        if not data:
            logging.warning(f"No data to export for {filename}")
            return 0
            
        # Filter to dict entries only (defensive)
        processed_data = [entry for entry in data if isinstance(entry, dict)]
        
        # Sort if requested
        if sort_key:
            processed_data = sorted(processed_data, key=lambda x: x.get(sort_key, ""))
            logging.debug(f"Data sorted by key: {sort_key}")
        
        # Apply standard processing
        processed_data = flatten_nested_fields_in_list(processed_data)
        processed_data = escape_multiline_strings_for_csv(processed_data)
        
        # Save the processed data
        success = DataExporter.save_data_to_output(processed_data, filename, api_function_name)
        
        if success:
            logging.info(f"Exported {len(processed_data)} records to {filename}")
            return len(processed_data)
        else:
            logging.error(f"Failed to export data to {filename}")
            return 0


def save_data_to_output(data: List[Dict[str, Any]], filename: str, api_function_name: Optional[str] = None) -> bool:
    """
    Wrapper function to replace write_dict_list_to_csv calls.
    Routes to appropriate output format based on global OUTPUT_FORMAT setting.
    
    Args:
        data (list): List of dictionaries containing the data to write
        filename (str): CSV filename or database table name
        api_function_name (str, optional): Name of the API function for SQLite strategy selection
    """
    return DataExporter.save_data_to_output(data, filename, api_function_name)

def fetch_and_display_api_data(title, api_call, filename, sort_key=None, display_fields=None, **kwargs):
    """
    Fetches data using the provided API call, processes it (flattening, sorting, escaping),
    writes it to a CSV file, and displays it in a PrettyTable. Adds detailed logging.
    
    Enhanced Error Handling:
        - Handles API rate limiting (HTTP 429) by saving partial results
        - Detects malformed API responses (missing 'results' key) and attempts recovery
        - Logs detailed response structure for debugging unexpected formats
        - Always saves partial data before exiting on error (prevents data loss)
        - Provides user-friendly messages about partial data saves
    
    Safety Features:
        - Emergency data saves on any exception
        - Detailed logging of response structure for troubleshooting
        - Graceful degradation when API returns unexpected formats
    """

    logging.debug(f"ENTRY: fetch_and_display_api_data(title={title}, api_call={api_call.__name__}, filename={filename}, sort_key={sort_key}, display_fields={display_fields}, kwargs={kwargs})")
    
    logging.info(f"Starting data fetch: {title}")
    print(title)
    org_id = get_cached_or_prompted_org_id()
    logging.debug(f"Using org_id: {org_id}")
    smoothed = None

    rawdata = []
    try:
        # Call the API and get all paginated results
        logging.debug(f"Making API call: {api_call.__name__} with kwargs: {kwargs}")
        response = api_call(apisession, org_id, **kwargs)
        smoothed, delay = get_rate_limited_delay(smoothed)
        logging.debug(f"Applying rate limit delay: {delay:.2f}s")
        time.sleep(delay)
        
        # Log response structure for debugging unexpected formats
        logging.debug(f"API response type: {type(response)}")
        if hasattr(response, 'data'):
            logging.debug(f"Response.data type: {type(response.data)}")
            if isinstance(response.data, dict):
                logging.debug(f"Response.data keys: {list(response.data.keys())}")
            elif isinstance(response.data, list):
                logging.debug(f"Response.data is list with {len(response.data)} items")
        
        try:
            rawdata = mistapi.get_all(response=response, mist_session=apisession)
            logging.debug(f"API call successful, retrieved {len(rawdata) if rawdata else 0} raw records")
        except KeyError as e:
            # Handle missing 'results' key or other structure issues
            logging.error(f"API response structure error - missing key: {e}")
            logging.error(f"Response details: type={type(response)}, hasattr(data)={hasattr(response, 'data')}")
            if hasattr(response, 'data'):
                logging.error(f"Response.data type={type(response.data)}")
                if isinstance(response.data, dict):
                    logging.error(f"Available keys: {list(response.data.keys())}")
                    # Try to extract data from common alternate structures
                    if 'data' in response.data:
                        rawdata = response.data.get('data', [])
                        logging.info(f"Recovered {len(rawdata)} records from response.data['data']")
                    elif isinstance(response.data, list):
                        rawdata = response.data
                        logging.info(f"Recovered {len(rawdata)} records from response.data (list)")
                elif isinstance(response.data, list):
                    rawdata = response.data
                    logging.info(f"Recovered {len(rawdata)} records from response.data (direct list)")
            
            # Save whatever we recovered
            if rawdata:
                print(f"! API returned unexpected structure. Recovered {len(rawdata)} records.")
                DataExporter.save_data_to_output(rawdata, filename, api_function_name=api_call.__name__)
                logging.info(f"Recovered data saved to {filename} ({len(rawdata)} rows)")
            else:
                print(f"! API response missing expected 'results' key. No data could be recovered.")
                logging.error(f"Unable to recover any data from malformed response for {title}")
                logging.debug(f"EXIT: fetch_and_display_api_data - structure error, no recovery")
                return
                
        except Exception as e:
            # Handle other exceptions during data retrieval
            logging.error(f"Exception occurred during API data retrieval: {e}")
            logging.error(f"Exception type: {type(e).__name__}")
            print(f"! Exception occurred during API call: {e}")
            
            # Check for HTTP 429 (rate limit exceeded)
            status_code = getattr(getattr(e, "response", None), "status_code", None)
            if status_code == 429:
                logging.warning("API rate limit (HTTP 429) reached. Saving partial results and exiting.")
                if rawdata:
                    DataExporter.save_data_to_output(rawdata, filename, api_function_name=api_call.__name__)
                    logging.info(f"Partial results saved to {filename} ({len(rawdata)} rows) using {api_call.__name__} strategy.")
                    print(f"* Partial data saved: {len(rawdata)} records written to {filename}")
                logging.debug(f"EXIT: fetch_and_display_api_data - rate limited")
                return
            else:
                # For any other exception, save partial data before re-raising
                if rawdata:
                    try:
                        DataExporter.save_data_to_output(rawdata, filename, api_function_name=api_call.__name__)
                        logging.info(f"Emergency save: {len(rawdata)} partial records saved to {filename} before error exit")
                        print(f"* Emergency save: {len(rawdata)} partial records written to {filename}")
                    except Exception as save_error:
                        logging.error(f"Failed to save partial data during error handling: {save_error}")
                logging.debug(f"EXIT: fetch_and_display_api_data - API error")
                raise

        if rawdata is None:
            logging.warning(f"! No data returned from API for {title}. Skipping.")
            logging.debug(f"EXIT: fetch_and_display_api_data - no data")
            return

        logging.info(f"Fetched {len(rawdata)} raw records from API.")

        # Process and export data using DataExporter
        record_count = DataExporter.export_with_processing(rawdata, filename, sort_key=sort_key, api_function_name=api_call.__name__)
        print(f"! {len(rawdata)} records exported to {filename}")
        
        # Get processed data for display (reprocess for table display)
        data = [entry for entry in rawdata if isinstance(entry, dict)]
        if sort_key:
            data = sorted(data, key=lambda x: x.get(sort_key, ""))
        data = flatten_nested_fields_in_list(data)
        data = escape_multiline_strings_for_csv(data)
        
        # Determine all unique fields for table display
        fields = get_all_unique_dict_keys(data)
        logging.debug(f"Unique fields for table: {fields}")

        # Prepare and display PrettyTable
        table = PrettyTable()
        table.field_names = display_fields if display_fields else fields
        table.valign = "t"
        for item in tqdm(data, desc="Processing", unit="record"):
            row = [item.get(field, "") for field in table.field_names]
            table.add_row(row)
        logging.debug("\n" + table.get_string())
        logging.debug(f"EXIT: fetch_and_display_api_data - success")

    except Exception as e:
        logging.error(f"! Error during data fetch for {title}: {e}")
        logging.error(f"Exception type: {type(e).__name__}, Traceback info available in logs")
        
        # Always save whatever data was collected so far
        if rawdata:
            try:
                DataExporter.save_data_to_output(rawdata, filename, api_function_name=api_call.__name__)
                logging.info(f"Partial results saved to {filename} ({len(rawdata)} rows) using {api_call.__name__} strategy.")
                print(f"\n!! PARTIAL DATA SAVED !!")
                print(f"   * Despite the error, {len(rawdata)} records were successfully saved to {filename}")
                print(f"   * Error: {str(e)}")
                print(f"   * You can retry the operation later to get remaining data")
            except Exception as save_error:
                logging.error(f"Failed to save partial data in outer exception handler: {save_error}")
                print(f"! Critical: Could not save partial data. Error: {save_error}")
        else:
            print(f"! No data was collected before the error occurred")
            
        logging.debug(f"EXIT: fetch_and_display_api_data - error")
        raise

def execute_with_connection_pool_management(work_items: List[Any], worker_function: Any, batch_description: str = "items", retry_function: Optional[Any] = None) -> Tuple[List[Any], List[Any]]:
    """
    Execute a list of work items using connection pool management and configurable threading.
    
    This is a reusable helper that any function can use to benefit from:
    - Connection-aware vs CPU-aware threading
    - Semaphore-based connection limiting 
    - Configurable batch processing
    - Automatic retry handling
    - Progress tracking
    
    Args:
        work_items: List of items to process
        worker_function: Function to call for each item. Should accept (item, connection_semaphore) parameters
        batch_description: Description for progress tracking (e.g., "devices", "sites")
        retry_function: Optional function to call for retry logic. Should accept (failed_items, connection_semaphore) parameters
        
    Returns:
        Tuple of (successful_results, failed_items)
    """
    
    if not work_items:
        logging.info(f"* No {batch_description} to process.")
        return [], []
        
    logging.info(f"* Processing {len(work_items)} {batch_description} with connection pool management...")
    
    # Determine threading strategy from environment variables
    if FAST_MODE_USE_CONNECTION_AWARE_THREADING:
        # Connection-aware threading: limit threads to connection pool capacity
        max_threads = FAST_MODE_MAX_CONCURRENT_CONNECTIONS
        threading_mode = "connection-aware"
        logging.info(f"! Connection-aware threading: Using {max_threads} threads (respects connection pool limit)")
    else:
        # CPU-aware threading: use maximum CPU threads available
        max_threads = os.cpu_count() or FAST_MODE_FALLBACK_THREADS
        threading_mode = "CPU-aware"
        logging.info(f"! CPU-aware threading: Using {max_threads} threads (maximum CPU utilization)")
    
    # Create a semaphore to limit concurrent API connections
    connection_semaphore = threading.Semaphore(FAST_MODE_MAX_CONCURRENT_CONNECTIONS)
    logging.info(f"* Connection pool protection: Maximum {FAST_MODE_MAX_CONCURRENT_CONNECTIONS} concurrent API calls")
    
    # Calculate optimal batch size using configurable devices per thread
    devices_per_thread = FAST_MODE_DEVICES_PER_THREAD
    batch_size = max_threads * devices_per_thread
    successful_results = []
    failed_items = []
    
    # Process items in batches
    for batch_index in range(0, len(work_items), batch_size):
        try:
            batch = work_items[batch_index:batch_index + batch_size]
            batch_number = (batch_index // batch_size) + 1
            total_batches = (len(work_items) + batch_size - 1) // batch_size
            logging.info(f"! Processing batch {batch_number}/{total_batches} ({len(batch)} {batch_description}, ~{len(batch)/max_threads:.0f} per thread)")
            with ThreadPoolExecutor(max_workers=max_threads) as executor:
                # Submit batch tasks with connection semaphore
                future_to_item = {
                    executor.submit(worker_function, item, connection_semaphore): item 
                    for item in batch
                }
                batch_desc = f"Batch {batch_number}/{total_batches}"
                # Strategy: optionally avoid as_completed entirely using wait loop for stability
                use_wait_loop = True  # Default to True after repeated environment anomalies
                first_result_logged = False
                if use_wait_loop:
                    pending = set(future_to_item.keys())
                    pbar_total = len(pending)
                    with tqdm(total=pbar_total, desc=batch_desc, unit=batch_description.rstrip('s')) as pbar:
                        while pending:
                            done, pending = wait(pending, return_when=FIRST_COMPLETED)
                            for future in done:
                                item = future_to_item[future]
                                try:
                                    result = future.result()
                                    if result:
                                        successful_results.append(result)
                                        if not first_result_logged:
                                            logging.debug(f"! First future result type: {type(result)}")
                                            first_result_logged = True
                                    else:
                                        failed_items.append(item)
                                except Exception as e:
                                    logging.error(f"! Future exception for {batch_description.rstrip('s')} {item}: {e}")
                                    failed_items.append(item)
                                finally:
                                    try:
                                        pbar.update(1)
                                    except Exception as upd_err:
                                        logging.error(f"! Progress bar update failed: {upd_err}")
                else:
                    # Retained fallback path (not expected to be used now)
                    for future in as_completed(future_to_item):
                        item = future_to_item[future]
                        try:
                            result = future.result()
                            if result:
                                successful_results.append(result)
                            else:
                                failed_items.append(item)
                        except Exception as e:
                            logging.error(f"! Future exception for {batch_description.rstrip('s')} {item}: {e}")
                            failed_items.append(item)
        except Exception as batch_exc:
            # Log detailed context about the batch to aid debugging (e.g., dict+float arithmetic errors outside futures)
            logging.error(f"! Batch-level exception in execute_with_connection_pool_management: {batch_exc}")
            logging.error(f"! Batch context: batch_index={batch_index}, batch_size={batch_size}, max_threads={max_threads}, threading_mode={threading_mode}")
            try:
                import traceback as _tb2
                formatted = ''.join(_tb2.format_exception(type(batch_exc), batch_exc, batch_exc.__traceback__))
                for line in formatted.rstrip().splitlines():
                    logging.error(line)
            except Exception as trace_log_err:
                logging.error(f"! Failed to log batch exception traceback: {trace_log_err}")
            # Re-raise to allow outer handlers / global excepthook to capture as well
            raise
    
    # Handle retries if retry function is provided
    if failed_items and retry_function:
        logging.info(f"! Retrying {len(failed_items)} failed {batch_description}...")
        retry_results, still_failed = retry_function(failed_items, connection_semaphore)
        successful_results.extend(retry_results)
        failed_items = still_failed
    
    logging.info(f"! Processed {len(successful_results)} {batch_description} successfully, {len(failed_items)} failed")
    return successful_results, failed_items

def prompt_select_device_id_from_inventory(site_id: str, device_type: str = "all", csv_filename: str = "SiteInventory.csv") -> Optional[str]:
    """
    Prompts the user to select a device by index or name from the device inventory at a given site.
    Returns the corresponding device ID, or None if not found.
    
    SECURITY: Always fetch all device types from API first (type=all), then filter locally
    to avoid Mist API's default behavior of only returning APs.
    """
    # IMPORTANT: Always use type=all to get all device types (APs, switches, gateways)
    # The Mist API defaults to only APs unless explicitly specified
    rawdata = mistapi.api.v1.sites.devices.listSiteDevices(apisession, site_id, type="all").data
    if not rawdata:
        print("No devices found for the selected site.")
        logging.warning(f"No devices found for site_id: {site_id}")
        return None

    # Filter devices locally based on requested device types
    if device_type != "all":
        requested_types = [dtype.strip() for dtype in device_type.split(",")]
        filtered_data = []
        for device in rawdata:
            device_device_type = device.get("type", "").lower()
            if device_device_type in requested_types:
                filtered_data.append(device)
        rawdata = filtered_data
        
        if not rawdata:
            print(f"No devices of type '{device_type}' found at the selected site.")
            logging.warning(f"No devices of type '{device_type}' found for site_id: {site_id}")
            return None

    # Sort, flatten, and sanitize the inventory data for display and CSV export
    inventory = sorted(rawdata, key=lambda x: x.get("model", ""))
    inventory = flatten_nested_fields_in_list(inventory)
    inventory = escape_multiline_strings_for_csv(inventory)
    DataExporter.save_data_to_output(inventory, csv_filename)
    logging.info(f"Device inventory for site_id {site_id} written to {csv_filename}")

    # Prepare PrettyTable for user selection
    table = PrettyTable()
    table.field_names = ["Index", "name", "mac", "model", "serial"]
    index_to_device = {}
    name_to_device = {}

    # Populate the table and lookup dictionaries
    for idx, item in enumerate(inventory):
        table.add_row([idx, item.get("name", ""), item.get("mac", ""), item.get("model", ""), item.get("serial", "")])
        index_to_device[idx] = item
        name_to_device[item.get("name", "")] = item

    print(table)
    logging.info("Displayed device selection table to user.")

    user_input = input("Enter the index or name of the device to view device: ").strip()
    logging.debug(f"User input for device selection: {user_input}")

    # Try index selection
    if user_input.isdigit():
        idx = int(user_input)
        if idx in index_to_device:
            device_id = index_to_device[idx].get("id")
            logging.info(f"User selected device by index: {idx} (device_id: {device_id})")
            return device_id
        else:
            logging.error(" Invalid index.")
            return None

    # Try name selection
    if user_input in name_to_device:
        device_id = name_to_device[user_input].get("id")
        logging.info(f"User selected device by name: {user_input} (device_id: {device_id})")
        return device_id

    logging.error(" Device not found by name or index.")
    return None

def show_site_device_inventory(site_id, device_type="all", csv_filename="SiteInventory.csv"):
    """
    Fetches and displays the device inventory for a given site.
    - site_id: The ID of the site to fetch inventory for.
    - device_type: The type of device to filter (default: "all").
    - csv_filename: The filename to write the inventory CSV to.
    
    SECURITY: Always fetch all device types from API first (type=all), then filter locally
    to avoid Mist API's default behavior of only returning APs.
    """
    logging.info(f"Fetching device inventory for site_id={site_id}, device_type={device_type}")
    
    # IMPORTANT: Always use type=all to get all device types (APs, switches, gateways)
    # The Mist API defaults to only APs unless explicitly specified
    rawdata = mistapi.api.v1.sites.devices.listSiteDevices(apisession, site_id, type="all").data
    if not rawdata:
        print("No devices found for the selected site.")
        logging.warning(f"No devices found for site_id={site_id}")
        return

    # Filter devices locally based on requested device types
    if device_type != "all":
        requested_types = [dtype.strip() for dtype in device_type.split(",")]
        filtered_data = []
        for device in rawdata:
            device_device_type = device.get("type", "").lower()
            if device_device_type in requested_types:
                filtered_data.append(device)
        rawdata = filtered_data
        
        if not rawdata:
            print(f"No devices of type '{device_type}' found at the selected site.")
            logging.warning(f"No devices of type '{device_type}' found for site_id: {site_id}")
            return

    # Sort inventory by model for easier viewing
    inventory = sorted(rawdata, key=lambda x: x.get("model", ""))
    # Flatten nested fields for CSV and table compatibility
    inventory = flatten_nested_fields_in_list(inventory)
    # Escape multiline strings for CSV compatibility
    inventory = escape_multiline_strings_for_csv(inventory)
    # Get all unique fields for CSV/table columns
    fields = get_all_unique_dict_keys(inventory)
    # Write inventory to CSV
    DataExporter.save_data_to_output(inventory, csv_filename)
    logging.info(f"Device inventory written to {csv_filename} ({len(inventory)} rows)")

    # Prepare PrettyTable for display
    table = PrettyTable()
    table.field_names = fields

    # Attempt to sort the table by 'model' if present
    if "model" in fields:
        try:
            table.sortby = "model"
        except Exception as e:
            logging.warning(f"! Could not sort table by 'model': {e}")

    # Add each device as a row in the table
    for item in inventory:
        row = [item.get(field, "") for field in fields]
        table.add_row(row)

    # Log the table output for reference (debug mode only)
    logging.debug("\n" + table.get_string())

def prompt_select_site_id_from_csv(csv_file: str = "SiteList.csv") -> Optional[str]:
    """
    Prompts the user to select a site by index or name from SiteList.csv.
    Returns the corresponding site ID.
    """
    # Ensure the site list CSV is fresh or generate it if missing/stale
    check_and_generate_csv(csv_file, export_all_sites_to_csv)

    # Get the full path to the CSV file in the data directory
    csv_file_path = get_csv_file_path(csv_file)
    
    # Load the site list from CSV
    with open(csv_file_path, mode='r', encoding='utf-8') as file:
        reader = list(csv.DictReader(file))
        index_to_site = {i: row for i, row in enumerate(reader)}
        name_to_site = {row["name"]: row for row in reader if "name" in row}

    # Display available sites to the user
    print("\nAvailable Sites:")
    for idx, row in index_to_site.items():
        print(f"[{idx}] {row.get('name', 'Unnamed')}")

    user_input = input("\nEnter site index or name: ").strip()
    logging.debug(f"User input for site selection: {user_input}")

    # Try index selection
    if user_input.isdigit():
        idx = int(user_input)
        if idx in index_to_site:
            site_id = index_to_site[idx].get("id")
            print(f"! Selected site: {index_to_site[idx].get('name')} (ID: {site_id})")
            logging.info(f"User selected site by index: {idx} (site_id: {site_id})")
            return site_id
        else:
            print(" Invalid index.")
            logging.warning(f"Invalid site index entered: {idx}")
            return None

    # Try name selection
    if user_input in name_to_site:
        site_id = name_to_site[user_input].get("id")
        print(f"! Selected site: {user_input} (ID: {site_id})")
        logging.info(f"User selected site by name: {user_input} (site_id: {site_id})")
        return site_id

    print(" Site not found by name or index.")
    logging.warning(f"Site not found by name or index: {user_input}")
    return None

def prompt_and_log_site_selection() -> Optional[str]:
    """
    Prompts the user to select a site from the CSV list and logs the selection.
    """
    logging.info("Prompting user to select a site from SiteList.csv...")
    site_id = prompt_select_site_id_from_csv()
    if site_id:
        logging.info(f"! Selected site ID: {site_id}")
        # You can store or use the selected site_id as needed here
    else:
        logging.error(" No site selected. User may have entered an invalid value or cancelled the prompt.")
    
    # CRITICAL: Return the site_id so callers can use it
    return site_id

def prompt_site_selection() -> Optional[str]:
    """
    Prompts the user to select a site and returns the site_id.
    Uses the existing CSV-based site selection functionality.
    """
    return prompt_select_site_id_from_csv()

def prompt_device_selection(site_id: str, device_type: str = "all") -> Optional[str]:
    """
    Prompts the user to select a device from the specified site and returns the device_id.
    
    Args:
        site_id (str): The site ID to filter devices by
        device_type (str): Filter by device type ("all", "switch", "gateway", "ap")
    
    Returns:
        str: The selected device ID or None if no selection made
    """
    return prompt_select_device_id_from_inventory(site_id, device_type)

def prompt_select_ap_mac_from_site(site_id: str) -> Optional[str]:
    """
    Prompts the user to select an AP from the specified site and returns the AP MAC address.
    
    Args:
        site_id (str): The site ID to filter APs by
    
    Returns:
        str: The selected AP MAC address (normalized) or None if no selection made
    """
    logging.debug(f"Fetching APs for site: {site_id}")
    
    # Fetch all devices, filter for APs
    try:
        rawdata = mistapi.api.v1.sites.devices.listSiteDevices(apisession, site_id, type="ap").data
        if not rawdata:
            print("\n! No APs found at the selected site.")
            logging.warning(f"No APs found for site_id: {site_id}")
            return None
        
        logging.info(f"Found {len(rawdata)} APs at site")
        
        # Sort by name for easier selection
        aps = sorted(rawdata, key=lambda x: x.get("name", ""))
        
        # Prepare selection table
        table = PrettyTable()
        table.field_names = ["Index", "Name", "MAC", "Model", "Status"]
        index_to_ap = {}
        
        for idx, ap in enumerate(aps):
            table.add_row([
                idx,
                ap.get("name", "Unknown"),
                ap.get("mac", "Unknown"),
                ap.get("model", "Unknown"),
                ap.get("status", "Unknown")
            ])
            index_to_ap[idx] = ap
        
        print("\n" + "=" * 80)
        print(" SELECT ACCESS POINT")
        print("=" * 80)
        print(table)
        print("\nSpecial options:")
        print("  'all' - Select all APs (launches simultaneous captures)")
        
        user_input = safe_input("\nEnter the index number of the AP or 'all': ", context="ap_selection").strip()
        logging.debug(f"User input for AP selection: {user_input}")
        
        # Check for 'all' option
        if user_input.lower() == 'all':
            print(f"\n! Selected: All APs ({len(aps)} APs)")
            logging.info(f"User selected all APs: {len(aps)} APs")
            return 'ALL_APS'  # Special marker for all APs
        
        # Validate index selection
        if user_input.isdigit():
            idx = int(user_input)
            if idx in index_to_ap:
                ap_mac = index_to_ap[idx].get("mac")
                ap_name = index_to_ap[idx].get("name", "Unknown")
                print(f"\n! Selected AP: {ap_name} (MAC: {ap_mac})")
                logging.info(f"User selected AP by index: {idx} (name: {ap_name}, mac: {ap_mac})")
                return ap_mac
            else:
                print("\n! Invalid index")
                logging.error(f"Invalid AP index: {idx}")
                return None
        else:
            print("\n! Please enter a valid index number")
            logging.error(f"Non-numeric AP selection: {user_input}")
            return None
            
    except Exception as error:
        print(f"\n! Error fetching APs: {error}")
        logging.error(f"Exception in prompt_select_ap_mac_from_site: {error}", exc_info=True)
        return None

def get_all_ap_macs_from_site(site_id: str) -> list:
    """
    Fetch all AP MAC addresses from a site.
    
    Args:
        site_id (str): The site ID to fetch APs from
    
    Returns:
        list: List of AP MAC addresses, or empty list if error/none found
    """
    logging.debug(f"Fetching all AP MACs for site: {site_id}")
    
    try:
        rawdata = mistapi.api.v1.sites.devices.listSiteDevices(apisession, site_id, type="ap").data
        if not rawdata:
            logging.warning(f"No APs found for site_id: {site_id}")
            return []
        
        ap_macs = [ap.get("mac") for ap in rawdata if ap.get("mac")]
        logging.info(f"Found {len(ap_macs)} AP MACs at site")
        return ap_macs
        
    except Exception as error:
        logging.error(f"Exception in get_all_ap_macs_from_site: {error}", exc_info=True)
        return []

def prompt_select_client_mac_from_site(site_id: str) -> Optional[str]:
    """
    Prompts the user to select a client from currently connected clients at the site.
    
    Args:
        site_id (str): The site ID to fetch clients from
    
    Returns:
        str: The selected client MAC address (normalized) or None if no selection made
    """
    logging.debug(f"Fetching connected clients for site: {site_id}")
    
    try:
        # Fetch wireless clients using search endpoint (from clients module)
        wireless_response = mistapi.api.v1.sites.clients.searchSiteWirelessClients(apisession, site_id)
        wireless_clients = wireless_response.data.get('results', []) if hasattr(wireless_response.data, 'get') else wireless_response.data
        
        # Fetch wired clients using search endpoint (from separate wired_clients module)
        wired_response = mistapi.api.v1.sites.wired_clients.searchSiteWiredClients(apisession, site_id)
        wired_clients = wired_response.data.get('results', []) if hasattr(wired_response.data, 'get') else wired_response.data
        
        all_clients = []
        
        # Process wireless clients
        if wireless_clients:
            for client in wireless_clients:
                client['connection_type'] = 'Wireless'
                all_clients.append(client)
        
        # Process wired clients
        if wired_clients:
            for client in wired_clients:
                client['connection_type'] = 'Wired'
                all_clients.append(client)
        
        if not all_clients:
            print("\n! No connected clients found at the selected site.")
            logging.warning(f"No clients found for site_id: {site_id}")
            return None
        
        logging.info(f"Found {len(all_clients)} connected clients at site ({len(wireless_clients or [])} wireless, {len(wired_clients or [])} wired)")
        
        # Sort by hostname/username for easier selection
        all_clients = sorted(all_clients, key=lambda x: (x.get("hostname", ""), x.get("username", "")))
        
        # Prepare selection table
        table = PrettyTable()
        table.field_names = ["Index", "Hostname/User", "MAC", "IP", "Type", "SSID/VLAN"]
        table.max_width["Hostname/User"] = 25
        index_to_client = {}
        
        for idx, client in enumerate(all_clients):
            hostname = client.get("hostname", client.get("username", "Unknown"))[:25]
            mac = client.get("mac", "Unknown")
            ip = client.get("ip", "Unknown")
            conn_type = client.get("connection_type", "Unknown")
            
            # SSID for wireless, VLAN for wired
            if conn_type == "Wireless":
                network = client.get("ssid", "N/A")
            else:
                network = f"VLAN {client.get('vlan_id', 'N/A')}"
            
            table.add_row([
                idx,
                hostname,
                mac,
                ip,
                conn_type,
                network
            ])
            index_to_client[idx] = client
        
        print("\n" + "=" * 80)
        print(" SELECT CONNECTED CLIENT")
        print("=" * 80)
        print(f"  Found {len(all_clients)} connected clients")
        print("=" * 80)
        print(table)
        print("\nOptions:")
        print("  - Enter index number to select a client")
        print("  - Enter 'm' to manually type MAC address")
        print("  - Enter 'c' to cancel")
        
        user_input = safe_input("\nEnter your choice: ", context="client_selection").strip()
        logging.debug(f"User input for client selection: {user_input}")
        
        # Check for manual entry
        if user_input.lower() == 'm':
            manual_mac = safe_input("Enter client MAC address: ", context="manual_mac")
            logging.info(f"User chose manual MAC entry: {manual_mac}")
            return manual_mac
        
        # Check for cancel
        if user_input.lower() == 'c':
            logging.info("User cancelled client selection")
            return None
        
        # Validate index selection
        if user_input.isdigit():
            idx = int(user_input)
            if idx in index_to_client:
                client_mac = index_to_client[idx].get("mac")
                client_hostname = index_to_client[idx].get("hostname", index_to_client[idx].get("username", "Unknown"))
                conn_type = index_to_client[idx].get("connection_type", "Unknown")
                print(f"\n! Selected: {client_hostname} ({conn_type}) - MAC: {client_mac}")
                logging.info(f"User selected client by index: {idx} (hostname: {client_hostname}, mac: {client_mac}, type: {conn_type})")
                return client_mac
            else:
                print("\n! Invalid index")
                logging.error(f"Invalid client index: {idx}")
                return None
        else:
            print("\n! Please enter a valid index number, 'm' for manual, or 'c' to cancel")
            logging.error(f"Invalid client selection input: {user_input}")
            return None
            
    except Exception as error:
        print(f"\n! Error fetching clients: {error}")
        logging.error(f"Exception in prompt_select_client_mac_from_site: {error}", exc_info=True)
        return None

def prompt_select_gateway_mac_from_site(site_id: str) -> Optional[str]:
    """
    Prompts the user to select a gateway from the specified site and returns the gateway MAC address.
    
    Args:
        site_id (str): The site ID to filter gateways by
    
    Returns:
        str: The selected gateway MAC address (normalized) or None if no selection made
    """
    logging.debug(f"Fetching gateways for site: {site_id}")
    
    # Fetch all devices, filter for gateways
    try:
        rawdata = mistapi.api.v1.sites.devices.listSiteDevices(apisession, site_id, type="gateway").data
        if not rawdata:
            print("\n! No gateways found at the selected site.")
            logging.warning(f"No gateways found for site_id: {site_id}")
            return None
        
        logging.info(f"Found {len(rawdata)} gateways at site")
        
        # Sort by name for easier selection
        gateways = sorted(rawdata, key=lambda x: x.get("name", ""))
        
        # Prepare selection table
        table = PrettyTable()
        table.field_names = ["Index", "Name", "MAC", "Model", "Status"]
        index_to_gateway = {}
        
        for idx, gateway in enumerate(gateways):
            table.add_row([
                idx,
                gateway.get("name", "Unknown"),
                gateway.get("mac", "Unknown"),
                gateway.get("model", "Unknown"),
                gateway.get("status", "Unknown")
            ])
            index_to_gateway[idx] = gateway
        
        print("\n" + "=" * 80)
        print(" SELECT GATEWAY")
        print("=" * 80)
        print(table)
        
        user_input = safe_input("\nEnter the index number of the gateway: ", context="gateway_selection").strip()
        logging.debug(f"User input for gateway selection: {user_input}")
        
        # Validate index selection
        if user_input.isdigit():
            idx = int(user_input)
            if idx in index_to_gateway:
                gateway_mac = index_to_gateway[idx].get("mac")
                gateway_name = index_to_gateway[idx].get("name", "Unknown")
                print(f"\n! Selected gateway: {gateway_name} (MAC: {gateway_mac})")
                logging.info(f"User selected gateway by index: {idx} (name: {gateway_name}, mac: {gateway_mac})")
                return gateway_mac
            else:
                print("\n! Invalid index")
                logging.error(f"Invalid gateway index: {idx}")
                return None
        else:
            print("\n! Please enter a valid index number")
            logging.error(f"Non-numeric gateway selection: {user_input}")
            return None
            
    except Exception as error:
        print(f"\n! Error fetching gateways: {error}")
        logging.error(f"Exception in prompt_select_gateway_mac_from_site: {error}", exc_info=True)
        return None

def prompt_select_switch_mac_from_site(site_id: str) -> Optional[str]:
    """
    Prompts the user to select a switch from the specified site and returns the switch MAC address.
    
    Args:
        site_id (str): The site ID to filter switches by
    
    Returns:
        str: The selected switch MAC address (normalized) or None if no selection made
    """
    logging.debug(f"Fetching switches for site: {site_id}")
    
    # Fetch all devices, filter for switches
    try:
        rawdata = mistapi.api.v1.sites.devices.listSiteDevices(apisession, site_id, type="switch").data
        if not rawdata:
            print("\n! No switches found at the selected site.")
            logging.warning(f"No switches found for site_id: {site_id}")
            return None
        
        logging.info(f"Found {len(rawdata)} switches at site")
        
        # Sort by name for easier selection
        switches = sorted(rawdata, key=lambda x: x.get("name", ""))
        
        # Prepare selection table
        table = PrettyTable()
        table.field_names = ["Index", "Name", "MAC", "Model", "Status"]
        index_to_switch = {}
        
        for idx, switch in enumerate(switches):
            table.add_row([
                idx,
                switch.get("name", "Unknown"),
                switch.get("mac", "Unknown"),
                switch.get("model", "Unknown"),
                switch.get("status", "Unknown")
            ])
            index_to_switch[idx] = switch
        
        print("\n" + "=" * 80)
        print(" SELECT SWITCH")
        print("=" * 80)
        print(table)
        
        user_input = safe_input("\nEnter the index number of the switch: ", context="switch_selection").strip()
        logging.debug(f"User input for switch selection: {user_input}")
        
        # Validate index selection
        if user_input.isdigit():
            idx = int(user_input)
            if idx in index_to_switch:
                switch_mac = index_to_switch[idx].get("mac")
                switch_name = index_to_switch[idx].get("name", "Unknown")
                print(f"\n! Selected switch: {switch_name} (MAC: {switch_mac})")
                logging.info(f"User selected switch by index: {idx} (name: {switch_name}, mac: {switch_mac})")
                return switch_mac
            else:
                print("\n! Invalid index")
                logging.error(f"Invalid switch index: {idx}")
                return None
        else:
            print("\n! Please enter a valid index number")
            logging.error(f"Non-numeric switch selection: {user_input}")
            return None
            
    except Exception as error:
        print(f"\n! Error fetching switches: {error}")
        logging.error(f"Exception in prompt_select_switch_mac_from_site: {error}", exc_info=True)
        return None

def _expand_port_range_string(port_range_string: str) -> list:
    """
    Expands a port range string from device config into individual port names.
    
    Examples:
        "ge-0/0/0" -> ["ge-0/0/0"]
        "ge-0/0/0-2" -> ["ge-0/0/0", "ge-0/0/1", "ge-0/0/2"]
        "ge-0/0/0-2, ge-0/1/2-3" -> ["ge-0/0/0", "ge-0/0/1", "ge-0/0/2", "ge-0/1/2", "ge-0/1/3"]
        "mge-0/2/0, xe-0/1/0-3" -> ["mge-0/2/0", "xe-0/1/0", "xe-0/1/1", "xe-0/1/2", "xe-0/1/3"]
    
    Args:
        port_range_string (str): Port name or range specification from port_config
    
    Returns:
        list: List of individual port names
    """
    import re
    expanded_ports = []
    
    # Split by comma to handle multiple ranges
    port_parts = [part.strip() for part in port_range_string.split(',')]
    
    for port_part in port_parts:
        # Check if this is a range (e.g., "ge-0/0/0-2")
        if '-' in port_part:
            # Try to match pattern like "ge-0/0/0-2"
            match = re.match(r'^(.+/)(\d+)-(\d+)$', port_part)
            if match:
                prefix = match.group(1)  # e.g., "ge-0/0/"
                start_num = int(match.group(2))  # e.g., 0
                end_num = int(match.group(3))    # e.g., 2
                
                # Expand the range
                for port_num in range(start_num, end_num + 1):
                    expanded_ports.append(f"{prefix}{port_num}")
            else:
                # Couldn't parse as range, treat as single port
                expanded_ports.append(port_part)
        else:
            # Single port name
            expanded_ports.append(port_part)
    
    return expanded_ports

def prompt_select_ports_from_device(site_id: str, device_mac: str, device_type: str = "switch", return_available: bool = False):
    """
    Prompts the user to select one or more ports from a device (switch or gateway).
    Displays port status information from device stats.
    
    Args:
        site_id (str): The site ID where the device is located
        device_mac (str): The MAC address of the device
        device_type (str): Type of device ("switch" or "gateway") for display purposes
        return_available (bool): If True, return tuple of (selected_ports, available_ports)
    
    Returns:
        list or tuple: List of selected port names (e.g., ["ge-0/0/0", "ge-0/0/1"]) or empty list for all ports, or None on error.
                      If return_available=True, returns (selected_ports, available_ports) where available_ports is list of (name, data) tuples.
    """
    logging.debug(f"Fetching port information for {device_type} {device_mac} at site {site_id}")
    
    try:
        # Normalize the input MAC for comparison (remove colons, lowercase)
        normalized_input_mac = str(device_mac).replace(":", "").replace("-", "").lower()
        logging.debug(f"Normalized input MAC for comparison: {normalized_input_mac}")
        
        # First get device ID from MAC
        devices_response = mistapi.api.v1.sites.devices.listSiteDevices(apisession, site_id, type=device_type)
        devices = devices_response.data
        
        device = None
        for dev in devices:
            dev_mac = dev.get("mac", "")
            # Normalize device MAC for comparison
            normalized_dev_mac = str(dev_mac).replace(":", "").replace("-", "").lower()
            logging.debug(f"Comparing device {dev.get('name', 'Unknown')}: {dev_mac} (normalized: {normalized_dev_mac})")
            
            if normalized_dev_mac == normalized_input_mac:
                device = dev
                logging.debug(f"MAC match found for device: {dev.get('name', 'Unknown')}")
                break
        
        if not device:
            print(f"\n! Could not find {device_type} with MAC {device_mac}")
            logging.error(f"Device not found with MAC: {device_mac} (normalized: {normalized_input_mac})")
            logging.error(f"Available devices: {[d.get('mac') for d in devices]}")
            return None
        
        device_id = device.get("id")
        device_name = device.get("name", "Unknown")
        
        # Get device stats which includes port information
        logging.debug(f"Fetching device stats for device_id: {device_id}")
        # Get port statistics using the appropriate API for the device type
        # Switches and Gateways: Use searchSiteSwOrGwPorts for detailed port-level stats
        # APs: Use getSiteDeviceStats which includes port_stat
        port_stat = {}
        
        if device_type in ["switch", "gateway"]:
            # For switches/gateways, use dedicated port search API
            logging.info(f"Fetching switch/gateway port stats using searchSiteSwOrGwPorts for device {device_id}")
            try:
                ports_search_response = mistapi.api.v1.sites.stats.searchSiteSwOrGwPorts(
                    apisession, 
                    site_id,
                    mac=device_mac,  # Filter by device MAC
                    limit=1000  # Get all ports
                )
                ports_results = ports_search_response.data.get("results", [])
                logging.info(f"Retrieved {len(ports_results)} port stat entries from searchSiteSwOrGwPorts")
                
                # Convert array of port objects to dict keyed by port_id
                for port_obj in ports_results:
                    port_id = port_obj.get("port_id")
                    if port_id:
                        port_stat[port_id] = port_obj
                        
                if port_stat:
                    logging.info(f"Successfully converted {len(port_stat)} switch/gateway ports to dict format")
                    # Log sample port for debugging
                    if port_stat:
                        sample_port = list(port_stat.keys())[0]
                        sample_data = port_stat[sample_port]
                        logging.debug(f"Sample port '{sample_port}' data: speed={sample_data.get('speed')}, full_duplex={sample_data.get('full_duplex')}, up={sample_data.get('up')}")
                else:
                    logging.warning(f"searchSiteSwOrGwPorts returned no port data for device {device_mac}")
                    
            except Exception as port_search_error:
                logging.error(f"Error fetching switch/gateway port stats: {port_search_error}")
                logging.debug(f"Traceback: ", exc_info=True)
        else:
            # For APs, use getSiteDeviceStats which includes port_stat
            logging.info(f"Fetching AP port stats using getSiteDeviceStats for device {device_id}")
            stats_response = mistapi.api.v1.sites.stats.getSiteDeviceStats(apisession, site_id, device_id)
            stats_data = stats_response.data
            
            if "port_stat" in stats_data:
                port_stat = stats_data.get("port_stat", {})
                logging.info(f"Found port_stat (AP-style) with {len(port_stat)} ports")
            else:
                logging.warning(f"No port_stat found in AP stats for device {device_id}")
        
        # Also get device config for port profiles and descriptions
        # This is needed regardless of whether port_stat exists
        logging.debug(f"Fetching device config for port profiles and descriptions")
        try:
            device_config_response = mistapi.api.v1.sites.devices.getSiteDevice(apisession, site_id, device_id)
            device_config = device_config_response.data
            port_config = device_config.get("port_config", {})
        except Exception as cfg_error:
            logging.warning(f"Could not fetch device config for port details: {cfg_error}")
            port_config = {}
        
        # Build a mapping from individual port names to their config (handles port ranges)
        port_to_config = {}
        if port_config:
            logging.info(f"Building port_to_config mapping from {len(port_config)} port_config entries")
            for port_range_key, cfg in port_config.items():
                expanded_ports = _expand_port_range_string(port_range_key)
                logging.debug(f"Port config key '{port_range_key}' expands to {len(expanded_ports)} ports, has profile: {cfg.get('port_profile', 'NONE')}")
                for individual_port in expanded_ports:
                    port_to_config[individual_port] = cfg
            logging.info(f"Created port_to_config mapping with {len(port_to_config)} individual port entries")
        else:
            logging.warning("No port_config available to build mapping")
        
        if not port_stat:
            logging.warning(f"No port_stat found in device stats for device {device_id}")
            logging.debug(f"Attempting to get port configuration from device config instead")
            
            # Fallback: Try to get port information from device configuration
            # Note: port_config was already fetched above, so we can use it here
            try:
                if port_config:
                    logging.info(f"Found {len(port_config)} configured port entries in device config")
                    # Use port_config as source instead of port_stat
                    # Convert port_config format to port_stat-like format for consistency
                    # NOTE: port_config keys may be port RANGES like "ge-0/0/0-2, ge-0/1/2-3"
                    # We need to expand these to individual port names for display and API submission
                    port_stat = {}
                    for port_range_key, port_cfg in port_config.items():
                        # Expand port ranges to individual port names
                        expanded_ports = _expand_port_range_string(port_range_key)
                        logging.debug(f"Expanded port range '{port_range_key}' to {len(expanded_ports)} ports: {expanded_ports}")
                        
                        for individual_port in expanded_ports:
                            # Create a port_stat-like entry from port_config
                            # NOTE: These are CONFIGURED values, not actual operational stats
                            # When stats are unavailable, we show config as a fallback
                            usage = port_cfg.get("usage", "")
                            
                            # Determine if port is up based on usage
                            port_up = usage not in ["disabled", "", None]
                            
                            # Get speed from port_config (configured, not actual)
                            speed_value = port_cfg.get("speed", "N/A")
                            
                            # Get duplex from port_config (configured, not actual)
                            duplex_value = port_cfg.get("duplex", "N/A")
                            full_duplex = duplex_value == "full" or duplex_value == "auto"
                            
                            port_stat[individual_port] = {
                                "up": port_up,
                                "speed": speed_value,
                                "full_duplex": full_duplex,
                                "duplex": duplex_value,
                                "_fallback": True  # Flag to indicate this is config, not stats
                            }
                    
                    logging.info(f"Expanded to {len(port_stat)} individual ports")
                else:
                    print(f"\n! No port information available for {device_type}: {device_name}")
                    print(f"  This device may be offline or not yet reporting statistics.")
                    logging.warning(f"No port_stat or port_config found for device {device_id}")
                    return None
                    
            except Exception as config_error:
                print(f"\n! No port information available for {device_type}: {device_name}")
                print(f"  This device may be offline or not yet reporting statistics.")
                logging.error(f"Could not fetch device config: {config_error}")
                return None
        
        # NOTE: We do NOT enrich port_stat with config data for speed/duplex
        # Speed and duplex should come from live stats (actual operational values)
        # Only port_profile and description come from config
        
        # Filter and sort ports (exclude management/internal ports and DOWN ports)
        # Management/service ports to exclude: fxp, em, me, vme, irb, lo, vlan, bme, cbp, jsrv, pip
        exclude_prefixes = ["fxp", "em", "me", "vme", "irb", "lo", "vlan", "bme", "cbp", "jsrv", "pip"]
        available_ports = []
        
        for port_name, port_info in port_stat.items():
            # Skip internal/management/service ports
            if any(port_name.startswith(prefix) for prefix in exclude_prefixes):
                logging.debug(f"Excluding management/service port: {port_name}")
                continue
            
            # Skip DOWN ports to keep table focused on active connections
            port_up = port_info.get("up", False)
            if not port_up:
                logging.debug(f"Excluding DOWN port: {port_name}")
                continue
                
            available_ports.append((port_name, port_info))
        
        if not available_ports:
            print(f"\n! No network ports available for {device_type}: {device_name}")
            logging.warning(f"No user-facing ports found for device {device_id}")
            return None
        
        # Sort ports naturally (ge-0/0/0, ge-0/0/1, etc.)
        def natural_sort_key(port_tuple):
            port_name = port_tuple[0]
            # Extract numbers for sorting
            import re
            parts = re.split(r'(\d+)', port_name)
            return [int(part) if part.isdigit() else part for part in parts]
        
        available_ports = sorted(available_ports, key=natural_sort_key)
        
        # Prepare selection table with profile and description
        table = PrettyTable()
        table.field_names = ["Index", "Port Name", "Status", "Speed", "Duplex", "Profile", "Description"]
        table.max_width = 120  # Allow wider table for descriptions
        table.align["Description"] = "l"  # Left-align descriptions
        table.align["Profile"] = "l"  # Left-align profiles
        index_to_port = {}
        
        for idx, (port_name, port_info) in enumerate(available_ports):
            port_up = port_info.get("up", False)
            status = "UP" if port_up else "DOWN"
            
            # Get speed - handle various formats
            speed = port_info.get("speed", "N/A")
            if isinstance(speed, str):
                # Handle string values like "auto", "1g", "10g"
                speed_str = speed.upper()
                if speed_str == "AUTO":
                    speed_str = "Auto"
                elif speed_str.endswith("G"):
                    # Convert "1g" to "1000 Mbps", "10g" to "10000 Mbps"
                    try:
                        gig_value = int(speed_str[:-1])
                        speed_str = f"{gig_value * 1000} Mbps"
                    except ValueError:
                        speed_str = speed
            elif isinstance(speed, (int, float)) and speed > 0:
                speed_str = f"{speed} Mbps"
            else:
                speed_str = "N/A"
            
            # Get duplex - show actual mode (Full/Half/Auto)
            duplex_value = port_info.get("duplex", "")
            if duplex_value:
                if duplex_value == "full":
                    duplex_str = "Full"
                elif duplex_value == "half":
                    duplex_str = "Half"
                elif duplex_value == "auto":
                    duplex_str = "Auto"
                else:
                    duplex_str = str(duplex_value).capitalize()
            else:
                # Fallback to full_duplex boolean
                full_duplex = port_info.get("full_duplex", False)
                duplex_str = "Full" if full_duplex else "Half"
            
            # Get port profile and description from config mapping
            port_cfg = port_to_config.get(port_name, {})
            port_profile = port_cfg.get("port_profile", "N/A")
            port_description = port_cfg.get("description", "")
            
            # Log if we have config but no profile
            if port_cfg and port_profile == "N/A":
                logging.debug(f"Port {port_name} has config but no port_profile field. Config keys: {list(port_cfg.keys())}")
            
            # Truncate description if too long
            if len(port_description) > 30:
                port_description = port_description[:27] + "..."
            if not port_description:
                port_description = "-"
            
            table.add_row([
                idx,
                port_name,
                status,
                speed_str,
                duplex_str,
                port_profile,
                port_description
            ])
            index_to_port[idx] = port_name
        
        print("\n" + "=" * 80)
        print(f" SELECT PORTS FROM {device_type.upper()}: {device_name}")
        print("=" * 80)
        print(f"  Device MAC: {device_mac}")
        print(f"  Available Ports: {len(available_ports)}")
        
        # Check if we're showing fallback data
        using_fallback = any(port_info.get("_fallback", False) for _, port_info in available_ports)
        if using_fallback:
            print(f"  NOTE: Speed/Duplex showing configured values (device stats unavailable)")
        
        print("=" * 80)
        print(table)
        print("\n" + "!" * 80)
        print("  API LIMITATION: Maximum 6 ports per capture")
        print("!" * 80)
        print("\nPort Selection Options:")
        print("  - Enter a single index (e.g., '0') for one port")
        print("  - Enter multiple indices separated by commas (e.g., '0,2,5')")
        print("  - Enter a range (e.g., '0-3' for ports 0, 1, 2, 3)")
        if len(available_ports) <= 6:
            print("  - Press Enter with no input to capture on ALL ports (default)")
        else:
            print("  - Press Enter with no input to capture on ALL ports (NOT AVAILABLE - exceeds 6 port limit)")
        print("  - Enter 'c' to cancel")
        
        user_input = safe_input("\nEnter your choice (up to 6 ports): ", context="port_selection", allow_empty=True).strip()
        logging.debug(f"User input for port selection: {user_input}")
        
        # Handle cancel
        if user_input.lower() == 'c':
            print("\n! Port selection cancelled")
            logging.info("Port selection cancelled by user")
            return None
        
        # Handle empty input or 'all' - check if within 6-port limit
        if not user_input or user_input.lower() == 'all':
            if len(available_ports) > 6:
                print(f"\n! ERROR: Cannot select all {len(available_ports)} ports - API maximum is 6 ports per capture")
                print(f"  Please select up to 6 specific ports from the list above")
                logging.error(f"User attempted to select all {len(available_ports)} ports, exceeds API limit of 6")
                return None
            
            print(f"\n! Selected ALL {len(available_ports)} ports for capture")
            logging.info(f"User selected all {len(available_ports)} ports (within 6-port limit)")
            if return_available:
                return [], available_ports
            return []  # Empty list signals "all ports"
        
        # Parse user selection
        selected_indices = set()
        
        try:
            # Split by commas
            parts = user_input.split(',')
            for part in parts:
                part = part.strip()
                
                # Check for range (e.g., "0-3")
                if '-' in part:
                    range_parts = part.split('-')
                    if len(range_parts) == 2:
                        start_idx = int(range_parts[0].strip())
                        end_idx = int(range_parts[1].strip())
                        for i in range(start_idx, end_idx + 1):
                            if i in index_to_port:
                                selected_indices.add(i)
                            else:
                                print(f"\n! Warning: Index {i} is out of range, skipping")
                                logging.warning(f"Invalid port index in range: {i}")
                else:
                    # Single index
                    idx = int(part)
                    if idx in index_to_port:
                        selected_indices.add(idx)
                    else:
                        print(f"\n! Warning: Index {idx} is out of range, skipping")
                        logging.warning(f"Invalid port index: {idx}")
            
            if not selected_indices:
                print("\n! No valid ports selected")
                logging.error("No valid port indices provided")
                return None
            
            # Convert indices to port names
            selected_ports = [index_to_port[idx] for idx in sorted(selected_indices)]
            
            # Validate 6-port API limit
            if len(selected_ports) > 6:
                print(f"\n! ERROR: Selected {len(selected_ports)} ports, but API maximum is 6 ports per capture")
                print(f"  Please refine your selection to 6 or fewer ports")
                logging.error(f"User selected {len(selected_ports)} ports, exceeds API limit of 6")
                return None
            
            print(f"\n! Selected {len(selected_ports)} port(s): {', '.join(selected_ports)}")
            logging.info(f"User selected ports: {selected_ports}")
            if return_available:
                return selected_ports, available_ports
            return selected_ports
            
        except ValueError as value_error:
            print(f"\n! Invalid input format: {value_error}")
            logging.error(f"Port selection parse error: {value_error}")
            return None
            
    except Exception as error:
        print(f"\n! Error fetching port information: {error}")
        logging.error(f"Exception in prompt_select_ports_from_device: {error}", exc_info=True)
        return None

def export_site_specific_data(api_call, data_type, sort_key="name", **api_kwargs):
    """
    Generic function to export site-specific data to CSV.
    
    Args:
        api_call: The mistapi function to call
        data_type: Description of the data type (e.g., "port stats", "clients")
        sort_key: Field to sort results by
        **api_kwargs: Additional arguments to pass to the API call
    
    Returns:
        None
    """
    logging.info(f"Starting export of site {data_type}...")
    
    # Get site selection
    site_id = prompt_site_selection()
    if not site_id:
        logging.error("No site selected. Exiting.")
        return
    
    # Get site name for display
    try:
        response = mistapi.api.v1.orgs.sites.listOrgSites(apisession, get_cached_or_prompted_org_id())
        sites = mistapi.get_all(response=response, mist_session=apisession)
        site_name = next((site["name"] for site in sites if site["id"] == site_id), site_id)
    except Exception as e:
        logging.error(f"Error getting site name: {e}")
        site_name = site_id
    
    logging.info(f"Exporting {data_type} for site: {site_name}")
    
    # Create filename from data_type
    safe_data_type = data_type.replace(" ", "").replace("-", "").title()
    safe_site_name = site_name.replace(" ", "_").replace("-", "_")
    filename = f"Site{safe_data_type}_{safe_site_name}.csv"
    
    # For site-specific API calls, we need to use a custom approach since
    # fetch_and_display_api_data expects org_id as the second parameter
    try:
        logging.debug(f"Making site-specific API call: {api_call.__name__} with site_id: {site_id}")
        
        # Try to determine if the API function supports 'limit' parameter
        # Use introspection to check function signature
        try:
            sig = inspect.signature(api_call)
            supports_limit = 'limit' in sig.parameters
        except Exception:
            # If introspection fails, assume limit is supported (safer default for most APIs)
            supports_limit = True
        
        # Call API with or without limit parameter based on support
        if supports_limit:
            response = api_call(apisession, site_id, limit=1000, **api_kwargs)
        else:
            logging.debug(f"API function {api_call.__name__} does not support 'limit' parameter")
            response = api_call(apisession, site_id, **api_kwargs)
        
        rawdata = mistapi.get_all(response=response, mist_session=apisession)
        if rawdata is None:
            logging.warning(f"! No data returned from API for {data_type} at site {site_name}. Skipping.")
            return

        logging.info(f"Fetched {len(rawdata)} raw records for {data_type} from site {site_name}.")

        # Sort data if a sort key is provided
        if sort_key:
            rawdata = sorted(rawdata, key=lambda x: x.get(sort_key, ""))

        # Flatten nested fields for CSV compatibility
        data = flatten_nested_fields_in_list(rawdata)
        
        # Escape multiline strings for CSV
        data = escape_multiline_strings_for_csv(data)

        # Write processed data to output
        DataExporter.save_data_to_output(data, filename)
        
        # Determine the full file path for console output (matches CSV writer logic)
        if not os.path.dirname(filename):
            full_file_path = os.path.join("data", filename)
        else:
            full_file_path = filename
            
        print(f"! {len(data)} records exported to {full_file_path}")
        logging.info(f"Site {data_type} data written to {filename} ({len(data)} rows).")

        # Display the data in a table (only in debug mode, otherwise just log summary)
        if is_debug_mode():
            fields = get_all_unique_dict_keys(data)
            table = PrettyTable()
            table.field_names = fields
            table.valign = "t"
            for item in tqdm(data, desc="Processing", unit="record"):
                row = [item.get(field, "") for field in table.field_names]
                table.add_row(row)
            print(table)
            logging.debug("Site data displayed in table format (debug mode).")
        else:
            logging.info(f"Site {data_type} export completed - {len(data)} records saved to {filename}.")
        
    except Exception as e:
        logging.error(f"! Error during site {data_type} export for {site_name}: {e}")
        raise

def export_org_specific_data(api_call, data_type, sort_key="name", **api_kwargs):
    """
    Generic function to export organization-specific data to CSV.
    
    Args:
        api_call: The mistapi function to call
        data_type: Description of the data type (e.g., "licenses", "templates")
        sort_key: Field to sort results by
        **api_kwargs: Additional arguments to pass to the API call
    
    Returns:
        None
    """
    logging.info(f"Starting export of organization {data_type}...")
    
    # Create filename from data_type
    safe_data_type = data_type.replace(" ", "").replace("-", "").title()
    filename = f"Org{safe_data_type}.csv"
    
    fetch_and_display_api_data(
        title=f"Organization {data_type.title()}:",
        api_call=api_call,
        filename=filename,
        sort_key=sort_key,
        limit=1000,
        **api_kwargs
    )

def export_open_org_alarms_to_csv():
    """
    Fetches all open organization alarms from the past 24 hours and writes them to OrgAlarms.csv.
    """
    logging.debug("ENTRY: export_open_org_alarms_to_csv()")
    hours = get_dynamic_lookback_hours(24, 1)
    log_dynamic_lookback("open org alarms export", hours)
    logging.info(f"Starting search for all open org alarms in the past {hours} hours...")
    
    try:
        fetch_and_display_api_data(
            title="Search all Org Alarms:",
            api_call=mistapi.api.v1.orgs.alarms.searchOrgAlarms,
            filename="OrgAlarms.csv",
            limit=1000,
            duration=f"{hours}h",
            status="open"
        )
        logging.info("Completed export_open_org_alarms_to_csv and wrote results to OrgAlarms.csv.")
        logging.debug("EXIT: export_open_org_alarms_to_csv - success")
    except Exception as e:
        logging.error(f"Failed to export open org alarms: {e}")
        logging.debug("EXIT: export_open_org_alarms_to_csv - error")
        raise

def export_recent_device_events_to_csv():
    """
    Export all device events from the past 24 hours to OrgDeviceEvents.csv.
    """
    logging.info("Search Org Device Events:")
    org_id = get_cached_or_prompted_org_id()
    # Dynamic lookback window (24h normal / 1h test by default)
    hours = get_dynamic_lookback_hours(24, 1)
    log_dynamic_lookback("recent device events export", hours)
    # Use explicit duration parameter rather than start/end to avoid mistapi defaulting to duration=1d
    # If we provided only start/end previously, the library still appended duration=1d in the request.
    # Explicitly passing duration ensures correct reduced window in test mode.
    duration_param = f"{hours}h"
    response = mistapi.api.v1.orgs.devices.searchOrgDeviceEvents(
        apisession,
        org_id,
        device_type="all",
        limit=1000,
        duration=duration_param
    )
    # Retrieve all paginated results
    rawdata = mistapi.get_all(response=response, mist_session=apisession)
    events = rawdata
    logging.info(f"Fetched {len(events)} device events from the past {hours} hours (duration={duration_param}).")
    # Write the events to a CSV file
    DataExporter.save_data_to_output(events, "OrgDeviceEvents.csv")
    logging.info(f"Device events written to OrgDeviceEvents.csv ({len(events)} rows).")
    print(f"! {len(events)} device events exported to OrgDeviceEvents.csv")
    # Optionally log the first few events for debugging
    if events:
        logging.debug("Sample device events: %s", json.dumps(events[:3], indent=2))

def export_all_org_device_events_52w_to_csv():
    """
    Export all org device events from the last 52 weeks to OrgDeviceEvents_52w.csv.
    Fetches all data into memory, then writes to CSV in one operation.
    """
    logging.info("Exporting all org device events from the last 52 weeks...")
    org_id = get_cached_or_prompted_org_id()
    # Use Mist API to search for device events in the last 52 weeks
    # Use duration=52w, do NOT use last_by
    response = mistapi.api.v1.orgs.devices.searchOrgDeviceEvents(
        apisession, org_id, device_type="all", limit=1000, duration="52w"
    )
    # Retrieve all paginated results into memory
    events = mistapi.get_all(response=response, mist_session=apisession)
    logging.info(f"Fetched {len(events)} device events from the last 52 weeks.")
    # Flatten and sanitize for CSV
    events = flatten_nested_fields_in_list(events)
    events = escape_multiline_strings_for_csv(events)
    # Write all data to CSV in one operation
    DataExporter.save_data_to_output(events, "OrgDeviceEvents_52w.csv")
    logging.info(" All org device events (52w) exported to OrgDeviceEvents_52w.csv.")

def export_audit_logs_to_csv(full_history=False, duration=None):
    """
    Export organization audit logs to OrgAuditLogs.csv.
    Fetches all pages using mistapi.get_all.
    If full_history is True, pulls all audit logs (start=0).
    If False, pulls only the last 24 hours.
    If duration is provided, uses it as the duration parameter.
    """
    logging.debug(f"ENTRY: export_audit_logs_to_csv(full_history={full_history}, duration={duration})")
    logging.info("Starting export of organization audit logs...")
    
    try:
        org_id = get_cached_or_prompted_org_id()

        # Always include limit=1000 to reduce number of API calls
        kwargs = {"limit": 1000}

        if duration:
            # Caller explicitly provided duration; honor it.
            kwargs["duration"] = duration
            logging.info(f"Exporting audit logs for duration: {duration}")
        elif not full_history:
            # Use dynamic hour window and pass duration directly to avoid mistapi auto-appending duration=1d
            hours = get_dynamic_lookback_hours(24, 1)
            log_dynamic_lookback("audit logs export", hours)
            kwargs["duration"] = f"{hours}h"
            logging.info(f"Exporting only last {hours} hours of audit logs (duration={hours}h).")
        else:
            kwargs["start"] = 0
            logging.info("Exporting full audit log history (start=0).")

        # Call the API and fetch all pages
        logging.debug(f"Making API call with parameters: {kwargs}")
        response = mistapi.api.v1.orgs.logs.listOrgAuditLogs(apisession, org_id, **kwargs)
        rawdata = mistapi.get_all(response=response, mist_session=apisession)

        if not rawdata:
            logging.warning(" No audit logs returned from API.")
            logging.debug("EXIT: export_audit_logs_to_csv - no data")
            return

        # Flatten and sanitize for CSV
        data = flatten_nested_fields_in_list(rawdata)
        data = escape_multiline_strings_for_csv(data)
        DataExporter.save_data_to_output(data, "OrgAuditLogs.csv")
        print(f"! {len(data)} audit logs exported to OrgAuditLogs.csv")
        logging.info("Completed export_audit_logs_to_csv and wrote results to OrgAuditLogs.csv.")
        logging.debug("EXIT: export_audit_logs_to_csv - success")
        
    except Exception as e:
        logging.error(f"Failed to export audit logs: {e}")
        logging.debug("EXIT: export_audit_logs_to_csv - error")
        raise


# ============================================================================
# WEBSOCKET COMMAND FUNCTIONS
# ============================================================================

class WebSocketCommands:
    """
    WebSocket Commands Class for Mist API device operations.
    
    This class organizes all WebSocket-based device command functions following
    the agents guide requirement that "All features, or helpers need to live 
    under the appropriately titled/named 'Class's for code clarity and organization."
    
    All methods are static since they don't require instance state and can be
    called directly from menu actions.
    
    SECURITY: All methods use authenticated WebSocket connections with session-based
    command demultiplexing for concurrent command safety.
    """
    
    @staticmethod
    def ping_device():
        """
        Execute ping command on a network device via WebSocket.
        
        Follows the documented Mist API pattern:
        1. Connect to WebSocket
        2. Subscribe to device command channel
        3. Issue POST ping command
        4. Await results via WebSocket stream
        
        SECURITY: Uses authenticated WebSocket connection with session-based
        command demultiplexing for concurrent command safety.
        """
        return ping_device_websocket()
    
    @staticmethod
    def show_mac_table():
        """
        Execute show MAC table command on a switch device via WebSocket.
        
        MAC tables are a Layer 2 switching feature and are only meaningful on switches.
        Routers/gateways operate at Layer 3 and typically don't maintain MAC tables.
        
        Follows the documented Mist API pattern:
        1. Connect to WebSocket
        2. Subscribe to device command channel
        3. Issue POST show_mac_table command
        4. Await results via WebSocket stream
        
        SECURITY: Uses authenticated WebSocket connection with session-based
        command demultiplexing for concurrent command safety.
        """
        return show_mac_table_websocket()
    
    @staticmethod
    def arp_device():
        """
        Execute ARP command on a network device via WebSocket.
        
        Follows the documented Mist API pattern for ARP commands:
        1. Subscribe to WebSocket channel
        2. POST ARP command
        3. Receive results via WebSocket stream with session-based demultiplexing
        
        SECURITY: Uses authenticated WebSocket connection with session-based
        command demultiplexing for concurrent command safety.
        """
        return arp_device_websocket()
    
    @staticmethod
    def service_ping_device():
        """
        Execute service ping command on SSR gateway devices via WebSocket.
        Service ping allows ping packets to follow the same path as specific services.
        
        Follows the documented Mist API pattern for service ping commands:
        1. Subscribe to WebSocket channel
        2. POST service ping command with service-specific parameters
        3. Receive results via WebSocket stream with session-based demultiplexing
        
        SECURITY: Uses authenticated WebSocket connection with session-based
        command demultiplexing for concurrent command safety.
        """
        return service_ping_device_websocket()
    
    @staticmethod
    def show_forwarding_table():
        """
        Execute show forwarding table command on a gateway/SSR device via WebSocket.
        
        Displays Layer 3 routing table information for gateway/SSR devices.
        This is a routing table diagnostic command specifically for devices that
        perform Layer 3 forwarding functions.
        
        SECURITY: Uses authenticated WebSocket connection with session-based
        command demultiplexing for concurrent command safety.
        """
        return show_forwarding_table_websocket()
    
    @staticmethod
    def show_routing_table():
        """
        Execute show route command on switches, routers, and SSR devices via WebSocket.
        
        Displays routing table information (RIB - Routing Information Base) for network devices.
        This shows the routing protocol information maintained by routing protocols like BGP, OSPF, 
        static routes, etc. Different from forwarding table (FIB) which shows actual forwarding entries.
        
        Supported on:
        - Switches with Layer 3 capabilities
        - SRX routers  
        - SSR gateways
        - Other routing-capable devices
        
        SECURITY: Uses authenticated WebSocket connection with session-based
        command demultiplexing for concurrent command safety.
        """
        return show_routing_table_websocket()
    
    @staticmethod
    def show_ssr_routes():
        """
        Execute SSR/SRX routing table command using dedicated API function.
        
        Uses the dedicated mistapi.api.v1.sites.devices.showSiteSsrAndSrxRoutes function
        which provides structured routing table queries specifically optimized for
        SSR and SRX devices with proper parameter validation and device-specific formatting.
        
        This is the preferred method for SSR/SRX routing table queries as it provides:
        - Structured parameter input (protocol, neighbor, prefix, vrf, node, route direction)
        - Device-specific optimization for SSR/SRX platforms
        - Proper BGP neighbor route analysis (received/advertised)
        - VRF-aware routing table queries
        - HA cluster node selection for multi-node deployments
        
        Supported devices:
        - SSR gateways (128T Session Smart Routers)
        - SRX routers (Juniper SRX series)
        
        SECURITY: Uses authenticated API session with proper parameter validation
        and device capability checking for safe routing table operations.
        """
        return show_ssr_routes_dedicated()


def ping_device_websocket():
    """
    Execute ping command on a network device via WebSocket.
    
    Follows the documented Mist API pattern:
    1. Connect to WebSocket
    2. Subscribe to device command channel
    3. Issue POST ping command
    4. Await results via WebSocket stream
    
    SECURITY: Uses authenticated WebSocket connection with session-based
    command demultiplexing for concurrent command safety.
    """
    logging.info("Starting WebSocket ping operation...")
    
    # Check for debug mode from command line args
    debug_mode = '--debug' in sys.argv or '-d' in sys.argv
    
    if debug_mode:
        logging.getLogger().setLevel(logging.DEBUG)
        print("[DEBUG] DEBUG MODE ENABLED")
    
    logging.info("Starting WebSocket ping operation...")
    logging.debug("ENTER: ping_device_websocket")
    
    try:
        # Interactive site and device selection
        site_id = prompt_select_site_id_from_csv()
        if not site_id:
            print("! No site selected. Operation cancelled.")
            return
        
        if debug_mode:
            print(f"[DEBUG] Selected site_id = {site_id}")
            
        # Get device selection  
        device_id = prompt_select_device_id_from_inventory(site_id, device_type="all")
        if not device_id:
            print("! No device selected. Operation cancelled.")
            return
        
        if debug_mode:
            print(f"[DEBUG] Selected device_id = {device_id}")
            
        # Get ping target from user (default to 8.8.8.8)
        target_input = input("Enter the target hostname or IP address to ping (default: 8.8.8.8): ").strip()
        target_host = target_input if target_input else "8.8.8.8"
            
        # Validate target host
        if not _validate_ping_target(target_host):
            print(f"! Invalid ping target: {target_host}")
            return
        
        if debug_mode:
            print(f"[DEBUG] Target host = {target_host}")
            
        # Get ping count (optional)
        ping_count_input = input("Enter number of ping packets (default: 4): ").strip()
        ping_count = 4
        if ping_count_input:
            try:
                ping_count = int(ping_count_input)
                if ping_count < 1 or ping_count > 100:
                    print("! Ping count must be between 1 and 100. Using default: 4")
                    ping_count = 4
            except ValueError:
                print("! Invalid ping count. Using default: 4")
                ping_count = 4
        
        if debug_mode:
            print(f"[DEBUG] Ping count = {ping_count}")
        
        print(f"\n-> Executing ping to {target_host} on device {device_id}...")
        print(f"-> Ping count: {ping_count}")
        print("-> Establishing WebSocket connection...")
        
        # Initialize WebSocket manager
        websocket_manager = WebSocketManager(apisession)
        
        if debug_mode:
            print("[DEBUG] WebSocketManager initialized")
        
        # Connect to WebSocket
        if not websocket_manager.connect():
            print("! Failed to establish WebSocket connection")
            return
            
        if debug_mode:
            print("[DEBUG] WebSocket connection established")
            
        # Subscribe to device command channel
        command_channel = f"/sites/{site_id}/devices/{device_id}/cmd"
        if not websocket_manager.subscribe_to_channel(command_channel):
            print("! Failed to subscribe to device command channel")
            websocket_manager.disconnect()
            return
        
        if debug_mode:
            print(f"[DEBUG] Subscribed to channel: {command_channel}")
            
        print("-> WebSocket connected and subscribed")
        
        # Wait a moment for subscription to be established
        time.sleep(1)
        
        # Issue ping command via REST API
        ping_payload = {
            "host": target_host,
            "count": ping_count
        }
        
        print("-> Issuing ping command...")
        logging.debug(f"Ping payload: {ping_payload}")
        
        if debug_mode:
            print(f"[DEBUG] Ping payload = {ping_payload}")
        
        # Get authentication details for direct HTTP request
        mist_host = getattr(apisession, "host", None) or os.getenv("MIST_HOST")
        mist_apitoken = getattr(apisession, "apitoken", None) or os.getenv("MIST_APITOKEN")
        
        if not mist_host or not mist_apitoken:
            print("! Mist host or API token not found in session or environment")
            websocket_manager.disconnect()
            return
        
        if debug_mode:
            print(f"[DEBUG] mist_host = {mist_host}")
            print(f"[DEBUG] API token length = {len(mist_apitoken) if mist_apitoken else 0}")
        
        # Make direct POST request to trigger ping
        ping_url = f"https://{mist_host}/api/v1/sites/{site_id}/devices/{device_id}/ping"
        headers = {'Authorization': f'Token {mist_apitoken}', 'Content-Type': 'application/json'}
        
        if debug_mode:
            print(f"[DEBUG] POST URL = {ping_url}")
            print(f"[DEBUG] Headers = {{'Authorization': 'Token [REDACTED]', 'Content-Type': 'application/json'}}")
        
        ping_response = requests.post(ping_url, headers=headers, json=ping_payload)
        
        if debug_mode:
            print(f"[DEBUG] HTTP Response Status = {ping_response.status_code}")
            print(f"[DEBUG] HTTP Response Body = {ping_response.text}")
        
        if ping_response.status_code != 200:
            print(f"! Failed to issue ping command: {ping_response.status_code}")
            print(f"! Response: {ping_response.text}")
            websocket_manager.disconnect()
            return
            
        # Extract session ID from response
        response_data = ping_response.json()
        session_id = response_data.get("session")
        if not session_id:
            print("! No session ID returned from ping command")
            websocket_manager.disconnect()
            return
            
        print(f"-> Ping command issued (session: {session_id[:8]}...)")
        print("-> Waiting for ping results...")
        
        if debug_mode:
            print(f"[DEBUG] Full session ID = {session_id}")
            print("[DEBUG] Starting to wait for WebSocket results...")
        
        # Wait for ping results via WebSocket
        ping_result = websocket_manager.wait_for_command_result(session_id, timeout_seconds=30)
        
        if debug_mode:
            print(f"[DEBUG] wait_for_command_result returned: {ping_result is not None}")
            if ping_result:
                print(f"[DEBUG] Result keys: {list(ping_result.keys())}")
        
        if ping_result:
            print("\n" + "=" * 60)
            print("PING RESULTS:")
            print("=" * 60)
            
            # Display raw output (this is where ping results come according to documentation)
            raw_output = ping_result.get("raw", "")
            if raw_output:
                print("RAW OUTPUT:")
                print("-" * 40)
                print(raw_output)
            
            # Display any other output fields that might be present
            output_fields = ping_result.get("Output", "")
            if output_fields and output_fields != raw_output:
                print("\nOTHER OUTPUT:")
                print("-" * 40)
                print(output_fields)
                
            # Show all available fields for debugging
            available_fields = [key for key in ping_result.keys() if key not in ['raw', 'Output', 'session']]
            if available_fields:
                print(f"\nOTHER AVAILABLE FIELDS: {available_fields}")
                for field in available_fields:
                    field_value = ping_result.get(field)
                    if field_value:
                        print(f"{field}: {field_value}")
                
            if not raw_output and not output_fields:
                print("No output data received")
                print(f"Available result keys: {list(ping_result.keys())}")
                
            print("=" * 60)
            
            # Log the successful operation
            logging.info(f"WebSocket ping completed successfully for {target_host}")
            
        else:
            print("! Timeout waiting for ping results")
            logging.warning("WebSocket ping operation timed out")
            
            if debug_mode:
                print("[DEBUG] Checking WebSocket manager state...")
                print(f"[DEBUG] Connected = {websocket_manager.connected}")
                print(f"[DEBUG] Subscribed channels = {websocket_manager.subscribed_channels}")
                with websocket_manager.results_lock:
                    print(f"[DEBUG] Pending results = {list(websocket_manager.command_results.keys())}")
            
    except Exception as ping_error:
        error_message = f"WebSocket ping operation failed: {ping_error}"
        print(f"! {error_message}")
        logging.error(error_message)
        
        if debug_mode:
            print("[DEBUG] Exception details:")
            import traceback
            traceback.print_exc()
        
        logging.debug("EXIT: ping_device_websocket - error")
        
    finally:
        # Always cleanup WebSocket connection
        try:
            if 'websocket_manager' in locals():
                websocket_manager.disconnect()
                print("-> WebSocket connection closed")
                
                if debug_mode:
                    print("[DEBUG] WebSocket cleanup completed")
        except Exception as cleanup_error:
            logging.warning(f"WebSocket cleanup error: {cleanup_error}")
            
        logging.debug("EXIT: ping_device_websocket")


def show_mac_table_websocket():
    """
    Execute show MAC table command on a switch device via WebSocket.
    
    MAC tables are a Layer 2 switching feature and are only meaningful on switches.
    Routers/gateways operate at Layer 3 and typically don't maintain MAC tables.
    
    Follows the documented Mist API pattern:
    1. Connect to WebSocket
    2. Subscribe to device command channel
    3. Issue POST show_mac_table command
    4. Await results via WebSocket stream
    
    SECURITY: Uses authenticated WebSocket connection with session-based
    command demultiplexing for concurrent command safety.
    """
    # Check for debug mode from command line arguments
    debug_mode = '--debug' in sys.argv or '-d' in sys.argv
    
    if debug_mode:
        logging.getLogger().setLevel(logging.DEBUG)
        print("[DEBUG] DEBUG MODE ENABLED")
    
    logging.info("Starting WebSocket show MAC table operation...")
    logging.debug("ENTER: show_mac_table_websocket")
    
    try:
        # Interactive site selection
        site_id = prompt_select_site_id_from_csv()
        if not site_id:
            print("! No site selected. Operation cancelled.")
            return
        
        if debug_mode:
            print(f"[DEBUG] Selected site_id = {site_id}")
            
        # Get device selection - MAC table is a Layer 2 switching feature
        print("-> MAC table is available on switches (Layer 2 devices)")
        print("-> Routers/gateways operate at Layer 3 and typically don't maintain MAC tables")
        print("-> APs forward wireless traffic but don't maintain traditional MAC tables")
        device_id = prompt_select_device_id_from_inventory(site_id, device_type="switch")
        if not device_id:
            print("! No switch device selected. MAC table command requires Layer 2 switching devices.")
            print("! Only switches maintain MAC address learning tables for Ethernet forwarding.")
            return
        
        if debug_mode:
            print(f"[DEBUG] Selected device_id = {device_id}")
            
        print(f"\n-> Executing show MAC table on device {device_id}...")
        print("-> Establishing WebSocket connection...")
        
        # Initialize WebSocket manager
        websocket_manager = WebSocketManager(apisession)
        
        if debug_mode:
            print("[DEBUG] WebSocketManager initialized")
        
        # Connect to WebSocket
        if not websocket_manager.connect():
            print("! Failed to establish WebSocket connection")
            return
            
        if debug_mode:
            print("[DEBUG] WebSocket connection established")
            
        # Subscribe to device command channel
        command_channel = f"/sites/{site_id}/devices/{device_id}/cmd"
        if not websocket_manager.subscribe_to_channel(command_channel):
            print("! Failed to subscribe to device command channel")
            websocket_manager.disconnect()
            return
        
        if debug_mode:
            print(f"[DEBUG] Subscribed to channel: {command_channel}")
            
        print("-> WebSocket connected and subscribed")
        
        # Wait a moment for subscription to be established
        time.sleep(1)
        
        # Issue show MAC table command via REST API
        mac_table_payload = {}  # show_mac_table typically doesn't require additional parameters
        
        print("-> Issuing show MAC table command...")
        logging.debug(f"MAC table payload: {mac_table_payload}")
        
        if debug_mode:
            print(f"[DEBUG] MAC table payload = {mac_table_payload}")
        
        # Get authentication details for direct HTTP request
        mist_host = getattr(apisession, "host", None) or os.getenv("MIST_HOST")
        mist_apitoken = getattr(apisession, "apitoken", None) or os.getenv("MIST_APITOKEN")
        
        if not mist_host or not mist_apitoken:
            print("! Mist host or API token not found in session or environment")
            websocket_manager.disconnect()
            return
        
        if debug_mode:
            print(f"[DEBUG] mist_host = {mist_host}")
            print(f"[DEBUG] API token length = {len(mist_apitoken) if mist_apitoken else 0}")
        
        # Make direct POST request to trigger show MAC table
        mac_table_url = f"https://{mist_host}/api/v1/sites/{site_id}/devices/{device_id}/show_mac_table"
        headers = {'Authorization': f'Token {mist_apitoken}', 'Content-Type': 'application/json'}
        
        if debug_mode:
            print(f"[DEBUG] POST URL = {mac_table_url}")
            print(f"[DEBUG] Headers = {{'Authorization': 'Token [REDACTED]', 'Content-Type': 'application/json'}}")
        
        mac_table_response = requests.post(mac_table_url, headers=headers, json=mac_table_payload)
        
        if debug_mode:
            print(f"[DEBUG] HTTP Response Status = {mac_table_response.status_code}")
            print(f"[DEBUG] HTTP Response Body = {mac_table_response.text}")
        
        if mac_table_response.status_code != 200:
            print(f"! Failed to issue show MAC table command: {mac_table_response.status_code}")
            print(f"! Response: {mac_table_response.text}")
            websocket_manager.disconnect()
            return
            
        # Extract session ID from response
        response_data = mac_table_response.json()
        session_id = response_data.get("session")
        if not session_id:
            print("! No session ID returned from show MAC table command")
            websocket_manager.disconnect()
            return
            
        print(f"-> Show MAC table command issued (session: {session_id[:8]}...)")
        print("-> Waiting for MAC table results...")
        
        if debug_mode:
            print(f"[DEBUG] Full session ID = {session_id}")
            print("[DEBUG] Starting to wait for WebSocket results...")
        
        # Wait for MAC table results via WebSocket (longer timeout for potentially large tables)
        mac_table_result = websocket_manager.wait_for_command_result(session_id, timeout_seconds=60)
        
        if debug_mode:
            print(f"[DEBUG] wait_for_command_result returned: {mac_table_result is not None}")
            if mac_table_result:
                print(f"[DEBUG] Result keys: {list(mac_table_result.keys())}")
        
        if mac_table_result:
            print("\n" + "=" * 60)
            print("MAC TABLE RESULTS:")
            print("=" * 60)
            
            # Display raw output (this is where MAC table results come according to documentation)
            raw_output = mac_table_result.get("raw", "")
            if raw_output:
                print("RAW OUTPUT:")
                print("-" * 40)
                print(raw_output)
            
            # Display any other output fields that might be present
            output_fields = mac_table_result.get("Output", "")
            if output_fields and output_fields != raw_output:
                print("\nOTHER OUTPUT:")
                print("-" * 40)
                print(output_fields)
                
            # Show all available fields for debugging
            available_fields = [key for key in mac_table_result.keys() if key not in ['raw', 'Output', 'session']]
            if available_fields:
                print(f"\nOTHER AVAILABLE FIELDS: {available_fields}")
                for field in available_fields:
                    field_value = mac_table_result.get(field)
                    if field_value:
                        print(f"{field}: {field_value}")
                
            if not raw_output and not output_fields:
                print("No output data received")
                print(f"Available result keys: {list(mac_table_result.keys())}")
                
            print("=" * 60)
            
            # Log the successful operation
            logging.info("WebSocket show MAC table completed successfully")
            
        else:
            print("! Timeout waiting for MAC table results")
            print("! This may indicate:")
            print("  - The device doesn't support MAC table commands (common for routers/Layer 3 devices)")
            print("  - The device is busy or not responding")
            print("  - Network connectivity issues")
            print("! Note: MAC tables are primarily a Layer 2 (switch) feature")
            logging.warning("WebSocket show MAC table operation timed out")
            
            if debug_mode:
                print("[DEBUG] Checking WebSocket manager state...")
                print(f"[DEBUG] Connected = {websocket_manager.connected}")
                print(f"[DEBUG] Subscribed channels = {websocket_manager.subscribed_channels}")
                with websocket_manager.results_lock:
                    print(f"[DEBUG] Pending results = {list(websocket_manager.command_results.keys())}")
            
    except Exception as mac_table_error:
        error_message = f"WebSocket show MAC table operation failed: {mac_table_error}"
        print(f"! {error_message}")
        logging.error(error_message)
        
        if debug_mode:
            print("[DEBUG] Exception details:")
            import traceback
            traceback.print_exc()
        
        logging.debug("EXIT: show_mac_table_websocket - error")
        
    finally:
        # Always cleanup WebSocket connection
        try:
            if 'websocket_manager' in locals():
                websocket_manager.disconnect()
                print("-> WebSocket connection closed")
                
                if debug_mode:
                    print("[DEBUG] WebSocket cleanup completed")
        except Exception as cleanup_error:
            logging.warning(f"WebSocket cleanup error: {cleanup_error}")
            
        logging.debug("EXIT: show_mac_table_websocket")


def arp_device_websocket():
    """
    Execute ARP command on a network device via WebSocket.
    
    Follows the documented Mist API pattern for ARP commands:
    1. Subscribe to WebSocket channel
    2. POST ARP command
    3. Receive results via WebSocket stream with session-based demultiplexing
    
    SECURITY: Uses authenticated WebSocket connection with session-based
    command demultiplexing for concurrent command safety.
    """
    logging.info("Starting WebSocket ARP operation...")
    logging.debug("ENTER: arp_device_websocket")
    
    debug_mode = '--debug' in sys.argv or '-d' in sys.argv
    
    if debug_mode:
        print("[DEBUG] Starting ARP via WebSocket operation...")
    
    try:
        # Interactive site and device selection
        site_id = prompt_select_site_id_from_csv()
        if not site_id:
            print("! No site selected. Operation cancelled.")
            return
            
        if debug_mode:
            print(f"[DEBUG] Selected site_id = {site_id}")
        
        # Get device selection  
        device_id = prompt_select_device_id_from_inventory(site_id, device_type="all")
        if not device_id:
            print("! No device selected. Operation cancelled.")
            return
        
        if debug_mode:
            print(f"[DEBUG] Selected device_id = {device_id}")
        
        # Get device details to check type and model for compatibility
        device_info = None
        try:
            rawdata = mistapi.api.v1.sites.devices.listSiteDevices(apisession, site_id, type="all").data
            device_info = next((device for device in rawdata if device.get('id') == device_id), None)
            
            if device_info:
                device_type = device_info.get('type', 'unknown')
                device_model = device_info.get('model', 'unknown')
                device_name = device_info.get('name', f"Device {device_id[:8]}")
                
                if debug_mode:
                    print(f"[DEBUG] Device type: {device_type}, model: {device_model}, name: {device_name}")
                
                # Warn about device compatibility
                if device_type == 'switch':
                    print(f"!?  WARNING: Switch detected (Model: {device_model})")
                    print("   -> Switches may have limited WebSocket ARP support")
                    print("   -> Consider using SSH-based ARP commands instead")
                    print("   -> This operation may timeout or return limited results")
                    
                    response = input("   -> Continue anyway? (y/N): ").strip().lower()
                    if response not in ['y', 'yes']:
                        print("! Operation cancelled by user")
                        return
                        
                elif device_type == 'gateway':
                    print(f"!? Gateway detected (Model: {device_model})")
                    print("   -> Gateways have good WebSocket ARP support")
                    print("   -> Results may differ from Access Points")
                    
                elif device_type == 'ap':
                    print(f"!? Access Point detected (Model: {device_model})")
                    print("   -> Access Points have full WebSocket ARP support")
                    
                else:
                    print(f"? Unknown device type: {device_type} (Model: {device_model})")
                    print("   -> Proceeding with standard ARP command")
                    
        except Exception as device_check_error:
            logging.warning(f"Could not verify device compatibility: {device_check_error}")
            if debug_mode:
                print(f"[DEBUG] Device check failed: {device_check_error}")
            print("   -> Proceeding with standard ARP command")
        
        print(f"\n-> Executing ARP command on device {device_id}...")
        print("-> Establishing WebSocket connection...")
        
        if debug_mode:
            print("[DEBUG] WebSocketManager initialized")
        
        # Initialize WebSocket manager
        websocket_manager = WebSocketManager(apisession)
        
        # Connect to WebSocket
        if not websocket_manager.connect():
            print("! Failed to establish WebSocket connection")
            return
            
        if debug_mode:
            print("[DEBUG] WebSocket connection established")
        
        # Subscribe to device command channel
        command_channel = f"/sites/{site_id}/devices/{device_id}/cmd"
        if not websocket_manager.subscribe_to_channel(command_channel):
            print("! Failed to subscribe to device command channel")
            websocket_manager.disconnect()
            return
            
        if debug_mode:
            print(f"[DEBUG] Subscribed to channel: {command_channel}")
        
        print("-> WebSocket connected and subscribed")
        
        # Wait a moment for subscription to be established
        time.sleep(1)
        
        print("-> Issuing ARP command...")
        
        # Get authentication details for direct HTTP request
        mist_host = getattr(apisession, "host", None) or os.getenv("MIST_HOST")
        mist_apitoken = getattr(apisession, "apitoken", None) or os.getenv("MIST_APITOKEN")
        
        if debug_mode:
            print(f"[DEBUG] mist_host = {mist_host}")
            print(f"[DEBUG] API token length = {len(mist_apitoken) if mist_apitoken else 'None'}")
        
        if not mist_host or not mist_apitoken:
            print("! Mist host or API token not found in session or environment")
            websocket_manager.disconnect()
            return
        
        # Make direct POST request to trigger ARP command
        arp_url = f"https://{mist_host}/api/v1/sites/{site_id}/devices/{device_id}/arp"
        headers = {'Authorization': f'Token {mist_apitoken}', 'Content-Type': 'application/json'}
        
        if debug_mode:
            print(f"[DEBUG] POST URL = {arp_url}")
            print(f"[DEBUG] Headers = {{'Authorization': 'Token [REDACTED]', 'Content-Type': 'application/json'}}")
        
        # ARP command typically doesn't need a payload body
        arp_response = requests.post(arp_url, headers=headers, json={})
        
        if debug_mode:
            print(f"[DEBUG] HTTP Response Status = {arp_response.status_code}")
            print(f"[DEBUG] HTTP Response Body = {arp_response.text}")
        
        if arp_response.status_code != 200:
            print(f"! Failed to issue ARP command: {arp_response.status_code}")
            print(f"! Response: {arp_response.text}")
            websocket_manager.disconnect()
            return
            
        # Extract session ID from response
        response_data = arp_response.json()
        session_id = response_data.get("session")
        if not session_id:
            print("! No session ID returned from ARP command")
            websocket_manager.disconnect()
            return
            
        print(f"-> ARP command issued (session: {session_id[:8]}...)")
        print("-> Waiting for ARP results...")
        
        if debug_mode:
            print(f"[DEBUG] Full session ID = {session_id}")
            print("[DEBUG] Starting to wait for WebSocket results...")
        
        # Determine timeout based on device type
        if device_info:
            device_type = device_info.get('type', 'unknown')
            if device_type == 'switch':
                # Switches often timeout, give them more time
                timeout_seconds = 45
                print("   -> Using extended timeout for switch (45 seconds)")
            elif device_type == 'gateway':
                # Gateways work but may be slower
                timeout_seconds = 35
                print("   -> Using extended timeout for gateway (35 seconds)")
            else:
                # APs and unknown devices use standard timeout
                timeout_seconds = 30
        else:
            timeout_seconds = 30
        
        # Wait for ARP results via WebSocket
        arp_result = websocket_manager.wait_for_command_result(session_id, timeout_seconds=timeout_seconds)
        
        if debug_mode:
            print(f"[DEBUG] wait_for_command_result returned: {arp_result is not None}")
            if arp_result:
                print(f"[DEBUG] Result keys: {list(arp_result.keys())}")
        
        if arp_result:
            print("\n" + "=" * 60)
            print("ARP TABLE RESULTS:")
            print("=" * 60)
            
            # Add device-specific context
            if device_info:
                device_type = device_info.get('type', 'unknown')
                device_model = device_info.get('model', 'unknown')
                device_name = device_info.get('name', 'Unknown Device')
                
                print(f"Device: {device_name} ({device_type.upper()}: {device_model})")
                
                if device_type == 'switch':
                    print("Note: Switch ARP data may show forwarding table or limited ARP information")
                elif device_type == 'gateway':
                    print("Note: Gateway ARP data may include routing information")
                elif device_type == 'ap':
                    print("Note: Access Point ARP data shows client connectivity information")
                    
                print("-" * 60)
            
            # Display raw output if available
            raw_output = arp_result.get("raw", "")
            if raw_output:
                # Parse and display gateway data in table format if it's JSON
                if device_info and device_info.get('type') == 'gateway' and raw_output.strip().startswith('{'):
                    try:
                        import json
                        gateway_data = json.loads(raw_output)
                        
                        if gateway_data.get('status') == 'SUCCESS' and 'rows' in gateway_data:
                            print("PARSED ARP TABLE:")
                            print("-" * 40)
                            
                            # Get column headers
                            columns = gateway_data.get('columns', [])
                            if columns:
                                # Create header row
                                headers = [col.get('display_name', col.get('id', 'Unknown')) for col in columns]
                                
                                # Calculate column widths
                                rows = gateway_data.get('rows', [])
                                col_widths = []
                                for idx, header in enumerate(headers):
                                    max_width = len(header)
                                    for row in rows:
                                        col_id = columns[idx].get('id', '')
                                        cell_value = str(row.get(col_id, ''))
                                        max_width = max(max_width, len(cell_value))
                                    col_widths.append(min(max_width + 2, 20))  # Cap at 20 chars
                                
                                # Print header
                                header_line = " | ".join(header.ljust(col_widths[idx]) for idx, header in enumerate(headers))
                                print(header_line)
                                print("-" * len(header_line))
                                
                                # Print data rows
                                for row in rows:
                                    row_values = []
                                    for idx, col in enumerate(columns):
                                        col_id = col.get('id', '')
                                        cell_value = str(row.get(col_id, ''))
                                        # Truncate if too long
                                        if len(cell_value) > col_widths[idx] - 2:
                                            cell_value = cell_value[:col_widths[idx] - 5] + "..."
                                        row_values.append(cell_value.ljust(col_widths[idx]))
                                    print(" | ".join(row_values))
                                
                                print(f"\nTotal ARP Entries: {len(rows)}")
                                
                                # Also show raw for reference if debug mode
                                if debug_mode:
                                    print("\nRAW JSON OUTPUT (Debug):")
                                    print("-" * 40)
                                    print(raw_output)
                            else:
                                print("No column information available in gateway response")
                                print("RAW OUTPUT:")
                                print("-" * 40)
                                print(raw_output)
                        else:
                            print("Gateway response format not recognized")
                            print("RAW OUTPUT:")
                            print("-" * 40)
                            print(raw_output)
                            
                    except json.JSONDecodeError as json_error:
                        if debug_mode:
                            print(f"[DEBUG] Failed to parse gateway JSON: {json_error}")
                        print("Failed to parse gateway JSON output")
                        print("RAW OUTPUT:")
                        print("-" * 40)
                        print(raw_output)
                else:
                    # For APs and other devices, show raw output
                    print("RAW OUTPUT:")
                    print("-" * 40)
                    print(raw_output)
            
            # Display parsed output if available  
            parsed_output = arp_result.get("Output", "")
            if parsed_output and parsed_output != raw_output:
                print("\nPARSED OUTPUT:")
                print("-" * 40)
                print(parsed_output)
                
            if not raw_output and not parsed_output:
                print("No output data received")
                if device_info and device_info.get('type') == 'switch':
                    print("\nTroubleshooting for switches:")
                    print("-> Try using SSH-based commands instead")
                    print("-> Some switches require specific ARP command syntax")
                    print("-> WebSocket API may have limited switch support")
                
            print("=" * 60)
            
            # Log the successful operation with device context
            device_context = f"device {device_id}"
            if device_info:
                device_context = f"{device_info.get('type', 'unknown')} {device_info.get('name', device_id[:8])}"
            logging.info(f"WebSocket ARP completed successfully for {device_context}")
            
        else:
            print("! Timeout waiting for ARP results")
            
            # Provide device-specific troubleshooting advice
            if device_info:
                device_type = device_info.get('type', 'unknown')
                device_model = device_info.get('model', 'unknown')
                
                if device_type == 'switch':
                    print(f"\nSwitch troubleshooting ({device_model}):")
                    print("-> Switches often have limited WebSocket ARP support")
                    print("-> Try using SSH-based 'show arp' commands instead")
                    print("-> Some switch models require specific command syntax")
                    print("-> Consider using Menu option for SSH device commands")
                elif device_type == 'gateway':
                    print(f"\nGateway troubleshooting ({device_model}):")
                    print("-> Try increasing timeout or checking network connectivity")
                    print("-> Gateway may require different ARP command format")
                else:
                    print(f"\nGeneral troubleshooting ({device_type}):")
                    print("-> Check device connectivity and WebSocket support")
                    print("-> Some devices may require SSH-based commands")
            
            logging.warning("WebSocket ARP operation timed out")
            
    except Exception as arp_error:
        error_message = f"WebSocket ARP operation failed: {arp_error}"
        print(f"! {error_message}")
        logging.error(error_message)
        logging.debug("EXIT: arp_device_websocket - error")
        
    finally:
        # Always cleanup WebSocket connection
        try:
            if 'websocket_manager' in locals():
                websocket_manager.disconnect()
                print("-> WebSocket connection closed")
        except Exception as cleanup_error:
            logging.warning(f"WebSocket cleanup error: {cleanup_error}")
            
        logging.debug("EXIT: arp_device_websocket")


def service_ping_device_websocket():
    logging.debug("ENTER: service_ping_device_websocket")
    
    debug_mode = is_debug_mode()
    
    if debug_mode:
        print("[DEBUG] Starting Service Ping via WebSocket operation...")
        print(f"[DEBUG] Command line args: {sys.argv}")
        print(f"[DEBUG] Debug mode detected: {debug_mode}")
    
    try:
        # Interactive site and device selection
        site_id = prompt_select_site_id_from_csv()
        if not site_id:
            print("! No site selected. Operation cancelled.")
            return
            
        if debug_mode:
            print(f"[DEBUG] Selected site_id = {site_id}")
        
        # Get device selection - ONLY gateways for Service Ping
        device_id = prompt_select_device_id_from_inventory(site_id, device_type="gateway")
        if not device_id:
            print("! No gateway devices found or selected. Service Ping requires an SSR gateway.")
            return
        
        if debug_mode:
            print(f"[DEBUG] Selected device_id = {device_id}")
        
        # Get device details to check type and model for compatibility
        device_info = None
        try:
            rawdata = mistapi.api.v1.sites.devices.listSiteDevices(apisession, site_id, type="gateway").data
            device_info = next((device for device in rawdata if device.get('id') == device_id), None)
            
            if device_info:
                device_type = device_info.get('type', 'unknown')
                device_model = device_info.get('model', 'unknown')
                device_name = device_info.get('name', f"Device {device_id[:8]}")
                
                if debug_mode:
                    print(f"[DEBUG] Device type: {device_type}, model: {device_model}, name: {device_name}")
                
                # Provide device-specific guidance
                if device_type == 'gateway':
                    print(f"!? SSR Gateway detected (Model: {device_model})")
                    print("   -> Service Ping allows ping packets to follow service-specific paths")
                    print("   -> This is an SSR-specific feature")
                elif device_type == 'ap':
                    print(f"!? WARNING: Access Point detected (Model: {device_model})")
                    print("   -> Service Ping is designed for SSR gateways")
                    print("   -> This device may not support service ping functionality")
                    choice = input("   -> Continue anyway? (y/N): ").strip().lower()
                    if choice != 'y':
                        print("Operation cancelled.")
                        return
                elif device_type == 'switch':
                    print(f"!? WARNING: Switch detected (Model: {device_model})")
                    print("   -> Service Ping is designed for SSR gateways")
                    print("   -> Switches typically do not support service ping")
                    choice = input("   -> Continue anyway? (y/N): ").strip().lower()
                    if choice != 'y':
                        print("Operation cancelled.")
                        return
                else:
                    print(f"!? WARNING: Unknown device type detected (Model: {device_model})")
                    print("   -> Service Ping is designed for SSR gateways")
                    choice = input("   -> Continue anyway? (y/N): ").strip().lower()
                    if choice != 'y':
                        print("Operation cancelled.")
                        return
            
        except Exception as device_error:
            logging.warning(f"Could not retrieve device details: {device_error}")
            if debug_mode:
                print(f"[DEBUG] Device details error: {device_error}")
            print("!? Cannot determine device type - proceeding with caution")
            print("   -> Service Ping is designed for SSR gateways")
            choice = input("   -> Continue anyway? (y/N): ").strip().lower()
            if choice != 'y':
                print("Operation cancelled.")
                return
        
        # Fetch organization services first (primary source)
        print("\n-> Fetching organization services...")
        org_services = fetch_organization_services()
        available_org_services = []
        
        if org_services:
            available_org_services = [svc['name'] for svc in org_services if svc.get('name')]
            print(f"   -> Found {len(available_org_services)} organization-level services")
            if debug_mode:
                print(f"[DEBUG] Organization services: {available_org_services}")
        else:
            print("   -> No organization-level services found")
        
        # Fetch organization tenants from networks (primary source)
        print("-> Fetching organization tenants...")
        org_tenants = fetch_organization_tenants()
        available_org_tenants = []
        
        if org_tenants:
            available_org_tenants = org_tenants
            print(f"   -> Found {len(available_org_tenants)} organization-level tenants")
            if debug_mode:
                print(f"[DEBUG] Organization tenants: {available_org_tenants}")
        else:
            print("   -> No organization-level tenants found")
        
        # Fetch site tenants from derived networks (secondary source)
        print("-> Fetching site tenants...")
        site_tenants = fetch_site_tenants(site_id)
        available_site_tenants = []
        
        if site_tenants:
            available_site_tenants = site_tenants
            print(f"   -> Found {len(available_site_tenants)} site-level tenants")
            if debug_mode:
                print(f"[DEBUG] Site tenants: {available_site_tenants}")
        else:
            print("   -> No site-level tenants found")
        
        # Fetch tenants from service policies (tertiary source)
        print("-> Fetching service policy tenants...")
        service_policy_tenants = fetch_service_policy_tenants(site_id)
        available_service_policy_tenants = []
        
        if service_policy_tenants:
            available_service_policy_tenants = service_policy_tenants
            print(f"   -> Found {len(available_service_policy_tenants)} service policy tenants")
            if debug_mode:
                print(f"[DEBUG] Service policy tenants: {available_service_policy_tenants}")
        else:
            print("   -> No service policy tenants found")
        
        # Fetch tenants from gateway templates (quaternary source)
        print("-> Fetching gateway template tenants...")
        gateway_template_tenants = fetch_gateway_template_tenants(site_id)
        available_gateway_template_tenants = []
        
        if gateway_template_tenants:
            available_gateway_template_tenants = gateway_template_tenants
            print(f"   -> Found {len(available_gateway_template_tenants)} gateway template tenants")
            if debug_mode:
                print(f"[DEBUG] Gateway template tenants: {available_gateway_template_tenants}")
        else:
            print("   -> No gateway template tenants found")
        
        # Fetch device configuration for additional tenant and service options (fallback)
        print("-> Fetching device configuration for additional options...")
        device_config = None
        available_device_tenants = []
        available_device_services = []
        
        try:
            config_response = mistapi.api.v1.sites.devices.getSiteDevice(apisession, site_id, device_id)
            device_config = getattr(config_response, "data", {})
            
            if debug_mode:
                print(f"[DEBUG] Device config keys: {list(device_config.keys())}")
            
            # Extract tenants and services from device configuration/stats
            tenants_set = set()
            services_set = set()
            
            # Method 1: Check service_policies
            service_policies = device_config.get("service_policies", [])
            for policy in service_policies:
                if isinstance(policy, dict):
                    tenant_name = policy.get("tenant", "")
                    if tenant_name:
                        tenants_set.add(tenant_name)
                    
                    # Extract services from policies
                    services = policy.get("services", [])
                    if isinstance(services, list):
                        for service_item in services:
                            if isinstance(service_item, dict):
                                service_name = service_item.get("name", "")
                                if service_name:
                                    services_set.add(service_name)
                            elif isinstance(service_item, str):
                                services_set.add(service_item)
            
            # Method 2: Check routing instances for tenants
            routing_instances = device_config.get("routing_instances", [])
            for instance in routing_instances:
                if isinstance(instance, dict):
                    tenant_name = instance.get("name", "")
                    if tenant_name and not tenant_name.startswith("_"):
                        tenants_set.add(tenant_name)
            
            # Method 3: Check router configuration
            router_config = device_config.get("router", {})
            if isinstance(router_config, dict):
                # Check for tenants in router config
                tenants_config = router_config.get("tenants", [])
                if isinstance(tenants_config, list):
                    for tenant_item in tenants_config:
                        if isinstance(tenant_item, dict):
                            tenant_name = tenant_item.get("name", "")
                            if tenant_name:
                                tenants_set.add(tenant_name)
                
                # Check for services in router config
                services_config = router_config.get("services", [])
                if isinstance(services_config, list):
                    for service_item in services_config:
                        if isinstance(service_item, dict):
                            service_name = service_item.get("name", "")
                            if service_name:
                                services_set.add(service_name)
            
            # Method 4: Check device stats for additional service/tenant info
            try:
                stats_response = mistapi.api.v1.sites.stats.getSiteDeviceStats(apisession, site_id, device_id)
                stats_data = getattr(stats_response, "data", {})
                
                # Look for tenant/service info in stats
                if "service_stat" in stats_data:
                    service_stats = stats_data.get("service_stat", [])
                    for service_stat in service_stats:
                        if isinstance(service_stat, dict):
                            service_name = service_stat.get("name", "")
                            if service_name and not service_name.startswith("_"):
                                services_set.add(service_name)
                
                if debug_mode:
                    print(f"[DEBUG] Stats keys: {list(stats_data.keys())}")
                    
            except Exception as stats_error:
                if debug_mode:
                    print(f"[DEBUG] Could not fetch stats: {stats_error}")
            
            # Convert to sorted lists and filter out internal/system entries
            available_device_tenants = sorted([t for t in tenants_set if t and not t.startswith("_")])
            available_device_services = sorted([s for s in services_set if s and not s.startswith("_")])
            
            if debug_mode:
                print(f"[DEBUG] Found tenants from device: {available_device_tenants}")
                print(f"[DEBUG] Found services from device: {available_device_services}")
            
            # Report device configuration results
            if not available_device_tenants:
                print("   -> No tenants found in device configuration")
            else:
                print(f"   -> Found {len(available_device_tenants)} tenants from device configuration")
                
            if not available_device_services:
                print("   -> No additional services found in device configuration")
            else:
                print(f"   -> Found {len(available_device_services)} additional services from device configuration")
                
        except Exception as config_error:
            logging.warning(f"Could not fetch device configuration: {config_error}")
            if debug_mode:
                print(f"[DEBUG] Config error: {config_error}")
            print("!? Cannot retrieve device configuration")
        
        # Combine organization, site, service policy, gateway template, and device tenants 
        # (precedence: org > site > service policy > gateway template > device)
        all_available_tenants = list(available_org_tenants)  # Start with org tenants
        
        # Add site tenants (if not already present from org)
        for site_tenant in available_site_tenants:
            if site_tenant not in all_available_tenants:
                all_available_tenants.append(site_tenant)
        
        # Add service policy tenants (if not already present from org or site)
        for policy_tenant in available_service_policy_tenants:
            if policy_tenant not in all_available_tenants:
                all_available_tenants.append(policy_tenant)
        
        # Add gateway template tenants (if not already present)
        for template_tenant in available_gateway_template_tenants:
            if template_tenant not in all_available_tenants:
                all_available_tenants.append(template_tenant)
        
        # Add device tenants (if not already present from any other source)
        for device_tenant in available_device_tenants:
            if device_tenant not in all_available_tenants:
                all_available_tenants.append(device_tenant)
        
        # Combine organization and device services (org services take precedence)
        all_available_services = list(available_org_services)  # Start with org services
        for device_service in available_device_services:
            if device_service not in all_available_services:
                all_available_services.append(device_service)
        
        # Force add default tenant "testing-tools" if not already present
        if "testing-tools" not in all_available_tenants:
            all_available_tenants.append("testing-tools")
            if debug_mode:
                print(f"[DEBUG] Added default tenant: testing-tools")
        
        # Force add default service "web-session" if not already present
        if "web-session" not in all_available_services:
            all_available_services.append("web-session")
            if debug_mode:
                print(f"[DEBUG] Added default service: web-session")
        
        if debug_mode:
            print(f"[DEBUG] Combined tenant list: {all_available_tenants}")
            print(f"[DEBUG] Combined service list: {all_available_services}")
        
        # Get service ping parameters from user
        print("\n" + "=" * 50)
        print("SERVICE PING CONFIGURATION")
        print("=" * 50)
        
        # Tenant selection (organization, site, service policy, gateway template, and device tenants combined)
        tenant = None
        if all_available_tenants:
            print("\nAvailable Tenants:")
            
            current_index = 0
            
            # Show organization tenants first
            org_tenant_count = len(available_org_tenants)
            if org_tenant_count > 0:
                print(f"  Organization Tenants ({org_tenant_count}):")
                for tenant_name in available_org_tenants:
                    print(f"    [{current_index}] {tenant_name} (org networks)")
                    current_index += 1
            
            # Show site tenants (additional to org tenants)
            site_only_tenants = [tenant for tenant in available_site_tenants if tenant not in available_org_tenants]
            if site_only_tenants:
                print(f"  Site Tenants ({len(site_only_tenants)}):")
                for tenant_name in site_only_tenants:
                    print(f"    [{current_index}] {tenant_name} (site networks)")
                    current_index += 1
            
            # Show service policy tenants (additional to org and site tenants)
            policy_only_tenants = [tenant for tenant in available_service_policy_tenants if tenant not in available_org_tenants and tenant not in available_site_tenants]
            if policy_only_tenants:
                print(f"  Service Policy Tenants ({len(policy_only_tenants)}):")
                for tenant_name in policy_only_tenants:
                    print(f"    [{current_index}] {tenant_name} (service policies)")
                    current_index += 1
            
            # Show gateway template tenants (additional to org, site, and policy tenants)
            template_only_tenants = [tenant for tenant in available_gateway_template_tenants 
                                   if tenant not in available_org_tenants 
                                   and tenant not in available_site_tenants 
                                   and tenant not in available_service_policy_tenants]
            if template_only_tenants:
                print(f"  Gateway Template Tenants ({len(template_only_tenants)}):")
                for tenant_name in template_only_tenants:
                    print(f"    [{current_index}] {tenant_name} (gateway templates)")
                    current_index += 1
            
            # Show device tenants (additional to all other sources)
            device_only_tenants = [tenant for tenant in available_device_tenants 
                                 if tenant not in available_org_tenants 
                                 and tenant not in available_site_tenants 
                                 and tenant not in available_service_policy_tenants
                                 and tenant not in available_gateway_template_tenants]
            if device_only_tenants:
                print(f"  Device Configuration Tenants ({len(device_only_tenants)}):")
                for tenant_name in device_only_tenants:
                    print(f"    [{current_index}] {tenant_name} (device config)")
                    current_index += 1
            
            # Show any remaining tenants that don't fit into the above categories (e.g., defaults)
            remaining_tenants = [tenant for tenant in all_available_tenants 
                               if tenant not in available_org_tenants 
                               and tenant not in available_site_tenants 
                               and tenant not in available_service_policy_tenants
                               and tenant not in available_gateway_template_tenants
                               and tenant not in available_device_tenants]
            if remaining_tenants:
                print(f"  Additional Tenants ({len(remaining_tenants)}):")
                for tenant_name in remaining_tenants:
                    print(f"    [{current_index}] {tenant_name} (default/custom)")
                    current_index += 1
            
            # Find the index of "testing-tools" for default selection
            testing_tools_index = None
            if "testing-tools" in all_available_tenants:
                testing_tools_index = all_available_tenants.index("testing-tools")
            
            print(f"  [{len(all_available_tenants)}] Skip tenant selection")
            
            while True:
                try:
                    if testing_tools_index is not None:
                        selection = input(f"\nSelect tenant index (0-{len(all_available_tenants)}) [default: {testing_tools_index} (testing-tools)]: ").strip()
                    else:
                        selection = input(f"\nSelect tenant index (0-{len(all_available_tenants)}) [default: skip]: ").strip()
                    
                    if not selection:
                        # Default behavior
                        if testing_tools_index is not None:
                            tenant = all_available_tenants[testing_tools_index]
                            print(f"!? Using default tenant: {tenant}")
                        break
                    
                    selection_index = int(selection)
                    if 0 <= selection_index < len(all_available_tenants):
                        tenant = all_available_tenants[selection_index]
                        
                        # Show which source the tenant came from
                        if tenant in available_org_tenants:
                            print(f"!? Selected organization tenant: {tenant}")
                        elif tenant in available_site_tenants:
                            print(f"!? Selected site tenant: {tenant}")
                        elif tenant in available_service_policy_tenants:
                            print(f"!? Selected service policy tenant: {tenant}")
                        elif tenant in available_gateway_template_tenants:
                            print(f"!? Selected gateway template tenant: {tenant}")
                        elif tenant in available_device_tenants:
                            print(f"!? Selected device configuration tenant: {tenant}")
                        else:
                            print(f"!? Selected default/custom tenant: {tenant}")
                        break
                    elif selection_index == len(all_available_tenants):
                        # Skip tenant selection
                        print("!? Skipping tenant selection")
                        break
                    else:
                        print(f"Please enter a number between 0 and {len(all_available_tenants)}")
                except ValueError:
                    print("Please enter a valid number")
                except KeyboardInterrupt:
                    print("\nOperation cancelled")
                    return
        else:
            print("\n-> No tenants found in organization networks, site networks, service policies, gateway templates, or device configuration")
            
            # For SSR service ping, tenant is often required, so offer manual entry
            manual_tenant = input("-> Enter tenant name manually (or press Enter to skip): ").strip()
            if manual_tenant:
                tenant = manual_tenant
                print(f"!? Manual tenant: {tenant}")
            else:
                print("-> Proceeding without tenant (may cause service ping to fail)")
                tenant = None
        
        # Service selection - now using combined organization and device services
        service = None
        if all_available_services:
            print("\nAvailable Services:")
            
            # Show organization services first
            org_service_count = len(available_org_services)
            if org_service_count > 0:
                print(f"  Organization Services ({org_service_count}):")
                for index, service_name in enumerate(available_org_services):
                    # Find the service details from org_services
                    service_details = next((svc for svc in org_services if svc['name'] == service_name), {})
                    service_type = service_details.get('type', 'custom')
                    service_desc = service_details.get('description', '')
                    if service_desc:
                        print(f"    [{index}] {service_name} ({service_type}) - {service_desc}")
                    else:
                        print(f"    [{index}] {service_name} ({service_type})")
            
            # Show device services if any (additional to org services)
            device_only_services = [svc for svc in available_device_services if svc not in available_org_services]
            device_service_start_index = org_service_count
            if device_only_services:
                print(f"  Device Configuration Services ({len(device_only_services)}):")
                for index, service_name in enumerate(device_only_services, start=device_service_start_index):
                    print(f"    [{index}] {service_name} (device config)")
            
            # Show any remaining services that don't fit into the above categories (e.g., defaults like "any")
            remaining_services = [svc for svc in all_available_services 
                                if svc not in available_org_services 
                                and svc not in available_device_services]
            remaining_service_start_index = device_service_start_index + len(device_only_services)
            if remaining_services:
                print(f"  Additional Services ({len(remaining_services)}):")
                for index, service_name in enumerate(remaining_services, start=remaining_service_start_index):
                    print(f"    [{index}] {service_name} (default/custom)")
            
            # Find the index of "web-session" for default selection
            web_session_service_index = None
            if "web-session" in all_available_services:
                web_session_service_index = all_available_services.index("web-session")
            
            print(f"  [{len(all_available_services)}] Enter custom service name")
            
            while True:
                try:
                    if web_session_service_index is not None:
                        selection = input(f"\nSelect service index (0-{len(all_available_services)}) or enter custom [default: {web_session_service_index} (web-session)]: ").strip()
                    else:
                        selection = input(f"\nSelect service index (0-{len(all_available_services)}) or enter custom: ").strip()
                    
                    # Check if it's a number (index selection)
                    try:
                        if not selection:
                            # Default behavior
                            if web_session_service_index is not None:
                                service = all_available_services[web_session_service_index]
                                print(f"!? Using default service: {service}")
                                break
                            else:
                                print("Please enter a service name or select from the list")
                                continue
                        
                        selection_index = int(selection)
                        if 0 <= selection_index < len(all_available_services):
                            service = all_available_services[selection_index]
                            
                            # Show which source the service came from
                            if service in available_org_services:
                                print(f"!? Selected organization service: {service}")
                                # Show service details if available
                                service_details = next((svc for svc in org_services if svc['name'] == service), {})
                                if service_details.get('description'):
                                    print(f"  Description: {service_details['description']}")
                                if service_details.get('type'):
                                    print(f"  Type: {service_details['type']}")
                            elif service in available_device_services:
                                print(f"!? Selected device configuration service: {service}")
                            else:
                                print(f"!? Selected default/custom service: {service}")
                            break
                        elif selection_index == len(all_available_services):
                            # Enter custom service name
                            service = input("Enter custom service name: ").strip()
                            if service:
                                print(f"!? Custom service: {service}")
                                break
                            else:
                                print("Service name cannot be empty")
                        else:
                            print(f"Please enter a number between 0 and {len(all_available_services)}")
                    except ValueError:
                        # Not a number, treat as custom service name
                        if selection:
                            service = selection
                            print(f"!? Custom service: {service}")
                            break
                        else:
                            print("Please enter a service name or select from the list")
                except KeyboardInterrupt:
                    print("\nOperation cancelled")
                    return
        else:
            # If no services found in either source, require manual input
            print("\n-> No services found in organization or device configuration")
            while True:
                service = input("Enter service name: ").strip()
                if service:
                    print(f"!? Custom service: {service}")
                    break
                print("Service is required. Please enter a service name.")
        
        # Required: Host to ping (with default)
        host = input("\nEnter target host/IP to ping [default: 8.8.8.8]: ").strip()
        if not host:
            host = "8.8.8.8"
            print("!? Using default destination: 8.8.8.8")
        
        # Optional: Count (default 4)
        count_input = input("Enter ping count [default: 4]: ").strip()
        try:
            count = int(count_input) if count_input else 4
            if count <= 0:
                count = 4
        except ValueError:
            count = 4
        
        # Optional: Size (default 56, min 56, max 65535)
        size_input = input("Enter packet size in bytes [default: 56]: ").strip()
        try:
            size = int(size_input) if size_input else 56
            if size < 56:
                size = 56
            elif size > 65535:
                size = 65535
        except ValueError:
            size = 56
        
        # Optional: HA node (node0, node1)
        node_input = input("Enter HA node (node0/node1) [optional]: ").strip().lower()
        node = node_input if node_input in ['node0', 'node1'] else None
        
        # Build service ping payload
        # Based on Mist API documentation schema: utils_service_ping
        # Example: {"count": 10, "host": "1.1.1.1", "service": "web-session"}
        service_ping_payload = {
            "host": host,
            "service": service,
            "count": count,
            "size": size
        }
        
        # Add optional parameters if specified
        if tenant:
            service_ping_payload["tenant"] = tenant
        if node:
            service_ping_payload["node"] = node
        
        print("\n" + "-" * 50)
        print(f"Service Ping Configuration:")
        print(f"  Host: {host}")
        print(f"  Service: {service}")
        print(f"  Count: {count}")
        print(f"  Size: {size} bytes")
        if tenant:
            print(f"  Tenant: {tenant}")
        if node:
            print(f"  HA Node: {node}")
        print("-" * 50)
        
        # Validate service name format
        if debug_mode:
            if service in ["web-session", "LANS", "RBO_SSH"]:
                print(f"[DEBUG] Using known valid service: {service}")
            else:
                print(f"[DEBUG] Using custom service: {service} (may not exist on device)")
        
        print(f"\n-> Executing Service Ping on device {device_id}...")
        
        # Initialize WebSocket manager
        websocket_manager = WebSocketManager(apisession)
        
        # Connect to WebSocket
        if not websocket_manager.connect():
            print("! Failed to establish WebSocket connection")
            return
            
        if debug_mode:
            print("[DEBUG] WebSocket connection established")
        
        # Subscribe to device command channel
        command_channel = f"/sites/{site_id}/devices/{device_id}/cmd"
        if not websocket_manager.subscribe_to_channel(command_channel):
            print("! Failed to subscribe to device command channel")
            return
            
        if debug_mode:
            print(f"[DEBUG] Subscribed to channel: {command_channel}")
        
        print("-> WebSocket connected and subscribed")
        
        # Wait for subscription confirmation before sending command
        print("-> Waiting for subscription confirmation...")
        if not websocket_manager.wait_for_subscription_confirmation(command_channel, timeout_seconds=15):
            print("! Subscription confirmation not received within timeout")
            print("! Proceeding anyway, but results may not be received")
        else:
            print("-> Subscription confirmed")
        
        print("-> Issuing Service Ping command...")
        
        # Execute service ping using mistapi library
        if debug_mode:
            print(f"[DEBUG] Using mistapi.api.v1.sites.devices.servicePingFromSsr")
            print(f"[DEBUG] Service ping payload being sent:")
            print(f"[DEBUG]   {service_ping_payload}")
            logging.debug(f"Service ping payload: {service_ping_payload}")
        
        # Always log the service ping request details to the log file
        logging.info(f"Sending service ping via mistapi to device: {device_id}")
        logging.info(f"Service ping payload: {service_ping_payload}")
        
        try:
            # Use mistapi library instead of direct requests
            response = mistapi.api.v1.sites.devices.servicePingFromSsr(
                apisession, site_id, device_id, service_ping_payload
            )
            
            # Always log the response details to the log file
            logging.info(f"Service ping mistapi response status: {response.status_code}")
            logging.info(f"Service ping mistapi response data: {response.data}")
            
            if debug_mode:
                print(f"[DEBUG] mistapi Response Status = {response.status_code}")
                print(f"[DEBUG] mistapi Response Data = {response.data}")
                
            if response.status_code == 200:
                result = response.data
                session_id = result.get("session", "")
                if debug_mode:
                    print(f"[DEBUG] Response data parsed: {result}")
                    print(f"[DEBUG] Extracted session_id: {session_id}")
                    logging.debug(f"Service ping response data: {result}")
                    logging.debug(f"Service ping session_id: {session_id}")
                if session_id:
                    short_session_id = session_id[:8] + "..." if len(session_id) > 8 else session_id
                    print(f"-> Service Ping command issued (session: {short_session_id})")
                    if debug_mode:
                        print(f"[DEBUG] Full session ID: {session_id}")
                else:
                    print("-> Service Ping command issued (no session ID returned)")
                    session_id = None
            else:
                error_msg = f"Failed to issue Service Ping command. mistapi status {response.status_code}: {response.data}"
                print(error_msg)
                logging.error(error_msg)
                if debug_mode:
                    print(f"[DEBUG] mistapi request failed - Status: {response.status_code}")
                    print(f"[DEBUG] mistapi response data: {response.data}")
                    logging.debug(f"Service ping failed - mistapi response: {response.data}")
                return
                
        except Exception as api_error:
            error_msg = f"Error issuing Service Ping command via mistapi: {api_error}"
            print(error_msg)
            logging.error(error_msg)
            if debug_mode:
                print(f"[DEBUG] mistapi exception details: {type(api_error).__name__}: {api_error}")
                logging.debug(f"Service ping mistapi exception: {type(api_error).__name__}: {api_error}")
            return
        
        # Proceed only if we have a session ID
        if not session_id:
            print("! No session ID received - cannot wait for results")
            return
        
        # Wait for WebSocket results
        print("-> Waiting for Service Ping results...")
        
        if debug_mode:
            print(f"[DEBUG] Full session ID = {session_id}")
            print("[DEBUG] Starting to wait for WebSocket results...")
        
        # Use device-specific timeout
        if device_info and device_info.get('type') == 'gateway':
            timeout_seconds = 45  # Extended timeout for gateways
            activity_timeout_seconds = 5  # Longer activity timeout for service ping
            print("   -> Using extended timeout for SSR gateway (45 seconds total, 5 seconds activity)")
        else:
            timeout_seconds = 30  # Standard timeout for other devices
            activity_timeout_seconds = 3  # Moderate activity timeout for non-gateways
        
        service_ping_result = websocket_manager.wait_for_command_result(
            session_id, 
            timeout_seconds=timeout_seconds, 
            activity_timeout_seconds=activity_timeout_seconds
        )
        
        if debug_mode:
            print(f"[DEBUG] wait_for_command_result returned: {service_ping_result is not None}")
            if service_ping_result:
                print(f"[DEBUG] Result keys: {list(service_ping_result.keys())}")
                print(f"[DEBUG] Service ping operation completed successfully")
            else:
                print(f"[DEBUG] Service ping operation timed out or failed")
        
        # Safety check: Ensure WebSocket manager is cleaned up
        try:
            if hasattr(websocket_manager, 'ws') and websocket_manager.ws:
                websocket_manager.close_connection()
                if debug_mode:
                    print(f"[DEBUG] WebSocket connection cleaned up")
        except Exception as cleanup_error:
            if debug_mode:
                print(f"[DEBUG] WebSocket cleanup warning: {cleanup_error}")
            logging.warning(f"WebSocket cleanup issue: {cleanup_error}")
        
        # Display results
        if service_ping_result:
            print("\n" + "=" * 60)
            print("SERVICE PING RESULTS:")
            print("=" * 60)
            
            # Add device-specific context
            if device_info:
                device_type = device_info.get('type', 'unknown')
                device_model = device_info.get('model', 'unknown')
                device_name = device_info.get('name', 'Unknown Device')
                
                print(f"Device: {device_name} ({device_type.upper()}: {device_model})")
                print(f"Service: {service} -> Host: {host}")
                
                if device_type == 'gateway':
                    print("Note: Service-specific routing path used for ping packets")
                else:
                    print("Note: Device may not fully support service ping functionality")
                    
                print("-" * 60)
            
            # Display raw output if available
            raw_output = service_ping_result.get("raw", "")
            if raw_output:
                print("PING OUTPUT:")
                print("-" * 40)
                print(raw_output)
            
            # Display parsed output if available  
            parsed_output = service_ping_result.get("Output", "")
            if parsed_output and parsed_output != raw_output:
                print("\nPARSED OUTPUT:")
                print("-" * 40)
                print(parsed_output)
            
            if not raw_output and not parsed_output:
                print("No output data received")
                if device_info and device_info.get('type') != 'gateway':
                    print("\nTroubleshooting for non-gateway devices:")
                    print("-> Service Ping is designed specifically for SSR gateways")
                    print("-> Try using regular ping (Menu 87) instead")
                    print("-> Verify device supports service ping functionality")
                
            print("=" * 60)
            
            # Log the successful operation with device context
            if device_info:
                device_name = device_info.get('name', 'Unknown Device')
                device_type = device_info.get('type', 'unknown')
                logging.info(f"Service ping completed for {device_name} ({device_type}) - Service: {service}, Host: {host}")
            else:
                logging.info(f"Service ping completed for device {device_id} - Service: {service}, Host: {host}")
        else:
            print("\nNo Service Ping results received within timeout period.")
            
            # Provide device-specific troubleshooting guidance
            if device_info:
                device_type = device_info.get('type', 'unknown')
                device_name = device_info.get('name', 'Unknown Device')
                print(f"Device: {device_name} ({device_type})")
                
                if device_type == 'gateway':
                    print("\nTroubleshooting for SSR gateways:")
                    print("-> Verify service name is valid for this SSR")
                    print("-> Check if host is reachable through the specified service")
                    print("-> Confirm SSR routing configuration for the service")
                    print("-> Try with a different service name")
                elif device_type == 'switch':
                    print("\nNote: Switches typically do not support service ping")
                    print("-> Try using regular ping (Menu 87) for basic connectivity")
                    print("-> Service ping is an SSR-specific feature")
                elif device_type == 'ap':
                    print("\nNote: Access Points do not support service ping")
                    print("-> Try using regular ping (Menu 87) for basic connectivity") 
                    print("-> Service ping is an SSR-specific feature")
                else:
                    print("\nNote: Service ping is designed for SSR gateways")
                    print("-> Try using regular ping (Menu 87) for basic connectivity")
                
            logging.warning(f"Service ping timeout - no results received for device {device_id}")
            
    except KeyboardInterrupt:
        print("\nOperation cancelled by user")
        logging.info("Service ping operation cancelled by user")
        
    except Exception as error:
        print(f"Error during Service Ping operation: {error}")
        logging.error(f"Service ping error: {error}")
        logging.debug("EXIT: service_ping_device_websocket - error")
        
    finally:
        # Always cleanup WebSocket connection
        try:
            if 'websocket_manager' in locals():
                websocket_manager.disconnect()
                print("-> WebSocket connection closed")
        except Exception as cleanup_error:
            logging.warning(f"WebSocket cleanup error: {cleanup_error}")
            
        logging.debug("EXIT: service_ping_device_websocket")


def parse_forwarding_table_output(raw_output):
    """
    Parse the raw JSON forwarding table output into structured data.
    
    Args:
        raw_output (str): Raw JSON string containing forwarding table data
        
    Returns:
        list: List of parsed forwarding table entries
    """
    entries = []
    
    if not raw_output or not raw_output.strip():
        return entries
    
    try:
        # Split multiple JSON objects (WebSocket can send multiple chunks)
        json_chunks = []
        for line in raw_output.strip().split('\n'):
            line = line.strip()
            if line and line.startswith('{') and line.endswith('}'):
                try:
                    chunk = json.loads(line)
                    json_chunks.append(chunk)
                except json.JSONDecodeError:
                    continue
        
        # Process all chunks and extract forwarding table rows
        for chunk in json_chunks:
            if isinstance(chunk, dict) and 'rows' in chunk:
                rows = chunk.get('rows', [])
                for row in rows:
                    if isinstance(row, dict):
                        # Clean up None values and empty strings
                        cleaned_entry = {}
                        for key, value in row.items():
                            if value == "None" or value == "":
                                cleaned_entry[key] = "-"
                            else:
                                cleaned_entry[key] = value
                        entries.append(cleaned_entry)
    
    except Exception as e:
        logging.error(f"Error parsing forwarding table output: {e}")
        # Return raw data if parsing fails
        return [{"raw_data": raw_output}]
    
    return entries


def display_forwarding_table_summary(entries):
    """
    Display a user-friendly summary of forwarding table entries.
    
    Args:
        entries (list): List of forwarding table entry dictionaries
    """
    if not entries:
        print("-> No forwarding table entries found")
        return
    
    # Handle raw data fallback
    if len(entries) == 1 and "raw_data" in entries[0]:
        print("-> Raw forwarding table data (parsing failed):")
        print(entries[0]["raw_data"][:1000] + "..." if len(entries[0]["raw_data"]) > 1000 else entries[0]["raw_data"])
        return
    
    print(f"-> Total forwarding table entries: {len(entries)}")
    
    # Analyze the data for summary statistics
    prefixes = set()
    services = set()
    tenants = set()
    protocols = set()
    interfaces = set()
    
    for entry in entries:
        if entry.get('ip_prefix') and entry['ip_prefix'] != '-':
            prefixes.add(entry['ip_prefix'])
        if entry.get('service') and entry['service'] != '-':
            services.add(entry['service'])
        if entry.get('tenant') and entry['tenant'] != '-':
            tenants.add(entry['tenant'])
        if entry.get('protocol') and entry['protocol'] != '-':
            protocols.add(entry['protocol'])
        if entry.get('next_hops_interface') and entry['next_hops_interface'] != '-':
            interfaces.add(entry['next_hops_interface'])
    
    # Display summary statistics
    print(f"-> Unique IP prefixes: {len(prefixes)}")
    print(f"-> Unique services: {len(services)}")
    print(f"-> Unique tenants: {len(tenants)}")
    print(f"-> Protocols: {', '.join(sorted(protocols)) if protocols else 'None'}")
    print(f"-> Next-hop interfaces: {len(interfaces)}")
    
    # Group entries by IP prefix for better readability
    prefix_groups = {}
    for entry in entries:
        prefix = entry.get('ip_prefix', 'Unknown')
        if prefix not in prefix_groups:
            prefix_groups[prefix] = []
        prefix_groups[prefix].append(entry)
    
    # Display detailed table for all prefixes (removed artificial limiting)
    print(f"\n-> Detailed forwarding table by IP prefix:")
    for prefix in sorted(prefix_groups.keys()):
        display_prefix_table(prefix, prefix_groups[prefix])
    
    # Show interface summary
    if interfaces:
        print(f"\n-> Active next-hop interfaces:")
        for interface in sorted(interfaces):
            if interface != '-':
                interface_entries = [e for e in entries if e.get('next_hops_interface') == interface]
                print(f"   {interface}: {len(interface_entries)} routes")


def display_prefix_table(prefix, entries):
    """
    Display a formatted table for entries with a specific IP prefix.
    
    Args:
        prefix (str): IP prefix to display
        entries (list): List of forwarding table entries for this prefix
    """
    if not entries:
        return
    
    print(f"\n-> Routes for {prefix} ({len(entries)} entries):")
    
    # Use prettytable if available, otherwise fall back to simple formatting
    try:
        table = PrettyTable()
        table.field_names = ["Port", "Protocol", "Service", "Tenant", "Next Hop Interface", "Vector", "Cost"]
        table.align = "l"
        table.max_width = 20
        
        # Show ALL entries (removed truncation limit)
        for entry in entries:
            table.add_row([
                entry.get('port', '-'),
                entry.get('protocol', '-'),
                entry.get('service', '-')[:18] + '..' if len(entry.get('service', '-')) > 20 else entry.get('service', '-'),
                entry.get('tenant', '-')[:15] + '..' if len(entry.get('tenant', '-')) > 17 else entry.get('tenant', '-'),
                entry.get('next_hops_interface', '-')[:15] + '..' if len(entry.get('next_hops_interface', '-')) > 17 else entry.get('next_hops_interface', '-'),
                entry.get('vector', '-'),
                entry.get('cost', '-')
            ])
        
        print(table)
            
    except Exception as e:
        # Fallback to simple text formatting if prettytable fails
        print("   Port   | Protocol | Service              | Tenant           | Next Hop Interface")
        print("   " + "-" * 80)
        # Show ALL entries (removed truncation limit)
        for entry in entries:
            port = entry.get('port', '-')[:6]
            protocol = entry.get('protocol', '-')[:8]
            service = entry.get('service', '-')[:20]
            tenant = entry.get('tenant', '-')[:16]
            interface = entry.get('next_hops_interface', '-')[:18]
            print(f"   {port:<6} | {protocol:<8} | {service:<20} | {tenant:<16} | {interface}")


def parse_routing_table_output(raw_output):
    """
    Parse routing table output from Mist device show route command.
    
    This function handles routing table output from various device types including:
    - Switches with Layer 3 capabilities
    - SRX routers
    - SSR gateways
    - Other routing-capable devices
    
    The output contains routing protocol information (RIB) showing routes learned
    from BGP, OSPF, static configuration, directly connected networks, etc.
    
    Args:
        raw_output (str): Raw output from show route command
        
    Returns:
        list: List of parsed routing table entries as dictionaries
    """
    if not raw_output:
        return []
    
    # Detect Juniper routing table format and use specialized parser
    if any(pattern in raw_output for pattern in ['inet.0:', 'inet6.0:', 'Limit/Threshold:']):
        return parse_juniper_routing_table(raw_output)
    
    # Fallback to generic parsing for other device types
    routes = []
    lines = raw_output.strip().split('\n')
    
    # Look for common routing table patterns across different device types
    # Handle various output formats from different device vendors/models
    
    for line_num, line in enumerate(lines):
        line = line.strip()
        if not line or line.startswith('#') or line.startswith('show'):
            continue
            
        # Parse common routing table patterns
        # Different devices may have different output formats
        
        # Pattern 1: Standard route entry with prefix, next-hop, protocol
        # Example: "192.168.1.0/24 via 10.0.0.1 dev eth0 proto bgp metric 100"
        if ' via ' in line or ' dev ' in line or ' proto ' in line:
            route_entry = parse_standard_route_line(line)
            if route_entry:
                routes.append(route_entry)
                
        # Pattern 2: Tabular format with columns
        # Look for lines that appear to be route entries based on common fields
        elif any(indicator in line.lower() for indicator in ['bgp', 'ospf', 'static', 'direct', 'connected']):
            route_entry = parse_protocol_route_line(line)
            if route_entry:
                routes.append(route_entry)
                
        # Pattern 3: JSON-like structured output (some devices)
        elif line.startswith('{') and line.endswith('}'):
            try:
                import json
                route_data = json.loads(line)
                route_entry = normalize_json_route_entry(route_data)
                if route_entry:
                    routes.append(route_entry)
            except:
                continue
                
        # Pattern 4: Space-separated tabular data
        elif len(line.split()) >= 3:
            route_entry = parse_tabular_route_line(line)
            if route_entry:
                routes.append(route_entry)
    
    return routes


def parse_juniper_routing_table(raw_output):
    """
    Parse Juniper-specific routing table output format.
    
    Juniper routing tables have a specific multi-line format:
    - Route table sections (inet.0:, inet6.0:)
    - Multi-line route entries with continuation lines
    - Route preferences indicated by >, *, +
    - Protocol information in brackets like *[Direct/0]
    
    Example format:
    0.0.0.0/0        *[Static/5] 00:01:02
                    > via 192.168.1.1, irb.0
    192.168.1.0/24   *[Direct/0] 1d 02:03:04
                    > via irb.0
    
    Args:
        raw_output (str): Raw routing table output from Juniper device
        
    Returns:
        list: Parsed route entries
    """
    routes = []
    lines = raw_output.strip().split('\n')
    current_route = None
    current_table = ""
    
    for line in lines:
        line_stripped = line.strip()
        
        # Skip empty lines and headers
        if not line_stripped:
            continue
            
        # Detect routing table sections
        if line_stripped.endswith(':') and any(table in line_stripped for table in ['inet.0', 'inet6.0', 'mpls.0']):
            current_table = line_stripped.replace(':', '')
            continue
            
        # Skip limit/threshold lines
        if 'Limit/Threshold' in line_stripped or line_stripped == '+':
            continue
            
        # Parse route entries
        parts = line_stripped.split()
        if not parts:
            continue
            
        # Main route line (starts with destination prefix, not > or *)
        if ('/' in parts[0] and not parts[0].startswith('>') and not parts[0].startswith('*') 
            and not line_stripped.startswith(' ') and not line_stripped.startswith('>')):
            
            # Save previous route if exists
            if current_route:
                routes.append(current_route)
                
            # Start new route entry
            current_route = {
                'destination': parts[0],
                'next_hop': '',
                'interface': '',
                'protocol': '',
                'metric': '',
                'admin_distance': '',
                'table': current_table,
                'active': False,
                'selected': False
            }
            
            # Look for protocol information in brackets on same line
            line_remainder = ' '.join(parts[1:])
            if '[' in line_remainder and ']' in line_remainder:
                # Extract protocol and admin distance from [Protocol/admin_dist]
                bracket_start = line_remainder.find('[')
                bracket_end = line_remainder.find(']')
                bracket_content = line_remainder[bracket_start + 1:bracket_end]
                
                # Mark as selected if starts with *
                if line_remainder.startswith('*'):
                    current_route['selected'] = True
                    
                if '/' in bracket_content:
                    protocol_parts = bracket_content.split('/')
                    current_route['protocol'] = protocol_parts[0]
                    current_route['admin_distance'] = protocol_parts[1]
                else:
                    current_route['protocol'] = bracket_content
                    
        # Continuation lines (indented or start with > or *)
        elif (current_route and (line_stripped.startswith('>') or line_stripped.startswith('*') 
                                or line.startswith(' ') or line.startswith('\t'))):
            
            # Mark route as active if line starts with >
            if line_stripped.startswith('>'):
                current_route['active'] = True
                line_stripped = line_stripped[1:].strip()
                
            # Parse via and interface information
            if ' via ' in line_stripped:
                # Extract next hop and interface
                via_parts = line_stripped.split(' via ')[1].strip()
                if ',' in via_parts:
                    # Format: "via 192.168.1.1, irb.0"
                    next_hop_part, interface_part = via_parts.split(',', 1)
                    current_route['next_hop'] = next_hop_part.strip()
                    current_route['interface'] = interface_part.strip()
                else:
                    # Format: "via irb.0" (direct interface)
                    if '.' in via_parts and not '/' in via_parts:
                        current_route['interface'] = via_parts.strip()
                    else:
                        current_route['next_hop'] = via_parts.strip()
            
            # Look for standalone interface (like "Local" or interface names)
            elif line_stripped in ['Local']:
                current_route['next_hop'] = 'Local'
            elif '.' in line_stripped and len(line_stripped.split()) == 1:
                current_route['interface'] = line_stripped
    
    # Add the last route
    if current_route:
        routes.append(current_route)
    
    return routes


def parse_standard_route_line(line):
    """Parse a standard route line with via/dev/proto keywords."""
    try:
        parts = line.split()
        if len(parts) < 2:
            return None
            
        route_entry = {
            'destination': parts[0] if parts[0] != '*' else 'default',
            'next_hop': '',
            'interface': '',
            'protocol': '',
            'metric': '',
            'admin_distance': ''
        }
        
        # Extract next hop
        if ' via ' in line:
            via_index = line.find(' via ')
            via_part = line[via_index + 5:].split()[0]
            route_entry['next_hop'] = via_part
            
        # Extract interface  
        if ' dev ' in line:
            dev_index = line.find(' dev ')
            dev_part = line[dev_index + 5:].split()[0]
            route_entry['interface'] = dev_part
            
        # Extract protocol
        if ' proto ' in line:
            proto_index = line.find(' proto ')
            proto_part = line[proto_index + 7:].split()[0]
            route_entry['protocol'] = proto_part
            
        # Extract metric
        if ' metric ' in line:
            metric_index = line.find(' metric ')
            metric_part = line[metric_index + 8:].split()[0]
            route_entry['metric'] = metric_part
            
        return route_entry
        
    except Exception as e:
        return None


def parse_protocol_route_line(line):
    """Parse a route line containing protocol information."""
    try:
        parts = line.split()
        if len(parts) < 2:
            return None
            
        route_entry = {
            'destination': '',
            'next_hop': '',
            'interface': '',
            'protocol': '',
            'metric': '',
            'admin_distance': ''
        }
        
        # Look for destination (first IP-like field)
        for i, part in enumerate(parts):
            if '/' in part or any(char.isdigit() for char in part) and '.' in part:
                route_entry['destination'] = part
                break
                
        # Identify protocol
        for protocol in ['bgp', 'ospf', 'static', 'direct', 'connected', 'kernel', 'evpn']:
            if protocol in line.lower():
                route_entry['protocol'] = protocol.upper()
                break
                
        # Look for next hop (IP address pattern)
        import re
        ip_pattern = r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b'
        ip_matches = re.findall(ip_pattern, line)
        if len(ip_matches) > 1:  # First might be destination
            route_entry['next_hop'] = ip_matches[1]
        elif len(ip_matches) == 1 and route_entry['destination'] != ip_matches[0]:
            route_entry['next_hop'] = ip_matches[0]
            
        return route_entry
        
    except Exception as e:
        return None


def parse_tabular_route_line(line):
    """Parse a space-separated tabular route line."""
    try:
        parts = line.split()
        if len(parts) < 3:
            return None
            
        route_entry = {
            'destination': parts[0] if parts[0] not in ['*', '>'] else (parts[1] if len(parts) > 1 else ''),
            'next_hop': '',
            'interface': '',
            'protocol': '',
            'metric': '',
            'admin_distance': ''
        }
        
        # Common patterns for different positions
        for i, part in enumerate(parts[1:], 1):
            # Look for next hop (IP address)
            if '.' in part and any(char.isdigit() for char in part) and part != route_entry['destination']:
                if not route_entry['next_hop']:
                    route_entry['next_hop'] = part
                    
            # Look for interface (often starts with common prefixes)
            elif any(part.lower().startswith(prefix) for prefix in ['eth', 'ge-', 'xe-', 'fe-', 'lo', 'vlan']):
                route_entry['interface'] = part
                
            # Look for protocol indicators
            elif part.upper() in ['BGP', 'OSPF', 'STATIC', 'DIRECT', 'CONNECTED', 'KERNEL']:
                route_entry['protocol'] = part.upper()
                
            # Look for metric (numeric)
            elif part.isdigit() and not route_entry['metric']:
                route_entry['metric'] = part
                
        return route_entry
        
    except Exception as e:
        return None


def normalize_json_route_entry(route_data):
    """Normalize a JSON route entry to standard format."""
    try:
        route_entry = {
            'destination': route_data.get('destination', route_data.get('prefix', '')),
            'next_hop': route_data.get('next_hop', route_data.get('nexthop', route_data.get('gateway', ''))),
            'interface': route_data.get('interface', route_data.get('dev', route_data.get('outgoing_interface', ''))),
            'protocol': route_data.get('protocol', route_data.get('proto', '')).upper(),
            'metric': str(route_data.get('metric', route_data.get('cost', ''))),
            'admin_distance': str(route_data.get('admin_distance', route_data.get('distance', '')))
        }
        return route_entry
    except:
        return None


def parse_ssr_routing_json(json_data):
    """
    Parse SSR/SRX routing table JSON data from the dedicated API.
    
    The SSR API returns structured JSON with columns definition and rows data:
    {
        "status": "SUCCESS",
        "finished": true,
        "message": "Bgp Routes Table",
        "columns": [
            {"id": "vrfName", "display_name": "Vrf Name", "type": "STRING"},
            {"id": "prefix", "display_name": "Prefix", "type": "STRING"},
            {"id": "name", "display_name": "Name", "type": "STRING"},
            {"id": "metric", "display_name": "Metric", "type": "NUMBER"},
            {"id": "weight", "display_name": "Weight", "type": "NUMBER"},
            {"id": "path", "display_name": "AS Path", "type": "STRING"},
            {"id": "localPreference", "display_name": "Local Preference", "type": "NUMBER"},
            {"id": "status", "display_name": "Status", "type": "STRING"},
            {"id": "selectionReason", "display_name": "Selection Reason", "type": "STRING"},
            {"id": "nextHops", "display_name": "Next Hops", "type": "STRING"}
        ],
        "rows": [...]
    }
    
    Args:
        json_data (str): Raw JSON string from SSR API
        
    Returns:
        list: List of parsed routing table entries as dictionaries
    """
    try:
        import json
        data = json.loads(json_data)
        
        if data.get("status") != "SUCCESS":
            return []
            
        columns = data.get("columns", [])
        rows = data.get("rows", [])
        
        if not columns or not rows:
            return []
        
        route_entries = []
        for row in rows:
            # Map SSR fields to standard routing table format
            route_entry = {
                'destination': row.get('prefix', ''),
                'next_hop': row.get('nextHops', ''),
                'interface': '',  # SSR doesn't provide interface in this API
                'protocol': 'BGP' if 'bgp' in data.get("message", "").lower() else 'Unknown',
                'admin_distance': '',  # Not directly provided
                'metric': str(row.get('metric', '')),
                'status': row.get('status', ''),
                'vrf': row.get('vrfName', 'default'),
                'name': row.get('name', ''),
                'weight': str(row.get('weight', '')),
                'as_path': row.get('path', ''),
                'local_preference': str(row.get('localPreference', '')),
                'selection_reason': row.get('selectionReason', '')
            }
            route_entries.append(route_entry)
            
        return route_entries
        
    except (json.JSONDecodeError, KeyError, TypeError) as e:
        logging.warning(f"Failed to parse SSR routing JSON: {e}")
        return []


def display_routing_table_summary(route_entries, query_params):
    """
    Display a formatted summary of routing table entries.
    
    Args:
        route_entries (list): List of parsed routing table entries
        query_params (dict): Original query parameters for context
    """
    if not route_entries:
        print("-> No routing table entries found")
        if query_params:
            print("  -> Try adjusting query parameters:")
            for key, value in query_params.items():
                print(f"    - {key}: {value}")
        return
    
    print(f"-> Total routing table entries: {len(route_entries)}")
    
    # Group routes by protocol for summary
    protocols = {}
    destinations = set()
    next_hops = set()
    interfaces = set()
    tables = set()
    active_routes = 0
    
    for entry in route_entries:
        protocol = entry.get('protocol', 'Unknown').upper()
        if protocol and protocol != 'UNKNOWN':
            protocols[protocol] = protocols.get(protocol, 0) + 1
        
        if entry.get('destination') and entry.get('destination') != '-':
            destinations.add(entry['destination'])
        if entry.get('next_hop') and entry.get('next_hop') not in ['-', '']:
            next_hops.add(entry['next_hop'])
        if entry.get('interface') and entry.get('interface') not in ['-', '']:
            interfaces.add(entry['interface'])
        if entry.get('table'):
            tables.add(entry['table'])
        if entry.get('active'):
            active_routes += 1
    
    # Display protocol summary  
    if protocols:
        print(f"-> Protocols: {', '.join([f'{proto}({count})' for proto, count in protocols.items()])}")
    
    # Display table summary if multiple tables
    if len(tables) > 1:
        print(f"-> Routing tables: {', '.join(sorted(tables))}")
    
    print(f"-> Unique destinations: {len(destinations)}")
    print(f"-> Unique next hops: {len(next_hops)}")
    print(f"-> Unique interfaces: {len(interfaces)}")
    
    if active_routes > 0:
        print(f"-> Active routes (marked with >): {active_routes}")
    
    # Display detailed routing table
    print(f"\n-> Detailed routing table:")
    display_routing_table_details(route_entries)


def display_routing_table_details(route_entries):
    """
    Display detailed routing table in a formatted table.
    
    Args:
        route_entries (list): List of parsed routing table entries
    """
    if not route_entries:
        return
        
    # Use prettytable if available, otherwise fall back to simple formatting
    try:
        table = PrettyTable()
        table.field_names = ["Status", "Destination", "Next Hop", "Interface", "Protocol", "Admin Dist"]
        table.align = "l"
        # Remove max_width to prevent truncation
        
        # Show all entries with full data (no truncation)
        for entry in route_entries:
            # Create status indicator
            status = ""
            if entry.get('active'):
                status += ">"
            if entry.get('selected'):
                status += "*"
            if not status:
                status = " "
                
            # Clean up fields - replace empty/None with dash
            dest = entry.get('destination', '-')
            if not dest or dest == '':
                dest = '-'
                
            next_hop = entry.get('next_hop', '-')
            if not next_hop or next_hop == '':
                next_hop = '-'
                
            interface = entry.get('interface', '-')
            if not interface or interface == '':
                interface = '-'
                
            protocol = entry.get('protocol', '-')
            if not protocol or protocol == '':
                protocol = '-'
                
            admin_dist = entry.get('admin_distance', '-')
            if not admin_dist or admin_dist == '':
                admin_dist = '-'
            
            # Add row without truncation
            table.add_row([
                status,
                dest,
                next_hop,
                interface,
                protocol,
                admin_dist
            ])
        
        print(table)
        
        # Add legend for status indicators
        print("\nStatus Legend:")
        print("  > = Active route (installed in forwarding table)")
        print("  * = Selected route (best route among alternatives)")
            
    except Exception as e:
        # Fallback to simple text formatting if prettytable fails
        print("   Status | Destination              | Next Hop        | Interface       | Protocol | Dist")
        print("   " + "-" * 95)
        for entry in route_entries:
            status = ""
            if entry.get('active'):
                status += ">"
            if entry.get('selected'):
                status += "*"
            if not status:
                status = " "
                
            dest = entry.get('destination', '-')
            next_hop = entry.get('next_hop', '-')
            interface = entry.get('interface', '-')
            protocol = entry.get('protocol', '-')
            admin_dist = entry.get('admin_distance', '-')
            print(f"   {status:<6} | {dest:<25} | {next_hop:<15} | {interface:<15} | {protocol:<8} | {admin_dist}")
        
        print("\nStatus Legend:")
        print("  > = Active route, * = Selected route")


def display_ssr_routing_table(route_entries, query_params):
    """
    Display SSR/SRX routing table with enhanced BGP-specific information.
    
    Args:
        route_entries (list): List of parsed SSR routing table entries
        query_params (dict): Original query parameters for context
    """
    if not route_entries:
        print("-> No routing table entries found")
        if query_params:
            print("  -> Try adjusting query parameters:")
            for key, value in query_params.items():
                print(f"    - {key}: {value}")
        return
    
    # Count and categorize routes
    total_routes = len(route_entries)
    protocols = {}
    vrfs = {}
    next_hops = set()
    
    for entry in route_entries:
        protocol = entry.get('protocol', 'Unknown')
        protocols[protocol] = protocols.get(protocol, 0) + 1
        
        vrf = entry.get('vrf', 'default')
        vrfs[vrf] = vrfs.get(vrf, 0) + 1
        
        next_hop = entry.get('next_hop', '')
        if next_hop and next_hop != '0.0.0.0':
            next_hops.add(next_hop)
    
    # Display summary
    print(f"-> Total routing table entries: {total_routes}")
    
    protocol_summary = ", ".join([f"{proto}({count})" for proto, count in protocols.items()])
    print(f"-> Protocols: {protocol_summary}")
    
    vrf_summary = ", ".join([f"{vrf}({count})" for vrf, count in vrfs.items()])
    print(f"-> VRFs: {vrf_summary}")
    
    print(f"-> Unique next hops: {len(next_hops)}")
    
    # Display detailed table using prettytable
    try:
        table = PrettyTable()
        table.field_names = ["Destination", "Next Hop", "Protocol", "Route Name", "Status", "Selection Reason", "Weight", "Metric", "Local Pref", "AS Path", "VRF"]
        table.align = "l"
        # Remove max_width to allow full data display
        
        for entry in route_entries:
            vrf = entry.get('vrf', 'default')
            destination = entry.get('destination', '-')
            next_hop = entry.get('next_hop', '-')
            protocol = entry.get('protocol', '-')
            status = entry.get('status', '-')
            weight = entry.get('weight', '-')
            metric = entry.get('metric', '-')
            local_pref = entry.get('local_preference', '-')
            as_path = entry.get('as_path', '-')
            route_name = entry.get('name', '-')
            selection_reason = entry.get('selection_reason', '-')
            
            # No truncation - show full data
            table.add_row([
                destination,
                next_hop,
                protocol,
                route_name,
                status,
                selection_reason,
                weight,
                metric,
                local_pref,
                as_path,
                vrf
            ])
        
        print(f"\n-> Detailed routing table:")
        print(table)
        
    except Exception as e:
        # Fallback to simple formatting
        print(f"\n-> Detailed routing table:")
        print("   Destination | Next Hop | Protocol | Route Name | Status | Selection Reason | Weight | Metric | Local Pref | AS Path | VRF")
        print("   " + "-" * 140)
        for entry in route_entries:
            dest = entry.get('destination', '-')
            next_hop = entry.get('next_hop', '-')
            protocol = entry.get('protocol', '-')
            route_name = entry.get('name', '-')
            status = entry.get('status', '-')
            selection_reason = entry.get('selection_reason', '-')
            weight = entry.get('weight', '-')
            metric = entry.get('metric', '-')
            local_pref = entry.get('local_preference', '-')
            as_path = entry.get('as_path', '-')
            vrf = entry.get('vrf', 'default')
            print(f"   {dest} | {next_hop} | {protocol} | {route_name} | {status} | {selection_reason} | {weight} | {metric} | {local_pref} | {as_path} | {vrf}")


def show_forwarding_table_websocket():
    """
    Execute show forwarding table command on a gateway/SSR device via WebSocket.
    
    Forwarding tables are a Layer 3 routing feature primarily used by routers and gateways
    to show the Forwarding Information Base (FIB) for packet forwarding decisions.
    This is different from MAC tables (Layer 2) and provides routing/forwarding information.
        
        Follows the documented Mist API pattern:
        1. Connect to WebSocket
        2. Subscribe to device command channel
        3. Issue POST show_forwarding_table command
        4. Await results via WebSocket stream
        
        SECURITY: Uses authenticated WebSocket connection with session-based
        command demultiplexing for concurrent command safety.
        """
    # Check for debug mode from command line arguments
    debug_mode = '--debug' in sys.argv or '-d' in sys.argv
    
    if debug_mode:
        logging.getLogger().setLevel(logging.DEBUG)
        print("[DEBUG] DEBUG MODE ENABLED")
    
    logging.info("Starting WebSocket show forwarding table operation...")
    logging.debug("ENTER: show_forwarding_table_websocket")
    
    try:
        # Interactive site selection
        site_id = prompt_select_site_id_from_csv()
        if not site_id:
            print("! No site selected. Operation cancelled.")
            return
        
        if debug_mode:
            print(f"[DEBUG] Selected site_id = {site_id}")
            
        # Get device selection - Forwarding table is a Layer 3 routing feature
        print("-> Forwarding table is available on routers and gateways (Layer 3 devices)")
        print("-> This shows the Forwarding Information Base (FIB) used for packet routing decisions")
        print("-> SSR gateways provide the most comprehensive forwarding table information")
        device_id = prompt_select_device_id_from_inventory(site_id, device_type="gateway")
        if not device_id:
            print("! No gateway device selected. Forwarding table command is optimized for Layer 3 routing devices.")
            print("! Gateways and SSR devices maintain forwarding tables for packet routing decisions.")
            return
        
        if debug_mode:
            print(f"[DEBUG] Selected device_id = {device_id}")
        
        # Get device details to check type and model for compatibility
        device_info = None
        try:
            rawdata = mistapi.api.v1.sites.devices.listSiteDevices(apisession, site_id, type="all").data
            device_info = next((device for device in rawdata if device.get('id') == device_id), None)
            
            if device_info:
                device_type = device_info.get('type', 'unknown')
                device_model = device_info.get('model', 'unknown')
                device_name = device_info.get('name', f"Device {device_id[:8]}")
                
                if debug_mode:
                    print(f"[DEBUG] Device type: {device_type}, model: {device_model}, name: {device_name}")
                
                # Provide device-specific guidance
                if device_type == 'gateway':
                    if 'SSR' in device_model.upper() or '128T' in device_model:
                        print(f"-> SSR gateway detected ({device_model}): Excellent forwarding table support")
                    else:
                        print(f"-> Gateway device detected ({device_model}): Good forwarding table support")
                elif device_type == 'switch':
                    print(f"!? Switch device ({device_model}): Limited forwarding table - primarily Layer 2")
                    print("  -> Consider using MAC table command for Layer 2 switching information")
                elif device_type == 'ap':
                    print(f"!? Access Point ({device_model}): No forwarding table - wireless bridging only")
                    print("  -> APs operate at Layer 2 and don't maintain routing tables")
                    
        except Exception as device_check_error:
            logging.warning(f"Could not verify device compatibility: {device_check_error}")
            if debug_mode:
                print(f"[DEBUG] Device check failed: {device_check_error}")
            print("   -> Proceeding with standard forwarding table command")
            
        print(f"\n-> Executing show forwarding table on device {device_id}...")
        print("-> Establishing WebSocket connection...")
        
        # Initialize WebSocket manager
        websocket_manager = WebSocketManager(apisession)
        
        if debug_mode:
            print("[DEBUG] WebSocketManager initialized")
        
        # Connect to WebSocket
        if not websocket_manager.connect():
            print("! Failed to establish WebSocket connection")
            return
            
        if debug_mode:
            print("[DEBUG] WebSocket connection established")
            
        # Subscribe to device command channel
        command_channel = f"/sites/{site_id}/devices/{device_id}/cmd"
        if not websocket_manager.subscribe_to_channel(command_channel):
            print("! Failed to subscribe to device command channel")
            websocket_manager.disconnect()
            return
        
        if debug_mode:
            print(f"[DEBUG] Subscribed to channel: {command_channel}")
            
        print("-> WebSocket connected and subscribed")
        
        # Wait a moment for subscription to be established
        time.sleep(1)
        
        # Issue show forwarding table command via REST API
        # According to Mist API docs: "prefix or/and service_name must be used for filtering forwarding table"
        print("\n=== Forwarding Table Lookup Parameters ===")
        print("The Mist API requires filtering parameters for forwarding table lookups.")
        print("You can provide:")
        print("  1. IP prefix (e.g., 192.168.1.0/24, 10.0.0.0/8)")
        print("  2. Service name (for SSR gateways)")
        print("  3. Both prefix and service name")
        print("  4. Leave empty to use default (0.0.0.0/0 - all routes)")
        
        # Get user input for filtering parameters
        prefix_input = input("\nEnter IP prefix (press Enter for default 0.0.0.0/0): ").strip()
        service_name_input = input("Enter service name (press Enter to skip): ").strip()
        vrf_input = input("Enter VRF name (press Enter to skip): ").strip()
        node_input = input("Enter node (node0/node1 for HA, press Enter to skip): ").strip()
        
        # Build the payload with required filtering
        forwarding_table_payload = {}
        
        # Use provided prefix or default to show all routes
        if prefix_input:
            forwarding_table_payload["prefix"] = prefix_input
        else:
            forwarding_table_payload["prefix"] = "0.0.0.0/0"  # Default to show all routes
            print("-> Using default prefix: 0.0.0.0/0 (all routes)")
        
        # Add optional parameters if provided
        if service_name_input:
            forwarding_table_payload["service_name"] = service_name_input
            
        if vrf_input:
            forwarding_table_payload["vrf"] = vrf_input
            
        if node_input and node_input.lower() in ["node0", "node1"]:
            forwarding_table_payload["node"] = node_input.lower()
        
        print("-> Issuing show forwarding table command...")
        logging.debug(f"Forwarding table payload: {forwarding_table_payload}")
        
        if debug_mode:
            print(f"[DEBUG] Forwarding table payload = {forwarding_table_payload}")
        
        # Get authentication details for direct HTTP request
        mist_host = getattr(apisession, "host", None) or os.getenv("MIST_HOST")
        mist_apitoken = getattr(apisession, "apitoken", None) or os.getenv("MIST_APITOKEN")
        
        if not mist_host or not mist_apitoken:
            print("! Mist host or API token not found in session or environment")
            websocket_manager.disconnect()
            return
        
        if debug_mode:
            print(f"[DEBUG] mist_host = {mist_host}")
            print(f"[DEBUG] API token length = {len(mist_apitoken) if mist_apitoken else 0}")
        
        # Make direct POST request to trigger show forwarding table
        forwarding_table_url = f"https://{mist_host}/api/v1/sites/{site_id}/devices/{device_id}/show_forwarding_table"
        headers = {'Authorization': f'Token {mist_apitoken}', 'Content-Type': 'application/json'}
        
        if debug_mode:
            print(f"[DEBUG] POST URL = {forwarding_table_url}")
            print(f"[DEBUG] Headers = {{'Authorization': 'Token [REDACTED]', 'Content-Type': 'application/json'}}")
        
        forwarding_table_response = requests.post(forwarding_table_url, headers=headers, json=forwarding_table_payload)
        
        if debug_mode:
            print(f"[DEBUG] HTTP Response Status = {forwarding_table_response.status_code}")
            print(f"[DEBUG] HTTP Response Body = {forwarding_table_response.text}")
        
        if forwarding_table_response.status_code != 200:
            print(f"! Failed to issue show forwarding table command: {forwarding_table_response.status_code}")
            print(f"! Response: {forwarding_table_response.text}")
            websocket_manager.disconnect()
            return
            
        # Extract session ID from response
        response_data = forwarding_table_response.json()
        session_id = response_data.get("session")
        if not session_id:
            print("! No session ID returned from show forwarding table command")
            websocket_manager.disconnect()
            return
            
        print(f"-> Show forwarding table command issued (session: {session_id[:8]}...)")
        print("-> Waiting for forwarding table results...")
        
        if debug_mode:
            print(f"[DEBUG] Full session ID = {session_id}")
            print("[DEBUG] Starting to wait for WebSocket results...")
        
        # Wait for forwarding table results via WebSocket (longer timeout for potentially large tables)
        forwarding_table_result = websocket_manager.wait_for_command_result(session_id, timeout_seconds=60)
        
        if debug_mode:
            print(f"[DEBUG] wait_for_command_result returned: {forwarding_table_result is not None}")
            if forwarding_table_result:
                print(f"[DEBUG] Result keys: {list(forwarding_table_result.keys())}")
        
        if forwarding_table_result:
            print("\n" + "=" * 80)
            print("FORWARDING TABLE RESULTS:")
            print("=" * 80)
            
            # Parse and display forwarding table data in a user-friendly format
            raw_output = forwarding_table_result.get("raw", "")
            if raw_output:
                forwarding_entries = parse_forwarding_table_output(raw_output)
                display_forwarding_table_summary(forwarding_entries)
            
            # Display any other output fields that might be present
            output_fields = forwarding_table_result.get("Output", "")
            if output_fields and output_fields != raw_output:
                print("\n" + "=" * 40)
                print("ADDITIONAL OUTPUT:")
                print("=" * 40)
                additional_entries = parse_forwarding_table_output(output_fields)
                display_forwarding_table_summary(additional_entries)
                
            # Show debug information if enabled
            if debug_mode:
                available_fields = [key for key in forwarding_table_result.keys() if key not in ['raw', 'Output', 'session']]
                if available_fields:
                    print(f"\n[DEBUG] OTHER AVAILABLE FIELDS: {available_fields}")
                    for field in available_fields:
                        field_value = forwarding_table_result.get(field)
                        if field_value:
                            print(f"[DEBUG] {field}: {field_value}")
                
            if not raw_output and not output_fields:
                print("! No forwarding table data received")
                print(f"Available result keys: {list(forwarding_table_result.keys())}")
            
            print("=" * 80)
            
            # Log the successful operation with device context
            device_context = f"device {device_id}"
            if device_info:
                device_context = f"{device_info.get('type', 'unknown')} {device_info.get('name', device_id[:8])}"
            logging.info(f"WebSocket show forwarding table completed successfully for {device_context}")
            
        else:
            print("! Timeout waiting for forwarding table results")
            print("! This may indicate:")
            print("  - The device doesn't support forwarding table commands (common for Layer 2-only devices)")
            print("  - The device is busy or not responding")
            print("  - Network connectivity issues")
            print("! Note: Forwarding tables are primarily a Layer 3 (routing) feature")
            
            # Provide device-specific troubleshooting advice
            if device_info:
                device_type = device_info.get('type', 'unknown')
                device_model = device_info.get('model', 'unknown')
                
                if device_type == 'gateway':
                    print(f"\nGateway troubleshooting ({device_model}):")
                    print("-> SSR gateways typically support forwarding table commands")
                    print("-> Ensure the device is online and reachable")
                    print("-> Check device CPU utilization - high load can delay responses")
                    print("-> Try the command again or use SSH-based routing commands")
                elif device_type == 'switch':
                    print(f"\nSwitch troubleshooting ({device_model}):")
                    print("-> Switches primarily operate at Layer 2")
                    print("-> Use 'Show MAC Table' command for Layer 2 forwarding information")
                    print("-> Layer 3 switches may support limited routing table commands")
                elif device_type == 'ap':
                    print(f"\nAccess Point troubleshooting ({device_model}):")
                    print("-> APs operate at Layer 2 and don't maintain forwarding tables")
                    print("-> Use wireless client statistics instead")
                    print("-> Check AP connectivity and bridging status")
                else:
                    print(f"\nGeneral troubleshooting ({device_model}):")
                    print("-> Verify device supports Layer 3 routing features")
                    print("-> Check device online status and connectivity")
                    print("-> Consider using SSH commands for advanced routing diagnostics")
            
            logging.warning("WebSocket show forwarding table operation timed out")
            
            if debug_mode:
                print("[DEBUG] Checking WebSocket manager state...")
                print(f"[DEBUG] Connected = {websocket_manager.connected}")
                print(f"[DEBUG] Subscribed channels = {websocket_manager.subscribed_channels}")
                with websocket_manager.results_lock:
                    print(f"[DEBUG] Pending results = {list(websocket_manager.command_results.keys())}")
            
    except Exception as forwarding_table_error:
        error_message = f"WebSocket show forwarding table operation failed: {forwarding_table_error}"
        print(f"! {error_message}")
        logging.error(error_message)
        
        if debug_mode:
            print("[DEBUG] Exception details:")
            import traceback
            traceback.print_exc()
        
        logging.debug("EXIT: show_forwarding_table_websocket - error")
        
    finally:
        # Always cleanup WebSocket connection
        try:
            if 'websocket_manager' in locals():
                websocket_manager.disconnect()
                print("-> WebSocket connection closed")
                
                if debug_mode:
                    print("[DEBUG] WebSocket cleanup completed")
        except Exception as cleanup_error:
            logging.warning(f"WebSocket cleanup error: {cleanup_error}")
            
        logging.debug("EXIT: show_forwarding_table_websocket")


def show_routing_table_websocket():
    """
    Execute show route command on switches, routers, and SSR devices via WebSocket.
    
    This function retrieves routing table information (RIB - Routing Information Base) from network devices.
    The routing table shows routes learned from various routing protocols like BGP, OSPF, static routes, etc.
    This is different from the forwarding table (FIB) which shows the actual forwarding entries used for packet forwarding.
    
    Supported devices:
    - Switches with Layer 3 routing capabilities 
    - SRX routers
    - SSR gateways
    - Other routing-capable network devices
    
    API Endpoint: POST /api/v1/sites/:site_id/devices/:device_id/show_route
    
    SECURITY: Uses authenticated WebSocket connection with session-based
    command demultiplexing for concurrent command safety.
    """
    # Check for debug mode from command line arguments
    debug_mode = '--debug' in sys.argv or '-d' in sys.argv
    
    if debug_mode:
        logging.getLogger().setLevel(logging.DEBUG)
        print("[DEBUG] DEBUG MODE ENABLED")
    
    logging.info("Starting WebSocket show routing table operation...")
    logging.debug("ENTER: show_routing_table_websocket")
    
    try:
        # Interactive site selection
        site_id = prompt_select_site_id_from_csv()
        if not site_id:
            print("! No site selected. Operation cancelled.")
            return
        
        if debug_mode:
            print(f"[DEBUG] Selected site_id = {site_id}")
            
        # Get device selection - Focus on switches with Layer 3 routing capabilities
        print("-> Switch routing table information (Layer 3 routing protocols)")
        print("-> This shows the Routing Information Base (RIB) maintained by routing protocols")
        print("-> Includes routes from BGP, OSPF, static routes, direct routes, etc.")
        print("-> For SSR/SRX devices, use Menu Option 8 (dedicated SSR/SRX routing API)")
        device_id = prompt_select_device_id_from_inventory(site_id, device_type="switch")
        if not device_id:
            print("! No device selected. Operation cancelled.")
            return
        
        if debug_mode:
            print(f"[DEBUG] Selected device_id = {device_id}")
        
        # Get device details to check type and model for switch routing compatibility
        device_info = None
        try:
            rawdata = mistapi.api.v1.sites.devices.listSiteDevices(apisession, site_id, type="switch").data
            device_info = next((device for device in rawdata if device.get('id') == device_id), None)
            
            if device_info:
                device_type = device_info.get('type', 'unknown')
                device_model = device_info.get('model', 'unknown')
                device_name = device_info.get('name', f"Device {device_id[:8]}")
                
                if debug_mode:
                    print(f"[DEBUG] Device type: {device_type}, model: {device_model}, name: {device_name}")
                
                # Provide device-specific guidance for switch routing table support
                if device_type == 'switch':
                    if 'EX' in device_model.upper():
                        print(f"!? Juniper EX switch detected ({device_model}): Excellent Layer 3 routing support")
                    elif 'QFX' in device_model.upper():
                        print(f"!? Juniper QFX switch detected ({device_model}): Good Layer 3 routing support")
                    else:
                        print(f"-> Switch device detected ({device_model}): Layer 3 routing table support")
                    print("  -> Shows routing protocol information if Layer 3 features are enabled")
                else:
                    print(f"!? Non-switch device detected ({device_type}/{device_model})")
                    print(f"  -> For SSR/SRX devices, use Menu Option 8 (dedicated SSR/SRX routing API)")
                    print(f"  -> For gateway forwarding tables, use Menu Option 6")
                    user_choice = input("Continue with switch routing command anyway? (y/N): ").strip().lower()
                    if user_choice not in ['y', 'yes']:
                        print("Operation cancelled.")
                        return
                    
        except Exception as device_check_error:
            logging.warning(f"Could not verify device compatibility: {device_check_error}")
            if debug_mode:
                print(f"[DEBUG] Device check failed: {device_check_error}")
            print("   -> Proceeding with standard routing table command")
            
        print(f"\n-> Executing show route on device {device_id}...")
        print("-> Establishing WebSocket connection...")
        
        # Initialize WebSocket manager
        websocket_manager = WebSocketManager(apisession)
        
        if debug_mode:
            print("[DEBUG] WebSocketManager initialized")
        
        # Connect to WebSocket
        if not websocket_manager.connect():
            print("! Failed to establish WebSocket connection")
            return
            
        if debug_mode:
            print("[DEBUG] WebSocket connection established")
            
        # Subscribe to device command channel
        command_channel = f"/sites/{site_id}/devices/{device_id}/cmd"
        if not websocket_manager.subscribe_to_channel(command_channel):
            print("! Failed to subscribe to device command channel")
            websocket_manager.disconnect()
            return
        
        if debug_mode:
            print(f"[DEBUG] Subscribed to channel: {command_channel}")
            
        print("-> WebSocket connected and subscribed")
        
        # Wait a moment for subscription to be established
        time.sleep(1)
        
        # Issue show route command via REST API
        print("\n=== Routing Table Query Parameters ===")
        print("Configure the routing table query (all parameters are optional):")
        print("  X  Prefix: Specific route prefix to look up (e.g., 192.168.1.0/24)")
        print("  X  Protocol: Filter by routing protocol (bgp, ospf, static, direct, evpn, any)")
        print("  X  VRF: Virtual Routing and Forwarding instance name")
        print("  X  Neighbor: BGP neighbor IP (shows received/advertised routes)")
        print("  X  Node: For HA devices (node0/node1)")
        
        # Get user input for routing table parameters
        prefix_input = input("\nEnter route prefix (press Enter to show all routes): ").strip()
        
        print("\nProtocol options: any (default - shows all routes), bgp, ospf, static, direct, evpn")
        protocol_input = input("Enter protocol filter (press Enter for default 'any'): ").strip()
        
        vrf_input = input("Enter VRF name (press Enter to skip): ").strip()
        neighbor_input = input("Enter BGP neighbor IP (press Enter to skip): ").strip()
        
        if neighbor_input:
            print("\nRoute direction options for BGP neighbor:")
            print("  X  received: Routes received from neighbor")
            print("  X  advertised: Routes advertised to neighbor") 
            print("  X  (empty): Both received and advertised routes")
            route_direction = input("Enter route direction (press Enter for both): ").strip()
        else:
            route_direction = ""
            
        node_input = input("Enter node (node0/node1 for HA, press Enter to skip): ").strip()
        
        # Build the payload for show route command
        route_payload = {}
        
        # Add parameters if provided
        if prefix_input:
            route_payload["prefix"] = prefix_input
            
        if protocol_input:
            if protocol_input.lower() in ["bgp", "ospf", "static", "direct", "evpn", "any"]:
                route_payload["protocol"] = protocol_input.lower()
            else:
                print(f"!? Invalid protocol '{protocol_input}', using default 'any'")
                route_payload["protocol"] = "any"
        else:
            route_payload["protocol"] = "any"  # Default protocol - shows all routes
            
        if vrf_input:
            route_payload["vrf"] = vrf_input
            
        if neighbor_input:
            route_payload["neighbor"] = neighbor_input
            if route_direction and route_direction.lower() in ["received", "advertised"]:
                route_payload["route"] = route_direction.lower()
            
        if node_input and node_input.lower() in ["node0", "node1"]:
            route_payload["node"] = node_input.lower()
        
        print("-> Issuing show route command...")
        logging.debug(f"Route payload: {route_payload}")
        
        if debug_mode:
            print(f"[DEBUG] Route payload = {route_payload}")
        
        # Get authentication details for direct HTTP request
        mist_host = getattr(apisession, "host", None) or os.getenv("MIST_HOST")
        mist_apitoken = getattr(apisession, "apitoken", None) or os.getenv("MIST_APITOKEN")
        
        if not mist_host or not mist_apitoken:
            print("! Mist host or API token not found in session or environment")
            websocket_manager.disconnect()
            return
        
        if debug_mode:
            print(f"[DEBUG] mist_host = {mist_host}")
            print(f"[DEBUG] API token length = {len(mist_apitoken) if mist_apitoken else 0}")
        
        # Make direct POST request to trigger show route
        route_url = f"https://{mist_host}/api/v1/sites/{site_id}/devices/{device_id}/show_route"
        headers = {'Authorization': f'Token {mist_apitoken}', 'Content-Type': 'application/json'}
        
        if debug_mode:
            print(f"[DEBUG] POST URL = {route_url}")
            print(f"[DEBUG] Headers = {{'Authorization': 'Token [REDACTED]', 'Content-Type': 'application/json'}}")
        
        route_response = requests.post(route_url, headers=headers, json=route_payload)
        
        if debug_mode:
            print(f"[DEBUG] HTTP Response Status = {route_response.status_code}")
            print(f"[DEBUG] HTTP Response Body = {route_response.text}")
        
        if route_response.status_code != 200:
            print(f"! Failed to issue show route command: {route_response.status_code}")
            print(f"! Response: {route_response.text}")
            websocket_manager.disconnect()
            return
            
        # Extract session ID from response
        response_data = route_response.json()
        session_id = response_data.get("session")
        if not session_id:
            print("! No session ID returned from show route command")
            websocket_manager.disconnect()
            return
            
        print(f"-> Show route command issued (session: {session_id[:8]}...)")
        print("-> Waiting for routing table results...")
        
        if debug_mode:
            print(f"[DEBUG] Full session ID = {session_id}")
            print("[DEBUG] Starting to wait for WebSocket results...")
        
        # Wait for routing table results via WebSocket (longer timeout for potentially large tables)
        route_result = websocket_manager.wait_for_command_result(session_id, timeout_seconds=60)
        
        if debug_mode:
            print(f"[DEBUG] wait_for_command_result returned: {route_result is not None}")
            if route_result:
                print(f"[DEBUG] Result keys: {list(route_result.keys())}")
        
        if route_result:
            print("\n" + "=" * 80)
            print("ROUTING TABLE RESULTS:")
            print("=" * 80)
            
            # Parse and display routing table data in a user-friendly format
            raw_output = route_result.get("raw", "")
            if raw_output:
                route_entries = parse_routing_table_output(raw_output)
                display_routing_table_summary(route_entries, route_payload)
            
            # Display any other output fields that might be present
            output_fields = route_result.get("Output", "")
            if output_fields and output_fields != raw_output:
                print("\n" + "=" * 40)
                print("ADDITIONAL OUTPUT:")
                print("=" * 40)
                additional_entries = parse_routing_table_output(output_fields)
                display_routing_table_summary(additional_entries, route_payload)
                
            # Show debug information if enabled
            if debug_mode:
                available_fields = [key for key in route_result.keys() if key not in ['raw', 'Output', 'session']]
                if available_fields:
                    print(f"\n[DEBUG] OTHER AVAILABLE FIELDS: {available_fields}")
                    for field in available_fields:
                        field_value = route_result.get(field)
                        if field_value:
                            print(f"[DEBUG] {field}: {field_value}")
                
            if not raw_output and not output_fields:
                print("! No routing table data received")
                print(f"Available result keys: {list(route_result.keys())}")
            
            print("=" * 80)
            
            # Log the successful operation with device context
            device_context = f"device {device_id}"
            if device_info:
                device_context = f"{device_info.get('type', 'unknown')} {device_info.get('name', device_id[:8])}"
            logging.info(f"WebSocket show routing table completed successfully for {device_context}")
            
        else:
            print("! Timeout waiting for routing table results")
            print("! This may indicate:")
            print("  - The device doesn't support routing table commands")
            print("  - The device has no routing protocols configured")
            print("  - The device is busy or not responding")
            print("  - Network connectivity issues")
            print("  - Invalid query parameters for this device type")
            
    except KeyboardInterrupt:
        print("\n! Operation interrupted by user")
        logging.info("WebSocket show routing table operation interrupted by user")
        
    except Exception as route_error:
        print(f"! Error during show routing table operation: {route_error}")
        logging.error(f"WebSocket show routing table error: {route_error}")
        
        if debug_mode:
            import traceback
            print("[DEBUG] Full traceback:")
            traceback.print_exc()
        
    finally:
        # Always cleanup WebSocket connection
        try:
            if 'websocket_manager' in locals():
                websocket_manager.disconnect()
                print("-> WebSocket connection closed")
                
                if debug_mode:
                    print("[DEBUG] WebSocket cleanup completed")
        except Exception as cleanup_error:
            logging.warning(f"WebSocket cleanup error: {cleanup_error}")
            
        logging.debug("EXIT: show_routing_table_websocket")


def show_ssr_routes_dedicated():
    """
    Execute SSR/SRX routing table command using dedicated API function.
    
    Uses the dedicated mistapi.api.v1.sites.devices.showSiteSsrAndSrxRoutes function
    which provides structured routing table queries specifically optimized for
    SSR and SRX devices with proper parameter validation and device-specific formatting.
    
    This function provides advanced routing table analysis capabilities including:
    - Protocol-specific filtering (BGP, OSPF, static, direct, EVPN, any)
    - BGP neighbor analysis (received/advertised routes)  
    - VRF-aware routing table queries
    - Route prefix lookups
    - HA cluster node selection
    - Structured output formatting
    
    API Endpoint: POST /api/v1/sites/:site_id/devices/:device_id/show_route
    Schema: utils_show_route (see OpenAPI documentation)
    
    Supported devices:
    - SSR gateways (128T Session Smart Routers)
    - SRX routers (Juniper SRX series)
    
    SECURITY: Uses authenticated API session with proper parameter validation
    and device capability checking for safe routing table operations.
    """
    # Check for debug mode from command line arguments
    debug_mode = '--debug' in sys.argv or '-d' in sys.argv
    
    if debug_mode:
        logging.getLogger().setLevel(logging.DEBUG)
        print("[DEBUG] DEBUG MODE ENABLED")
    
    logging.info("Starting SSR/SRX dedicated routing table operation...")
    logging.debug("ENTER: show_ssr_routes_dedicated")
    
    try:
        # Interactive site selection
        site_id = prompt_select_site_id_from_csv()
        if not site_id:
            print("! No site selected. Operation cancelled.")
            return
        
        if debug_mode:
            print(f"[DEBUG] Selected site_id = {site_id}")
            
        # Get device selection - Focus on SSR and SRX devices
        print("-> SSR/SRX routing table query using dedicated API function")
        print("-> This function is optimized for SSR (128T) and SRX devices")
        print("-> Provides structured routing table queries with advanced filtering")
        device_id = prompt_select_device_id_from_inventory(site_id, device_type="gateway")
        if not device_id:
            print("! No device selected. Operation cancelled.")
            return
        
        if debug_mode:
            print(f"[DEBUG] Selected device_id = {device_id}")
        
        # Get device details to verify SSR/SRX compatibility
        device_info = None
        device_compatible = False
        try:
            rawdata = mistapi.api.v1.sites.devices.listSiteDevices(apisession, site_id, type="gateway").data
            device_info = next((device for device in rawdata if device.get('id') == device_id), None)
            
            if device_info:
                device_type = device_info.get('type', 'unknown')
                device_model = device_info.get('model', 'unknown')
                device_name = device_info.get('name', f"Device {device_id[:8]}")
                
                if debug_mode:
                    print(f"[DEBUG] Device type: {device_type}, model: {device_model}, name: {device_name}")
                
                # Check device compatibility for dedicated SSR/SRX API
                if device_type == 'gateway':
                    if 'SSR' in device_model.upper() or '128T' in device_model:
                        print(f"!? SSR gateway detected ({device_model}): Fully compatible with dedicated API")
                        device_compatible = True
                    elif 'SRX' in device_model.upper():
                        print(f"!? SRX router detected ({device_model}): Fully compatible with dedicated API") 
                        device_compatible = True
                    else:
                        print(f"!? Gateway device ({device_model}): May have limited compatibility")
                        print("  -> This API function is optimized for SSR and SRX devices")
                        user_choice = input("Continue anyway? (y/N): ").strip().lower()
                        if user_choice not in ['y', 'yes']:
                            print("Operation cancelled.")
                            return
                        device_compatible = True
                else:
                    print(f"!? Non-gateway device detected ({device_type}/{device_model})")
                    print("  -> This API function is designed for SSR and SRX gateway devices")
                    user_choice = input("Continue anyway? (y/N): ").strip().lower()
                    if user_choice not in ['y', 'yes']:
                        print("Operation cancelled.")
                        return
                    device_compatible = True
                    
        except Exception as device_check_error:
            logging.warning(f"Could not verify device compatibility: {device_check_error}")
            if debug_mode:
                print(f"[DEBUG] Device check failed: {device_check_error}")
            print("   -> Proceeding with SSR/SRX routing table command")
            device_compatible = True
            
        if not device_compatible:
            print("! Device compatibility check failed. Operation cancelled.")
            return
            
        print(f"\n=== SSR/SRX Routing Table Query Parameters ===")
        print("Configure the routing table query (all parameters are optional):")
        print("  X  Protocol: Filter by routing protocol")
        print("  X  Prefix: Specific route prefix to look up")
        print("  X  VRF: Virtual Routing and Forwarding instance")
        print("  X  Neighbor: BGP neighbor IP for route analysis")
        print("  X  Route Direction: For BGP neighbors (received/advertised)")
        print("  X  Node: For HA clusters (node0/node1)")
        print("  X  Refresh: Real-time updates (interval/duration)")
        print("")
        print("-> Note: SSR devices work well with BGP protocol queries")
        print("-> For comprehensive routing table, use 'any' protocol")
        print("-> Or let the API choose its own default by skipping protocol")
        
        # Build the request body using utils_show_route schema
        request_body = {}
        
        # Protocol selection
        print("\nProtocol options:")
        print("  X  bgp - Border Gateway Protocol routes")
        print("  X  any - Show all routing protocols")
        print("  X  ospf - Open Shortest Path First routes")
        print("  X  static - Statically configured routes")
        print("  X  direct - Directly connected routes")
        print("  X  evpn - Ethernet VPN routes")
        print("  X  (none) - Let API use its default behavior")
        protocol_input = input("Enter protocol (press Enter to use API default): ").strip().lower()
        
        if protocol_input and protocol_input in ["any", "bgp", "ospf", "static", "direct", "evpn"]:
            request_body["protocol"] = protocol_input
        elif protocol_input:
            print(f"!? Invalid protocol '{protocol_input}', skipping protocol filter")
            # Don't set protocol in request_body - let API use its default
        
        # Route prefix
        prefix_input = input("\nEnter route prefix (e.g., 192.168.1.0/24, press Enter to skip): ").strip()
        if prefix_input:
            request_body["prefix"] = prefix_input
            
        # VRF name
        vrf_input = input("Enter VRF name (press Enter for default VRF): ").strip()
        if vrf_input:
            request_body["vrf"] = vrf_input
            
        # BGP neighbor analysis
        neighbor_input = input("Enter BGP neighbor IP (press Enter to skip): ").strip()
        if neighbor_input:
            request_body["neighbor"] = neighbor_input
            
            print("\nBGP route direction options:")
            print("  X  received - Routes received from neighbor")
            print("  X  advertised - Routes advertised to neighbor") 
            print("  X  (empty) - Both received and advertised routes")
            route_direction = input("Enter route direction (press Enter for both): ").strip().lower()
            
            if route_direction and route_direction in ["received", "advertised"]:
                request_body["route"] = route_direction
                
        # HA cluster node selection
        node_input = input("Enter HA cluster node (node0/node1, press Enter to skip): ").strip().lower()
        if node_input and node_input in ["node0", "node1"]:
            request_body["node"] = {"node": node_input}
            
        # Real-time refresh options
        print("\nReal-time refresh options (for monitoring dynamic changes):")
        interval_input = input("Refresh interval in seconds (0-10, press Enter for one-time): ").strip()
        if interval_input and interval_input.isdigit():
            interval_val = int(interval_input)
            if 0 <= interval_val <= 10:
                request_body["interval"] = interval_val
                
                if interval_val > 0:
                    duration_input = input("Refresh duration in seconds (0-300, press Enter for 30): ").strip()
                    if duration_input and duration_input.isdigit():
                        duration_val = int(duration_input)
                        if 0 <= duration_val <= 300:
                            request_body["duration"] = duration_val
                    else:
                        request_body["duration"] = 30  # Default 30 seconds
        
        print(f"\n-> Executing SSR/SRX routing table query on device {device_id}...")
        logging.debug(f"Request body: {request_body}")
        
        if debug_mode:
            print(f"[DEBUG] Request body = {request_body}")
        
        # Initialize WebSocket manager for receiving results
        print("-> Establishing WebSocket connection...")
        websocket_manager = WebSocketManager(apisession)
        
        if debug_mode:
            print("[DEBUG] WebSocketManager initialized")
        
        # Connect to WebSocket
        if not websocket_manager.connect():
            print("! Failed to establish WebSocket connection")
            return
            
        if debug_mode:
            print("[DEBUG] WebSocket connection established")
            
        # Subscribe to device command channel
        command_channel = f"/sites/{site_id}/devices/{device_id}/cmd"
        if not websocket_manager.subscribe_to_channel(command_channel):
            print("! Failed to subscribe to device command channel")
            websocket_manager.disconnect()
            return
        
        if debug_mode:
            print(f"[DEBUG] Subscribed to channel: {command_channel}")
            
        print("-> WebSocket connected and subscribed")
        
        # Wait a moment for subscription to be established
        time.sleep(1)
        
        # Execute the dedicated SSR/SRX routing table API call
        try:
            print("-> Calling dedicated SSR/SRX routing table API...")
            if debug_mode:
                print(f"[DEBUG] Calling mistapi.api.v1.sites.devices.showSiteSsrAndSrxRoutes")
                print(f"[DEBUG] Parameters: site_id={site_id}, device_id={device_id}")
                print(f"[DEBUG] Request body: {request_body}")
            
            response = mistapi.api.v1.sites.devices.showSiteSsrAndSrxRoutes(
                apisession, 
                site_id, 
                device_id, 
                request_body
            )
            
            if debug_mode:
                print(f"[DEBUG] API response type: {type(response)}")
                print(f"[DEBUG] API response hasattr data: {hasattr(response, 'data')}")
                print(f"[DEBUG] API response hasattr status_code: {hasattr(response, 'status_code')}")
                if hasattr(response, 'data'):
                    print(f"[DEBUG] Response data: {response.data}")
                if hasattr(response, 'status_code'):
                    print(f"[DEBUG] Status code: {response.status_code}")
                if hasattr(response, 'raw_data'):
                    print(f"[DEBUG] Raw response data: {response.raw_data}")
                if hasattr(response, 'headers'):
                    print(f"[DEBUG] Response headers: {response.headers}")
                if hasattr(response, 'url'):
                    print(f"[DEBUG] Request URL: {response.url}")
                if hasattr(response, 'next'):
                    print(f"[DEBUG] Next URL: {response.next}")
                if hasattr(response, 'proxy_error'):
                    print(f"[DEBUG] Proxy error: {response.proxy_error}")
                if hasattr(response, '__dict__'):
                    print(f"[DEBUG] Response attributes: {list(response.__dict__.keys())}")
                    # Show all attributes with their values
                    print("[DEBUG] === COMPLETE APIResponse OBJECT DUMP ===")
                    for attr_name, attr_value in response.__dict__.items():
                        print(f"[DEBUG] {attr_name}: {attr_value}")
                    print("[DEBUG] === END APIResponse OBJECT DUMP ===")
            
            # Process the API response and extract session ID
            session_id = None
            if hasattr(response, 'data') and response.data:
                session_id = response.data.get('session')
                if session_id:
                    print(f"-> Command initiated (session: {session_id[:8]}...)")
                    print("-> Waiting for SSR/SRX routing table results...")
                    
                    if debug_mode:
                        print(f"[DEBUG] Full session ID: {session_id}")
                        print(f"[DEBUG] Response data keys: {list(response.data.keys())}")
                        print(f"[DEBUG] WebSocket timeout will be 60 seconds")
                        
                else:
                    print("! No session ID returned from SSR/SRX routing API")
                    if debug_mode:
                        print(f"[DEBUG] Response data content: {response.data}")
                    websocket_manager.disconnect()
                    return
                            
            elif hasattr(response, 'status_code'):
                print(f"! API call returned status code: {response.status_code}")
                if hasattr(response, 'data'):
                    print(f"Response data: {response.data}")
                websocket_manager.disconnect()
                return
                    
            else:
                print("! Unexpected API response format")
                print(f"Response type: {type(response)}")
                if hasattr(response, '__dict__'):
                    print(f"Response attributes: {list(response.__dict__.keys())}")
                websocket_manager.disconnect()
                return
            
            # Wait for SSR/SRX routing table results via WebSocket (longer timeout for potentially large tables)
            if debug_mode:
                print("[DEBUG] Starting to wait for WebSocket results...")
                print(f"[DEBUG] WebSocket manager state: connected={websocket_manager.connected if hasattr(websocket_manager, 'connected') else 'unknown'}")
            
            route_result = websocket_manager.wait_for_command_result(session_id, timeout_seconds=60)
            
            if debug_mode:
                print(f"[DEBUG] wait_for_command_result returned: {route_result is not None}")
                if route_result:
                    print(f"[DEBUG] Result keys: {list(route_result.keys())}")
                    print(f"[DEBUG] Result content preview: {str(route_result)[:500]}...")
                else:
                    print("[DEBUG] No result received from WebSocket")
            
            if route_result:
                print("\n" + "=" * 80)
                print("SSR/SRX ROUTING TABLE RESULTS:")
                print("=" * 80)
                
                # Parse and display SSR/SRX routing table data in a user-friendly format
                raw_output = route_result.get("raw", "")
                if raw_output:
                    # Try SSR JSON parser first
                    route_entries = parse_ssr_routing_json(raw_output)
                    if route_entries:
                        display_ssr_routing_table(route_entries, request_body)
                    else:
                        # Fallback to generic parser for other formats
                        route_entries = parse_routing_table_output(raw_output)
                        display_routing_table_summary(route_entries, request_body)
                
                # Display any other output fields that might be present
                output_fields = route_result.get("Output", "")
                if output_fields and output_fields != raw_output:
                    print("\n" + "=" * 40)
                    print("ADDITIONAL OUTPUT:")
                    print("=" * 40)
                    additional_entries = parse_ssr_routing_json(output_fields)
                    if additional_entries:
                        display_ssr_routing_table(additional_entries, request_body)
                    else:
                        additional_entries = parse_routing_table_output(output_fields)
                        display_routing_table_summary(additional_entries, request_body)
                    
                # Show debug information if enabled
                if debug_mode:
                    available_fields = [key for key in route_result.keys() if key not in ['raw', 'Output', 'session']]
                    if available_fields:
                        print(f"\n[DEBUG] OTHER AVAILABLE FIELDS: {available_fields}")
                        for field in available_fields:
                            field_value = route_result.get(field)
                            if field_value:
                                print(f"[DEBUG] {field}: {field_value}")
                    
                if not raw_output and not output_fields:
                    print("! No routing table data received")
                    print(f"Available result keys: {list(route_result.keys())}")
                
                print("=" * 80)
                
                # Log the successful operation with device context
                device_context = f"device {device_id}"
                if device_info:
                    device_context = f"{device_info.get('type', 'unknown')} {device_info.get('name', device_id[:8])}"
                logging.info(f"SSR/SRX dedicated routing table completed successfully for {device_context}")
                
            else:
                print("! Timeout waiting for SSR/SRX routing table results")
                print("! This may indicate:")
                print("  - The device doesn't support SSR/SRX routing table commands")
                print("  - The device has no routing protocols configured")
                print("  - The device is busy or not responding")
                print("  - Network connectivity issues")
                print("  - Invalid query parameters for this device type")
                print("  - Try the generic routing table command (Menu 7) as fallback")
            
        except Exception as api_error:
            print(f"! Error calling SSR/SRX routing table API: {api_error}")
            logging.error(f"SSR/SRX routing table API error: {api_error}")
            
            if debug_mode:
                import traceback
                print("[DEBUG] Full API error traceback:")
                traceback.print_exc()
                
            print("\n-> Troubleshooting suggestions:")
            print("  X  Verify the device is an SSR or SRX gateway")
            print("  X  Check device connectivity and responsiveness")
            print("  X  Verify API permissions for device commands")
            print("  X  Try the generic routing table command (Menu 7) as fallback")
            
    except KeyboardInterrupt:
        print("\n! Operation interrupted by user")
        logging.info("SSR/SRX routing table operation interrupted by user")
        
    except Exception as main_error:
        print(f"! Error during SSR/SRX routing table operation: {main_error}")
        logging.error(f"SSR/SRX routing table main error: {main_error}")
        
        if debug_mode:
            import traceback
            print("[DEBUG] Full main error traceback:")
            traceback.print_exc()
            
    finally:
        # Always cleanup WebSocket connection
        try:
            if 'websocket_manager' in locals():
                websocket_manager.disconnect()
                print("-> WebSocket connection closed")
                
                if debug_mode:
                    print("[DEBUG] WebSocket cleanup completed")
        except Exception as cleanup_error:
            logging.warning(f"WebSocket cleanup error: {cleanup_error}")
            
        logging.debug("EXIT: show_ssr_routes_dedicated")


def _validate_ping_target(target):
    """
    Validate ping target hostname or IP address.
    
    Args:
        target (str): Target hostname or IP address
        
    Returns:
        bool: True if valid target, False otherwise
    """
    if not target or len(target.strip()) == 0:
        return False
        
    target = target.strip()
    
    # Check if it's a valid IP address
    try:
        ipaddress.ip_address(target)
        return True
    except ValueError:
        pass
        
    # Check if it's a valid hostname
    # Basic hostname validation: alphanumeric, dots, hyphens
    if re.match(r'^[a-zA-Z0-9.-]+$', target) and len(target) <= 253:
        # Ensure it doesn't start or end with a dot or hyphen
        if not target.startswith(('.', '-')) and not target.endswith(('.', '-')):
            return True
            
    return False


def export_all_sites_to_csv():
    """
    Fetches and exports the list of all sites in the organization.
    Output format determined by global OUTPUT_FORMAT setting.
    Uses fetch_and_display_api_data to handle API call and output writing.
    """
    logging.info("Starting export of organization site list...")
    fetch_and_display_api_data(
        title="Site List:",
        api_call=mistapi.api.v1.orgs.sites.listOrgSites,
        filename="SiteList",  # Format-agnostic filename (no extension)
        sort_key="name",  # or "site_id" if preferred
        limit=1000
    )
    output_desc = "SQLite table" if OUTPUT_FORMAT == "sqlite" else "CSV file"
    logging.info(f"Completed export_all_sites and wrote results to {output_desc}.")

def export_all_sites_list_to_csv():
    """
    Uses the 'list' sites API endpoint (not 'search') to export all sites to SiteList_ListAPI.csv,
    but only if the file does not already exist.
    """
    output_file = "SiteList_ListAPI.csv"
    if os.path.exists(output_file):
        logging.info(f"! Using cached {output_file} (already exists)")
        print(f"! Using cached {output_file} (already exists)")
        return

    logging.info("Fetching all sites using the 'list' sites API endpoint...")
    print("Fetching all sites using the 'list' sites API endpoint...")
    org_id = get_cached_or_prompted_org_id()
    sites = fetch_all_sites_with_limit(org_id)
    if not sites:
        logging.warning(" No sites returned from API.")
        print(" No sites returned from API.")
        return
    # Flatten and sanitize for CSV
    sites = flatten_nested_fields_in_list(sites)
    sites = escape_multiline_strings_for_csv(sites)
    DataExporter.save_data_to_output(sites, output_file)
    logging.info(f"! Sites exported to {output_file}")
    print(f"! Sites exported to {output_file}")

def export_device_inventory_to_csv():
    """
    Fetches and exports the full inventory of devices in the organization to OrgInventory.csv.
    Uses fetch_and_display_api_data to handle API call, CSV writing, and table display.
    """
    logging.info("Starting export of organization device inventory...")
    fetch_and_display_api_data(
        title="Org Inventory:",
        api_call=mistapi.api.v1.orgs.inventory.getOrgInventory,
        filename="OrgInventory.csv",
        sort_key="model",
        limit=1000
    )
    logging.info("Completed export_device_inventory_to_csv and wrote results to OrgInventory.csv.")

def export_device_stats_to_csv(fast: bool = False):
    """Export statistics for all devices in the organization to `OrgDeviceStats.csv`.

    Fast Mode Behavior:
        - If `fast` is True and a fresh CSV (mtime < CSV_FRESHNESS_MINUTES) exists, skip API call (cache hit).
        - Falls back to normal fetch otherwise (no change to data semantics).
    SECURITY: Read-only operation; safe to cache.
    """
    output_file = "OrgDeviceStats.csv"
    if fast and os.path.exists(output_file):
        try:
            mtime = os.path.getmtime(output_file)
            age_minutes = (time.time() - mtime) / 60.0
            if age_minutes < CSV_FRESHNESS_MINUTES:
                logging.info(f" Fast mode cache hit: {output_file} is fresh ({age_minutes:.1f}m < {CSV_FRESHNESS_MINUTES}m); skipping fetch.")
                print(f"* Fast mode: Using cached {output_file} (age {age_minutes:.1f}m)")
                return
        except Exception as e:  # pragma: no cover - defensive
            logging.debug(f"Fast mode freshness check failed for {output_file}: {e}")
    logging.info("Starting export of organization device statistics...")
    hours = get_dynamic_lookback_hours(24, 1)
    log_dynamic_lookback("org device statistics export", hours)
    fetch_and_display_api_data(
        title="Org Device Stats:",
        api_call=mistapi.api.v1.orgs.stats.listOrgDevicesStats,
        filename=output_file,
        sort_key="type",
        type="all",
        duration=f"{hours}h",
        limit=1000
    )

def export_device_port_stats_to_csv(fast: bool = False):
    """Export port-level statistics for all switches and gateways to `OrgDevicePortStats.csv`.

    Fast Mode Behavior:
        - Skips API call if recent CSV exists (freshness based on `CSV_FRESHNESS_MINUTES`).
        - Parallelizes data retrieval across sites for faster collection.
        - Uses connection pool management to limit concurrent API calls.
    
    Performance Optimization:
        - Non-fast mode: Single org-level API call with serial pagination (slow but simple)
        - Fast mode: Parallel site-level API calls (faster, scales with site count)
    
    SECURITY: Read-only aggregation; caching is safe.
    """
    output_file = "OrgDevicePortStats.csv"
    if fast and os.path.exists(output_file):
        try:
            mtime = os.path.getmtime(output_file)
            age_minutes = (time.time() - mtime) / 60.0
            if age_minutes < CSV_FRESHNESS_MINUTES:
                logging.info(f" Fast mode cache hit: {output_file} is fresh ({age_minutes:.1f}m < {CSV_FRESHNESS_MINUTES}m); skipping fetch.")
                print(f"* Fast mode: Using cached {output_file} (age {age_minutes:.1f}m)")
                return
        except Exception as e:  # pragma: no cover
            logging.debug(f"Fast mode freshness check failed for {output_file}: {e}")
    
    logging.info("Starting export of organization device port statistics...")
    hours = get_dynamic_lookback_hours(24, 1)
    log_dynamic_lookback("org device port statistics export", hours)
    
    if fast:
        # Fast mode: Parallelize by site for better performance
        logging.info("* Fast mode: Parallelizing port stats retrieval across sites")
        
        # Get org_id for API calls
        org_id = get_cached_or_prompted_org_id()
        
        # Get all sites (use cached CSV if available)
        try:
            check_and_generate_csv("SiteList.csv", export_all_sites_to_csv)
            site_list_path = get_csv_file_path("SiteList.csv")
            with open(site_list_path, mode="r", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                sites = [(row.get("id"), row.get("name", "Unknown")) for row in reader if row.get("id")]
            logging.info(f"* Loaded {len(sites)} sites from cached data")
            logging.debug(f"First site sample: {sites[0] if sites else 'No sites'}, type: {type(sites[0]) if sites else 'N/A'}")
        except Exception as e:
            logging.warning(f"* Could not use cached sites, fetching from API: {e}")
            site_response = mistapi.api.v1.orgs.sites.listOrgSites(apisession, org_id, limit=1000)
            site_data = mistapi.get_all(response=site_response, mist_session=apisession)
            sites = [(site.get("id"), site.get("name", "Unknown")) for site in site_data if site.get("id")]
            logging.info(f"* Fetched {len(sites)} sites from API")
            logging.debug(f"First site sample: {sites[0] if sites else 'No sites'}, type: {type(sites[0]) if sites else 'N/A'}")
        
        # Worker function to fetch port stats for a single site
        def fetch_site_port_stats(site_info, connection_semaphore):
            """Fetch port statistics for a single site with retry logic."""
            site_id, site_name = site_info
            
            for attempt in range(FAST_MODE_MAX_RETRIES + 1):
                try:
                    # Use semaphore to limit concurrent connections
                    with connection_semaphore:
                        response = mistapi.api.v1.sites.stats.searchSiteSwOrGwPorts(
                            apisession, 
                            site_id, 
                            duration=f"{hours}h",
                            limit=1000
                        )
                        port_stats = mistapi.get_all(response=response, mist_session=apisession)
                    
                    # SAFETY: Validate that port_stats is a list, not a dict or other type
                    if not isinstance(port_stats, list):
                        logging.error(f"! API returned non-list type for site {site_name}: type={type(port_stats)}, value={port_stats}")
                        return []
                    
                    # Add site information to each record
                    for stat in port_stats:
                        stat['site_id'] = site_id
                        stat['site_name'] = site_name
                    
                    if attempt > 0:
                        logging.info(f"! Retry {attempt} successful for site {site_name} ({len(port_stats)} records)")
                    else:
                        logging.debug(f"! Collected {len(port_stats)} port stats from site {site_name}")
                    return port_stats
                    
                except Exception as e:
                    if attempt < FAST_MODE_MAX_RETRIES:
                        backoff_delay = FAST_MODE_RETRY_DELAY * (FAST_MODE_BACKOFF_MULTIPLIER ** attempt)
                        logging.warning(f"! Attempt {attempt + 1} failed for site {site_name}: {e}")
                        logging.info(f"! Retrying in {backoff_delay:.1f}s (attempt {attempt + 2}/{FAST_MODE_MAX_RETRIES + 1})")
                        time.sleep(backoff_delay)
                    else:
                        logging.error(f"! Final attempt failed for site {site_name}: {e}")
                        return []
            return []
        
        # Retry function for failed sites
        def retry_failed_sites(failed_sites, connection_semaphore):
            retry_results = []
            still_failed = []
            retry_threads = min(FAST_MODE_RETRY_THREADS, len(failed_sites), max(1, FAST_MODE_MAX_CONCURRENT_CONNECTIONS - 2))
            
            if retry_threads <= 0:
                logging.warning(" FAST MODE: No available threads for retry; skipping retries")
                return [], failed_sites
                
            with ThreadPoolExecutor(max_workers=retry_threads) as executor:
                retry_futures = {
                    executor.submit(fetch_site_port_stats, site_info, connection_semaphore): site_info
                    for site_info in failed_sites
                }
                # Wrap the futures dict keys for tqdm progress tracking
                # CRITICAL FIX: Use fully qualified concurrent.futures.as_completed to bypass any monkey-patching
                # Direct import to avoid tqdm or other wrappers interfering with parameters
                import concurrent.futures
                retry_futures_list = list(retry_futures.keys())
                with tqdm(total=len(retry_futures_list), desc="Retrying Failed Sites", unit="site") as pbar:
                    for future in concurrent.futures.as_completed(retry_futures_list):
                        site_info = retry_futures[future]
                        try:
                            result = future.result()
                            if result:
                                retry_results.extend(result)
                                logging.info(f" FAST RETRY OK: {site_info[1]}")
                            else:
                                still_failed.append(site_info)
                                logging.warning(f" FAST RETRY EMPTY: {site_info[1]}")
                        except Exception as e:
                            still_failed.append(site_info)
                            logging.error(f" FAST RETRY EXC: {site_info[1]} -> {e}")
                        finally:
                            pbar.update(1)
            return retry_results, still_failed
        
        # Execute parallel site fetches
        start_time = time.time()
        logging.debug(f"Start time type: {type(start_time)}, value: {start_time}")
        
        # SAFETY: Validate start_time is actually a float
        if not isinstance(start_time, (int, float)):
            logging.error(f"! CRITICAL: start_time is not a number! type={type(start_time)}, value={start_time}")
            logging.error(f"! time module type: {type(time)}, time.time type: {type(time.time)}")
            raise TypeError(f"start_time must be a number, got {type(start_time)}")
        
        successful_results, failed_sites = execute_with_connection_pool_management(
            work_items=sites,
            worker_function=fetch_site_port_stats,
            batch_description="sites",
            retry_function=retry_failed_sites
        )
        
        logging.debug(f"execute_with_connection_pool_management returned - successful_results type: {type(successful_results)}, length: {len(successful_results) if isinstance(successful_results, list) else 'N/A'}")
        logging.debug(f"failed_sites type: {type(failed_sites)}, length: {len(failed_sites) if isinstance(failed_sites, list) else 'N/A'}")
        
        # Flatten results (each successful result is a list of port stats)
        all_port_stats = []
        for idx, result_list in enumerate(successful_results):
            logging.debug(f"Processing result {idx}: type={type(result_list)}, is_list={isinstance(result_list, list)}")
            if isinstance(result_list, list):
                all_port_stats.extend(result_list)
            else:
                logging.warning(f"Unexpected result type at index {idx}: {type(result_list)}, value: {result_list}")
        
        end_time = time.time()
        logging.debug(f"End time type: {type(end_time)}, value: {end_time}")
        duration = end_time - start_time
        logging.debug(f"Duration calculation successful: {duration}")
        
        logging.info(f" FAST MODE SUMMARY (port stats): sites_ok={len(sites) - len(failed_sites)} sites_fail={len(failed_sites)} records={len(all_port_stats)} elapsed={duration:.2f}s")
        print(f"* Fast mode: Collected {len(all_port_stats)} port stat records from {len(sites) - len(failed_sites)}/{len(sites)} sites in {duration:.1f}s")
        
        # Save results
        if all_port_stats:
            # Sort by MAC address if available
            try:
                all_port_stats = sorted(all_port_stats, key=lambda x: x.get('mac', ''))
            except Exception as e:
                logging.debug(f"Could not sort by MAC: {e}")
            
            # Process and save
            flattened = flatten_nested_fields_in_list(all_port_stats)
            sanitized = escape_multiline_strings_for_csv(flattened)
            DataExporter.save_data_to_output(sanitized, output_file, api_function_name='searchSiteSwOrGwPorts')
            print(f"! {len(all_port_stats)} port stat records exported to {output_file}")
            logging.info(f"! Port statistics saved to {output_file} ({len(all_port_stats)} records)")
        else:
            logging.warning(" No port statistics collected. CSV not created.")
            print("! No port statistics collected. CSV not created.")
    else:
        # Non-fast mode: Original org-level search (serial pagination)
        fetch_and_display_api_data(
            title="Org Device Port Stats:",
            api_call=mistapi.api.v1.orgs.stats.searchOrgSwOrGwPorts,
            filename=output_file,
            sort_key="mac",
            duration=f"{hours}h",
            limit=1000
        )

def export_vpn_peer_stats_to_csv(fast: bool = False):
    """Export VPN peer path statistics to `OrgVPNPeerStats.csv`.

    Fast Mode Behavior:
        - Skip API call on fresh cache (age < `CSV_FRESHNESS_MINUTES`).
        - Normal fetch otherwise.
    SECURITY: Read-only; safe to cache.
    """
    output_file = "OrgVPNPeerStats.csv"
    if fast and os.path.exists(output_file):
        try:
            mtime = os.path.getmtime(output_file)
            age_minutes = (time.time() - mtime) / 60.0
            if age_minutes < CSV_FRESHNESS_MINUTES:
                logging.info(f" Fast mode cache hit: {output_file} is fresh ({age_minutes:.1f}m < {CSV_FRESHNESS_MINUTES}m); skipping fetch.")
                print(f"* Fast mode: Using cached {output_file} (age {age_minutes:.1f}m)")
                return
        except Exception as e:  # pragma: no cover
            logging.debug(f"Fast mode freshness check failed for {output_file}: {e}")
    logging.info("Starting export of organization VPN peer path statistics...")
    hours = get_dynamic_lookback_hours(24, 1)
    log_dynamic_lookback("org vpn peer path statistics export", hours)
    fetch_and_display_api_data(
        title="Org VPN Peer Stats:",
        api_call=mistapi.api.v1.orgs.stats.searchOrgPeerPathStats,
        filename=output_file,
        sort_key="mac",
        duration=f"{hours}h",
        limit=1000
    )

def export_site_port_stats_to_csv():
    """Export port statistics for a specific site to SitePortStats.csv."""
    export_site_specific_data(
        api_call=mistapi.api.v1.sites.stats.searchSiteSwOrGwPorts,
        data_type="port stats",
        sort_key="mac"
    )

def export_site_device_virtual_chassis_to_csv():
    """
    Export virtual chassis information for switches at a specific site.
    Prompts user to select a site and device, then exports VC details.
    """
    print("Export Virtual Chassis Information:")
    logging.info("Starting export of site device virtual chassis information...")
    
    # Get site selection
    site_id = prompt_site_selection()
    if not site_id:
        logging.error("No site selected. Exiting.")
        return
    
    # Get device selection (filtered for switches)
    device_id = prompt_device_selection(site_id, device_type="switch")
    if not device_id:
        logging.error("No switch device selected. Exiting.")
        return
    
    # Get device name for display
    response = mistapi.api.v1.sites.devices.listSiteDevices(apisession, site_id, type='all')
    devices = mistapi.get_all(response=response, mist_session=apisession)
    device_name = next((dev["name"] for dev in devices if dev["id"] == device_id), device_id)
    
    logging.info(f"Exporting virtual chassis information for device: {device_name}")
    
    try:
        # Make API call to get virtual chassis information
        response = mistapi.api.v1.sites.devices.getSiteDeviceVirtualChassis(apisession, site_id, device_id)
        
        if response.data:
            # Convert to list format for CSV processing
            vc_data = [response.data] if isinstance(response.data, dict) else response.data
            
            # Process and save the data
            flattened = flatten_nested_fields_in_list(vc_data)
            sanitized = escape_multiline_strings_for_csv(flattened)
            
            filename = f"VirtualChassis_{device_name.replace(' ', '_')}.csv"
            DataExporter.save_data_to_output(sanitized, filename)
            
            logging.info(f"! Virtual chassis information exported to {filename}")
            
            # Display summary
            if sanitized:
                print(f"\n!! Virtual Chassis Summary for {device_name}:")
                print(f"   * Records exported: {len(sanitized)}")
                if 'members' in sanitized[0]:
                    print(f"   * VC members: {sanitized[0].get('members', 'N/A')}")
                if 'preprovisioned' in sanitized[0]:
                    print(f"   * Preprovisioned: {sanitized[0].get('preprovisioned', 'N/A')}")
                print(f"   * Data saved to: {filename}")
        else:
            logging.warning(f"! No virtual chassis data returned for device {device_name}")
            print(f"! No virtual chassis data found for device {device_name}")
            
    except Exception as e:
        logging.error(f"! Failed to export virtual chassis information: {e}")
        print(f"! Failed to export virtual chassis information: {e}")

def export_organization_templates_to_csv():
    """
    Export all organization templates (gateway, network, RF, site, AP) to CSV files.
    """
    logging.info("Starting export of organization templates...")
    
    # Gateway templates
    try:
        fetch_and_display_api_data(
            title="Gateway Templates:",
            api_call=mistapi.api.v1.orgs.gatewaytemplates.listOrgGatewayTemplates,
            filename="OrgGatewayTemplates.csv",
            sort_key="name",
            limit=1000
        )
    except Exception as e:
        logging.error(f"Failed to export gateway templates: {e}")
    
    # Network templates
    try:
        fetch_and_display_api_data(
            title="Network Templates:",
            api_call=mistapi.api.v1.orgs.networktemplates.listOrgNetworkTemplates,
            filename="OrgNetworkTemplates.csv",
            sort_key="name",
            limit=1000
        )
    except Exception as e:
        logging.error(f"Failed to export network templates: {e}")
    
    # RF templates
    try:
        fetch_and_display_api_data(
            title="RF Templates:",
            api_call=mistapi.api.v1.orgs.rftemplates.listOrgRfTemplates,
            filename="OrgRfTemplates.csv",
            sort_key="name",
            limit=1000
        )
    except Exception as e:
        logging.error(f"Failed to export RF templates: {e}")
    
    # Site templates
    try:
        fetch_and_display_api_data(
            title="Site Templates:",
            api_call=mistapi.api.v1.orgs.sitetemplates.listOrgSiteTemplates,
            filename="OrgSiteTemplates.csv",
            sort_key="name",
            limit=1000
        )
    except Exception as e:
        logging.error(f"Failed to export site templates: {e}")
    
    # AP templates
    try:
        fetch_and_display_api_data(
            title="AP Templates:",
            api_call=mistapi.api.v1.orgs.aptemplates.listOrgAptemplates,
            filename="OrgApTemplates.csv",
            sort_key="name",
            limit=1000
        )
    except Exception as e:
        logging.error(f"Failed to export AP templates: {e}")
    
    logging.info(" Organization templates export completed")

def export_site_clients_to_csv():
    """
    Export client statistics for a specific site to SiteClients.csv.
    Prompts user to select a site and exports connected client information.
    """
    print("Site Client Statistics:")
    logging.info("Starting export of site client statistics...")
    
    # Get site selection
    site_id = prompt_site_selection()
    if not site_id:
        logging.error("No site selected. Exiting.")
        return
    
    # Get site name for display
    sites = fetch_all_sites_with_limit(org_id)
    site_name = next((site["name"] for site in sites if site["id"] == site_id), site_id)
    
    logging.info(f"Exporting client statistics for site: {site_name}")
    
    # Fetch site client stats directly
    try:
        response = mistapi.api.v1.sites.stats.listSiteWirelessClientsStats(apisession, site_id, limit=1000)
        rawdata = mistapi.get_all(response=response, mist_session=apisession)
        
        if rawdata:
            # Process and save data
            flattened_data = flatten_nested_fields_in_list(rawdata)
            sanitized_data = escape_multiline_strings_for_csv(flattened_data)
            filename = f"SiteClients_{site_name.replace(' ', '_')}.csv"
            DataExporter.save_data_to_output(sanitized_data, filename)
            print(f"! {len(rawdata)} client records exported to {filename}")
        else:
            print("! No client data found for this site")
    except Exception as e:
        logging.error(f"Error fetching client stats for site {site_name}: {e}")
        print(f"! Error fetching client data: {e}")

def export_site_devices_to_csv():
    """
    Export device list for a specific site to SiteDevices.csv.
    Prompts user to select a site and exports device information.
    """
    print("Site Device List:")
    logging.info("Starting export of site device list...")
    
    # Get site selection
    site_id = prompt_site_selection()
    if not site_id:
        logging.error("No site selected. Exiting.")
        return
    
    # Get site name for display
    sites = fetch_all_sites_with_limit(org_id)
    site_name = next((site["name"] for site in sites if site["id"] == site_id), site_id)
    
    logging.info(f"Exporting device list for site: {site_name}")
    
    # Fetch site devices directly
    try:
        response = mistapi.api.v1.sites.devices.listSiteDevices(apisession, site_id, type="all")
        rawdata = getattr(response, 'data', [])
        
        if rawdata:
            # Process and save data
            flattened_data = flatten_nested_fields_in_list(rawdata)
            sanitized_data = escape_multiline_strings_for_csv(flattened_data)
            filename = f"SiteDevices_{site_name.replace(' ', '_')}.csv"
            DataExporter.save_data_to_output(sanitized_data, filename)
            print(f"! {len(rawdata)} devices exported to {filename}")
        else:
            print("! No devices found for this site")
    except Exception as e:
        logging.error(f"Error fetching devices for site {site_name}: {e}")
        print(f"! Error fetching device data: {e}")

def export_site_device_stats_to_csv():
    """
    Export device statistics for a specific site to SiteDeviceStats.csv.
    Prompts user to select a site and exports device statistics.
    """
    print("Site Device Statistics:")
    logging.info("Starting export of site device statistics...")
    
    # Get site selection
    site_id = prompt_site_selection()
    if not site_id:
        logging.error("No site selected. Exiting.")
        return
    
    # Get site name for display
    sites = fetch_all_sites_with_limit(org_id)
    site_name = next((site["name"] for site in sites if site["id"] == site_id), site_id)
    
    logging.info(f"Exporting device statistics for site: {site_name}")
    
    # Fetch site device stats directly
    try:
        response = mistapi.api.v1.sites.stats.listSiteDevicesStats(apisession, site_id, type="all", limit=1000)
        rawdata = mistapi.get_all(response=response, mist_session=apisession)
        
        if rawdata:
            # Process and save data
            flattened_data = flatten_nested_fields_in_list(rawdata)
            sanitized_data = escape_multiline_strings_for_csv(flattened_data)
            filename = f"SiteDeviceStats_{site_name.replace(' ', '_')}.csv"
            DataExporter.save_data_to_output(sanitized_data, filename)
            print(f"! {len(rawdata)} device stats exported to {filename}")
        else:
            print("! No device statistics found for this site")
    except Exception as e:
        logging.error(f"Error fetching device stats for site {site_name}: {e}")
        print(f"! Error fetching device statistics: {e}")

def export_org_wireless_clients_to_csv():
    """Export wireless client statistics for the entire organization to OrgWirelessClients.csv."""
    export_org_specific_data(
        api_call=mistapi.api.v1.orgs.clients.searchOrgWirelessClients,
        data_type="wireless clients",
        sort_key="mac"
    )

def export_org_wired_clients_to_csv():
    """Export wired client statistics for the entire organization to OrgWiredClients.csv."""
    export_org_specific_data(
        api_call=mistapi.api.v1.orgs.wired_clients.searchOrgWiredClients,
        data_type="wired clients",
        sort_key="mac"
    )

def export_org_security_events_to_csv():
    """Export security policies (OrgSecurityPolicies.csv), security intelligence profiles (OrgSecIntelProfiles.csv), and site rogue data (OrgRogueData.csv)."""
    print("Export Organization Security Data:")
    logging.info("Starting export of organization security policies, intelligence profiles, and rogue data...")
    org_id = get_cached_or_prompted_org_id()

    # 1. Security Policies
    policies = []
    try:
        logging.info("Fetching organization security policies (secpolicies)...")
        resp = mistapi.api.v1.orgs.secpolicies.listOrgSecPolicies(apisession, org_id, limit=1000)
        policies = mistapi.get_all(response=resp, mist_session=apisession) or []
        logging.debug(f"Security policies fetched: {len(policies)}")
    except Exception as e:
        logging.warning(f"Failed to fetch security policies: {e}")
    if policies:
        processed = flatten_nested_fields_in_list(policies)
        processed = escape_multiline_strings_for_csv(processed)
        DataExporter.save_data_to_output(processed, "OrgSecurityPolicies.csv")
        print(f"! {len(processed)} security policies exported to OrgSecurityPolicies.csv")
        logging.info(f"Exported {len(processed)} security policies to OrgSecurityPolicies.csv")
    else:
        print("! 0 security policies exported to OrgSecurityPolicies.csv (no policies found)")
        logging.warning("No data to export for OrgSecurityPolicies.csv (zero policies returned).")
        DataExporter.save_data_to_output([], "OrgSecurityPolicies.csv")

    # 2. Security Intelligence Profiles (use available endpoint)
    secintel_profiles = []
    try:
        logging.info("Fetching organization security intelligence profiles...")
        resp_secintel = mistapi.api.v1.orgs.secintelprofiles.listOrgSecIntelProfiles(apisession, org_id)
        secintel_profiles = mistapi.get_all(response=resp_secintel, mist_session=apisession) or []
        logging.debug(f"Security intelligence profiles fetched: {len(secintel_profiles)}")
    except Exception as e:
        logging.warning(f"Failed to fetch security intelligence profiles: {e}")
    if secintel_profiles:
        processed_si = flatten_nested_fields_in_list(secintel_profiles)
        processed_si = escape_multiline_strings_for_csv(processed_si)
        DataExporter.save_data_to_output(processed_si, "OrgSecIntelProfiles.csv")
        print(f"! {len(processed_si)} security intelligence profiles exported to OrgSecIntelProfiles.csv")
        logging.info(f"Exported {len(processed_si)} security intelligence profiles to OrgSecIntelProfiles.csv")
    else:
        print("! 0 security intelligence profiles exported to OrgSecIntelProfiles.csv (no profiles found)")
        logging.warning("No data to export for OrgSecIntelProfiles.csv (zero profiles returned).")
        DataExporter.save_data_to_output([], "OrgSecIntelProfiles.csv")

    # 3. Rogue Events (site-level via insights)
    logging.info("Fetching rogue APs and clients from all sites via insights...")
    check_and_generate_csv("SiteList.csv", export_all_sites_to_csv)
    all_rogue_aps = []
    all_rogue_clients = []
    try:
        site_list_path = get_csv_file_path("SiteList.csv")
        with open(site_list_path, mode="r", encoding="utf-8") as f:
            sites = list(csv.DictReader(f))
        for site in tqdm(sites, desc="Sites", unit="site"):
            site_id = site.get("id")
            site_name = site.get("name", "Unknown Site")
            if not site_id:
                continue
            try:
                # Get rogue APs
                response_aps = mistapi.api.v1.sites.insights.listSiteRogueAPs(apisession, site_id, duration="7d", limit=1000)
                site_rogue_aps = mistapi.get_all(response=response_aps, mist_session=apisession) or []
                for ap in site_rogue_aps:
                    ap["site_id"] = site_id
                    ap["site_name"] = site_name
                    ap["rogue_type"] = "AP"
                all_rogue_aps.extend(site_rogue_aps)
                
                # Get rogue clients  
                response_clients = mistapi.api.v1.sites.insights.listSiteRogueClients(apisession, site_id, duration="7d", limit=1000)
                site_rogue_clients = mistapi.get_all(response=response_clients, mist_session=apisession) or []
                for client in site_rogue_clients:
                    client["site_id"] = site_id
                    client["site_name"] = site_name
                    client["rogue_type"] = "Client"
                all_rogue_clients.extend(site_rogue_clients)
                
                logging.info(f"! Fetched {len(site_rogue_aps)} rogue APs and {len(site_rogue_clients)} rogue clients from site: {site_name}")
            except Exception as e:
                logging.warning(f"! Failed to fetch rogue data from site {site_name}: {e}")
                continue
            time.sleep(0.2)
    except Exception as e:
        logging.error(f"Failed to process sites for rogue data: {e}")

    # Combine all rogue data
    all_rogue_data = all_rogue_aps + all_rogue_clients

    if all_rogue_data:
        processed_r = flatten_nested_fields_in_list(all_rogue_data)
        processed_r = escape_multiline_strings_for_csv(processed_r)
        DataExporter.save_data_to_output(processed_r, "OrgRogueData.csv")
        print(f"! {len(processed_r)} rogue devices exported to OrgRogueData.csv")
        logging.info(f"Exported {len(processed_r)} rogue devices to OrgRogueData.csv")
    else:
        print("! 0 rogue devices exported to OrgRogueData.csv (no rogue devices found)")
        logging.info("No rogue devices found across all sites (OrgRogueData.csv written empty).")
        DataExporter.save_data_to_output([], "OrgRogueData.csv")

    print("Security data export completed (3 files generated)")
    logging.info("Completed security policies, intelligence profiles, and rogue data export aggregate.")

# ==============================
# INSIGHTS API FUNCTIONS - Organization & Site Analytics
# ==============================

def export_org_sle_metrics_to_csv():
    """Export organization-wide SLE (Service Level Experience) metrics to OrgSLEMetrics.csv."""
    print("Export Organization SLE Metrics:")
    logging.info("Starting export of organization SLE metrics...")
    org_id = get_cached_or_prompted_org_id()
    
    # Use the actual SLE service categories supported by the Mist platform
    # These are the core service level experience domains
    sle_categories = [
        "wifi",               # WiFi/wireless SLE metrics - CONFIRMED WORKING
        "wan",                # WAN connectivity SLE metrics - CONFIRMED WORKING  
        "wired",              # Wired network SLE metrics - CONFIRMED WORKING
    ]
    
    # Try to get organization-level SLE data using specialized SLE metrics
    # Based on what actually worked in option 83, try SLE-specific aggregation metrics
    org_sle_specialized_metrics = [
        "summary",            # Org summary SLE data - from const insights
        "sites-sle",          # Sites SLE aggregation - from const insights  
        "worst-sites-by-sle", # Worst performing sites SLE analysis
    ]
    
    all_sle_data = []
    metrics_retrieved = 0
    metrics_failed = 0
    
    print(f"! Retrieving organization SLE data using {len(sle_categories)} service categories...")
    print(f"! Also attempting {len(org_sle_specialized_metrics)} specialized SLE aggregation metrics...")
    
    try:
        # First, try using the specialized SLE aggregation metrics with getOrgSle
        for metric in org_sle_specialized_metrics:
            try:
                logging.debug(f"Attempting to retrieve specialized SLE metric: {metric}")
                
                # For metrics that analyze sites by SLE, use getOrgSitesSle instead of getOrgSle
                if "worst-sites" in metric or "sites-sle" in metric:
                    # These metrics require site-level SLE data analysis
                    for sle_category in sle_categories:
                        try:
                            response = mistapi.api.v1.orgs.insights.getOrgSitesSle(
                                apisession, 
                                org_id, 
                                sle=sle_category,
                                duration="7d",
                                limit=1000
                            )
                            sites_sle_data = mistapi.get_all(response=response, mist_session=apisession) or []
                            
                            if sites_sle_data:
                                # Create an aggregated metric result from sites data
                                aggregated_result = {
                                    'sle_metric_type': f"{metric}_{sle_category}",
                                    'org_id': org_id,
                                    'sle_category': sle_category,
                                    'data_source': 'org_sites_sle_aggregated',
                                    'total_sites': len(sites_sle_data),
                                    'sites_analyzed': sites_sle_data,
                                    'metric_name': metric
                                }
                                
                                # For worst-sites metrics, we could add analysis here
                                if "worst-sites" in metric:
                                    aggregated_result['analysis_type'] = 'worst_sites_identification'
                                
                                all_sle_data.append(aggregated_result)
                                metrics_retrieved += 1
                                logging.debug(f"Successfully retrieved sites SLE data for metric analysis: {metric} with SLE: {sle_category} ({len(sites_sle_data)} sites)")
                            else:
                                logging.debug(f"No sites SLE data available for metric: {metric} with SLE: {sle_category}")
                        except Exception as sites_error:
                            logging.debug(f"Failed to get sites SLE data for metric '{metric}' with SLE '{sle_category}': {sites_error}")
                            continue
                else:
                    # For other metrics, call getOrgSle directly
                    response = mistapi.api.v1.orgs.insights.getOrgSle(
                        apisession, 
                        org_id, 
                        metric,
                        duration="7d"
                    )
                    sle_data = getattr(response, 'data', response) or {}
                    
                    if sle_data:
                        sle_data['sle_metric_type'] = metric
                        sle_data['org_id'] = org_id
                        sle_data['data_source'] = 'org_sle_specialized'
                        all_sle_data.append(sle_data)
                        metrics_retrieved += 1
                        logging.debug(f"Successfully retrieved specialized SLE data for metric: {metric}")
                    else:
                        logging.debug(f"No data available for specialized SLE metric: {metric}")
                        metrics_failed += 1
                    
            except Exception as metric_error:
                metrics_failed += 1
                logging.debug(f"Failed to get specialized SLE data for metric '{metric}': {metric_error}")
                continue
        
        # Second, get aggregated SLE data for each service category using getOrgSitesSle 
        # but process it as organization-wide aggregated data
        for sle_category in sle_categories:
            try:
                logging.debug(f"Attempting to retrieve aggregated SLE data for category: {sle_category}")
                response = mistapi.api.v1.orgs.insights.getOrgSitesSle(
                    apisession, 
                    org_id, 
                    sle=sle_category, 
                    duration="7d", 
                    limit=1000
                )
                sites_sle_data = mistapi.get_all(response=response, mist_session=apisession) or []
                
                if sites_sle_data:
                    # Create an organization-level aggregation from sites data
                    org_aggregated = {
                        'sle_category': sle_category,
                        'org_id': org_id,
                        'data_source': 'org_aggregated_from_sites',
                        'total_sites': len(sites_sle_data),
                        'sites_data': sites_sle_data  # Include detailed sites for analysis
                    }
                    
                    # Calculate organization-level aggregations if possible
                    if sites_sle_data:
                        # Add summary statistics
                        org_aggregated['summary_calculated'] = True
                        
                    all_sle_data.append(org_aggregated)
                    metrics_retrieved += 1
                    logging.debug(f"Successfully aggregated SLE data for {len(sites_sle_data)} sites in category: {sle_category}")
                else:
                    logging.debug(f"No sites SLE data available for category: {sle_category}")
                    metrics_failed += 1
                    
            except Exception as category_error:
                metrics_failed += 1
                logging.debug(f"Failed to get SLE data for category '{sle_category}': {category_error}")
                continue
        
        # Report results
        print(f"! SLE data retrieval completed: {metrics_retrieved} successful, {metrics_failed} failed")
        logging.info(f"Org SLE data: {metrics_retrieved} retrieved successfully, {metrics_failed} failed")
        
        if all_sle_data:
            # Flatten and process the data for CSV export
            processed = flatten_nested_fields_in_list(all_sle_data)
            processed = escape_multiline_strings_for_csv(processed)
            DataExporter.save_data_to_output(processed, "OrgSLEMetrics.csv")
            print(f"! {metrics_retrieved} organization SLE data sources exported to OrgSLEMetrics.csv")
            logging.info(f"Exported {len(processed)} org SLE data points from {metrics_retrieved} sources to OrgSLEMetrics.csv")
        else:
            print("! 0 organization SLE metrics exported to OrgSLEMetrics.csv (no data available)")
            logging.warning("No org SLE data available - all sources failed or returned empty")
            DataExporter.save_data_to_output([], "OrgSLEMetrics.csv")
            
    except Exception as e:
        print(f"! Error exporting organization SLE metrics: {e}")
        logging.error(f"Failed to export org SLE metrics: {e}")
        DataExporter.save_data_to_output([], "OrgSLEMetrics.csv")

def export_org_sites_sle_summary_to_csv():
    """Export SLE summary metrics for all sites in the organization to OrgSitesSLESummary.csv."""
    print("Export Organization Sites SLE Summary:")
    logging.info("Starting export of sites SLE summary...")
    org_id = get_cached_or_prompted_org_id()
    
    # SLE types to export
    sle_types = ["wifi", "wired", "wan"]
    all_sites_sle_data = []
    
    for sle_type in sle_types:
        try:
            response = mistapi.api.v1.orgs.insights.getOrgSitesSle(apisession, org_id, sle=sle_type, duration="7d", limit=1000)
            sites_sle_data = mistapi.get_all(response=response, mist_session=apisession) or []
            
            for site_data in sites_sle_data:
                # Add SLE type identifier to the data
                site_data['sle_type'] = sle_type
                all_sites_sle_data.append(site_data)
            
            logging.debug(f"Retrieved SLE data for {len(sites_sle_data)} sites with SLE type: {sle_type}")
        except Exception as e:
            logging.warning(f"Failed to get sites SLE data for type {sle_type}: {e}")
            continue
    
    if all_sites_sle_data:
        processed = flatten_nested_fields_in_list(all_sites_sle_data)
        processed = escape_multiline_strings_for_csv(processed)
        DataExporter.save_data_to_output(processed, "OrgSitesSLESummary.csv")
        print(f"! {len(processed)} sites SLE summary exported to OrgSitesSLESummary.csv")
        logging.info(f"Exported {len(processed)} sites SLE summary to OrgSitesSLESummary.csv")
    else:
        print("! 0 sites SLE summary exported to OrgSitesSLESummary.csv (no data available)")
        logging.warning("No sites SLE data available for organization")
        DataExporter.save_data_to_output([], "OrgSitesSLESummary.csv")

def export_site_insight_metrics_to_csv():
    """Export general insight metrics for a selected site to SiteInsightMetrics_[SiteName].csv."""
    print("Export Site Insight Metrics:")
    logging.info("Starting export of site insight metrics...")
    
    # Get site selection
    site_id = prompt_site_selection()
    if not site_id:
        logging.error("No site selected. Exiting.")
        return
    
    # Get site name for filename
    try:
        response = mistapi.api.v1.sites.listSites(apisession, site_id)
        sites = mistapi.get_all(response=response, mist_session=apisession)
        site_name = next((site["name"] for site in sites if site["id"] == site_id), site_id)
    except:
        site_name = site_id
    
    sanitized_site_name = EnhancedSSHRunner.sanitize_filename(site_name or site_id)
    filename = f"SiteInsightMetrics_{sanitized_site_name}.csv"
    
    # First, refresh the available metrics from the API
    print("! Refreshing available insight metrics from Mist API...")
    export_const_insight_metrics_to_csv()
    
    # Get all metrics that support "site" scope
    site_metrics = get_insight_metrics_by_scope("site")
    
    if not site_metrics:
        print("! No metrics found for site scope. Check ConstInsightMetrics.csv file.")
        logging.error("No site-scope metrics found in const insight metrics")
        DataExporter.save_data_to_output([], filename)
        return
    
    all_insight_data = []
    metrics_retrieved = 0
    
    print(f"! Retrieving {len(site_metrics)} different site insight metrics...")
    
    try:
        for metric in site_metrics:
            try:
                response = mistapi.api.v1.sites.insights.getSiteInsightMetrics(apisession, site_id, metric)
                insight_data = getattr(response, 'data', response) or {}
                
                if insight_data:
                    # Add metric type identifier to each data point
                    insight_data['metric_type'] = metric
                    insight_data['site_id'] = site_id
                    insight_data['site_name'] = site_name
                    all_insight_data.append(insight_data)
                    metrics_retrieved += 1
                    logging.debug(f"Retrieved site insight data for metric: {metric}")
                else:
                    logging.debug(f"No data available for metric: {metric}")
            except Exception as e:
                logging.debug(f"Failed to get site insight data for metric {metric}: {e}")
                continue
        
        if all_insight_data:
            processed = flatten_nested_fields_in_list(all_insight_data)
            processed = escape_multiline_strings_for_csv(processed)
            DataExporter.save_data_to_output(processed, filename)
            print(f"! {metrics_retrieved} site insight metrics exported to {filename}")
            logging.info(f"Exported {metrics_retrieved} site insight metrics for {site_name} to {filename}")
        else:
            print(f"! 0 insight metrics exported to {filename} (no data available)")
            logging.warning(f"No insight data available for site {site_name}")
            DataExporter.save_data_to_output([], filename)
    except Exception as e:
        print(f"! Error exporting site insight metrics: {e}")
        logging.error(f"Failed to export site insight metrics for {site_name}: {e}")
        DataExporter.save_data_to_output([], filename)

def export_site_client_insights_to_csv():
    """Export client-specific insight metrics for a selected site to SiteClientInsights_[SiteName].csv."""
    print("Export Site Client Insights:")
    logging.info("Starting export of site client insights...")
    
    # First, refresh the available metrics from the API
    print("! Refreshing available insight metrics from Mist API...")
    export_const_insight_metrics_to_csv()
    
    # Get site selection
    site_id = prompt_site_selection()
    if not site_id:
        logging.error("No site selected. Exiting.")
        return
    
    # Get site name for filename
    try:
        response = mistapi.api.v1.sites.listSites(apisession, site_id)
        sites = mistapi.get_all(response=response, mist_session=apisession)
        site_name = next((site["name"] for site in sites if site["id"] == site_id), site_id)
    except:
        site_name = site_id
    
    sanitized_site_name = EnhancedSSHRunner.sanitize_filename(site_name or site_id)
    
    # Get available clients for the site to help user selection
    try:
        response = mistapi.api.v1.sites.stats.listSiteWirelessClientsStats(apisession, site_id)
        clients = mistapi.get_all(response=response, mist_session=apisession) or []
        
        if clients:
            print(f"\n! Found {len(clients)} clients at site {site_name}")
            print("Recent clients (showing first 5):")
            for i, client in enumerate(clients[:5]):
                mac = client.get('mac', 'Unknown')
                hostname = client.get('hostname', 'Unknown')
                last_seen = client.get('last_seen', 'Unknown')
                print(f"  [{i}] MAC: {mac}, Hostname: {hostname}, Last seen: {last_seen}")
        else:
            print(f"! No clients found at site {site_name}")
    except Exception as e:
        logging.warning(f"Could not retrieve client list: {e}")
        clients = []
    
    # Prompt for client MAC address
    print("\nEnter client MAC address or index number (or press Enter to skip):")
    client_input = input("Client MAC/Index: ").strip()
    
    if not client_input:
        print("! No client input provided. Skipping client insights export.")
        return
    
    # Check if input is a numeric index
    client_mac = None
    if client_input.isdigit():
        try:
            index = int(client_input)
            if 0 <= index < len(clients):
                client_mac = clients[index].get('mac', '')
                print(f"! Selected client by index: {client_mac}")
            else:
                print(f"! Invalid index {index}. Must be between 0 and {len(clients)-1}")
                return
        except (ValueError, IndexError):
            print(f"! Invalid index: {client_input}")
            return
    else:
        # Treat as MAC address
        client_mac = client_input
    
    if not client_mac:
        print("! Could not determine client MAC address.")
        return
    
    filename = f"SiteClientInsights_{sanitized_site_name}_{client_mac.replace(':', '')}.csv"
    
    # Get all metrics that support "client" scope
    client_metrics = get_insight_metrics_by_scope("client")
    
    if not client_metrics:
        print("! No metrics found for client scope. Check ConstInsightMetrics.csv file.")
        logging.error("No client-scope metrics found in const insight metrics")
        DataExporter.save_data_to_output([], filename)
        return
    
    all_client_data = []
    metrics_retrieved = 0
    
    print(f"! Retrieving {len(client_metrics)} different client insight metrics for {client_mac}...")
    
    try:
        for metric in client_metrics:
            try:
                response = mistapi.api.v1.sites.insights.getSiteInsightMetricsForClient(apisession, site_id, client_mac, metric)
                client_insight_data = getattr(response, 'data', response) or {}
                
                if client_insight_data:
                    # Add metric type identifier to each data point
                    client_insight_data['metric_type'] = metric
                    client_insight_data['site_id'] = site_id
                    client_insight_data['site_name'] = site_name
                    client_insight_data['client_mac'] = client_mac
                    all_client_data.append(client_insight_data)
                    metrics_retrieved += 1
                    logging.debug(f"Retrieved client insight data for metric: {metric}")
                else:
                    logging.debug(f"No data available for client metric: {metric}")
            except Exception as e:
                logging.debug(f"Failed to get client insight data for metric {metric}: {e}")
                continue
        
        if all_client_data:
            processed = flatten_nested_fields_in_list(all_client_data)
            processed = escape_multiline_strings_for_csv(processed)
            DataExporter.save_data_to_output(processed, filename)
            print(f"! {metrics_retrieved} client insight metrics exported to {filename}")
            logging.info(f"Exported {metrics_retrieved} client insight metrics for {client_mac} at {site_name} to {filename}")
        else:
            print(f"! 0 client insights exported to {filename} (no data available)")
            logging.warning(f"No client insight data available for {client_mac} at {site_name}")
            DataExporter.save_data_to_output([], filename)
    except Exception as e:
        print(f"! Error exporting client insights: {e}")
        logging.error(f"Failed to export client insights for {client_mac} at {site_name}: {e}")
        DataExporter.save_data_to_output([], filename)

def export_site_device_insights_to_csv():
    """Export device-specific insight metrics for a selected site to SiteDeviceInsights_[SiteName].csv."""
    print("Export Site Device Insights:")
    logging.info("Starting export of site device insights...")
    
    # First, refresh the available metrics from the API
    print("! Refreshing available insight metrics from Mist API...")
    export_const_insight_metrics_to_csv()
    
    # Get site selection
    site_id = prompt_site_selection()
    if not site_id:
        logging.error("No site selected. Exiting.")
        return
    
    # Get device selection
    device_id = prompt_device_selection(site_id)
    if not device_id:
        logging.error("No device selected. Exiting.")
        return
    
    # Get site and device names for filename
    try:
        response = mistapi.api.v1.sites.listSites(apisession, site_id)
        sites = mistapi.get_all(response=response, mist_session=apisession)
        site_name = next((site["name"] for site in sites if site["id"] == site_id), site_id)
    except:
        site_name = site_id
    
    try:
        response = mistapi.api.v1.sites.devices.listSiteDevices(apisession, site_id, type="all")
        devices = mistapi.get_all(response=response, mist_session=apisession)
        device = next((dev for dev in devices if dev["id"] == device_id), None)
        device_name = device["name"] if device else device_id
        device_mac = device["mac"] if device else None
    except:
        device_name = device_id
        device_mac = None
    
    if not device_mac:
        print(f"! Error: Could not find MAC address for device {device_name}")
        logging.error(f"Could not find MAC address for device {device_id}")
        return
    
    sanitized_site_name = EnhancedSSHRunner.sanitize_filename(site_name or site_id)
    sanitized_device_name = EnhancedSSHRunner.sanitize_filename(device_name or device_id)
    filename = f"SiteDeviceInsights_{sanitized_site_name}_{sanitized_device_name}.csv"
    
    # Get all metrics that support "device" scope
    device_metrics = get_insight_metrics_by_scope("device")
    
    if not device_metrics:
        print("! No metrics found for device scope. Check ConstInsightMetrics.csv file.")
        logging.error("No device-scope metrics found in const insight metrics")
        DataExporter.save_data_to_output([], filename)
        return
    
    all_device_data = []
    metrics_retrieved = 0
    
    print(f"! Retrieving {len(device_metrics)} different device insight metrics for {device_name}...")
    
    try:
        for metric in device_metrics:
            try:
                response = mistapi.api.v1.sites.insights.getSiteInsightMetricsForDevice(apisession, site_id, metric, device_mac)
                device_insight_data = getattr(response, 'data', response) or {}
                
                if device_insight_data:
                    # Add metric type identifier to each data point
                    device_insight_data['metric_type'] = metric
                    device_insight_data['site_id'] = site_id
                    device_insight_data['site_name'] = site_name
                    device_insight_data['device_id'] = device_id
                    device_insight_data['device_name'] = device_name
                    device_insight_data['device_mac'] = device_mac
                    all_device_data.append(device_insight_data)
                    metrics_retrieved += 1
                    logging.debug(f"Retrieved device insight data for metric: {metric}")
                else:
                    logging.debug(f"No data available for device metric: {metric}")
            except Exception as e:
                logging.debug(f"Failed to get device insight data for metric {metric}: {e}")
                continue
        
        if all_device_data:
            processed = flatten_nested_fields_in_list(all_device_data)
            processed = escape_multiline_strings_for_csv(processed)
            DataExporter.save_data_to_output(processed, filename)
            print(f"! {metrics_retrieved} device insight metrics exported to {filename}")
            logging.info(f"Exported {metrics_retrieved} device insight metrics for {device_name} at {site_name} to {filename}")
        else:
            print(f"! 0 device insights exported to {filename} (no data available)")
            logging.warning(f"No device insight data available for {device_name} at {site_name}")
            DataExporter.save_data_to_output([], filename)
    except Exception as e:
        print(f"! Error exporting device insights: {e}")
        logging.error(f"Failed to export device insights for {device_name} at {site_name}: {e}")
        DataExporter.save_data_to_output([], filename)

def export_all_const_definitions_to_csv():
    """Export all available const definitions from the Mist API to individual CSV files.
    
    Implements fully dynamic discovery and smart caching:
    - Automatically discovers all available const endpoints from mistapi library using introspection
    - Dynamically inspects each const module to find the correct function names
    - Checks if each Const{EndpointName}.csv exists and is fresh (< 24 hours old)
    - If fresh file exists, skips API call for that endpoint
    - If file is missing or stale (>= 24 hours old), fetches fresh data from API
    - Creates comprehensive const definition files for all available endpoints
    """
    import os
    import time
    import importlib
    import inspect
    import pkgutil
    from datetime import datetime, timedelta
    
    print("Export All Available Const Definitions (Dynamic Discovery):")
    logging.info("Starting comprehensive dynamic export of all const definitions...")
    
    cache_max_age_hours = 24  # Consider file stale after 24 hours
    
    try:
        # Dynamically discover all const modules in mistapi.api.v1.const
        import mistapi.api.v1.const as const_package
        
        discovered_endpoints = {}
        
        print("! Dynamically discovering const endpoints from mistapi library...")
        logging.info("Starting dynamic discovery of const endpoints")
        
        # Walk through all submodules in the const package
        for importer, modname, ispkg in pkgutil.iter_modules(const_package.__path__, const_package.__name__ + "."):
            if not ispkg:  # Only process non-package modules
                try:
                    # Extract just the endpoint name (last part after the dots)
                    endpoint_name = modname.split('.')[-1]
                    
                    # Skip if this looks like a private module
                    if endpoint_name.startswith('_'):
                        continue
                    
                    print(f"  ! Inspecting const module: {endpoint_name}")
                    
                    # Import the module dynamically
                    module = importlib.import_module(modname)
                    
                    # Find all callable functions in the module that look like API calls
                    functions = []
                    for name, obj in inspect.getmembers(module):
                        if inspect.isfunction(obj) and not name.startswith('_'):
                            # Check if function takes a mist_session parameter (API call signature)
                            sig = inspect.signature(obj)
                            param_names = list(sig.parameters.keys())
                            if ('mist_session' in param_names or 'apisession' in param_names) and len(param_names) >= 1:
                                functions.append(name)
                                logging.debug(f"Found potential API function in {endpoint_name}: {name}{sig}")
                    
                    if functions:
                        # For const endpoints, we typically want the "list" function
                        # Priority order: list*, get*, then first available
                        api_function = None
                        for func_name in functions:
                            if func_name.lower().startswith('list'):
                                api_function = func_name
                                break
                        
                        if not api_function:
                            for func_name in functions:
                                if func_name.lower().startswith('get'):
                                    api_function = func_name
                                    break
                        
                        if not api_function and functions:
                            api_function = functions[0]  # Use first available function
                        
                        if api_function:
                            # Create filename based on endpoint name with proper title casing
                            # Convert snake_case to TitleCase properly
                            parts = endpoint_name.split('_')
                            title_name = ''.join(word.capitalize() for word in parts)
                            filename = f"Const{title_name}.csv"
                            description = f"{endpoint_name.replace('_', ' ').title()} Definitions"
                            
                            # Check if this function requires additional parameters beyond mist_session
                            sig = inspect.signature(getattr(module, api_function))
                            required_params = [p for p in sig.parameters.values() 
                                             if p.default == inspect.Parameter.empty and p.name not in ['mist_session', 'apisession']]
                            
                            if required_params:
                                # Handle special cases that need additional parameters
                                param_names = [p.name for p in required_params]
                                
                                if endpoint_name == 'default_gateway_config' and 'model' in param_names:
                                    # Special handling for gateway config - call for all models
                                    print(f"    ! Found special endpoint {api_function}() requiring 'model' parameter")
                                    print(f"    ! Will call for all available gateway models -> {filename}")
                                    discovered_endpoints[endpoint_name] = {
                                        'module': module,
                                        'function': api_function,
                                        'filename': filename,
                                        'description': description,
                                        'modname': modname,
                                        'special_handling': 'all_models'
                                    }
                                elif endpoint_name == 'states' and 'country_code' in param_names:
                                    # Special handling for states - call for all countries
                                    print(f"    ! Found special endpoint {api_function}() requiring 'country_code' parameter")
                                    print(f"    ! Will call for all available countries -> {filename}")
                                    discovered_endpoints[endpoint_name] = {
                                        'module': module,
                                        'function': api_function,
                                        'filename': filename,
                                        'description': description,
                                        'modname': modname,
                                        'special_handling': 'all_countries'
                                    }
                                else:
                                    # Skip functions that require other additional parameters we can't provide
                                    print(f"    ! Skipping {api_function}() - requires additional parameters: {param_names}")
                                    logging.info(f"Skipping {endpoint_name}.{api_function}() - requires parameters: {param_names}")
                                    continue
                            else:
                                # Standard endpoint with no extra parameters required
                                discovered_endpoints[endpoint_name] = {
                                    'module': module,
                                    'function': api_function,
                                    'filename': filename,
                                    'description': description,
                                    'modname': modname,
                                    'special_handling': None
                                }
                            
                            print(f"    ! Found API function: {api_function}() -> {filename}")
                            logging.debug(f"Discovered {endpoint_name}: {api_function}() -> {filename}")
                        else:
                            print(f"    ! No suitable API functions found in {endpoint_name}")
                            logging.warning(f"No API functions with mist_session parameter found in {endpoint_name}")
                    else:
                        print(f"    ! No API functions found in {endpoint_name}")
                        logging.warning(f"No functions found in {endpoint_name}")
                        
                except Exception as e:
                    print(f"    ! Error inspecting {endpoint_name}: {e}")
                    logging.error(f"Error inspecting const module {endpoint_name}: {e}")
                    continue
        
        if not discovered_endpoints:
            print("! No const endpoints discovered from mistapi library")
            logging.error("Dynamic discovery found no const endpoints")
            return
        
        print(f"! Successfully discovered {len(discovered_endpoints)} const endpoints dynamically")
        logging.info(f"Dynamic discovery completed: {len(discovered_endpoints)} endpoints found")
        
        endpoints_processed = 0
        endpoints_skipped_fresh = 0
        endpoints_updated = 0
        endpoints_failed = 0
        
        # Process each discovered const endpoint individually
        for endpoint_name, endpoint_config in discovered_endpoints.items():
            try:
                filename = endpoint_config['filename']
                description = endpoint_config['description']
                module = endpoint_config['module']
                function_name = endpoint_config['function']
                
                print(f"\n! Processing {description} ({endpoint_name})...")
                
                # Check if file exists and determine its freshness
                file_path = os.path.join('data', filename)
                file_exists = os.path.exists(file_path)
                file_is_fresh = False
                
                if file_exists:
                    try:
                        file_mtime = os.path.getmtime(file_path)
                        file_age_hours = (time.time() - file_mtime) / 3600
                        file_is_fresh = file_age_hours < cache_max_age_hours
                        
                        file_timestamp = datetime.fromtimestamp(file_mtime).strftime('%Y-%m-%d %H:%M:%S')
                        if file_is_fresh:
                            print(f"  ! Found fresh {filename} (created {file_timestamp}, {file_age_hours:.1f}h old)")
                            print(f"  ! Skipping API call - using cached data (cache valid for {cache_max_age_hours}h)")
                            logging.info(f"Using cached {endpoint_name} file (age: {file_age_hours:.1f}h)")
                            endpoints_skipped_fresh += 1
                            endpoints_processed += 1
                            continue  # Skip to next endpoint - file is fresh enough
                        else:
                            print(f"  ! Found stale {filename} (created {file_timestamp}, {file_age_hours:.1f}h old)")
                            print(f"  ! File is older than {cache_max_age_hours}h threshold - fetching fresh data from API...")
                            logging.info(f"Refreshing stale {endpoint_name} file (age: {file_age_hours:.1f}h)")
                    except Exception as e:
                        print(f"  ! Error checking file timestamp: {e}")
                        logging.warning(f"Could not check {endpoint_name} file timestamp, will fetch fresh data: {e}")
                        file_is_fresh = False
                else:
                    print(f"  ! {filename} not found - fetching fresh data from API...")
                    logging.info(f"{filename} not found, fetching from API")
                
                # Only reach this point if file is missing or stale - fetch fresh data
                try:
                    print(f"  ! Requesting fresh {description.lower()} from Mist API using {function_name}()...")
                    
                    # Handle special endpoints that require additional parameters
                    special_handling = endpoint_config.get('special_handling')
                    
                    if special_handling == 'all_models':
                        # Handle default_gateway_config - call for all available gateway models
                        print(f"  ! Special handling: Calling {function_name}() for all available gateway models...")
                        
                        # First get the list of gateway models from device_models
                        try:
                            device_models_module = importlib.import_module('mistapi.api.v1.const.device_models')
                            device_models_function = getattr(device_models_module, 'listDeviceModels')
                            models_response = device_models_function(apisession)
                            device_models_data = getattr(models_response, 'data', models_response) or {}
                            
                            # Filter for gateway models only (NOT switches)
                            gateway_models = []
                            if isinstance(device_models_data, dict):
                                # Handle dictionary format
                                for model_name, model_details in device_models_data.items():
                                    if isinstance(model_details, dict):
                                        model_type = model_details.get('type', '').lower()
                                        if model_type == 'gateway':
                                            gateway_models.append(model_name)
                            elif isinstance(device_models_data, list):
                                # Handle list format
                                for model_item in device_models_data:
                                    if isinstance(model_item, dict):
                                        model_name = model_item.get('model', model_item.get('name', ''))
                                        model_type = model_item.get('type', '').lower()
                                        if model_name and model_type == 'gateway':
                                            gateway_models.append(model_name)
                                        
                            if not gateway_models:
                                # Fallback to common gateway models only (not switches)
                                gateway_models = ['SRX300', 'SRX320', 'SRX320-POE', 'SRX340', 'SRX345', 'SRX380']
                                print(f"    ! Using fallback gateway models: {len(gateway_models)} models")
                            else:
                                print(f"    ! Discovered {len(gateway_models)} gateway models from device definitions")
                            
                            # Call the function for each gateway model
                            all_gateway_configs = []
                            successful_models = 0
                            failed_models = 0
                            
                            for model in gateway_models:
                                try:
                                    api_function = getattr(module, function_name)
                                    model_response = api_function(apisession, model=model)
                                    model_data = getattr(model_response, 'data', model_response) or {}
                                    
                                    if model_data:
                                        # Add model identifier to each record
                                        if isinstance(model_data, dict):
                                            model_record = {'model': model}
                                            model_record.update(model_data)
                                            all_gateway_configs.append(model_record)
                                        elif isinstance(model_data, list):
                                            for item in model_data:
                                                if isinstance(item, dict):
                                                    item['model'] = model
                                                all_gateway_configs.extend(model_data)
                                        else:
                                            all_gateway_configs.append({'model': model, 'config': str(model_data)})
                                        successful_models += 1
                                except Exception as model_error:
                                    logging.warning(f"Failed to get gateway config for model {model}: {model_error}")
                                    failed_models += 1
                                    continue
                            
                            const_data = all_gateway_configs
                            print(f"    ! Successfully retrieved configs for {successful_models} models, {failed_models} failed")
                            
                        except Exception as e:
                            print(f"    ! Error getting gateway models list: {e}")
                            logging.error(f"Failed to get gateway models for {endpoint_name}: {e}")
                            const_data = {}
                            
                    elif special_handling == 'all_countries':
                        # Handle states - call for all available countries
                        print(f"  ! Special handling: Calling {function_name}() for all available countries...")
                        
                        # First get the list of countries
                        try:
                            countries_module = importlib.import_module('mistapi.api.v1.const.countries')
                            countries_function = getattr(countries_module, 'listCountryCodes')
                            countries_response = countries_function(apisession)
                            countries_data = getattr(countries_response, 'data', countries_response) or {}
                            
                            # Extract country codes
                            country_codes = []
                            if isinstance(countries_data, dict):
                                country_codes = list(countries_data.keys())
                            elif isinstance(countries_data, list):
                                for item in countries_data:
                                    if isinstance(item, dict) and 'code' in item:
                                        country_codes.append(item['code'])
                                    elif isinstance(item, dict) and 'name' in item:
                                        # Extract code from name or use first 2 chars
                                        code = item.get('alpha2', item.get('code', item['name'][:2].upper()))
                                        country_codes.append(code)
                            
                            if not country_codes:
                                # Fallback to major countries if we can't get the full list
                                country_codes = ['US', 'CA', 'GB', 'AU', 'DE', 'FR', 'JP', 'CN', 'IN', 'BR']
                                print(f"    ! Using fallback country codes: {len(country_codes)} countries")
                            else:
                                print(f"    ! Discovered {len(country_codes)} country codes from country definitions")
                            
                            # Call the function for each country
                            all_states = []
                            successful_countries = 0
                            failed_countries = 0
                            
                            for country_code in country_codes:
                                try:
                                    api_function = getattr(module, function_name)
                                    country_response = api_function(apisession, country_code=country_code)
                                    country_data = getattr(country_response, 'data', country_response) or {}
                                    
                                    if country_data:
                                        # Add country identifier to each record
                                        if isinstance(country_data, dict):
                                            for state_code, state_data in country_data.items():
                                                if isinstance(state_data, dict):
                                                    state_record = {'country_code': country_code, 'state_code': state_code}
                                                    state_record.update(state_data)
                                                    all_states.append(state_record)
                                                else:
                                                    all_states.append({
                                                        'country_code': country_code, 
                                                        'state_code': state_code, 
                                                        'state_name': str(state_data)
                                                    })
                                        elif isinstance(country_data, list):
                                            for item in country_data:
                                                if isinstance(item, dict):
                                                    item['country_code'] = country_code
                                                all_states.extend(country_data)
                                        successful_countries += 1
                                except Exception as country_error:
                                    logging.warning(f"Failed to get states for country {country_code}: {country_error}")
                                    failed_countries += 1
                                    continue
                            
                            const_data = all_states
                            print(f"    ! Successfully retrieved states for {successful_countries} countries, {failed_countries} failed")
                            
                        except Exception as e:
                            print(f"    ! Error getting countries list: {e}")
                            logging.error(f"Failed to get countries for {endpoint_name}: {e}")
                            const_data = {}
                    else:
                        # Standard endpoint - call normally
                        api_function = getattr(module, function_name)
                        response = api_function(apisession)
                        const_data = getattr(response, 'data', response) or {}
                    
                    if const_data:
                        # Handle different data structures returned by different endpoints
                        if isinstance(const_data, dict):
                            # Convert dictionary to list of records for CSV processing
                            if endpoint_name == 'insight_metrics':
                                # Special handling for insight metrics with complex nested structure
                                data_list = []
                                for metric_name, metric_details in const_data.items():
                                    # Flatten the metric details into a single row
                                    metric_row = {
                                        'metric_name': metric_name,
                                        'description': metric_details.get('description', ''),
                                        'type': metric_details.get('type', ''),
                                        'unit': metric_details.get('unit', ''),
                                        'scopes': ', '.join(metric_details.get('scopes', [])),
                                        'report_scopes': ', '.join(metric_details.get('report_scopes', [])),
                                    }
                                    
                                    # Add interval information if available
                                    intervals = metric_details.get('intervals', {})
                                    if intervals:
                                        interval_info = []
                                        for interval_name, interval_data in intervals.items():
                                            interval_str = f"{interval_name}({interval_data.get('interval', 'N/A')}s, max_age:{interval_data.get('max_age', 'N/A')}s)"
                                            interval_info.append(interval_str)
                                        metric_row['intervals'] = '; '.join(interval_info)
                                    else:
                                        metric_row['intervals'] = ''
                                    
                                    # Add report interval information if available
                                    report_intervals = metric_details.get('report_intervals', {})
                                    if report_intervals:
                                        report_interval_info = []
                                        for interval_name, interval_data in report_intervals.items():
                                            interval_str = f"{interval_name}({interval_data.get('interval', 'N/A')}s)"
                                            report_interval_info.append(interval_str)
                                        metric_row['report_intervals'] = '; '.join(report_interval_info)
                                    else:
                                        metric_row['report_intervals'] = ''
                                    
                                    data_list.append(metric_row)
                            else:
                                # Standard dictionary to list conversion for other endpoints
                                data_list = []
                                for key, value in const_data.items():
                                    if isinstance(value, dict):
                                        # Flatten nested dictionary
                                        row = {'name': key}
                                        row.update(value)
                                        data_list.append(row)
                                    else:
                                        # Simple key-value pair
                                        data_list.append({'name': key, 'value': str(value)})
                        elif isinstance(const_data, list):
                            # Data is already a list - use directly
                            data_list = const_data
                        else:
                            # Single item, convert to list
                            data_list = [const_data] if const_data else []
                        
                        # Process and save the data
                        processed = escape_multiline_strings_for_csv(data_list)
                        DataExporter.save_data_to_output(processed, filename)
                        print(f"  ! {len(processed)} {description.lower()} exported to {filename}")
                        logging.info(f"Exported {len(processed)} fresh {description.lower()} to {filename}")
                        endpoints_updated += 1
                    else:
                        print(f"  ! 0 {description.lower()} exported to {filename} (no data available)")
                        logging.warning(f"No {description.lower()} data available from {endpoint_name} endpoint")
                        DataExporter.save_data_to_output([], filename)
                        endpoints_updated += 1
                        
                except Exception as e:
                    print(f"  ! Error exporting {description.lower()}: {e}")
                    logging.error(f"Failed to export {description.lower()} from {endpoint_name}: {e}")
                    DataExporter.save_data_to_output([], filename)
                    endpoints_failed += 1
                    
                endpoints_processed += 1
                
            except Exception as e:
                print(f"! Critical error processing {endpoint_name}: {e}")
                logging.error(f"Critical error processing {endpoint_name}: {e}")
                endpoints_failed += 1
                endpoints_processed += 1
        
        # Summary report
        print(f"\n! Dynamic Const Export Summary:")
        print(f"  ! Total endpoints discovered: {len(discovered_endpoints)}")
        print(f"  ! Total endpoints processed: {endpoints_processed}")
        print(f"  ! Fresh files skipped: {endpoints_skipped_fresh}")
        print(f"  ! Files updated/created: {endpoints_updated}")
        print(f"  ! Failed endpoints: {endpoints_failed}")
        logging.info(f"Dynamic const export completed: {len(discovered_endpoints)} discovered, {endpoints_processed} processed, {endpoints_skipped_fresh} skipped (fresh), {endpoints_updated} updated, {endpoints_failed} failed")
        
    except Exception as e:
        print(f"! Critical error during dynamic const discovery: {e}")
        logging.error(f"Critical error during dynamic const discovery: {e}")


def export_const_insight_metrics_to_csv():
    """Legacy function maintained for backward compatibility.
    
    This function now calls the comprehensive dynamic export_all_const_definitions_to_csv()
    but still provides individual insight metrics functionality for existing code.
    """
    print("Export Available Insight Metrics (Legacy Mode):")
    print("! Note: This function now uses the dynamic comprehensive const export system")
    print("! For best results, consider using Menu 82: Export All Const Definitions")
    logging.info("Legacy const insight metrics export called - using dynamic comprehensive system")
    
    # Call the comprehensive function which will handle insight metrics along with all others
    export_all_const_definitions_to_csv()
    
    # Provide specific feedback about insight metrics
    import os
    insight_metrics_file = os.path.join('data', 'ConstInsightMetrics.csv')
    if os.path.exists(insight_metrics_file):
        print("! ConstInsightMetrics.csv is available in the dynamic export results")
    else:
        print("! Warning: ConstInsightMetrics.csv was not created during dynamic export")

def get_insight_metrics_by_scope(target_scope):
    """
    Read ConstInsightMetrics.csv and return metrics that support the specified scope.
    
    Args:
        target_scope (str): The scope to filter by (e.g., 'site', 'client', 'device', 'ap')
        
    Returns:
        list: List of metric names that support the target scope
    """
    import csv
    import os
    
    csv_path = os.path.join('data', 'ConstInsightMetrics.csv')
    metrics_for_scope = []
    
    try:
        if not os.path.exists(csv_path):
            logging.warning(f"ConstInsightMetrics.csv not found at {csv_path}")
            return []
            
        with open(csv_path, 'r', encoding='utf-8') as csvfile:
            reader = csv.DictReader(csvfile)
            for row in reader:
                scopes = row.get('scopes', '')
                metric_name = row.get('metric_name', '')
                
                # Skip rows with empty metric names
                if not metric_name:
                    continue
                    
                # Check if target scope is in the scopes list
                if scopes and target_scope in scopes:
                    metrics_for_scope.append(metric_name)
                    
        logging.debug(f"Found {len(metrics_for_scope)} metrics for scope '{target_scope}': {metrics_for_scope}")
        return metrics_for_scope
        
    except Exception as e:
        logging.error(f"Error reading ConstInsightMetrics.csv: {e}")
        return []

# ==============================
# NORMALIZED ORG INSIGHT METRICS FUNCTIONS
# ==============================

def parse_insight_metric_to_normalized_data(metric_data, org_id):
    """
    Parse a single insight metric into normalized data structures.
    
    Args:
        metric_data (dict): Raw insight metric data from API
        org_id (str): Organization ID
        
    Returns:
        dict: Containing 'summary', 'time_series', 'results', 'sites_data' lists
    """
    normalized_data = {
        'summary': [],
        'time_series': [],
        'results': [],
        'sites_data': []
    }
    
    try:
        metric_type = metric_data.get('metric_type', 'unknown')
        
        # Extract summary data
        summary_data = {
            'org_id': org_id,
            'metric_type': metric_type,
            'data_source': metric_data.get('data_source', ''),
            'start_time': metric_data.get('start', ''),
            'end_time': metric_data.get('end', ''),
            'interval_seconds': metric_data.get('interval', ''),
            'limit': metric_data.get('limit', ''),
            'total_sites': metric_data.get('total_sites', ''),
            'page': metric_data.get('page', ''),
            'sle_category': metric_data.get('sle_category', ''),
            'original_metric': metric_data.get('original_metric', ''),
            'roaming': metric_data.get('roaming', ''),
            'total': metric_data.get('total', ''),
            'totalTunnelCount': metric_data.get('totalTunnelCount', ''),
            'total_sites': metric_data.get('total_sites', '')
        }
        
        # Add scalar metrics to summary
        scalar_fields = [
            'ap-health', 'ap-redundancy', 'capacity', 'coverage', 
            'num_active_wan_tunnels', 'num_aps', 'num_auth', 'num_auth_failure', 
            'num_auth_total', 'num_client', 'num_clients', 'num_gateways', 
            'num_mdm_client', 'num_mxedges', 'num_mxtunnels', 'num_nac_clients', 
            'num_switches', 'num_wan_clients', 'num_wired_clients',
            'successful-connect', 'throughput', 'time-to-connect'
        ]
        
        for field in scalar_fields:
            if field in metric_data:
                summary_data[field] = metric_data[field]
        
        normalized_data['summary'].append(summary_data)
        
        # Extract time series data
        rt_field = metric_data.get('rt', '')
        if rt_field and isinstance(rt_field, str) and ',' in rt_field:
            timestamps = rt_field.split(',')
            
            # Process multiple time series fields
            time_series_fields = ['num_clients', 'num_aps', 'num_gateways', 'num_switches', 'num_mxedges', 'num_mxtunnels']
            
            for field in time_series_fields:
                field_data = metric_data.get(field, '')
                if field_data and isinstance(field_data, str) and ',' in field_data:
                    values = field_data.split(',')
                    for i, (timestamp, value) in enumerate(zip(timestamps, values)):
                        if value and value != 'None':
                            time_series_record = {
                                'org_id': org_id,
                                'metric_type': metric_type,
                                'timestamp': timestamp.strip(),
                                'value': value.strip(),
                                'value_type': field,
                                'sequence_order': i
                            }
                            normalized_data['time_series'].append(time_series_record)
        
        # Extract results array data
        results_data = []
        for key, value in metric_data.items():
            if key.startswith('results_') and '_' in key:
                parts = key.split('_', 2)
                if len(parts) >= 3:
                    result_index = parts[1]
                    result_field = parts[2]
                    
                    # Find or create result record
                    existing_result = None
                    for result in results_data:
                        if result['result_index'] == result_index:
                            existing_result = result
                            break
                    
                    if existing_result is None:
                        existing_result = {
                            'org_id': org_id,
                            'metric_type': metric_type,
                            'result_index': int(result_index) if result_index.isdigit() else result_index
                        }
                        results_data.append(existing_result)
                    
                    existing_result[result_field] = value
        
        normalized_data['results'] = results_data
        
        # Extract sites data
        sites_data = metric_data.get('sites_data', [])
        if isinstance(sites_data, list):
            for site_data in sites_data:
                if isinstance(site_data, dict):
                    site_record = {
                        'org_id': org_id,
                        'metric_type': metric_type
                    }
                    site_record.update(site_data)
                    normalized_data['sites_data'].append(site_record)
        
        # Also parse individual sites_data_X fields
        for key, value in metric_data.items():
            if key.startswith('sites_data_') and '_' in key:
                parts = key.split('_', 2)
                if len(parts) >= 3:
                    site_index = parts[2]
                    site_field = parts[3] if len(parts) > 3 else 'value'
                    
                    # Find or create site record
                    existing_site = None
                    for site in normalized_data['sites_data']:
                        if site.get('site_index') == site_index and site.get('metric_type') == metric_type:
                            existing_site = site
                            break
                    
                    if existing_site is None:
                        existing_site = {
                            'org_id': org_id,
                            'metric_type': metric_type,
                            'site_index': site_index
                        }
                        normalized_data['sites_data'].append(existing_site)
                    
                    existing_site[site_field] = value
        
        logging.debug(f"Normalized metric {metric_type}: {len(normalized_data['summary'])} summary, {len(normalized_data['time_series'])} time series, {len(normalized_data['results'])} results, {len(normalized_data['sites_data'])} sites")
        
    except Exception as e:
        logging.error(f"Error parsing insight metric data: {e}")
        logging.debug(f"Failed metric data structure: {metric_data}")
    
    return normalized_data


def export_org_insight_metrics_to_csv():
    """Export organization-wide insight metrics to normalized CSV files."""
    print("Export Organization Insight Metrics (Normalized):")
    logging.info("Starting export of organization insight metrics with normalized structure...")
    
    # First, refresh the available metrics from the API
    print("! Refreshing available insight metrics from Mist API...")
    export_const_insight_metrics_to_csv()
    
    # Get all metrics that support "org" scope
    org_metrics = get_insight_metrics_by_scope("org")
    
    if not org_metrics:
        print("! No metrics found for org scope. Check ConstInsightMetrics.csv file.")
        logging.error("No org-scope metrics found in const insight metrics")
        # Create empty normalized files
        DataExporter.save_data_to_output([], "OrgMetricsSummary.csv")
        DataExporter.save_data_to_output([], "OrgMetricsTimeSeries.csv")
        DataExporter.save_data_to_output([], "OrgMetricsResults.csv")
        DataExporter.save_data_to_output([], "OrgSitesData.csv")
        return
    
    org_id = get_cached_or_prompted_org_id()
    
    # Initialize normalized data collections
    all_summary_data = []
    all_time_series_data = []
    all_results_data = []
    all_sites_data = []
    
    all_insight_data = []
    metrics_retrieved = 0
    metrics_failed = 0
    
    print(f"! Retrieving {len(org_metrics)} different organization insight metrics...")
    print("! Processing each metric individually with proper error handling...")
    
    try:
        # Iterate through each org-scoped metric and retrieve it individually
        for metric in org_metrics:
            try:
                logging.debug(f"Attempting to retrieve org insight metric: {metric}")
                
                # For metrics that analyze sites, use getOrgSitesSle instead of getOrgSle
                if "worst-sites" in metric or metric in ["sites-sle", "sites-sle-filtered"]:
                    # These metrics require site-level SLE data analysis
                    sle_categories = ["wifi", "wan", "wired"]
                    
                    for sle_category in sle_categories:
                        try:
                            response = mistapi.api.v1.orgs.insights.getOrgSitesSle(
                                apisession, 
                                org_id, 
                                sle=sle_category,
                                duration="7d",
                                limit=1000
                            )
                            sites_data = mistapi.get_all(response=response, mist_session=apisession) or []
                            
                            if sites_data:
                                # Create an aggregated insight result from sites data
                                insight_result = {
                                    'metric_type': f"{metric}_{sle_category}",
                                    'org_id': org_id,
                                    'sle_category': sle_category,
                                    'data_source': 'sites_sle_analysis',
                                    'total_sites': len(sites_data),
                                    'sites_data': sites_data,
                                    'original_metric': metric
                                }
                                
                                all_insight_data.append(insight_result)
                                metrics_retrieved += 1
                                logging.debug(f"Successfully retrieved sites data for insight metric: {metric} with SLE: {sle_category} ({len(sites_data)} sites)")
                            else:
                                logging.debug(f"No sites data available for insight metric: {metric} with SLE: {sle_category}")
                        except Exception as sites_error:
                            logging.debug(f"Failed to get sites data for insight metric '{metric}' with SLE '{sle_category}': {sites_error}")
                            continue
                else:
                    # For other metrics, use getOrgSle directly
                    response = mistapi.api.v1.orgs.insights.getOrgSle(
                        apisession, 
                        org_id, 
                        metric,
                        duration="7d"
                    )
                    insight_data = getattr(response, 'data', response) or {}
                    
                    if insight_data:
                        # Add metric type identifier to each data point
                        insight_data['metric_type'] = metric
                        insight_data['org_id'] = org_id
                        all_insight_data.append(insight_data)
                        metrics_retrieved += 1
                        logging.debug(f"Successfully retrieved org insight data for metric: {metric}")
                    else:
                        logging.debug(f"No data available for org metric: {metric}")
                        metrics_failed += 1
                    
            except Exception as metric_error:
                metrics_failed += 1
                logging.debug(f"Failed to get org insight data for metric '{metric}': {metric_error}")
                # Continue with next metric instead of failing entirely
                continue
        
        # Also try getOrgSitesSle for sites summary data 
        try:
            logging.debug("Attempting to retrieve org sites SLE summary")
            response = mistapi.api.v1.orgs.insights.getOrgSitesSle(apisession, org_id, duration="7d", limit=100)
            sites_data = mistapi.get_all(response=response, mist_session=apisession) or []
            if sites_data:
                for item in sites_data:
                    item['metric_type'] = 'org_sites_sle_summary'
                    item['org_id'] = org_id
                    all_insight_data.append(item)
                metrics_retrieved += 1
                logging.debug(f"Successfully retrieved org sites SLE data for {len(sites_data)} sites")
        except Exception as sites_error:
            metrics_failed += 1
            logging.debug(f"Failed to get org sites SLE summary: {sites_error}")
        
        # Report results
        print(f"! Metric retrieval completed: {metrics_retrieved} successful, {metrics_failed} failed")
        logging.info(f"Org insight metrics: {metrics_retrieved} retrieved successfully, {metrics_failed} failed")
        
        if all_insight_data:
            print("! Parsing metrics into normalized data structures...")
            
            # Parse each metric into normalized structures
            for metric_data in all_insight_data:
                normalized = parse_insight_metric_to_normalized_data(metric_data, org_id)
                all_summary_data.extend(normalized['summary'])
                all_time_series_data.extend(normalized['time_series'])
                all_results_data.extend(normalized['results'])
                all_sites_data.extend(normalized['sites_data'])
            
            # Export to separate CSV files
            print("! Exporting to normalized CSV files...")
            
            # Summary data
            processed_summary = escape_multiline_strings_for_csv(all_summary_data)
            DataExporter.save_data_to_output(processed_summary, "OrgMetricsSummary.csv")
            print(f"  !? {len(processed_summary)} summary records -> OrgMetricsSummary.csv")
            
            # Time series data
            processed_time_series = escape_multiline_strings_for_csv(all_time_series_data)
            DataExporter.save_data_to_output(processed_time_series, "OrgMetricsTimeSeries.csv")
            print(f"  !? {len(processed_time_series)} time series records -> OrgMetricsTimeSeries.csv")
            
            # Results data
            processed_results = escape_multiline_strings_for_csv(all_results_data)
            DataExporter.save_data_to_output(processed_results, "OrgMetricsResults.csv")
            print(f"  !? {len(processed_results)} results records -> OrgMetricsResults.csv")
            
            # Sites data
            processed_sites = escape_multiline_strings_for_csv(all_sites_data)
            DataExporter.save_data_to_output(processed_sites, "OrgSitesData.csv")
            print(f"  !? {len(processed_sites)} sites records -> OrgSitesData.csv")
            
            print(f"\n! Successfully exported {metrics_retrieved} organization insight metrics to 4 normalized CSV files")
            logging.info(f"Exported {len(all_insight_data)} org insight data points from {metrics_retrieved} metrics to normalized CSV files")
            
            # Also save a legacy combined file for compatibility
            processed_legacy = flatten_nested_fields_in_list(all_insight_data)
            processed_legacy = escape_multiline_strings_for_csv(processed_legacy)
            DataExporter.save_data_to_output(processed_legacy, "OrgInsightMetrics_Legacy.csv")
            print(f"  !? Legacy format maintained -> OrgInsightMetrics_Legacy.csv")
            
        else:
            print(f"! 0 organization insight metrics exported (no data available)")
            logging.warning("No org insight data available - all metrics failed or returned empty")
            # Create empty normalized files
            DataExporter.save_data_to_output([], "OrgMetricsSummary.csv")
            DataExporter.save_data_to_output([], "OrgMetricsTimeSeries.csv")
            DataExporter.save_data_to_output([], "OrgMetricsResults.csv")
            DataExporter.save_data_to_output([], "OrgSitesData.csv")
            DataExporter.save_data_to_output([], "OrgInsightMetrics_Legacy.csv")
            
    except Exception as e:
        print(f"! Error exporting organization insight metrics: {e}")
        logging.error(f"Failed to export org insight metrics: {e}")
        # Create empty normalized files in case of error
        DataExporter.save_data_to_output([], "OrgMetricsSummary.csv")
        DataExporter.save_data_to_output([], "OrgMetricsTimeSeries.csv")
        DataExporter.save_data_to_output([], "OrgMetricsResults.csv")
        DataExporter.save_data_to_output([], "OrgSitesData.csv")
        DataExporter.save_data_to_output([], "OrgInsightMetrics_Legacy.csv")

def export_org_rogue_clients_to_csv():
    """Export rogue client detections from all sites to OrgRogueClients.csv."""
    logging.info("Starting export of rogue clients from all sites...")
    check_and_generate_csv("SiteList.csv", export_all_sites_to_csv)
    
    all_rogue_clients = []
    
    try:
        # Load sites
        site_list_path = get_csv_file_path("SiteList.csv")
        with open(site_list_path, mode="r", encoding="utf-8") as f:
            sites = list(csv.DictReader(f))
            
        for site in tqdm(sites, desc="Sites", unit="site"):
            site_id = site.get("id")
            site_name = site.get("name", "Unknown Site")
            
            if not site_id:
                continue
                
            try:
                # Get rogue clients for this site
                response = mistapi.api.v1.sites.insights.listSiteRogueClients(
                    apisession, site_id, duration="7d", limit=1000
                )
                clients = mistapi.get_all(response=response, mist_session=apisession)
                
                # Add site context to each client
                for client in clients:
                    client["site_id"] = site_id
                    client["site_name"] = site_name
                    
                all_rogue_clients.extend(clients)
                logging.info(f"! Fetched {len(clients)} rogue clients from site: {site_name}")
                
            except Exception as e:
                logging.warning(f"! Failed to fetch rogue clients from site {site_name}: {e}")
                continue
                
            # Rate limiting
            time.sleep(0.5)
                
    except Exception as e:
        logging.error(f"Failed to process sites for rogue clients: {e}")
        return
        
    # Save rogue clients
    if all_rogue_clients:
        flattened = flatten_nested_fields_in_list(all_rogue_clients)
        sanitized = escape_multiline_strings_for_csv(flattened)
        DataExporter.save_data_to_output(sanitized, "OrgRogueClients")
        logging.info(f"! {len(all_rogue_clients)} rogue clients exported to OrgRogueClients")
        print(f"! {len(all_rogue_clients)} rogue clients exported to OrgRogueClients")
    else:
        logging.info("No rogue clients found across all sites")
        print(" No rogue clients detected across all sites")

def export_org_rogue_aps_to_csv():
    """Export rogue AP detections from all sites to OrgRogueAPs.csv."""
    logging.info("Starting export of rogue APs from all sites...")
    check_and_generate_csv("SiteList.csv", export_all_sites_to_csv)
    
    all_rogue_aps = []
    
    try:
        # Load sites
        site_list_path = get_csv_file_path("SiteList.csv")
        with open(site_list_path, mode="r", encoding="utf-8") as f:
            sites = list(csv.DictReader(f))
            
        for site in tqdm(sites, desc="Sites", unit="site"):
            site_id = site.get("id")
            site_name = site.get("name", "Unknown Site")
            
            if not site_id:
                continue
                
            try:
                # Get rogue APs for this site
                response = mistapi.api.v1.sites.insights.listSiteRogueAPs(
                    apisession, site_id, duration="7d", limit=1000
                )
                aps = mistapi.get_all(response=response, mist_session=apisession)
                
                # Add site context to each AP
                for ap in aps:
                    ap["site_id"] = site_id
                    ap["site_name"] = site_name
                    
                all_rogue_aps.extend(aps)
                logging.info(f"! Fetched {len(aps)} rogue APs from site: {site_name}")
                
            except Exception as e:
                logging.warning(f"! Failed to fetch rogue APs from site {site_name}: {e}")
                continue
                
            # Rate limiting
            time.sleep(0.5)
                
    except Exception as e:
        logging.error(f"Failed to process sites for rogue APs: {e}")
        return
        
    # Save rogue APs
    if all_rogue_aps:
        flattened = flatten_nested_fields_in_list(all_rogue_aps)
        sanitized = escape_multiline_strings_for_csv(flattened)
        DataExporter.save_data_to_output(sanitized, "OrgRogueAPs")
        logging.info(f"! {len(all_rogue_aps)} rogue APs exported to OrgRogueAPs")
        print(f"! {len(all_rogue_aps)} rogue APs exported to OrgRogueAPs")
    else:
        logging.info("No rogue APs found across all sites")
        print(" No rogue APs detected across all sites")

def export_org_licenses_to_csv():
    """Export organization license entitlements to OrgLicenses.csv using the canonical list endpoint.

    Rationale: Per user directive, remove multi-endpoint fallback / probing logic. We use the
    detailed list endpoint only. If it returns zero records we log and still emit an empty file.
    """
    logging.info("Starting export of organization licenses (canonical endpoint)...")
    filename = "OrgLicenses.csv"
    org_id = get_cached_or_prompted_org_id()

    try:
        # Canonical endpoint: listOrgLicenses (paginated). We deliberately do NOT call the summary endpoint.
        list_func = getattr(mistapi.api.v1.orgs.licenses, 'listOrgLicenses', None)
        if list_func is None:
            # Library wrapper absent - this is a version compatibility shim, not an alternate data source.
            logging.debug("listOrgLicenses wrapper not present in mistapi library; performing direct GET /licenses")
            raw_url = f"/api/v1/orgs/{org_id}/licenses"
            response = apisession.mist_get(raw_url)
            raw_items = getattr(response, 'data', response) or []
        else:
            response = list_func(apisession, org_id, limit=1000)
            raw_items = mistapi.get_all(response=response, mist_session=apisession) or []

        if not isinstance(raw_items, list):
            # Defensive normalization: if API ever returns a dict, convert to single-element list.
            logging.debug("License endpoint returned non-list payload; normalizing to list")
            raw_items = [raw_items]

        if not raw_items:
            logging.info("No license records returned from canonical endpoint; writing empty OrgLicenses.csv")
            DataExporter.save_data_to_output([], filename)
            return

        processed = flatten_nested_fields_in_list(raw_items)
        processed = escape_multiline_strings_for_csv(processed)
        DataExporter.save_data_to_output(processed, filename)
        logging.info(f"Exported {len(processed)} license records to {filename}.")
    except Exception as e:
        logging.error(f"Failed to export licenses: {e}")
        # Emit an empty file to keep test harness consistent, then re-raise for visibility
        try:
            DataExporter.save_data_to_output([], filename)
        except Exception:
            pass
        raise

def export_org_psks_to_csv():
    """Export PSK (Pre-Shared Key) information for the organization to OrgPsks.csv."""
    export_org_specific_data(
        api_call=mistapi.api.v1.orgs.psks.listOrgPsks,
        data_type="psks",
        sort_key="name"
    )

def export_org_webhooks_to_csv():
    """Export webhook configuration for the organization to OrgWebhooks.csv."""
    export_org_specific_data(
        api_call=mistapi.api.v1.orgs.webhooks.listOrgWebhooks,
        data_type="webhooks",
        sort_key="name"
    )

def export_org_wlans_to_csv():
    """Export WLAN configuration for the organization to OrgWlans.csv."""
    export_org_specific_data(
        api_call=mistapi.api.v1.orgs.wlans.listOrgWlans,
        data_type="wlans",
        sort_key="ssid"
    )

def export_org_api_tokens_to_csv():
    """Export API token information for the organization to OrgApiTokens.csv."""
    logging.info("Starting export of organization api tokens...")
    
    # Create filename from data_type
    filename = "OrgApiTokens.csv"
    
    fetch_and_display_api_data(
        title="Organization Api Tokens:",
        api_call=mistapi.api.v1.orgs.apitokens.listOrgApiTokens,
        filename=filename,
        sort_key="name"
        # Note: no limit parameter as this function doesn't accept it
    )

def export_org_admins_to_csv():
    """Export administrator information for the organization to OrgAdmins.csv."""
    logging.info("Starting export of organization admins...")
    
    # Create filename from data_type  
    filename = "OrgAdmins.csv"
    
    fetch_and_display_api_data(
        title="Organization Admins:",
        api_call=mistapi.api.v1.orgs.admins.listOrgAdmins,
        filename=filename,
        sort_key="name"
        # Note: no limit parameter as this function doesn't accept it
    )

def export_org_sso_to_csv():
    """Export SSO (Single Sign-On) information for the organization to OrgSso.csv."""
    export_org_specific_data(
        api_call=mistapi.api.v1.orgs.ssos.listOrgSsos,
        data_type="sso",
        sort_key="name"
    )

def export_org_usage_to_csv():
    """Export license usage information for the organization to OrgUsage.csv."""
    logging.info("Starting export of organization license usage...")
    
    fetch_and_display_api_data(
        title="Organization License Usage:",
        api_call=mistapi.api.v1.orgs.licenses.getOrgLicensesBySite,
        filename="OrgUsage",
        sort_key="site_id"
    )
    
    logging.info(" License usage data exported to OrgUsage")
    print(" License usage data exported to OrgUsage")

def export_org_msp_to_csv():
    """Export MSP (Managed Service Provider) information for the organization to OrgMsp.csv."""
    logging.warning(" MSP data is available only at MSP level, not organization level")
    print(" MSP data is available only at MSP level, not organization level")
    print(" To access MSP data, use the Mist API MSP endpoints directly:")
    print("   - GET /api/v1/msps (list MSPs)")
    print("   - GET /api/v1/msps/{msp_id} (get MSP details)")
    print("   - GET /api/v1/msps/{msp_id}/orgs (list organizations under MSP)")
    print("   This organization-level export is not applicable for MSP data.")

def export_org_mx_edges_to_csv():
    """Export MX Edge information for the organization to OrgMxEdges.csv."""
    export_org_specific_data(
        api_call=mistapi.api.v1.orgs.mxedges.listOrgMxEdges,
        data_type="mx edges",
        sort_key="name"
    )

def create_test_sites_from_csv():
    """
    Create 137 test sites from NorthAmericanTestSites.csv in the data directory.
    
    DESTRUCTIVE: Creates new sites in the organization.
    Sites are based on real North American landmarks and locations across 13 countries.
    
    Geographic distribution:
    - United States: 85 sites (monuments, parks, museums, landmarks)
    - Canada: 11 sites (national parks, cities, attractions)
    - Mexico: 10 sites (archaeological sites, colonial cities, resorts)
    - Costa Rica: 4 sites (volcanoes, national parks, cloud forests)
    - Guatemala: 4 sites (Mayan ruins, colonial cities, lakes)
    - Panama: 4 sites (canal, islands, highlands)
    - Bahamas: 3 sites (Nassau, resorts, marine attractions)
    - Belize: 3 sites (barrier reef, islands, diving sites)
    - Cuba: 3 sites (Havana, beaches, colonial towns)
    - Honduras: 3 sites (Bay Islands, Mayan ruins, national parks)
    - Jamaica: 3 sites (Kingston, resorts, waterfalls)
    - Dominican Republic: 3 sites (colonial zone, resorts, beaches)
    - Haiti: 2 sites (capital, fortress UNESCO site)
    
    CSV columns:
    - name: Site name (required, no spaces)
    - address: Full street address
    - country_code: Two-letter ISO country code
    - lat: Latitude coordinate
    - lng: Longitude coordinate
    - timezone: IANA timezone string
    - notes: Description of the location
    
    SECURITY: Requires explicit 'CREATE' confirmation before execution.
    """
    logging.debug("ENTRY: create_test_sites_from_csv()")
    print("\n========================================")
    print(" DESTRUCTIVE OPERATION WARNING")
    print("========================================")
    print(" This will CREATE 137 new test sites in your organization")
    print(" Sites span 13 North American countries:")
    print(" US, Canada, Mexico, Guatemala, Costa Rica, Panama,")
    print(" Honduras, Belize, Bahamas, Cuba, Jamaica,")
    print(" Dominican Republic, and Haiti")
    print(" Each site includes address, coordinates, and timezone")
    print("========================================\n")
    
    # Safety confirmation
    confirmation = safe_input(
        "Type 'CREATE' (uppercase) to proceed with site creation: ",
        context="site creation confirmation"
    )
    
    if confirmation != "CREATE":
        print(" Site creation cancelled - confirmation phrase not matched")
        logging.info("Site creation cancelled by user - did not provide 'CREATE' confirmation")
        logging.debug("EXIT: create_test_sites_from_csv - cancelled")
        return
    
    logging.info("Starting creation of test sites from CSV...")
    
    # Get organization ID
    org_id = get_cached_or_prompted_org_id()
    if not org_id:
        logging.error("No organization ID provided - cannot create sites")
        print(" ERROR: No organization ID provided")
        logging.debug("EXIT: create_test_sites_from_csv - no org_id")
        return
    
    # Load CSV file
    csv_file_path = get_csv_file_path("NorthAmericanTestSites.csv")
    
    if not os.path.exists(csv_file_path):
        logging.error(f"CSV file not found: {csv_file_path}")
        print(f" ERROR: CSV file not found: {csv_file_path}")
        logging.debug("EXIT: create_test_sites_from_csv - file not found")
        return
    
    try:
        with open(csv_file_path, mode="r", encoding="utf-8") as csv_file:
            sites_data = list(csv.DictReader(csv_file))
        
        logging.info(f"Loaded {len(sites_data)} sites from CSV file")
        print(f"\n Loaded {len(sites_data)} sites from CSV file")
        
    except Exception as read_error:
        logging.error(f"Failed to read CSV file: {read_error}")
        print(f" ERROR: Failed to read CSV file: {read_error}")
        logging.debug("EXIT: create_test_sites_from_csv - read error")
        return
    
    # Create sites using the Mist API
    created_sites = []
    failed_sites = []
    
    print(f"\n Creating sites in organization {org_id}...")
    print(" This may take a few minutes...\n")
    
    for index, site_data in enumerate(sites_data, start=1):
        site_name = site_data.get("name", "").strip()
        
        if not site_name:
            logging.warning(f"Skipping row {index} - no site name provided")
            failed_sites.append({"row": index, "name": "MISSING", "error": "No site name"})
            continue
        
        # Build site creation payload
        site_payload = {
            "name": site_name
        }
        
        # Add optional fields if present
        if site_data.get("address"):
            site_payload["address"] = site_data["address"].strip()
        
        if site_data.get("country_code"):
            site_payload["country_code"] = site_data["country_code"].strip()
        
        # Add lat/lng if both are present
        lat_str = site_data.get("lat", "").strip()
        lng_str = site_data.get("lng", "").strip()
        if lat_str and lng_str:
            try:
                site_payload["latlng"] = {
                    "lat": float(lat_str),
                    "lng": float(lng_str)
                }
            except ValueError as coord_error:
                logging.warning(f"Invalid coordinates for {site_name}: {coord_error}")
        
        if site_data.get("timezone"):
            site_payload["timezone"] = site_data["timezone"].strip()
        
        if site_data.get("notes"):
            site_payload["notes"] = site_data["notes"].strip()
        
        # Create the site via API
        try:
            logging.debug(f"Creating site {index}/{len(sites_data)}: {site_name}")
            response = mistapi.api.v1.orgs.sites.createOrgSite(
                apisession,
                org_id,
                body=site_payload
            )
            
            # Check response
            if hasattr(response, 'data') and response.data:
                created_site_id = response.data.get('id', 'unknown')
                created_sites.append({
                    "name": site_name,
                    "id": created_site_id,
                    "row": index
                })
                print(f" [{index}/{len(sites_data)}] Created: {site_name} (ID: {created_site_id})")
                logging.info(f"Successfully created site: {site_name} (ID: {created_site_id})")
            else:
                failed_sites.append({
                    "row": index,
                    "name": site_name,
                    "error": "No data in response"
                })
                logging.warning(f"Site creation returned no data: {site_name}")
                print(f" [{index}/{len(sites_data)}] FAILED: {site_name} - No data returned")
                
        except Exception as create_error:
            error_message = str(create_error)
            failed_sites.append({
                "row": index,
                "name": site_name,
                "error": error_message
            })
            logging.error(f"Failed to create site {site_name}: {create_error}")
            print(f" [{index}/{len(sites_data)}] FAILED: {site_name} - {error_message}")
        
        # Rate limiting - small delay between creates
        time.sleep(0.5)
    
    # Summary report
    print("\n========================================")
    print(" SITE CREATION SUMMARY")
    print("========================================")
    print(f" Total sites in CSV: {len(sites_data)}")
    print(f" Successfully created: {len(created_sites)}")
    print(f" Failed: {len(failed_sites)}")
    print("========================================\n")
    
    logging.info(f"Site creation complete: {len(created_sites)} created, {len(failed_sites)} failed")
    
    # Export results to CSV
    if created_sites:
        output_filename = "CreatedTestSites.csv"
        DataExporter.save_data_to_output(created_sites, output_filename)
        print(f" Created sites exported to {output_filename}")
        logging.info(f"Created sites list exported to {output_filename}")
    
    if failed_sites:
        failure_filename = "FailedTestSites.csv"
        DataExporter.save_data_to_output(failed_sites, failure_filename)
        print(f" Failed sites exported to {failure_filename}")
        logging.warning(f"Failed sites list exported to {failure_filename}")
    
    logging.debug("EXIT: create_test_sites_from_csv - complete")

def create_country_rf_templates_and_assign():
    """
    Menu 108: Create Country-Specific RF Templates and Assign to Sites (DESTRUCTIVE)
    
    This function automates RF template deployment by country:
    1. Scans all organization sites to identify unique country codes
    2. Creates one RF template per country with default/auto settings
    3. Assigns each site to its corresponding country RF template
    
    RF Template Configuration:
    - name: "RF-{country_code}" (e.g., "RF-US", "RF-CA", "RF-MX")
    - country_code: Matches site country code
    - band_24: Auto channels, 20MHz bandwidth, auto power
    - band_5: Auto channels, 40MHz bandwidth, auto power
    - band_6: Auto channels, 80MHz bandwidth, auto power (future-ready for WiFi 6E)
    
    Safety: Requires uppercase 'CREATE' confirmation before execution.
    
    Note: Sites without country_code will be skipped with warning.
    """
    logging.debug("ENTRY: create_country_rf_templates_and_assign")
    
    print("\n" + "=" * 70)
    print(" Menu 108: Create Country-Specific RF Templates and Assign")
    print("=" * 70)
    
    # Initialize API session if needed
    if not apisession:
        logging.error("API session not initialized")
        print(" ERROR: Mist API session not initialized")
        return
    
    org_id = get_cached_or_prompted_org_id()
    if not org_id:
        logging.warning("No org_id provided - operation cancelled")
        print(" No organization ID provided. Exiting.")
        return
    
    # Step 1: Fetch all sites to identify unique countries
    print("\n  Step 1: Scanning organization sites for unique country codes...")
    logging.info(f"Fetching all sites for org {org_id} to identify countries")
    
    try:
        sites_response = mistapi.api.v1.orgs.sites.listOrgSites(
            apisession,
            org_id,
            limit=DEFAULT_API_PAGE_LIMIT
        )
        sites = mistapi.get_all(response=sites_response, mist_session=apisession)
        
        if not sites:
            print(" No sites found in organization.")
            logging.warning("No sites found in organization")
            return
        
        print(f" Found {len(sites)} sites in organization")
        logging.info(f"Found {len(sites)} total sites")
        
    except Exception as error:
        logging.error(f"Failed to fetch sites: {error}")
        print(f" ERROR: Failed to fetch sites - {error}")
        return
    
    # Step 2: Extract unique country codes
    country_codes = set()
    sites_by_country = {}
    sites_without_country = []
    
    for site in sites:
        country_code = site.get("country_code", "").strip().upper()
        site_id = site.get("id")
        site_name = site.get("name", "Unknown")
        
        if country_code:
            country_codes.add(country_code)
            if country_code not in sites_by_country:
                sites_by_country[country_code] = []
            sites_by_country[country_code].append({
                "id": site_id,
                "name": site_name
            })
        else:
            sites_without_country.append({
                "id": site_id,
                "name": site_name
            })
    
    if not country_codes:
        print(" WARNING: No sites have country codes assigned.")
        print(" Please assign country codes to sites before running this operation.")
        logging.warning("No sites with country codes found")
        return
    
    print(f"\n  Found {len(country_codes)} unique countries:")
    for country in sorted(country_codes):
        site_count = len(sites_by_country[country])
        print(f"   - {country}: {site_count} sites")
        logging.info(f"Country {country} has {site_count} sites")
    
    if sites_without_country:
        print(f"\n  WARNING: {len(sites_without_country)} sites have no country code and will be skipped:")
        for site_info in sites_without_country[:5]:  # Show first 5
            print(f"   - {site_info['name']}")
        if len(sites_without_country) > 5:
            print(f"   ... and {len(sites_without_country) - 5} more")
        logging.warning(f"{len(sites_without_country)} sites without country codes will be skipped")
    
    # Step 3: Check for existing RF templates with same naming pattern
    print("\n  Step 2: Checking for existing RF templates...")
    logging.info("Checking for existing RF templates")
    
    try:
        templates_response = mistapi.api.v1.orgs.rftemplates.listOrgRfTemplates(
            apisession,
            org_id,
            limit=DEFAULT_API_PAGE_LIMIT
        )
        existing_templates = mistapi.get_all(response=templates_response, mist_session=apisession) or []
        
        existing_template_names = {template.get("name"): template.get("id") for template in existing_templates}
        logging.info(f"Found {len(existing_templates)} existing RF templates")
        
    except Exception as error:
        logging.error(f"Failed to fetch existing RF templates: {error}")
        print(f" ERROR: Failed to fetch existing RF templates - {error}")
        return
    
    # Identify which templates need to be created vs updated
    templates_to_create = []
    templates_to_update = []
    
    for country in sorted(country_codes):
        template_name = f"RF-{country}"
        if template_name in existing_template_names:
            templates_to_update.append({
                "country": country,
                "name": template_name,
                "id": existing_template_names[template_name]
            })
        else:
            templates_to_create.append({
                "country": country,
                "name": template_name
            })
    
    # Prompt user about handling existing templates
    update_mode = "skip"  # Default: skip existing templates
    
    if templates_to_update:
        print(f"\n  Found {len(templates_to_update)} existing RF templates:")
        for template_info in templates_to_update[:5]:
            print(f"   - {template_info['name']} (ID: {template_info['id']})")
        if len(templates_to_update) > 5:
            print(f"   ... and {len(templates_to_update) - 5} more")
        
        print("\n  How should existing templates be handled?")
        print("   1. SKIP - Keep existing templates as-is (recommended)")
        print("   2. UPDATE - Update existing templates with new settings (DESTRUCTIVE)")
        
        while True:
            choice = safe_input("\n  Enter choice (1 or 2): ", context="template_update_mode").strip()
            if choice == "1":
                update_mode = "skip"
                print("  Using existing templates without changes.")
                break
            elif choice == "2":
                update_mode = "update"
                print("  Will update existing templates with new settings.")
                break
            else:
                print("  Invalid choice. Please enter 1 or 2.")
    
    if templates_to_create:
        print(f"\n  Will create {len(templates_to_create)} new RF templates:")
        for template_info in templates_to_create[:5]:
            print(f"   - {template_info['name']}")
        if len(templates_to_create) > 5:
            print(f"   ... and {len(templates_to_create) - 5} more")
    
    if not templates_to_create and update_mode == "skip":
        print("\n  All country RF templates already exist and will be used as-is.")
    
    # Step 4: Safety confirmation
    print("\n  " + "!" * 66)
    print("  WARNING: DESTRUCTIVE OPERATION")
    print("  " + "!" * 66)
    print("  This will:")
    if templates_to_create:
        print(f"  - CREATE {len(templates_to_create)} new RF templates")
    if update_mode == "update" and templates_to_update:
        print(f"  - UPDATE {len(templates_to_update)} existing RF templates")
    print(f"  - ASSIGN {sum(len(sites_by_country[c]) for c in country_codes)} sites to country templates")
    print("  - OVERRIDE any existing RF template assignments")
    print("  " + "!" * 66)
    
    confirmation = safe_input("\n  Type 'CREATE' to proceed: ", context="create_country_rf_templates")
    
    if confirmation != "CREATE":
        print(" Operation cancelled.")
        logging.info("Operation cancelled by user at confirmation prompt")
        return
    
    # Step 5: Update existing RF templates if requested
    updated_templates = {}
    
    if update_mode == "update" and templates_to_update:
        print(f"\n  Step 3: Updating {len(templates_to_update)} existing RF templates...")
        
        for template_info in templates_to_update:
            country = template_info["country"]
            template_name = template_info["name"]
            template_id = template_info["id"]
            
            # Default RF template configuration (auto/default settings)
            rf_template_payload = {
                "name": template_name,
                "country_code": country,
                "band_24": {
                    "disabled": False,
                    "allow_rrm_disable": "auto",
                    "channels": None,  # Auto channel selection
                    "bandwidth": 20,
                    "power_min": None,
                    "power_max": None,
                    "power": None,  # Auto power
                    "preamble": "short"
                },
                "band_5": {
                    "disabled": False,
                    "channels": None,  # Auto channel selection
                    "bandwidth": 40,
                    "power_min": None,
                    "power_max": None,
                    "power": None,  # Auto power
                    "preamble": "short"
                },
                "band_6": {
                    "disabled": False,
                    "channels": None,  # Auto channel selection
                    "bandwidth": 80,
                    "power_min": None,
                    "power_max": None,
                    "power": None,  # Auto power
                    "preamble": "short"
                },
                "band_24_usage": "auto"
            }
            
            try:
                logging.debug(f"Updating RF template: {template_name} (ID: {template_id}) for country {country}")
                response = mistapi.api.v1.orgs.rftemplates.updateOrgRfTemplate(
                    apisession,
                    org_id,
                    template_id,
                    body=rf_template_payload
                )
                
                if response.status_code == 200:
                    updated_templates[country] = {
                        "id": template_id,
                        "name": template_name
                    }
                    print(f"  Updated: {template_name} (ID: {template_id})")
                    logging.info(f"Successfully updated RF template {template_name} with ID {template_id}")
                else:
                    logging.error(f"Failed to update RF template {template_name}: HTTP {response.status_code}")
                    print(f"  FAILED: {template_name} (HTTP {response.status_code})")
                
                # Rate limiting delay
                time.sleep(0.5)
                
            except Exception as error:
                logging.error(f"Exception updating RF template {template_name}: {error}")
                print(f"  ERROR: {template_name} - {error}")
    
    # Step 6: Create new RF templates
    created_templates = {}
    
    if templates_to_create:
        step_number = 4 if update_mode == "update" and templates_to_update else 3
        print(f"\n  Step {step_number}: Creating {len(templates_to_create)} new RF templates...")
        
        for template_info in templates_to_create:
            country = template_info["country"]
            template_name = template_info["name"]
            
            # Default RF template configuration (auto/default settings)
            rf_template_payload = {
                "name": template_name,
                "country_code": country,
                "band_24": {
                    "disabled": False,
                    "allow_rrm_disable": "auto",
                    "channels": None,  # Auto channel selection
                    "bandwidth": 20,
                    "power_min": None,
                    "power_max": None,
                    "power": None,  # Auto power
                    "preamble": "short"
                },
                "band_5": {
                    "disabled": False,
                    "channels": None,  # Auto channel selection
                    "bandwidth": 40,
                    "power_min": None,
                    "power_max": None,
                    "power": None,  # Auto power
                    "preamble": "short"
                },
                "band_6": {
                    "disabled": False,
                    "channels": None,  # Auto channel selection
                    "bandwidth": 80,
                    "power_min": None,
                    "power_max": None,
                    "power": None,  # Auto power
                    "preamble": "short"
                },
                "band_24_usage": "auto"
            }
            
            try:
                logging.debug(f"Creating RF template: {template_name} for country {country}")
                response = mistapi.api.v1.orgs.rftemplates.createOrgRfTemplate(
                    apisession,
                    org_id,
                    rf_template_payload
                )
                
                if response.status_code == 200:
                    created_template_id = response.data.get("id")
                    created_templates[country] = {
                        "id": created_template_id,
                        "name": template_name
                    }
                    print(f" Created: {template_name} (ID: {created_template_id})")
                    logging.info(f"Successfully created RF template {template_name} with ID {created_template_id}")
                else:
                    logging.error(f"Failed to create RF template {template_name}: HTTP {response.status_code}")
                    print(f" FAILED: {template_name} (HTTP {response.status_code})")
                
                # Rate limiting delay
                time.sleep(0.5)
                
            except Exception as error:
                logging.error(f"Exception creating RF template {template_name}: {error}")
                print(f"  ERROR: {template_name} - {error}")
    
    # Merge all templates (existing/updated + newly created)
    country_template_mapping = {}
    
    # Add updated templates
    if update_mode == "update":
        country_template_mapping.update(updated_templates)
    
    # Add skipped (unchanged) templates
    if update_mode == "skip":
        for template_info in templates_to_update:
            country_template_mapping[template_info["country"]] = {
                "id": template_info["id"],
                "name": template_info["name"]
            }
    
    # Add newly created templates
    country_template_mapping.update(created_templates)
    
    print(f"\n  RF Template Summary:")
    print(f"   Total countries: {len(country_codes)}")
    print(f"   Templates created: {len(created_templates)}")
    if update_mode == "update":
        print(f"   Templates updated: {len(updated_templates)}")
    else:
        print(f"   Templates existing (unchanged): {len(templates_to_update)}")
    print(f"   Templates ready for assignment: {len(country_template_mapping)}")
    
    # Step 7: Assign sites to their country RF templates
    step_number = 5 if update_mode == "update" and templates_to_update else 4
    if templates_to_create:
        step_number += 1
    print(f"\n  Step {step_number}: Assigning sites to country RF templates...")
    
    success_assignments = []
    failed_assignments = []
    
    for country in sorted(country_codes):
        if country not in country_template_mapping:
            logging.warning(f"No template available for country {country} - skipping sites")
            continue
        
        template_id = country_template_mapping[country]["id"]
        template_name = country_template_mapping[country]["name"]
        sites_to_assign = sites_by_country[country]
        
        print(f"\n  Assigning {len(sites_to_assign)} sites to {template_name}...")
        
        for site_info in sites_to_assign:
            site_id = site_info["id"]
            site_name = site_info["name"]
            
            try:
                # Update site with rftemplate_id
                site_update_payload = {
                    "rftemplate_id": template_id
                }
                
                logging.debug(f"Assigning site {site_name} (ID: {site_id}) to RF template {template_name} (ID: {template_id})")
                
                response = mistapi.api.v1.sites.sites.updateSiteInfo(
                    apisession,
                    site_id,
                    body=site_update_payload
                )
                
                if response.status_code == 200:
                    success_assignments.append({
                        "site_name": site_name,
                        "site_id": site_id,
                        "country": country,
                        "template_name": template_name,
                        "template_id": template_id
                    })
                    logging.info(f"Successfully assigned site {site_name} to RF template {template_name}")
                else:
                    failed_assignments.append({
                        "site_name": site_name,
                        "site_id": site_id,
                        "country": country,
                        "template_name": template_name,
                        "error": f"HTTP {response.status_code}"
                    })
                    logging.error(f"Failed to assign site {site_name}: HTTP {response.status_code}")
                
                # Rate limiting delay
                time.sleep(0.3)
                
            except Exception as error:
                failed_assignments.append({
                    "site_name": site_name,
                    "site_id": site_id,
                    "country": country,
                    "template_name": template_name,
                    "error": str(error)
                })
                logging.error(f"Exception assigning site {site_name}: {error}")
    
    # Step 7: Summary and output
    print("\n" + "=" * 70)
    print(" OPERATION COMPLETE")
    print("=" * 70)
    print(f"  RF Templates Created: {len(created_templates)}")
    if update_mode == "update":
        print(f"  RF Templates Updated: {len(updated_templates)}")
    print(f"  Sites Successfully Assigned: {len(success_assignments)}")
    print(f"  Sites Failed: {len(failed_assignments)}")
    print(f"  Sites Skipped (no country): {len(sites_without_country)}")
    
    # Export results to CSV
    if success_assignments:
        success_filename = "SuccessfulRFTemplateAssignments.csv"
        DataExporter.save_data_to_output(success_assignments, success_filename)
        print(f"\n Successful assignments exported to {success_filename}")
        logging.info(f"Successful assignments exported to {success_filename}")
    
    if failed_assignments:
        failure_filename = "FailedRFTemplateAssignments.csv"
        DataExporter.save_data_to_output(failed_assignments, failure_filename)
        print(f" Failed assignments exported to {failure_filename}")
        logging.warning(f"Failed assignments exported to {failure_filename}")
    
    logging.debug("EXIT: create_country_rf_templates_and_assign - complete")

def create_ap_model_device_profiles():
    """
    Scan organization for all AP device models and create a Device Profile for each unique model.
    
    Handles sub-model revisions (AP41US, AP41WW, etc.) as separate profiles.
    All settings are set to inherit/auto for maximum flexibility.
    
    DESTRUCTIVE: Creates new device profiles in the organization.
    """
    logging.debug("ENTER: create_ap_model_device_profiles")
    print("\n" + "=" * 70)
    print(" CREATE AP MODEL DEVICE PROFILES")
    print("=" * 70)
    
    org_id = get_cached_or_prompted_org_id()
    
    # Step 1: Scan all devices for unique AP models
    print("\n  Step 1: Scanning organization for AP device models...")
    logging.info("Scanning organization inventory to identify unique AP models")
    
    try:
        inventory_response = mistapi.api.v1.orgs.inventory.getOrgInventory(
            apisession,
            org_id,
            type="ap",
            limit=DEFAULT_API_PAGE_LIMIT
        )
        all_devices = mistapi.get_all(response=inventory_response, mist_session=apisession) or []
        logging.info(f"Retrieved {len(all_devices)} AP devices from organization inventory")
        
    except Exception as error:
        logging.error(f"Failed to fetch organization inventory: {error}")
        print(f" ERROR: Failed to fetch inventory - {error}")
        return
    
    if not all_devices:
        print(" No AP devices found in organization.")
        logging.info("No AP devices found - exiting")
        return
    
    # Extract unique models (including sub-model variants like AP41US, AP41WW)
    ap_models = set()
    models_without_info = []
    
    for device in all_devices:
        model = device.get("model")
        if model:
            ap_models.add(model)
        else:
            device_name = device.get("name", device.get("mac", "unknown"))
            models_without_info.append(device_name)
    
    if models_without_info:
        print(f"\n  Warning: {len(models_without_info)} devices have no model information:")
        for device_name in models_without_info[:5]:
            print(f"   - {device_name}")
        if len(models_without_info) > 5:
            print(f"   ... and {len(models_without_info) - 5} more")
        logging.warning(f"{len(models_without_info)} devices without model information")
    
    print(f"\n  Found {len(ap_models)} unique AP models:")
    for model in sorted(ap_models):
        print(f"   - {model}")
    
    # Step 2: Check for existing device profiles
    print("\n  Step 2: Checking for existing Device Profiles...")
    logging.info("Checking for existing AP device profiles")
    
    try:
        profiles_response = mistapi.api.v1.orgs.deviceprofiles.listOrgDeviceProfiles(
            apisession,
            org_id,
            type="ap",
            limit=DEFAULT_API_PAGE_LIMIT
        )
        existing_profiles = mistapi.get_all(response=profiles_response, mist_session=apisession) or []
        
        existing_profile_names = {profile.get("name"): profile.get("id") for profile in existing_profiles}
        logging.info(f"Found {len(existing_profiles)} existing AP device profiles")
        
    except Exception as error:
        logging.error(f"Failed to fetch existing device profiles: {error}")
        print(f" ERROR: Failed to fetch existing device profiles - {error}")
        return
    
    # Identify which profiles need to be created
    profiles_to_create = []
    profiles_to_skip = []
    
    for model in sorted(ap_models):
        profile_name = f"AP-{model}"
        if profile_name in existing_profile_names:
            profiles_to_skip.append({
                "model": model,
                "name": profile_name,
                "id": existing_profile_names[profile_name]
            })
        else:
            profiles_to_create.append({
                "model": model,
                "name": profile_name
            })
    
    if profiles_to_skip:
        print(f"\n  Found {len(profiles_to_skip)} existing Device Profiles (will skip):")
        for profile_info in profiles_to_skip[:10]:
            print(f"   - {profile_info['name']} (ID: {profile_info['id']})")
        if len(profiles_to_skip) > 10:
            print(f"   ... and {len(profiles_to_skip) - 10} more")
    
    if profiles_to_create:
        print(f"\n  Will create {len(profiles_to_create)} new Device Profiles:")
        for profile_info in profiles_to_create[:10]:
            print(f"   - {profile_info['name']}")
        if len(profiles_to_create) > 10:
            print(f"   ... and {len(profiles_to_create) - 10} more")
    else:
        print("\n  All AP model Device Profiles already exist.")
        logging.info("All AP model device profiles already exist - nothing to create")
        return
    
    # Step 3: Safety confirmation
    print("\n  " + "!" * 66)
    print("  WARNING: DESTRUCTIVE OPERATION")
    print("  " + "!" * 66)
    print(f"  This will CREATE {len(profiles_to_create)} new Device Profiles")
    print("  All settings will be set to inherit/auto")
    print("  " + "!" * 66)
    
    confirmation = safe_input("\n  Type 'CREATE' to proceed: ", context="create_ap_model_profiles")
    
    if confirmation != "CREATE":
        print(" Operation cancelled.")
        logging.info("Operation cancelled by user at confirmation prompt")
        return
    
    # Step 4: Create new Device Profiles
    created_profiles = []
    failed_profiles = []
    
    print(f"\n  Step 3: Creating {len(profiles_to_create)} new Device Profiles...")
    
    for profile_info in profiles_to_create:
        model = profile_info["model"]
        profile_name = profile_info["name"]
        
        # Minimal device profile payload - all settings inherit/auto
        device_profile_payload = {
            "name": profile_name,
            "type": "ap"
            # All other settings omitted to inherit from site/template/org defaults
        }
        
        try:
            logging.debug(f"Creating Device Profile: {profile_name} for model {model}")
            response = mistapi.api.v1.orgs.deviceprofiles.createOrgDeviceProfile(
                apisession,
                org_id,
                body=device_profile_payload
            )
            
            if response.status_code == 200:
                created_profile_id = response.data.get("id")
                created_profiles.append({
                    "model": model,
                    "name": profile_name,
                    "id": created_profile_id
                })
                print(f"  Created: {profile_name} (ID: {created_profile_id})")
                logging.info(f"Successfully created Device Profile {profile_name} with ID {created_profile_id}")
            else:
                failed_profiles.append({
                    "model": model,
                    "name": profile_name,
                    "error": f"HTTP {response.status_code}"
                })
                logging.error(f"Failed to create Device Profile {profile_name}: HTTP {response.status_code}")
                print(f"  FAILED: {profile_name} (HTTP {response.status_code})")
            
            # Rate limiting delay
            time.sleep(0.5)
            
        except Exception as error:
            failed_profiles.append({
                "model": model,
                "name": profile_name,
                "error": str(error)
            })
            logging.error(f"Exception creating Device Profile {profile_name}: {error}")
            print(f"  ERROR: {profile_name} - {error}")
    
    # Step 5: Summary and output
    print("\n" + "=" * 70)
    print(" OPERATION COMPLETE")
    print("=" * 70)
    print(f"  Device Profiles Created: {len(created_profiles)}")
    print(f"  Device Profiles Failed: {len(failed_profiles)}")
    print(f"  Device Profiles Skipped (existing): {len(profiles_to_skip)}")
    
    # Export results to CSV
    if created_profiles:
        success_filename = "CreatedAPModelDeviceProfiles.csv"
        DataExporter.save_data_to_output(created_profiles, success_filename)
        print(f"\n Created profiles exported to {success_filename}")
        logging.info(f"Created profiles exported to {success_filename}")
    
    if failed_profiles:
        failure_filename = "FailedAPModelDeviceProfiles.csv"
        DataExporter.save_data_to_output(failed_profiles, failure_filename)
        print(f" Failed profiles exported to {failure_filename}")
        logging.warning(f"Failed profiles exported to {failure_filename}")
    
    logging.debug("EXIT: create_ap_model_device_profiles - complete")

def assign_aps_to_matching_device_profiles():
    """
    Assign AP devices to Device Profiles that match their model type.
    
    Scans organization AP inventory and assigns each AP to its corresponding
    Device Profile (AP-{model}) if that profile exists. Skips APs where no
    matching profile exists.
    
    DESTRUCTIVE: Modifies device assignments in the organization.
    """
    logging.debug("ENTER: assign_aps_to_matching_device_profiles")
    print("\n" + "=" * 70)
    print(" ASSIGN APS TO MATCHING DEVICE PROFILES")
    print("=" * 70)
    
    org_id = get_cached_or_prompted_org_id()
    
    # Step 1: Get all AP inventory
    print("\n  Step 1: Fetching AP inventory from organization...")
    logging.info("Fetching AP inventory from organization")
    
    try:
        inventory_response = mistapi.api.v1.orgs.inventory.getOrgInventory(
            apisession,
            org_id,
            type="ap",
            limit=DEFAULT_API_PAGE_LIMIT
        )
        all_aps = mistapi.get_all(response=inventory_response, mist_session=apisession) or []
        logging.info(f"Retrieved {len(all_aps)} APs from organization inventory")
        
    except Exception as error:
        logging.error(f"Failed to fetch organization AP inventory: {error}")
        print(f" ERROR: Failed to fetch AP inventory - {error}")
        return
    
    if not all_aps:
        print(" No APs found in organization inventory.")
        logging.info("No APs found - exiting")
        return
    
    print(f"  Found {len(all_aps)} APs in organization")
    
    # Step 2: Get all existing Device Profiles
    print("\n  Step 2: Fetching existing Device Profiles...")
    logging.info("Fetching existing AP Device Profiles")
    
    try:
        profiles_response = mistapi.api.v1.orgs.deviceprofiles.listOrgDeviceProfiles(
            apisession,
            org_id,
            type="ap",
            limit=DEFAULT_API_PAGE_LIMIT
        )
        existing_profiles = mistapi.get_all(response=profiles_response, mist_session=apisession) or []
        
        # Create mapping: profile name -> profile ID
        profile_map = {}
        for profile in existing_profiles:
            profile_name = profile.get("name")
            profile_id = profile.get("id")
            if profile_name and profile_id:
                profile_map[profile_name] = profile_id
        
        logging.info(f"Found {len(profile_map)} AP Device Profiles")
        
    except Exception as error:
        logging.error(f"Failed to fetch Device Profiles: {error}")
        print(f" ERROR: Failed to fetch Device Profiles - {error}")
        return
    
    if not profile_map:
        print(" No Device Profiles found in organization.")
        logging.info("No Device Profiles found - exiting")
        return
    
    print(f"  Found {len(profile_map)} Device Profiles")
    
    # Step 3: Analyze AP to profile matching
    aps_with_profile = []
    aps_without_profile = []
    aps_without_model = []
    
    for ap in all_aps:
        ap_mac = ap.get("mac", "unknown")
        ap_name = ap.get("name", ap_mac)
        ap_model = ap.get("model")
        
        if not ap_model:
            aps_without_model.append({
                "mac": ap_mac,
                "name": ap_name
            })
            continue
        
        expected_profile_name = f"AP-{ap_model}"
        
        if expected_profile_name in profile_map:
            aps_with_profile.append({
                "mac": ap_mac,
                "name": ap_name,
                "model": ap_model,
                "profile_name": expected_profile_name,
                "profile_id": profile_map[expected_profile_name]
            })
        else:
            aps_without_profile.append({
                "mac": ap_mac,
                "name": ap_name,
                "model": ap_model,
                "expected_profile": expected_profile_name
            })
    
    print(f"\n  Analysis:")
    print(f"   APs with matching profiles: {len(aps_with_profile)}")
    print(f"   APs without matching profiles: {len(aps_without_profile)}")
    print(f"   APs without model info: {len(aps_without_model)}")
    
    if aps_without_profile:
        print(f"\n  APs without matching profiles (first 10):")
        for ap_info in aps_without_profile[:10]:
            print(f"   - {ap_info['name']} ({ap_info['model']}) - needs {ap_info['expected_profile']}")
        if len(aps_without_profile) > 10:
            print(f"   ... and {len(aps_without_profile) - 10} more")
    
    if not aps_with_profile:
        print("\n  No APs have matching Device Profiles to assign.")
        logging.info("No APs have matching profiles - exiting")
        return
    
    # Step 4: Safety confirmation
    print("\n  " + "!" * 66)
    print("  WARNING: DESTRUCTIVE OPERATION")
    print("  " + "!" * 66)
    print(f"  This will ASSIGN {len(aps_with_profile)} APs to their matching Device Profiles")
    print(f"  APs without matching profiles will be SKIPPED: {len(aps_without_profile)}")
    print("  " + "!" * 66)
    
    confirmation = safe_input("\n  Type 'ASSIGN' to proceed: ", context="assign_aps_to_profiles")
    
    if confirmation != "ASSIGN":
        print(" Operation cancelled.")
        logging.info("Operation cancelled by user at confirmation prompt")
        return
    
    # Step 5: Assign APs to Device Profiles
    print(f"\n  Step 3: Assigning {len(aps_with_profile)} APs to Device Profiles...")
    
    successful_assignments = []
    failed_assignments = []
    
    for ap_info in aps_with_profile:
        ap_mac = ap_info["mac"]
        ap_name = ap_info["name"]
        profile_id = ap_info["profile_id"]
        profile_name = ap_info["profile_name"]
        
        try:
            # Assign device to profile using the assign endpoint
            logging.debug(f"Assigning AP {ap_name} (MAC: {ap_mac}) to Device Profile {profile_name} (ID: {profile_id})")
            
            response = mistapi.api.v1.orgs.deviceprofiles.assignOrgDeviceProfile(
                apisession,
                org_id,
                profile_id,
                body={"macs": [ap_mac]}
            )
            
            if response.status_code == 200:
                successful_assignments.append({
                    "mac": ap_mac,
                    "name": ap_name,
                    "model": ap_info["model"],
                    "profile_name": profile_name,
                    "profile_id": profile_id
                })
                logging.info(f"Successfully assigned AP {ap_name} to Device Profile {profile_name}")
            else:
                failed_assignments.append({
                    "mac": ap_mac,
                    "name": ap_name,
                    "model": ap_info["model"],
                    "profile_name": profile_name,
                    "error": f"HTTP {response.status_code}"
                })
                logging.error(f"Failed to assign AP {ap_name}: HTTP {response.status_code}")
            
            # Rate limiting delay
            time.sleep(0.3)
            
        except Exception as error:
            failed_assignments.append({
                "mac": ap_mac,
                "name": ap_name,
                "model": ap_info["model"],
                "profile_name": profile_name,
                "error": str(error)
            })
            logging.error(f"Exception assigning AP {ap_name}: {error}")
    
    # Step 6: Summary and output
    print("\n" + "=" * 70)
    print(" OPERATION COMPLETE")
    print("=" * 70)
    print(f"  APs Successfully Assigned: {len(successful_assignments)}")
    print(f"  APs Failed: {len(failed_assignments)}")
    print(f"  APs Skipped (no matching profile): {len(aps_without_profile)}")
    print(f"  APs Skipped (no model info): {len(aps_without_model)}")
    
    # Export results to CSV
    if successful_assignments:
        success_filename = "SuccessfulAPProfileAssignments.csv"
        DataExporter.save_data_to_output(successful_assignments, success_filename)
        print(f"\n Successful assignments exported to {success_filename}")
        logging.info(f"Successful assignments exported to {success_filename}")
    
    if failed_assignments:
        failure_filename = "FailedAPProfileAssignments.csv"
        DataExporter.save_data_to_output(failed_assignments, failure_filename)
        print(f" Failed assignments exported to {failure_filename}")
        logging.warning(f"Failed assignments exported to {failure_filename}")
    
    if aps_without_profile:
        skipped_filename = "SkippedAPsNoMatchingProfile.csv"
        DataExporter.save_data_to_output(aps_without_profile, skipped_filename)
        print(f" Skipped APs (no profile) exported to {skipped_filename}")
        logging.info(f"Skipped APs exported to {skipped_filename}")
    
    logging.debug("EXIT: assign_aps_to_matching_device_profiles - complete")

def export_org_network_templates_to_csv():
    """Export network template information for the organization to OrgNetworkTemplates.csv."""
    export_org_specific_data(
        api_call=mistapi.api.v1.orgs.networktemplates.listOrgNetworkTemplates,
        data_type="network templates",
        sort_key="name"
    )

def export_org_rf_templates_to_csv():
    """Export RF template information for the organization to OrgRfTemplates.csv."""
    export_org_specific_data(
        api_call=mistapi.api.v1.orgs.rftemplates.listOrgRfTemplates,
        data_type="rf templates",
        sort_key="name"
    )

def export_org_ap_templates_to_csv():
    """Export AP device profiles (templates) to OrgApTemplates.csv via canonical filtered endpoint.

    Single call only (type='ap'). If zero returned we write an empty file without secondary probing.
    """
    print("Export Organization AP Templates:")
    logging.info("Starting export of organization AP templates (canonical deviceprofiles type=ap)...")
    org_id = get_cached_or_prompted_org_id()
    filename = "OrgApTemplates.csv"
    try:
        response = mistapi.api.v1.orgs.deviceprofiles.listOrgDeviceProfiles(apisession, org_id, type="ap", limit=1000)
        ap_profiles = mistapi.get_all(response=response, mist_session=apisession) or []
        if not ap_profiles:
            print("! 0 AP templates exported to OrgApTemplates.csv (no templates found)")
            logging.info("No AP templates returned from canonical endpoint; writing empty OrgApTemplates.csv")
            DataExporter.save_data_to_output([], filename)
            return
        processed = flatten_nested_fields_in_list(ap_profiles)
        processed = escape_multiline_strings_for_csv(processed)
        DataExporter.save_data_to_output(processed, filename)
        print(f"! {len(processed)} AP templates exported to {filename}")
        logging.info(f"Exported {len(processed)} AP templates to {filename}.")
    except Exception as e:
        logging.error(f"Failed to export AP templates: {e}")
        try:
            DataExporter.save_data_to_output([], filename)
        except Exception:
            pass
        raise

def export_org_switch_templates_to_csv():
    """Export switch device profiles (templates) to OrgSwitchTemplates.csv via canonical filtered endpoint.

    Single call only (type='switch'). If zero returned we emit an empty file without retries.
    """
    print("Export Organization Switch Templates:")
    logging.info("Starting export of organization switch templates (canonical networktemplates)...")
    org_id = get_cached_or_prompted_org_id()
    filename = "OrgSwitchTemplates.csv"
    try:
        response = mistapi.api.v1.orgs.networktemplates.listOrgNetworkTemplates(apisession, org_id, limit=1000)
        switch_profiles = mistapi.get_all(response=response, mist_session=apisession) or []
        if not switch_profiles:
            print("! 0 switch templates exported to OrgSwitchTemplates.csv (no templates found)")
            logging.info("No switch templates returned from canonical endpoint; writing empty OrgSwitchTemplates.csv")
            DataExporter.save_data_to_output([], filename)
            return
        processed = flatten_nested_fields_in_list(switch_profiles)
        processed = escape_multiline_strings_for_csv(processed)
        DataExporter.save_data_to_output(processed, filename)
        print(f"! {len(processed)} switch templates exported to {filename}")
        logging.info(f"Exported {len(processed)} switch templates to {filename}.")
    except Exception as e:
        logging.error(f"Failed to export switch templates: {e}")
        try:
            DataExporter.save_data_to_output([], filename)
        except Exception:
            pass
        raise

def export_site_wlans_to_csv():
    """Export WLAN configuration for a specific site to SiteWlans.csv."""
    export_site_specific_data(
        api_call=mistapi.api.v1.sites.wlans.listSiteWlans,
        data_type="wlans",
        sort_key="ssid"
    )

def export_site_beacons_to_csv():
    """Export beacon information for a specific site to SiteBeacons.csv."""
    export_site_specific_data(
        api_call=mistapi.api.v1.sites.beacons.listSiteBeacons,
        data_type="beacons",
        sort_key="name"
    )

def export_site_maps_to_csv():
    """Export map information for a specific site to SiteMaps.csv."""
    export_site_specific_data(
        api_call=mistapi.api.v1.sites.maps.listSiteMaps,
        data_type="maps",
        sort_key="name"
    )

def export_site_zones_to_csv():
    """Export zone information for a specific site to SiteZones.csv."""
    export_site_specific_data(
        api_call=mistapi.api.v1.sites.zones.listSiteZones,
        data_type="zones",
        sort_key="name"
    )

def export_site_insights_to_csv():
    """Export SLE (Service Level Experience) metrics insights for a specific site to SiteInsights.csv."""
    export_site_specific_data(
        api_call=mistapi.api.v1.sites.sle.listSiteSlesMetrics,
        data_type="sle_metrics_insights",
        sort_key="name"
    )

def continuous_data_collection_loop():
    """
    Continuously collect core organizational data as specified in script needs.txt.
    This runs the 5 key API calls in a loop with proper rate limiting:
    1. Site list
    2. Organization inventory
    3. Organization device stats
    4. Organization device port stats
    5. VPN peer path stats
    """
    logging.info("Starting continuous data collection loop...")
    print(" Starting continuous data collection loop...")
    print("   This will collect core organizational data every 5 seconds")
    print("   Press CTRL+C to stop or create 'stop_loop.txt' file")
    
    loop_count = 0
    
    try:
        while True:
            loop_count += 1
            print(f"\n  Loop iteration {loop_count} - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            
            # Check for stop file
            if os.path.exists("stop_loop.txt"):
                print(" Stop file detected. Ending continuous loop.")
                os.remove("stop_loop.txt")
                break
            
            try:
                # 1. Site list
                print("  Collecting site list...")
                export_all_sites_to_csv()
                time.sleep(0.75)  # Rate limiting
                
                # 2. Organization inventory
                print("  Collecting organization inventory...")
                export_device_inventory_to_csv()
                time.sleep(0.75)
                
                # 3. Organization device stats
                print("  Collecting organization device stats...")
                export_device_stats_to_csv()
                time.sleep(0.75)
                
                # 4. Organization device port stats
                print("  Collecting organization device port stats...")
                export_device_port_stats_to_csv()
                time.sleep(0.75)
                
                # 5. VPN peer path stats
                print("  Collecting VPN peer path stats...")
                export_vpn_peer_stats_to_csv()
                time.sleep(0.75)
                
                print(f"  Loop {loop_count} completed successfully")
                
            except KeyboardInterrupt:
                print("\n   Keyboard interrupt detected. Stopping loop...")
                break
            except Exception as e:
                logging.error(f"Error in continuous loop iteration {loop_count}: {e}")
                print(f"  Error in loop {loop_count}: {e}")
                print("  Continuing to next iteration...")
                time.sleep(5)  # Wait longer on error
                
    except KeyboardInterrupt:
        print("\n  Continuous data collection loop stopped by user.")
    except Exception as e:
        logging.error(f"Fatal error in continuous loop: {e}")
        print(f"! Fatal error in continuous loop: {e}")
    
    print(" Continuous data collection loop ended.")

# === NAC (Network Access Control) Functions ===

def export_org_nac_clients_to_csv():
    """Export NAC client information for the organization to OrgNacClients.csv."""
    export_org_specific_data(
        api_call=mistapi.api.v1.orgs.nac_clients.searchOrgNacClients,
        data_type="nac clients",
        sort_key="mac"
    )

def export_org_nac_tags_to_csv():
    """Export NAC tags/policies for the organization to OrgNacTags.csv."""
    export_org_specific_data(
        api_call=mistapi.api.v1.orgs.nactags.listOrgNacTags,
        data_type="nac tags",
        sort_key="name"
    )

def export_org_nac_portals_to_csv():
    """Export NAC portals configuration for the organization to OrgNacPortals.csv."""
    export_org_specific_data(
        api_call=mistapi.api.v1.orgs.nacportals.listOrgNacPortals,
        data_type="nac portals",
        sort_key="name"
    )

def export_org_nac_rules_to_csv():
    """Export NAC rules/policies for the organization to OrgNacRules.csv."""
    export_org_specific_data(
        api_call=mistapi.api.v1.orgs.nacrules.listOrgNacRules,
        data_type="nac rules",
        sort_key="name"
    )

def export_org_nac_events_to_csv():
    """Export NAC events for the organization to OrgNacEvents.csv."""
    hours = get_dynamic_lookback_hours(24, 1)
    log_dynamic_lookback("org NAC events export", hours)
    export_org_specific_data(
        api_call=mistapi.api.v1.orgs.nac_clients.searchOrgNacClientEvents,
        data_type="nac events",
        sort_key="timestamp",
        duration=f"{hours}h"
    )

# === Statistics & Analytics Functions ===

def export_org_assets_to_csv():
    """Export asset tracking statistics for the organization to OrgAssets.csv."""
    export_org_specific_data(
        api_call=mistapi.api.v1.orgs.stats.searchOrgAssets,
        data_type="assets",
        sort_key="name"
    )

def export_org_bgp_peers_to_csv():
    """Export BGP peer statistics for the organization to OrgBgpPeers.csv."""
    export_org_specific_data(
        api_call=mistapi.api.v1.orgs.stats.searchOrgBgpPeers,
        data_type="bgp peers",
        sort_key="peer_ip"
    )

def export_org_tunnel_stats_to_csv():
    """Export tunnel statistics for the organization to OrgTunnelStats.csv."""
    export_org_specific_data(
        api_call=mistapi.api.v1.orgs.stats.searchOrgTunnels,
        data_type="tunnel stats",
        sort_key="name"
    )

def export_org_site_stats_to_csv():
    """Export site-level statistics for the organization to OrgSiteStats.csv."""
    export_org_specific_data(
        api_call=mistapi.api.v1.orgs.stats.listOrgSitesStats,
        data_type="site stats",
        sort_key="name"
    )

def export_org_mxedge_stats_to_csv():
    """Export MX Edge statistics for the organization to OrgMxEdgeStats.csv."""
    export_org_specific_data(
        api_call=mistapi.api.v1.orgs.stats.listOrgMxEdgesStats,
        data_type="mx edge stats",
        sort_key="name"
    )

# === Configuration & Management Functions ===

def export_org_alarm_templates_to_csv():
    """Export alarm template configurations for the organization to OrgAlarmTemplates.csv."""
    export_org_specific_data(
        api_call=mistapi.api.v1.orgs.alarmtemplates.listOrgAlarmTemplates,
        data_type="alarm templates",
        sort_key="name"
    )

def export_org_security_intel_profiles_to_csv():
    """Export security intelligence profiles for the organization to OrgSecurityIntelProfiles.csv."""
    export_org_specific_data(
        api_call=mistapi.api.v1.orgs.secintelprofiles.listOrgSecIntelProfiles,
        data_type="security intel profiles",
        sort_key="name"
    )

def export_org_invites_to_csv():
    """Export pending admin invitations for the organization to OrgInvites.csv."""
    export_org_specific_data(
        api_call=mistapi.api.v1.orgs.invites.listOrgInvites,
        data_type="invites",
        sort_key="email"
    )

def export_org_events_to_csv():
    """Export general organization events to OrgEvents.csv."""
    hours = get_dynamic_lookback_hours(24, 1)
    log_dynamic_lookback("org events export", hours)
    export_org_specific_data(
        api_call=mistapi.api.v1.orgs.events.searchOrgEvents,
        data_type="events",
        sort_key="timestamp",
        duration=f"{hours}h"
    )

# === Site-Level Functions ===

def export_site_system_events_to_csv():
    """Export system events for a specific site to SiteSystemEvents.csv."""
    hours = get_dynamic_lookback_hours(24, 1)
    log_dynamic_lookback("site system events export", hours)
    export_site_specific_data(
        api_call=mistapi.api.v1.sites.events.searchSiteSystemEvents,
        data_type="system events",
        sort_key="timestamp",
        duration=f"{hours}h"
    )

def export_site_fast_roam_events_to_csv():
    """Export fast roam events for a specific site to SiteFastRoamEvents.csv."""
    hours = get_dynamic_lookback_hours(24, 1)
    log_dynamic_lookback("site fast roam events export", hours)
    export_site_specific_data(
        api_call=mistapi.api.v1.sites.events.searchSiteFastRoamEvents,
        data_type="fast roam events",
        sort_key="timestamp",
        duration=f"{hours}h"
    )

def interactive_display_site_inventory():
    """
    Prompts the user to select a site and displays its device inventory.
    """
    logging.info("Prompting user to select a site for device inventory view...")
    print("Select a Site to View Device Inventory:")
    site_id = prompt_select_site_id_from_csv()
    if site_id:
        logging.info(f"User selected site_id: {site_id} for inventory display.")
        show_site_device_inventory(site_id)
    else:
        logging.warning("No site selected or invalid input provided for site selection.")

def interactive_display_device_stats(site_id=None, device_id=None):
    """
    Fetches and displays detailed statistics for a specific device (by site_id/device_id if provided, else prompts user).
    """
    logging.info("Prompting user to select a device for detailed statistics view...")
    interactive_fetch_device_data_to_csv(
        fetch_function=mistapi.api.v1.sites.stats.getSiteDeviceStats,
        filename="DeviceStats.csv",
        description="Fetching detailed stats",
        site_id=site_id,
        device_id=device_id
    )
    logging.info("Completed interactive_display_device_stats execution.")

def interactive_display_device_tests():
    """
    Prompts user to select a gateway device and displays its synthetic test stats.
    """
    logging.info("Prompting user to select a gateway device for synthetic test stats view...")
    # Call the interactive_fetch_device_data_to_csv helper with the appropriate Mist API function
    interactive_fetch_device_data_to_csv(
        fetch_function=mistapi.api.v1.sites.devices.getSiteDeviceSyntheticTest,
        filename="DeviceTestResults.csv",
        description="Fetching synthetic test stats",
        device_type="gateway"
    )
    logging.info("Completed interactive_display_device_tests execution.")

def interactive_display_device_config():
    """
    Prompts user to select a device and displays its configuration details.
    """
    logging.info("Prompting user to select a device for configuration details view...")  # Log start
    # Call the interactive_fetch_device_data_to_csv helper with the appropriate Mist API function
    interactive_fetch_device_data_to_csv(
        fetch_function=mistapi.api.v1.sites.devices.getSiteDevice,
        filename="DeviceConfig.csv",
        description="Fetching device configuration"
    )
    logging.info("Completed interactive_display_device_config execution.")  # Log completion

def export_all_devices_to_csv():
    """
    Fetches and exports a list of all devices in the organization to OrgDevices.csv.
    Uses fetch_and_display_api_data to handle API call, CSV writing, and table display.
    """
    logging.info("Starting export of all organization devices...")  # Log start of function
    fetch_and_display_api_data(
        title="Org Devices:",
        api_call=mistapi.api.v1.orgs.devices.listOrgDevices,
        filename="OrgDevices.csv",
        sort_key="type"
    )
    logging.info("Completed export_all_devices_to_csv and wrote results to OrgDevices.csv.")  # Log completion

def fetch_all_site_settings_from_api(apisession, org_id, limit=1000):
    """
    Fetches configuration settings for all sites in the organization.

    Args:
        apisession: The Mist API session object.
        org_id: The organization ID.
        limit: (Unused) Maximum number of sites to fetch per API call.

    Returns:
        List of dictionaries, each containing the settings for a site.
    """
    logging.info("Fetching all site settings...")

    # Use mistapi.get_all to ensure pagination is handled for all sites
    sites = fetch_all_sites_with_limit(org_id)

    all_configs = []
    for site in tqdm(sites, desc="Sites", unit="site"):
        site_id = site.get("id")
        site_name = site.get("name", "Unnamed Site")
        try:
            # Fetch the site settings using the Mist API
            config = mistapi.api.v1.sites.setting.getSiteSetting(apisession, site_id).data
            config["site_id"] = site_id
            config["site_name"] = site_name
            all_configs.append(config)
            logging.info(f"! Fetched config for site: {site_name} (ID: {site_id})")
        except Exception as e:
            logging.warning(f"! Failed to fetch config for {site_name} (ID: {site_id}): {e}")

    logging.info(f"Fetched settings for {len(all_configs)} sites.")
    return all_configs

def export_site_settings_to_csv():
    """
    Fetches and exports configuration settings for all sites in the organization to AllSiteConfigs.csv.
    Adds detailed logging at each step.
    """
    print("Site Configuration Settings:")
    logging.info("Starting export of all site configuration settings...")  # Log start
    org_id = get_cached_or_prompted_org_id()
    logging.debug(f"Using org_id: {org_id} for site settings export.")

    # Fetch all site settings using the helper function
    data = fetch_all_site_settings_from_api(apisession, org_id, limit=1000)
    if data:
        logging.info(f"Fetched settings for {len(data)} sites. Flattening and sanitizing data...")
        # Flatten nested fields for CSV compatibility
        data = flatten_nested_fields_in_list(data)
        # Escape multiline strings for CSV compatibility
        data = escape_multiline_strings_for_csv(data)
        # Write the processed data to a CSV file
        DataExporter.save_data_to_output(data, "AllSiteConfigs.csv")
        print(f"! {len(data)} site configurations exported to AllSiteConfigs.csv")
        logging.info(" Site configs saved to AllSiteConfigs.csv")
    else:
        logging.warning(" No site configs found.")
        print("! No site configurations found.")

def export_gateway_synthetic_tests_to_csv(fast=False):
    """
    Collects and exports synthetic test stats for all gateways in the organization.
    Optimized to use cached inventory data and concurrent processing when fast=True.
    
    Args:
        fast (bool): If True, enables concurrent processing and uses cached inventory data
                    to minimize API calls.
    """
    # DEBUG: Log invocation context early so harness vs direct calls can be distinguished
    logging.debug(f"[DEBUG] export_gateway_synthetic_tests_to_csv invoked with fast={fast}")
    logging.info("[INFO] Collecting synthetic test stats for all gateways in the org...")
    if fast:
        logging.info(" Fast mode enabled: Using cached data and concurrent processing (synthetic tests)")
    
    org_id = get_cached_or_prompted_org_id()
    gateway_devices = get_gateway_devices_with_sites(apisession, org_id, fast=fast)
    all_stats = []

    if not gateway_devices:
        logging.warning("[WARN] No gateway devices found. Exiting export_gateway_synthetic_tests_to_csv.")
        return

    def fetch_synthetic_test_stats_with_retry(device_info, max_retries=None, retry_delay=None, connection_semaphore=None):
        """
        Fetch synthetic test stats for a single gateway device with retry logic and connection pool management.
        
        Args:
            device_info: Tuple of (site_id, device_id, device_name, site_name)
            max_retries: Maximum number of retry attempts (uses env var if None)
            retry_delay: Base delay between retries (uses env var if None)
            connection_semaphore: Semaphore to limit concurrent connections (optional)
        """
        # Use environment variables as defaults if not provided
        if max_retries is None:
            max_retries = FAST_MODE_MAX_RETRIES
        if retry_delay is None:
            retry_delay = FAST_MODE_RETRY_DELAY
            
        site_id, device_id, device_name, site_name = device_info
        
        for attempt in range(max_retries + 1):
            try:
                # Validate inputs before making API calls
                validate_site_id(site_id, "export_gateway_synthetic_tests_to_csv")
                validate_device_id(device_id, "export_gateway_synthetic_tests_to_csv")
                
                # Use semaphore to limit concurrent connections if provided
                if connection_semaphore:
                    with connection_semaphore:
                        stats = mistapi.api.v1.sites.devices.getSiteDeviceSyntheticTest(apisession, site_id, device_id).data
                else:
                    stats = mistapi.api.v1.sites.devices.getSiteDeviceSyntheticTest(apisession, site_id, device_id).data
                
                stats["site_id"] = site_id
                stats["site_name"] = site_name
                stats["device_id"] = device_id
                stats["device_name"] = device_name
                
                if attempt > 0:
                    logging.info(f"! Retry {attempt} successful for device {device_name} at site {site_name}")
                else:
                    logging.info(f"! Collected synthetic test stats for device {device_name} at site {site_name}")
                return stats
                
            except Exception as e:
                if attempt < max_retries:
                    # Fast mode: reduced backoff delay for quicker retries
                    backoff_delay = retry_delay * (FAST_MODE_BACKOFF_MULTIPLIER ** attempt)  # Configurable backoff
                    logging.warning(f"! Attempt {attempt + 1} failed for device {device_id} at site {site_id}: {e}")
                    logging.info(f"! Fast retry in {backoff_delay:.1f}s (attempt {attempt + 2}/{max_retries + 1})")
                    time.sleep(backoff_delay)
                else:
                    logging.error(f"! Final attempt failed for device {device_id} at site {site_id}: {e}")
                    return None
        
        return None

    if fast:
        # Concurrent processing mode with connection-aware threading + summary instrumentation
        start_time = time.time()

        # Define worker function for the connection pool helper
        def fetch_device_stats(device_info, connection_semaphore):
            """Worker function that fetches synthetic test stats for a single device."""
            return fetch_synthetic_test_stats_with_retry(device_info, connection_semaphore=connection_semaphore)

        # Define retry function for failed devices (unchanged logic, clearer logging prefix FAST)
        def retry_failed_devices(failed_devices, connection_semaphore):
            retry_results = []
            still_failed = []
            retry_threads = min(FAST_MODE_RETRY_THREADS, len(failed_devices), max(1, FAST_MODE_MAX_CONCURRENT_CONNECTIONS - 2))
            if retry_threads <= 0:
                logging.warning(" FAST MODE: No available threads for retry; skipping retries")
                return [], failed_devices
            with ThreadPoolExecutor(max_workers=retry_threads) as executor:
                retry_futures = {
                    executor.submit(fetch_synthetic_test_stats_with_retry, device_info, max_retries=FAST_MODE_RETRY_MAX_RETRIES, connection_semaphore=connection_semaphore): device_info
                    for device_info in failed_devices
                }
                for future in tqdm(as_completed(retry_futures), total=len(retry_futures), desc="Retrying Failed", unit="device"):
                    device_info = retry_futures[future]
                    try:
                        result = future.result()
                        if result:
                            retry_results.append(result)
                            logging.info(f" FAST RETRY OK: {device_info[2]}")
                        else:
                            still_failed.append(device_info)
                            logging.error(f" FAST RETRY FAIL: {device_info[2]}")
                    except Exception as e:
                        still_failed.append(device_info)
                        logging.error(f" FAST RETRY EXC: {device_info[2]} -> {e}")
            return retry_results, still_failed

        successful_results, failed_devices = execute_with_connection_pool_management(
            work_items=gateway_devices,
            worker_function=fetch_device_stats,
            batch_description="devices",
            retry_function=retry_failed_devices
        )

        duration = time.time() - start_time
        all_stats.extend(successful_results)
        logging.info(f" FAST MODE SUMMARY (synthetic tests): ok={len(successful_results)} fail={len(failed_devices)} total={len(gateway_devices)} elapsed={duration:.2f}s")
    else:
        # Sequential processing with rate limiting (original behavior)
        smoothed = None
        for device_info in tqdm(gateway_devices, desc="Gateway Devices", unit="device"):
            result = fetch_synthetic_test_stats_with_retry(device_info, max_retries=FAST_MODE_SEQUENTIAL_MAX_RETRIES)
            if result:
                all_stats.append(result)
            
            # Apply rate limiting only in non-fast mode
            smoothed, delay = get_rate_limited_delay(smoothed)
            logging.info(f"[INFO] Sleeping for {delay:.2f}s.")
            time.sleep(delay)

    if all_stats:
        filename = "AllGatewaySyntheticTests.csv"
        flattened = flatten_nested_fields_in_list(all_stats)
        sanitized = escape_multiline_strings_for_csv(flattened)
        DataExporter.save_data_to_output(sanitized, filename)
        print(f"! {len(all_stats)} gateway synthetic test results exported to {filename}")
        logging.info(f"! Synthetic test results saved to {filename} ({len(all_stats)} records).")
        logging.info(f"! API Optimization: Saved {len(gateway_devices)} listSiteDevices calls by using cached inventory")
    else:
        logging.warning(" No synthetic test results found. CSV not created.")
        print("! No synthetic test results found. CSV not created.")

def get_gateway_devices_with_sites(apisession, org_id, fast=False):
    """
    Efficiently fetches all gateway devices with their site information.
    Uses cached data when fast=True to minimize API calls.

    Args:
        apisession: The Mist API session object.
        org_id: The organization ID.
        fast (bool): If True, uses cached CSV data instead of making fresh API calls.

    Returns:
        List of tuples: (site_id, device_id, device_name, site_name) for each gateway device.
    """
    logging.info("[INFO] Fetching gateway devices with site information...")
    
    if fast:
        # Use cached data approach
        try:
            # Ensure required CSV files exist using caching
            check_and_generate_csv("OrgInventory.csv", export_device_inventory_to_csv)
            check_and_generate_csv("SiteList.csv", export_all_sites_to_csv)
            
            # Load inventory from cached CSV
            gateway_devices = []
            inventory_path = get_csv_file_path("OrgInventory.csv")
            with open(inventory_path, mode="r", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                gateways = [row for row in reader if row.get("type") == "gateway" and row.get("site_id") and row.get("id")]
            
            # Load site names from cached CSV
            site_name_lookup = {}
            site_list_path = get_csv_file_path("SiteList.csv")
            with open(site_list_path, mode="r", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                site_name_lookup = {row.get("id"): row.get("name", "Unknown Site") for row in reader}
            
            # Build device list with site names
            for device in gateways:
                site_id = device.get("site_id")
                device_id = device.get("id")
                device_name = device.get("name", "")
                site_name = site_name_lookup.get(site_id, "Unknown Site")
                gateway_devices.append((site_id, device_id, device_name, site_name))
            
            logging.info(f"! Fast mode: Loaded {len(gateway_devices)} gateway devices from cached data")
            return gateway_devices
            
        except Exception as e:
            logging.warning(f"! Fast mode failed, falling back to API calls: {e}")
            # Fall through to API mode
    
    # Original API-based approach
    logging.info("[INFO] Fetching org inventory to find gateway devices...")
    # Fetch the full org inventory (all devices)
    devices = fetch_all_inventory_with_limit(org_id)
    logging.info(f"[INFO] Retrieved {len(devices)} devices from org inventory.")
    
    # Get site names
    site_response = mistapi.api.v1.orgs.sites.listOrgSites(apisession, org_id, limit=1000)
    sites = mistapi.get_all(response=site_response, mist_session=apisession)
    site_name_lookup = {site["id"]: site.get("name", "Unknown Site") for site in sites}
    
    # Filter for gateway devices and build tuples
    gateway_devices = []
    for device in devices:
        if device.get("type") == "gateway" and device.get("site_id") and device.get("id"):
            site_id = device.get("site_id")
            device_id = device.get("id")
            device_name = device.get("name", "")
            site_name = site_name_lookup.get(site_id, "Unknown Site")
            gateway_devices.append((site_id, device_id, device_name, site_name))
    
    logging.info(f"[INFO] Found {len(gateway_devices)} gateway devices across the organization.")
    return gateway_devices

def get_site_ids_with_gateway_devices(apisession, org_id):
    """
    Fetches all sites in the organization that have at least one gateway device.

    Args:
        apisession: The Mist API session object.
        org_id: The organization ID.

    Returns:
        List of site IDs that have at least one gateway device.
    """
    logging.info("[INFO] Fetching org inventory to find sites with gateways...")
    # Fetch the full org inventory (all devices)
    devices = fetch_all_inventory_with_limit(org_id)
    logging.info(f"[INFO] Retrieved {len(devices)} devices from org inventory.")

    # Collect unique site_ids for devices of type 'gateway'
    gateway_sites = {device["site_id"] for device in devices 
                     if device.get("type") == "gateway" 
                     and device.get("site_id") 
                     and str(device.get("site_id")).strip()}
    logging.info(f"[INFO] Found {len(gateway_sites)} sites with at least one gateway.")

    return list(gateway_sites)

def export_gateway_test_results_by_site_to_csv(fast: bool = False):
    """Export all synthetic test results (including speed tests) for all sites with gateways.

    When fast=True:
      * Uses cached inventory + site list CSVs (generates them if missing) to derive site IDs quickly.
      * Processes sites concurrently using the shared connection pool helper.
      * Skips per-site adaptive rate limiting delays (relies on pool throttling instead).

    Args:
        fast (bool): Enable cached data usage + concurrent site processing.
    """
    print("Gateway Synthetic Test Results:")
    logging.info("[INFO] Searching all test results (including speed tests) for sites with gateways...")
    if fast:
        logging.info(" Fast mode enabled: Using cached inventory/site data and concurrent site processing")

    org_id = get_cached_or_prompted_org_id()

    # Fast path: derive site IDs from cached CSVs to avoid full inventory refetch if possible
    site_ids = []
    if fast:
        try:
            # Ensure cached CSVs present
            check_and_generate_csv("OrgInventory.csv", export_device_inventory_to_csv)
            inventory_path = get_csv_file_path("OrgInventory.csv")
            with open(inventory_path, mode="r", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                # Filter out None/empty site_ids and only include gateways
                site_ids = sorted({row.get("site_id") for row in reader 
                                 if row.get("type") == "gateway" and row.get("site_id") and row.get("site_id").strip()})
            logging.info(f"! Fast mode: Loaded {len(site_ids)} site_ids with gateways from cached inventory")
        except Exception as e:
            logging.warning(f"! Fast mode site derivation failed, falling back to API discovery: {e}")
            site_ids = []  # Force fallback

    if not site_ids:
        site_ids = get_site_ids_with_gateway_devices(apisession, org_id)

    if not site_ids:
        logging.warning(" No sites with gateways found.")
        return

    all_results = []

    def fetch_site_tests(site_id, connection_semaphore):
        """Worker to fetch all synthetic test results for one site (with optional semaphore)."""
        try:
            validate_site_id(site_id, "export_gateway_test_results_by_site_to_csv")
            if connection_semaphore:
                with connection_semaphore:
                    response = mistapi.api.v1.sites.synthetic_test.searchSiteSyntheticTest(apisession, site_id)
            else:
                response = mistapi.api.v1.sites.synthetic_test.searchSiteSyntheticTest(apisession, site_id)
            if not hasattr(response, "data"):
                logging.warning(f"! No data attribute in response for site {site_id}")
                return []
            results = response.data.get("results", []) if isinstance(response.data, dict) else []
            for r in results:
                r["site_id"] = site_id
            logging.info(f"[{site_id}] Retrieved {len(results)} test results.")
            return results
        except Exception as e:
            logging.warning(f"! Failed to fetch test results for site {site_id}: {e}")
            return []

    if fast:
        start_time = time.time()
        # Use generic connection pool executor (treat each site as a work item)
        # Reuse execute_with_connection_pool_management pattern by adapting worker signature
        def worker(site_id, connection_semaphore):
            return fetch_site_tests(site_id, connection_semaphore)

        successful_results, failed_sites = execute_with_connection_pool_management(
            work_items=site_ids,
            worker_function=worker,
            batch_description="sites"
        )
        # successful_results is a list of lists (each site's results); flatten
        flattened_results = []
        for site_list in successful_results:
            if isinstance(site_list, list):
                flattened_results.extend(site_list)
        all_results = flattened_results
        duration = time.time() - start_time
        logging.info(f" FAST MODE SUMMARY (site synthetic tests): ok_sites={len(successful_results)} fail_sites={len(failed_sites)} total_sites={len(site_ids)} records={len(all_results)} elapsed={duration:.2f}s")
    else:
        smoothed = None
        for site_id in tqdm(site_ids, desc="Sites", unit="site"):
            results = fetch_site_tests(site_id, connection_semaphore=None)
            if results:
                all_results.extend(results)
            smoothed, delay = get_rate_limited_delay(smoothed)
            time.sleep(delay)

    if all_results:
        filename = "AllGatewayTestResults.csv"
        flattened = flatten_nested_fields_in_list(all_results)
        sanitized = escape_multiline_strings_for_csv(flattened)
        DataExporter.save_data_to_output(sanitized, filename)
        print(f"! {len(all_results)} gateway test results exported to {filename}")
        logging.info(f"! All test results saved to {filename} ({len(all_results)} records).")
        if fast:
            logging.info(f"! API Optimization: Saved site-level repeat lookups by using cached inventory for site derivation")
    else:
        logging.warning(" No test results found. CSV not created.")
        print("! No gateway test results found. CSV not created.")

def export_gateway_device_stats_to_csv_with_freshness_check(fast=False):
    """
    Wrapper function that checks if AllGatewayDeviceStats.csv exists and is fresh before generating it.
    This ensures we don't unnecessarily regenerate data that's already current.
    """
    output_file = "AllGatewayDeviceStats.csv"
    
    # Check if file exists and is fresh
    if check_and_generate_csv(output_file, lambda: export_gateway_device_stats_to_csv(fast=fast)):
        logging.info(f"! {output_file} already exists and is fresh - using cached data")
    else:
        logging.info(f"! {output_file} was generated or refreshed")

def export_gateway_device_stats_to_csv(fast=False):
    """
    Collects and exports detailed device statistics for all gateways in the organization.
    Makes individual getSiteDeviceStats API calls for each gateway device.
    Optimized to use cached inventory data and concurrent processing when fast=True.
    
    Args:
        fast (bool): If True, enables concurrent processing and uses cached inventory data
                    to minimize API calls.
    """
    logging.info("[INFO] Collecting detailed device statistics for all gateways in the org...")
    if fast:
        logging.info(" Fast mode enabled: Using cached data and concurrent processing")
    
    org_id = get_cached_or_prompted_org_id()
    gateway_devices = get_gateway_devices_with_sites(apisession, org_id, fast=fast)
    all_stats = []

    if not gateway_devices:
        logging.warning("[WARN] No gateway devices found. Exiting export_gateway_device_stats_to_csv.")
        return

    def fetch_device_stats_with_retry(device_info, max_retries=None, retry_delay=None, connection_semaphore=None):
        """
        Fetch device statistics for a single gateway device with retry logic and connection pool management.
        
        Args:
            device_info: Tuple of (site_id, device_id, device_name, site_name)
            max_retries: Maximum number of retry attempts (uses env var if None)
            retry_delay: Base delay between retries (uses env var if None)
            connection_semaphore: Semaphore to limit concurrent connections (optional)
        """
        # Use environment variables as defaults if not provided
        if max_retries is None:
            max_retries = FAST_MODE_MAX_RETRIES
        if retry_delay is None:
            retry_delay = FAST_MODE_RETRY_DELAY
            
        site_id, device_id, device_name, site_name = device_info
        
        for attempt in range(max_retries + 1):
            try:
                # Validate inputs before making API calls
                validate_site_id(site_id, "export_gateway_device_stats_to_csv")
                validate_device_id(device_id, "export_gateway_device_stats_to_csv")
                
                # Use semaphore to limit concurrent connections if provided
                if connection_semaphore:
                    with connection_semaphore:
                        stats = mistapi.api.v1.sites.stats.getSiteDeviceStats(apisession, site_id, device_id).data
                else:
                    stats = mistapi.api.v1.sites.stats.getSiteDeviceStats(apisession, site_id, device_id).data
                
                # Add contextual information to the stats
                stats["site_id"] = site_id
                stats["site_name"] = site_name
                stats["device_id"] = device_id
                stats["device_name"] = device_name
                
                if attempt > 0:
                    logging.info(f"! Retry {attempt} successful for device {device_name} at site {site_name}")
                else:
                    logging.debug(f"! Collected device stats for gateway {device_name} at site {site_name}")
                return stats
                
            except Exception as e:
                if attempt < max_retries:
                    # Fast mode: reduced backoff delay for quicker retries
                    backoff_delay = retry_delay * (2 ** attempt) if not fast else retry_delay
                    logging.warning(f"! Attempt {attempt + 1} failed for device {device_name} at site {site_name}: {e}")
                    logging.info(f"! Retrying in {backoff_delay} seconds...")
                    time.sleep(backoff_delay)
                else:
                    logging.error(f"! Failed to fetch device stats for {device_name} at site {site_name} after {max_retries + 1} attempts: {e}")
                    # Return a minimal record with error information
                    return {
                        "site_id": site_id,
                        "site_name": site_name,
                        "device_id": device_id,
                        "device_name": device_name,
                        "error": str(e),
                        "status": "failed"
                    }

    # Process devices with appropriate threading based on fast mode
    if fast and len(gateway_devices) > 10:
        # Use concurrent processing for large numbers of devices
        logging.info(f"! Fast mode: Processing {len(gateway_devices)} gateway devices concurrently...")
        
        # Limit concurrent connections to prevent overwhelming the API
        max_workers = min(10, len(gateway_devices))  # Cap at 10 concurrent requests
        connection_semaphore = threading.Semaphore(max_workers)
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(
                    fetch_device_stats_with_retry, 
                    device_info, 
                    connection_semaphore=connection_semaphore
                ): device_info for device_info in gateway_devices
            }
            
            # Progress bar for concurrent processing
            for future in tqdm(concurrent.futures.as_completed(futures), 
                             total=len(futures), 
                             desc="Gateway Device Stats", 
                             unit="device"):
                device_info = futures[future]
                try:
                    result = future.result()
                    if result:
                        all_stats.append(result)
                except Exception as e:
                    site_id, device_id, device_name, site_name = device_info
                    logging.error(f"! Concurrent processing failed for device {device_name} at site {site_name}: {e}")
    else:
        # Sequential processing for smaller datasets or normal mode
        logging.info(f"! Processing {len(gateway_devices)} gateway devices sequentially...")
        for i, device_info in enumerate(tqdm(gateway_devices, desc="Gateway Device Stats", unit="device"), 1):
            site_id, device_id, device_name, site_name = device_info
            logging.debug(f"! Processing device {i}/{len(gateway_devices)}: {device_name} at {site_name}")
            
            result = fetch_device_stats_with_retry(device_info)
            if result:
                all_stats.append(result)

    # Save results to CSV
    if all_stats:
        # Sanitize and flatten nested data structures
        sanitized = []
        for stats in all_stats:
            flat_record = flatten_dict_recursively(stats)
            sanitized.append(flat_record)
        
        filename = "AllGatewayDeviceStats.csv"
        DataExporter.save_data_to_output(sanitized, filename)
        logging.info(f"! Gateway device statistics saved to {filename} ({len(all_stats)} records).")
        logging.info(f"! API Optimization: Collected detailed stats for {len(gateway_devices)} gateways")
        
        # Log summary of successful vs failed requests
        successful_requests = len([s for s in all_stats if s.get("status") != "failed"])
        failed_requests = len(all_stats) - successful_requests
        if failed_requests > 0:
            logging.warning(f"! {failed_requests} requests failed out of {len(all_stats)} total")
        else:
            logging.info(f"! All {successful_requests} requests completed successfully")
    else:
        logging.warning(" No gateway device statistics found. CSV not created.")

def export_gateways_with_wan_port_conflicts_to_csv():
    """
    Checks if AllGatewayDeviceStats.csv exists and is fresh. If not, generates it.
    Then exports a filtered CSV showing gateways that have IP address conflicts WITHIN the same device:
    - Same IP address assigned to multiple WAN ports (0/0/0, 0/0/1, 0/0/2)
    
    Note: Next hop gateway conflicts are not checked since this data is not available in the CSV.
    """
    logging.info(" Starting WAN port IP conflict analysis for individual gateway devices...")
    
    # Check if AllGatewayDeviceStats.csv exists and is fresh using existing helper
    stats_file = "AllGatewayDeviceStats.csv"
    
    check_and_generate_csv(stats_file, lambda: export_gateway_device_stats_to_csv(fast=True))
    
    # Load the gateway device stats using CSV reader
    stats_path = get_csv_file_path(stats_file)
    try:
        gateway_data = []
        with open(stats_path, 'r', encoding='utf-8') as csvfile:
            reader = csv.DictReader(csvfile)
            gateway_data = list(reader)
        
        logging.info(f"! Loaded {len(gateway_data)} gateway device records for analysis")
    except Exception as e:
        logging.error(f"! Failed to load {stats_file}: {e}")
        print(f"! Failed to load {stats_file}: {e}")
        return
    
    # Define WAN port columns to analyze
    wan_ports = ['0/0/0', '0/0/1', '0/0/2'] 
    ip_columns = [f'if_stat_ge-{port}_ips' for port in wan_ports]
    
    # Find gateways with internal WAN port IP conflicts
    logging.info(" Analyzing individual gateways for internal WAN port IP conflicts...")
    
    conflicts_found = []
    
    for i, row in enumerate(gateway_data):
        device_name = row.get('device_name', row.get('name', f"Device_{i}"))
        site_name = row.get('site_name', 'Unknown Site')
        
        # Collect IP addresses for this device's WAN ports
        device_ips = {}  # {ip: [port1, port2, ...]}
        
        # Collect IP addresses from WAN ports
        for col in ip_columns:
            if col in row and row[col] and str(row[col]).strip():
                ip_value = str(row[col]).strip()
                if ip_value not in ['', 'nan', 'None', 'null']:
                    port = col.replace('if_stat_ge-', '').replace('_ips', '')
                    
                    if ip_value not in device_ips:
                        device_ips[ip_value] = []
                    device_ips[ip_value].append(port)
        
        # Check for IP conflicts within this gateway (same IP on multiple ports)
        ip_conflicts = []
        for ip, ports in device_ips.items():
            if len(ports) > 1:
                ip_conflicts.append({
                    'type': 'IP Address Conflict',
                    'value': ip,
                    'ports': ports,
                    'port_count': len(ports)
                })
                logging.warning(f"! IP conflict in {device_name}: {ip} assigned to ports {', '.join(ports)}")
        
        # If this gateway has IP conflicts, add simplified records for each conflicted port
        if ip_conflicts:
            # Add records for IP conflicts - one line per conflicted port
            for conflict in ip_conflicts:
                for port in conflict['ports']:
                    conflicts_found.append({
                        'device_name': device_name,
                        'site_name': site_name,
                        'port_name': f"ge-{port}",
                        'port_ip': conflict['value'],
                        'conflict_type': 'IP Address Conflict',
                        'conflict_with_ports': ', '.join([p for p in conflict['ports'] if p != port])
                    })
    
    # Export results
    if conflicts_found:
        output_file = "GatewayWANPortConflicts.csv"
        
        # Sort by device name and port name
        conflicts_found.sort(key=lambda x: (x.get('device_name', ''), x.get('port_name', '')))
        
        # Save to CSV using existing helper
        DataExporter.save_data_to_output(conflicts_found, output_file)
        
        logging.info(f"! WAN port IP conflicts exported to {output_file} ({len(conflicts_found)} conflicted port records)")
        print(f"! WAN port IP conflicts exported to {output_file} ({len(conflicts_found)} conflicted port records)")
        
        # Display summary
        unique_gateways = set()
        ip_conflict_ports = len(conflicts_found)
        
        for record in conflicts_found:
            unique_gateways.add(record.get('device_name', 'Unknown'))
        
        logging.info(f"! Summary: {len(unique_gateways)} gateways with IP conflicts ({ip_conflict_ports} conflicted ports)")
        print(f"! Summary: {len(unique_gateways)} gateways with IP conflicts ({ip_conflict_ports} conflicted ports)")
        
        # Show sample of conflicted ports
        print(f"\n  Sample WAN Port IP Conflicts Found:")
        for i, record in enumerate(conflicts_found[:10], 1):
            print(f"{i:2d}. {record.get('device_name', 'Unknown')} ({record.get('site_name', 'Unknown Site')})")
            print(f"    Port {record.get('port_name', 'Unknown')} has IP {record.get('port_ip', 'Unknown')}")
            print(f"    Conflicts with port(s): {record.get('conflict_with_ports', 'Unknown')}")
            print()
        
        if len(conflicts_found) > 10:
            print(f"... and {len(conflicts_found) - 10} more conflicted ports")
        
    else:
        logging.info(" No internal WAN port IP conflicts found - all gateways have unique IP addresses per WAN port")
        print(" No internal WAN port IP conflicts found - all gateways have unique IP addresses per WAN port")
        print(" This indicates healthy WAN port configurations with no duplicate IP assignments within individual gateways")

def export_sites_with_location_to_csv():
    """
    Export a list of sites with all available fields to SitesWithLocations.csv.
    """
    print("Sites with Location and Timezone Info:")
    logging.info("Listing Sites with Full Info:")
    org_id = get_cached_or_prompted_org_id()
    logging.debug(f"Using org_id: {org_id} for site location export.")

    # Fetch all sites using unified page size helper
    sites = fetch_all_sites_with_limit(org_id)
    logging.info(f"Fetched {len(sites)} sites from the organization.")

    # Flatten and sanitize all site data
    flattened_sites = flatten_nested_fields_in_list(sites)
    sanitized_sites = escape_multiline_strings_for_csv(flattened_sites)

    # Write to CSV
    DataExporter.save_data_to_output(sanitized_sites, "SitesWithLocations.csv")
    print(f"! {len(sanitized_sites)} sites exported to SitesWithLocations.csv")
    logging.info(" Full site data written to SitesWithLocations.csv")

def export_gateways_with_site_info_to_csv():
    """
    Fetches all gateway devices in the organization, enriches them with site and address info,
    and exports the result to GatewaysWithSiteInfo.csv. Also logs and displays a summary table.
    """
    print("Gateways with Site and Address Info:")
    logging.info("Fetching Gateways with Site Info...")
    org_id = get_cached_or_prompted_org_id()

    # Fetch site list and build a lookup dictionary for site info
    sites = fetch_all_sites_with_limit(org_id)
    site_lookup = {
        site["id"]: {
            "name": site.get("name", ""),
            "address": site.get("address", "")
        } for site in sites
    }
    logging.debug(f"Loaded {len(site_lookup)} sites for lookup.")

    # Fetch org inventory (all devices)
    inventory = fetch_all_inventory_with_limit(org_id)
    logging.debug(f"Loaded {len(inventory)} devices from org inventory.")

    def split_address(address):
        """
        Splits a full address string into street, city, state, zip, and country.
        Returns empty strings if parsing fails.
        """
        try:
            parts = address.split(", ")
            street = parts[0]
            city = parts[1]
            state_zip = parts[2].split()
            state = state_zip[0]
            zip_code = state_zip[1]
            country = parts[3]
            return street, city, state, zip_code, country
        except Exception as e:
            logging.debug(f"Failed to split address '{address}': {e}")
            return address, "", "", "", ""

    # Filter for gateways and enrich with site info
    gateways = []
    for device in tqdm(inventory, desc="Processing Gateways", unit="device"):
        if device.get("type") == "gateway":
            site_id = device.get("site_id")
            site_info = site_lookup.get(site_id, {"name": "Unknown", "address": "Unknown"})
            device["site_name"] = site_info["name"]
            device["site_address"] = site_info["address"]
            street, city, state, zip_code, country = split_address(site_info["address"])
            device["street"] = street
            device["city"] = city
            device["state"] = state
            device["zip_code"] = zip_code
            device["country"] = country
            gateways.append(device)
    logging.info(f"Enriched {len(gateways)} gateway devices with site info.")

    # Flatten nested fields and escape multiline strings for CSV compatibility
    gateways = flatten_nested_fields_in_list(gateways)
    gateways = escape_multiline_strings_for_csv(gateways)
    gateways = sorted(gateways, key=lambda x: x.get("site_name", ""))
    DataExporter.save_data_to_output(gateways, "GatewaysWithSiteInfo.csv")
    print(f"! {len(gateways)} gateways exported to GatewaysWithSiteInfo.csv")
    logging.info("Gateway data written to GatewaysWithSiteInfo.csv")

    # Display a summary table in logs
    table = PrettyTable()
    table.field_names = ["name", "mac", "model", "serial", "site_name", "street", "city", "state", "zip_code", "country"]
    for gw in gateways:
        table.add_row([
            gw.get("name", ""),
            gw.get("mac", ""),
            gw.get("model", ""),
            gw.get("serial", ""),
            gw.get("site_name", ""),
            gw.get("street", ""),
            gw.get("city", ""),
            gw.get("state", ""),
            gw.get("zip_code", ""),
            gw.get("country", "")
        ])
    logging.debug("\n" + table.get_string())  # Log the table output (debug mode only)

def export_devices_with_site_info_to_csv(fast=False):
    """
    Fetches all devices in the organization, enriches them with site and address info,
    and exports the result to AllDevicesWithSiteInfo.csv. Also logs and displays a summary table.
    
    Args:
        fast (bool): If True, enables optimized processing mode with enhanced caching
                    and concurrent site lookups where applicable.
    """
    print("All Devices with Site and Address Info:")
    logging.info("Fetching All Devices with Site Info...")  # Log start of function
    if fast:
        logging.info(" Fast mode enabled for devices with site info export")
    
    org_id = get_cached_or_prompted_org_id()

    # Ensure required CSV files are available, using caching where possible
    if fast:
        # Use cached data when fast mode is enabled
        check_and_generate_csv("SiteList.csv", export_all_sites_to_csv)
        check_and_generate_csv("OrgInventory.csv", export_device_inventory_to_csv)
        
        # Load from cached CSV files instead of making API calls
        site_lookup = {}
        try:
            site_list_path = get_csv_file_path("SiteList.csv")
            with open(site_list_path, mode="r", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                site_lookup = {
                    row["id"]: {
                        "name": row.get("name", ""),
                        "address": row.get("address", "")
                    } for row in reader
                }
            logging.debug(f"Loaded {len(site_lookup)} sites from cached SiteList.csv")
        except Exception as e:
            logging.warning(f"Failed to load from cached SiteList.csv, falling back to API: {e}")
            # Fallback to API if cached data fails
            sites = fetch_all_sites_with_limit(org_id)
            site_lookup = {
                site["id"]: {
                    "name": site.get("name", ""),
                    "address": site.get("address", "")
                } for site in sites
            }
            logging.debug(f"Loaded {len(site_lookup)} sites from API fallback")

        # Load inventory from cached CSV
        inventory = []
        try:
            inventory_path = get_csv_file_path("OrgInventory.csv")
            with open(inventory_path, mode="r", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                inventory = list(reader)
            logging.debug(f"Loaded {len(inventory)} devices from cached OrgInventory.csv")
        except Exception as e:
            logging.warning(f"Failed to load from cached OrgInventory.csv, falling back to API: {e}")
            # Fallback to API if cached data fails
            inventory = fetch_all_inventory_with_limit(org_id)
            logging.debug(f"Loaded {len(inventory)} devices from API fallback")
    else:
        # Original behavior: fetch directly from API
        # Fetch all sites and build a lookup dictionary for site info
        sites = fetch_all_sites_with_limit(org_id)
        site_lookup = {
            site["id"]: {
                "name": site.get("name", ""),
                "address": site.get("address", "")
            } for site in sites
        }
        logging.debug(f"Loaded {len(site_lookup)} sites for lookup.")

        # Fetch org inventory (all devices)
        inventory = fetch_all_inventory_with_limit(org_id)
        logging.debug(f"Loaded {len(inventory)} devices from org inventory.")

    def split_address(address):
        """
        Splits a full address string into street, city, state, zip, and country.
        Returns empty strings if parsing fails.
        """
        try:
            parts = address.split(", ")
            street = parts[0]
            city = parts[1]
            state_zip = parts[2].split()
            state = state_zip[0]
            zip_code = state_zip[1]
            country = parts[3]
            return street, city, state, zip_code, country
        except Exception as e:
            logging.debug(f"Failed to split address '{address}': {e}")
            return address, "", "", "", ""

    enriched_devices = []
    for device in tqdm(inventory, desc="Processing Devices", unit="device"):
        site_id = device.get("site_id")
        site_info = site_lookup.get(site_id, {"name": "Unknown", "address": "Unknown"})
        device["site_name"] = site_info["name"]
        device["site_address"] = site_info["address"]
        street, city, state, zip_code, country = split_address(site_info["address"])
        device["street"] = street
        device["city"] = city
        device["state"] = state
        device["zip_code"] = zip_code
        device["country"] = country
        enriched_devices.append(device)
        logging.debug(f"Enriched device {device.get('name', '')} ({device.get('mac', '')}) with site info.")

    # Flatten nested fields and escape multiline strings for CSV compatibility
    enriched_devices = flatten_nested_fields_in_list(enriched_devices)
    enriched_devices = escape_multiline_strings_for_csv(enriched_devices)
    enriched_devices = sorted(enriched_devices, key=lambda x: x.get("site_name", ""))
    DataExporter.save_data_to_output(enriched_devices, "AllDevicesWithSiteInfo.csv")
    print(f"! {len(enriched_devices)} devices exported to AllDevicesWithSiteInfo.csv")
    logging.info(f"All device data written to AllDevicesWithSiteInfo.csv ({len(enriched_devices)} records).")

    # Display a summary table in logs
    table = PrettyTable()
    table.field_names = ["name", "mac", "model", "serial", "type", "site_name", "street", "city", "state", "zip_code", "country"]
    for dev in enriched_devices:
        table.add_row([
            dev.get("name", ""),
            dev.get("mac", ""),
            dev.get("model", ""),
            dev.get("serial", ""),
            dev.get("type", ""),
            dev.get("site_name", ""),
            dev.get("street", ""),
            dev.get("city", ""),
            dev.get("state", ""),
            dev.get("zip_code", ""),
            dev.get("country", "")
        ])
    logging.debug("\n" + table.get_string())  # Log the table output for reference (debug mode only)

def generate_support_package():
    logging.info("Generating support package for each site...")

    # List of required CSV files and their generation functions
    required_files = [
        ("OrgAlarms.csv", export_open_org_alarms_to_csv),
        ("OrgDeviceEvents.csv", export_recent_device_events_to_csv),
        ("SiteList.csv", export_all_sites_to_csv),
        ("OrgDevices.csv", export_all_devices_to_csv),
        ("OrgDeviceStats.csv", export_device_stats_to_csv),
        ("OrgDevicePortStats.csv", export_device_port_stats_to_csv),
        ("AllGatewayTestResults.csv", export_gateway_test_results_by_site_to_csv),
    ]

    # Ensure all required files are fresh or regenerate them
    for filename, func in required_files:
        logging.debug(f"Checking freshness of {filename}...")
        check_and_generate_csv(filename, func)  # freshness_minutes now comes from .env

    # Ensure SiteList.csv is generated before loading
    check_and_generate_csv('SiteList.csv', export_all_sites_to_csv)

    # Load the pulled data into dictionaries
    logging.debug("Loading CSV data into dictionaries for support package assembly...")
    site_data = load_csv_grouped_by_key('SiteList.csv', 'id')
    alarms_data = load_csv_grouped_by_key('OrgAlarms.csv', 'site_id')
    events_data = load_csv_grouped_by_key('OrgDeviceEvents.csv', 'site_id')
    devices_data = load_csv_grouped_by_key('OrgDevices.csv', 'name')
    device_stats_data = load_csv_grouped_by_key('OrgDeviceStats.csv', 'site_id')
    port_stats_data = load_csv_grouped_by_key('OrgDevicePortStats.csv', 'site_id')

    # Load speedtest data if available
    gateway_test_results_path = get_csv_file_path('AllGatewayTestResults.csv')
    if os.path.exists(gateway_test_results_path):
        logging.debug("Loading AllGatewayTestResults.csv for speedtest data...")
        speedtest_data = load_csv_grouped_by_key('AllGatewayTestResults.csv', 'site_id')
    else:
        logging.warning(" AllGatewayTestResults.csv not found. Skipping speedtest data.")
        speedtest_data = {}

    # Create a support package for each site with alarms or events
    for site_id, site_info in site_data.items():
        # Only generate support package if there are alarms or events for the site
        if not alarms_data.get(site_id) and not events_data.get(site_id):
            logging.info(f"Skipping site {site_id} !? no alarms or events.")
            continue

        logging.info(f"Generating support package for site: {site_id}")
        # Gather all relevant data for the site
        support_data = {
            'alarms': alarms_data.get(site_id, []),
            'events': events_data.get(site_id, []),
            'devices': devices_data.get(site_id, []),
            'device_stats': device_stats_data.get(site_id, []),
            'port_stats': port_stats_data.get(site_id, []),
            'speedtests': speedtest_data.get(site_id, []),
        }

        support_package_filename = f"SupportPackage_{site_id}.csv"
        logging.debug(f"Writing support package to {support_package_filename}...")
        write_support_data_to_csv(support_data, support_package_filename)
        logging.info(f"Support package written for site {site_id}.")

    logging.info(" Support packages generated for applicable sites.")
    logging.info(" Support packages generated for all sites!")

def load_csv_grouped_by_key(filename, key):
    """
    Loads CSV data into a dictionary keyed by the specified column.
    Each key maps to a list of rows (as dictionaries) that share the same key value.
    Adds logging for file loading and key distribution.
    """
    logging.info(f"Loading CSV file '{filename}' into dictionary keyed by '{key}'...")
    csv_file_path = get_csv_file_path(filename)
    with open(csv_file_path, mode='r', encoding='utf-8') as file:
        reader = csv.DictReader(file)  # Create a CSV reader
        data_dict = {}  # Initialize an empty dictionary
        row_count = 0
        for row in reader:
            data_key = row.get(key)  # Get the value to use as the key
            if data_key is None:
                logging.warning(f"Row missing key '{key}': {row}")
                continue
            if data_key not in data_dict:
                data_dict[data_key] = []  # Initialize a list for this key
            data_dict[data_key].append(row)  # Add the row to the dictionary
            row_count += 1
        logging.info(f"Loaded {row_count} rows from '{filename}'. Found {len(data_dict)} unique keys for '{key}'.")
    return data_dict  # Return the dictionary

def write_support_data_to_csv(data, filename):
    """
    Writes the support package data (a dict of lists of dicts) to a CSV file.
    Each section in 'data' is a list of dictionaries. All unique keys across all sections are used as CSV columns.
    """
    logging.debug(f"Preparing to write support package to {filename}...")

    fieldnames = set()  # Initialize a set to collect all field names
    # Collect all unique field names from all sections
    for section_name, section in data.items():
        logging.debug(f"Processing section '{section_name}' with {len(section)} rows.")
        for row in section:
            fieldnames.update(row.keys())  # Add all keys to the fieldnames set
    fieldnames = sorted(fieldnames)  # Sort the fieldnames for consistent column order

    logging.debug(f"Final CSV fieldnames: {fieldnames}")

    # SECURITY: Use proper file path handling to ensure files go to data/ directory
    csv_file_path = get_csv_file_path(filename)
    with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.DictWriter(file, fieldnames=fieldnames)  # Create a CSV writer
        writer.writeheader()  # Write the header row
        row_count = 0
        for section_name, section in data.items():
            for row in section:
                writer.writerow(row)  # Write each row to the CSV file
                row_count += 1
        logging.info(f"Wrote {row_count} rows to {csv_file_path} for support package.")

    logging.info(f"Support package written to {csv_file_path}")  # Log completion of the file write

def poll_marvis_actions():
    """
    Interactive Marvis (VNA - Virtual Network Assistant) troubleshooting function that allows users to:
    1. Perform targeted troubleshooting for specific clients
    2. Perform targeted troubleshooting for specific devices
    3. Analyze network connectivity issues
    4. View Marvis insights and recommendations
    
    This function guides users through the Marvis troubleshooting process step by step,
    using guided selection workflows instead of manual MAC address entry.
    """
    logging.info(" Starting Marvis (VNA) troubleshooting workflow...")
    logging.debug("MARVIS DEBUG: Entering poll_marvis_actions() function")
    print(" Starting Marvis (VNA - Virtual Network Assistant) Troubleshooting")
    print("=" * 65)
    print()
    
    org_id = get_cached_or_prompted_org_id()
    logging.debug(f"MARVIS DEBUG: Using org_id: {org_id} for Marvis troubleshooting")
    logging.debug(f"MARVIS DEBUG: Session state - authenticated: {apisession is not None}")

    print(" Marvis AI Troubleshooting Options:")
    print("1. Troubleshoot client connectivity issues (guided client selection)")
    print("2. Diagnose device performance problems (guided device selection)") 
    print("3. Analyze network connectivity issues (site-level analysis)")
    print("4. View organization Marvis insights and capabilities")
    print("5. Exit")
    print()
    
    choice = input("Select an option (1-5): ").strip()
    logging.debug(f"MARVIS DEBUG: User selected option: {choice}")
    
    if choice == "1":
        logging.debug("MARVIS DEBUG: Calling troubleshoot_client_connectivity()")
        troubleshoot_client_connectivity()
    elif choice == "2":
        logging.debug("MARVIS DEBUG: Calling troubleshoot_device_performance()")
        troubleshoot_device_performance()
    elif choice == "3":
        logging.debug("MARVIS DEBUG: Calling troubleshoot_network_connectivity()")
        troubleshoot_network_connectivity()
    elif choice == "4":
        logging.debug("MARVIS DEBUG: Calling view_marvis_insights()")
        view_marvis_insights()
    elif choice == "5":
        logging.debug("MARVIS DEBUG: User chose to exit")
        print("Exiting Marvis troubleshooting.")
        return
    else:
        print(" Invalid option selected.")
        logging.warning(f"MARVIS DEBUG: Invalid troubleshooting option selected: {choice}")
        logging.debug("MARVIS DEBUG: Exiting poll_marvis_actions() due to invalid choice")

def prompt_client_selection(site_id=None):
    """
    Prompts the user to select a client from available wireless or wired clients.
    
    Args:
        site_id (str, optional): If provided, searches within the specific site.
                                If None, searches across the entire organization.
    
    Returns:
        tuple: (client_mac, client_type, site_id) or (None, None, None) if no selection made
    """
    print("\n  Client Selection")
    print("=" * 30)
    
    # If no site_id provided, decide between site-specific or org-wide search
    if not site_id:
        scope_choice = input("Search scope - (s)ite-specific or (o)rganization-wide? [s/o]: ").strip().lower()
        if scope_choice == 's':
            site_id = prompt_site_selection()
            if not site_id:
                print(" No site selected.")
                return None, None, None
    
    org_id = get_cached_or_prompted_org_id()
    
    try:
        all_clients = []
        
        if site_id:
            # Site-specific client search
            print(f"! Searching for clients in selected site...")
            
            # Get wireless clients for the site
            try:
                wireless_response = mistapi.api.v1.sites.clients.searchSiteWirelessClients(apisession, site_id, limit=1000)
                wireless_clients = mistapi.get_all(response=wireless_response, mist_session=apisession) or []
                for client in wireless_clients:
                    client['client_type'] = 'wireless'
                    client['source_site_id'] = site_id
                all_clients.extend(wireless_clients)
                logging.info(f"Found {len(wireless_clients)} wireless clients in site")
            except Exception as e:
                logging.warning(f"Could not fetch wireless clients for site: {e}")
            
            # Get wired clients for the site (if API supports it)
            try:
                wired_response = mistapi.api.v1.sites.wired_clients.searchSiteWiredClients(apisession, site_id, limit=1000)
                wired_clients = mistapi.get_all(response=wired_response, mist_session=apisession) or []
                for client in wired_clients:
                    client['client_type'] = 'wired'
                    client['source_site_id'] = site_id
                all_clients.extend(wired_clients)
                logging.info(f"Found {len(wired_clients)} wired clients in site")
            except Exception as e:
                logging.warning(f"Could not fetch wired clients for site (may not be supported): {e}")
                
        else:
            # Organization-wide client search
            print(f"! Searching for clients across organization...")
            
            # Get wireless clients org-wide
            try:
                wireless_response = mistapi.api.v1.orgs.clients.searchOrgWirelessClients(apisession, org_id, limit=1000)
                wireless_clients = mistapi.get_all(response=wireless_response, mist_session=apisession) or []
                for client in wireless_clients:
                    client['client_type'] = 'wireless'
                all_clients.extend(wireless_clients)
                logging.info(f"Found {len(wireless_clients)} wireless clients in organization")
            except Exception as e:
                logging.warning(f"Could not fetch wireless clients for org: {e}")
            
            # Get wired clients org-wide
            try:
                wired_response = mistapi.api.v1.orgs.wired_clients.searchOrgWiredClients(apisession, org_id, limit=1000)
                wired_clients = mistapi.get_all(response=wired_response, mist_session=apisession) or []
                for client in wired_clients:
                    client['client_type'] = 'wired'
                all_clients.extend(wired_clients)
                logging.info(f"Found {len(wired_clients)} wired clients in organization")
            except Exception as e:
                logging.warning(f"Could not fetch wired clients for org: {e}")
        
        if not all_clients:
            print(" No clients found.")
            return None, None, None
        
        # Sort clients by hostname, then MAC
        all_clients = sorted(all_clients, key=lambda x: (x.get('hostname', ''), x.get('mac', '')))
        
        # Prepare table for selection
        table = PrettyTable()
        table.field_names = ["#", "Hostname", "MAC Address", "Type", "IP Address", "SSID/VLAN", "Site", "Status"]
        table.align["#"] = "r"
        table.align["Hostname"] = "l"
        table.align["MAC Address"] = "l"
        table.align["Type"] = "c"
        table.align["IP Address"] = "l"
        table.align["SSID/VLAN"] = "l"
        table.align["Site"] = "l"
        table.align["Status"] = "c"
        # Set max widths for better formatting
        table.max_width["Hostname"] = 20
        table.max_width["IP Address"] = 16
        table.max_width["SSID/VLAN"] = 15
        table.max_width["Site"] = 15
        index_to_client = {}
        
        # Fetch site list once and cache it
        sites_cache = {}
        try:
            print(" Loading site information...")
            sites = fetch_all_sites_with_limit(org_id)
            sites_cache = {site["id"]: site["name"] for site in sites}
            logging.info(f"Cached {len(sites_cache)} sites for client display")
        except Exception as e:
            logging.warning(f"Could not fetch sites for display: {e}")
        
        for idx, client in enumerate(all_clients):
            # Get site name from cache
            site_name = ""
            if 'site_id' in client and client['site_id'] in sites_cache:
                site_name = sites_cache[client['site_id']]
            elif 'site_id' in client:
                site_name = client['site_id']  # Fallback to site ID if name not found
            
            # Determine connection status
            status = "??" if client.get('connected', True) else "??"
            if 'last_seen' in client:
                last_seen = client.get('last_seen', 0)
                current_time = int(time.time())
                if current_time - last_seen > 300:  # More than 5 minutes ago
                    status = "??"
            
            # Format hostname/name
            hostname = client.get('hostname', client.get('name', ''))
            if not hostname or hostname in ['[]', '']:
                hostname = 'Unknown'
            if len(hostname) > 20:
                hostname = hostname[:17] + "..."
            
            # Format IP address - handle both strings and arrays
            ip_address = client.get('ip', '')
            if isinstance(ip_address, list):
                if ip_address:
                    ip_address = ip_address[0]  # Take first IP if multiple
                else:
                    ip_address = 'N/A'
            elif not ip_address or ip_address == '[]':
                ip_address = 'N/A'
            
            # Format SSID/VLAN - handle both strings and arrays
            ssid_vlan = client.get('ssid', client.get('vlan', ''))
            if isinstance(ssid_vlan, list):
                if ssid_vlan:
                    ssid_vlan = str(ssid_vlan[0])  # Take first value if multiple
                else:
                    ssid_vlan = 'N/A'
            elif not ssid_vlan or ssid_vlan == '[]':
                ssid_vlan = 'N/A'
            
            if len(ssid_vlan) > 15:
                ssid_vlan = ssid_vlan[:12] + "..."
            
            # Format site name with better truncation
            if len(site_name) > 15:
                site_name = site_name[:12] + "..."
            
            table.add_row([
                idx,
                hostname,
                client.get('mac', 'Unknown'),
                client.get('client_type', 'unknown')[:8],
                ip_address,
                ssid_vlan,
                site_name,
                status
            ])
            index_to_client[idx] = client
        
        print(f"\n  Found {len(all_clients)} clients:")
        print(table)
        
        # Show summary statistics
        wireless_count = sum(1 for c in all_clients if c.get('client_type') == 'wireless')
        wired_count = sum(1 for c in all_clients if c.get('client_type') == 'wired')
        print(f"\n  Summary: {wireless_count} wireless, {wired_count} wired clients")
        
        # Show legend
        print("\n  = Online  = Recently seen  = Offline")
        print("---" * 20)
        
        # Get user selection
        try:
            max_index = len(all_clients) - 1
            user_input = input(f"\n  Enter client index (0-{max_index}) or 'q' to quit: ").strip()
                
            if user_input.lower() in ['q', 'quit', 'exit']:
                print(" Exiting client selection...")
                return None, None, None
                
            idx = int(user_input)
            if 0 <= idx <= max_index:
                selected_client = index_to_client[idx]
                client_mac = selected_client.get('mac')
                client_type = selected_client.get('client_type', 'unknown')
                client_site_id = selected_client.get('site_id', site_id)
                hostname = selected_client.get('hostname', selected_client.get('name', 'Unknown'))
                
                print(f"\n Selected client:")
                print(f"   Name: {hostname}")
                print(f"   MAC: {client_mac}")
                print(f"   Type: {client_type}")
                if client_site_id and client_site_id in sites_cache:
                    print(f"   Site: {sites_cache[client_site_id]}")
                
                logging.info(f"User selected client: MAC={client_mac}, type={client_type}, site={client_site_id}")
                return client_mac, client_type, client_site_id
            else:
                print(f"! Invalid index. Please enter a number between 0 and {max_index}.")
                return None, None, None
                
        except ValueError:
            print(" Please enter a valid number or 'q' to quit.")
            return None, None, None
            
    except Exception as e:
        logging.error(f"Error during client selection: {e}")
        print(f"! Error searching for clients: {e}")
        return None, None, None

def troubleshoot_client_connectivity():
    """
    Troubleshoot client connectivity issues using Marvis AI.
    Uses guided client selection instead of manual MAC address entry.
    """
    print("\n  Client Connectivity Troubleshooting")
    print("=" * 50)
    
    # Use guided client selection
    client_mac, client_type, site_id = prompt_client_selection()
    if not client_mac:
        print(" No client selected. Returning to main menu.")
        return
    
    org_id = get_cached_or_prompted_org_id()
    
    try:
        print(f"! Running Marvis AI analysis for client {client_mac}...")
        print(f"   Client Type: {client_type}")
        if site_id:
            print(f"   Site ID: {site_id}")
        
        logging.info(f"Starting Marvis client troubleshooting for MAC: {client_mac}, type: {client_type}, site: {site_id}")
        
        # Prepare parameters for troubleshoot call
        params = {"mac": client_mac}
        if site_id:
            params["site_id"] = site_id
            
        # Add client type parameter for proper troubleshooting context
        if client_type in ["wired", "wireless"]:
            params["type"] = client_type
            logging.debug(f"MARVIS DEBUG: Added type parameter: {client_type}")
        
        logging.debug(f"MARVIS DEBUG: About to call troubleshootOrg with params: {params}")
        
        # Call Marvis troubleshoot endpoint
        response = mistapi.api.v1.orgs.troubleshoot.troubleshootOrg(apisession, org_id, **params)
        
        if response.data:
            print(" Marvis AI analysis completed!")
            print(f"! Analysis results available.")
            
            # Save results to CSV with optimized formatting
            data = format_marvis_data_for_csv(response.data, "client")
            
            filename = f"MarvisInsights_Client_{client_mac.replace(':', '')}_{client_type}.csv"
            DataExporter.save_data_to_output(data, filename)
            print(f"! Results saved to {filename}")
            
            # Display summary
            if isinstance(response.data, dict):
                if 'results' in response.data:
                    print("\n  Marvis Analysis Summary:")
                    for result in response.data.get('results', []):
                        print(f"  !? {result.get('description', 'Analysis result')}")
                        if result.get('action'):
                            print(f"    Recommended Action: {result['action']}")
                elif 'insights' in response.data:
                    print("\n  Marvis Insights:")
                    insights = response.data.get('insights', [])
                    for insight in insights:
                        print(f"  !? {insight.get('description', insight)}")
                else:
                    print(f"\n  Analysis Data: {len(data)} items processed")
        else:
            print(" No specific connectivity issues found for this client.")
            print(" This could indicate the client is functioning normally.")
            
    except Exception as e:
        logging.error(f"Failed to troubleshoot client {client_mac}: {e}")
        print(f"! Failed to troubleshoot client: {e}")
        print(" This may indicate:")
        print("   - Marvis (VNA) is not enabled for your organization")
        print("   - The client is not currently active or found")
        print("   - Insufficient permissions for Marvis troubleshooting")
        print("   - API connectivity issues")

def troubleshoot_device_performance():
    """
    Troubleshoot device performance issues using Marvis AI.
    Uses guided site and device selection workflow.
    """
    logging.debug("MARVIS DEBUG: Entering troubleshoot_device_performance()")
    print("\n  Device Performance Troubleshooting")
    print("=" * 50)
    
    # Get site selection first
    site_id = prompt_site_selection()
    if not site_id:
        print(" No site selected.")
        logging.debug("MARVIS DEBUG: No site selected for device troubleshooting")
        return
    
    logging.debug(f"MARVIS DEBUG: Selected site_id: {site_id}")
    
    # Get device selection
    device_id = prompt_device_selection(site_id)
    if not device_id:
        print(" No device selected.")
        logging.debug("MARVIS DEBUG: No device selected")
        return
    
    logging.debug(f"MARVIS DEBUG: Selected device_id: {device_id}")
    org_id = get_cached_or_prompted_org_id()
    logging.debug(f"MARVIS DEBUG: Using org_id: {org_id}")
    
    try:
        # Get device MAC address from device ID
        print(f"! Looking up device details...")
        logging.debug(f"MARVIS DEBUG: About to get device details for device_id: {device_id} in site: {site_id}")
        
        device_response = mistapi.api.v1.sites.devices.getSiteDevice(apisession, site_id, device_id)
        logging.debug(f"MARVIS DEBUG: Device lookup response status: {device_response.status if hasattr(device_response, 'status') else 'unknown'}")
        
        if not device_response.data:
            print(" Could not retrieve device details.")
            logging.debug("MARVIS DEBUG: Device response data is None")
            return
            
        logging.debug(f"MARVIS DEBUG: Device data keys: {list(device_response.data.keys()) if isinstance(device_response.data, dict) else 'not a dict'}")
        
        device_mac = device_response.data.get('mac')
        device_name = device_response.data.get('name', 'Unknown Device')
        
        logging.debug(f"MARVIS DEBUG: Device MAC: {device_mac}")
        logging.debug(f"MARVIS DEBUG: Device name: {device_name}")
        
        if not device_mac:
            print(" Could not determine device MAC address.")
            logging.debug("MARVIS DEBUG: Device MAC is None or empty")
            return
        
        print(f"! Running Marvis AI performance analysis...")
        print(f"   Device: {device_name} ({device_mac})")
        print(f"   Site ID: {site_id}")
        
        logging.info(f"Starting Marvis device performance analysis for device: {device_name} (MAC: {device_mac})")
        logging.debug(f"MARVIS DEBUG: About to call troubleshootOrg with mac={device_mac}, site_id={site_id}")
        
        # Call Marvis troubleshoot endpoint for device using MAC address
        response = mistapi.api.v1.orgs.troubleshoot.troubleshootOrg(
            apisession, org_id, 
            mac=device_mac, 
            site_id=site_id
        )
        
        logging.debug(f"MARVIS DEBUG: Device troubleshoot response status: {response.status if hasattr(response, 'status') else 'unknown'}")
        logging.debug(f"MARVIS DEBUG: Device response data type: {type(response.data)}")
        logging.debug(f"MARVIS DEBUG: Device response data is None: {response.data is None}")
        
        if response.data:
            logging.debug(f"MARVIS DEBUG: Device response data keys: {list(response.data.keys()) if isinstance(response.data, dict) else 'not a dict'}")
            logging.debug(f"MARVIS DEBUG: Device response data: {json.dumps(response.data, indent=2, default=str)}")
            
            print(" Marvis AI device analysis completed!")
            
            # Save results to CSV with optimized formatting
            data = format_marvis_data_for_csv(response.data, "device")
            logging.debug(f"MARVIS DEBUG: Formatted device data length: {len(data) if data else 0}")
            
            filename = f"MarvisInsights_Device_{device_mac.replace(':', '')}_{device_name.replace(' ', '_')}.csv"
            DataExporter.save_data_to_output(data, filename)
            print(f"! Results saved to {filename}")
            
            # Display summary if available
            if isinstance(response.data, dict):
                if 'results' in response.data:
                    results = response.data.get('results', [])
                    logging.debug(f"MARVIS DEBUG: Found {len(results)} device results")
                    print("\n  Device Performance Analysis:")
                    for result in results:
                        print(f"  !? {result.get('description', 'Analysis result')}")
                        if result.get('action'):
                            print(f"    Recommended Action: {result['action']}")
                elif 'insights' in response.data:
                    print("\n  Marvis Device Insights:")
                    insights = response.data.get('insights', [])
                    logging.debug(f"MARVIS DEBUG: Found {len(insights)} device insights")
                    for insight in insights:
                        print(f"  !? {insight.get('description', insight)}")
                else:
                    logging.debug("MARVIS DEBUG: No results or insights in device response")
                    print(f"\n  Analysis Data: {len(data)} items processed")
            
        else:
            logging.debug("MARVIS DEBUG: Device response data is None or empty")
            print(" No performance issues detected for this device.")
            print(" This could indicate the device is operating within normal parameters.")
            
    except Exception as e:
        logging.error(f"MARVIS DEBUG: Exception in troubleshoot_device_performance: {e}")
        logging.error(f"MARVIS DEBUG: Exception type: {type(e)}")
        logging.error(f"MARVIS DEBUG: Exception traceback: ", exc_info=True)
        print(f"! Failed to troubleshoot device: {e}")
        print(" This may indicate:")
        print("   - The device is not found or not supported by Marvis")
        print("   - Marvis (VNA) is not enabled for your organization")
        print("   - Insufficient permissions for device troubleshooting")
    
    logging.debug("MARVIS DEBUG: Exiting troubleshoot_device_performance()")

def troubleshoot_network_connectivity():
    """
    Troubleshoot general network connectivity issues using Marvis AI.
    Provides site-level network analysis and insights.
    """
    logging.debug("MARVIS DEBUG: Entering troubleshoot_network_connectivity()")
    print("\n  Network Connectivity Troubleshooting")
    print("=" * 50)
    
    # Get site selection
    site_id = prompt_site_selection()
    if not site_id:
        print(" No site selected.")
        logging.debug("MARVIS DEBUG: No site selected, exiting network troubleshooting")
        return
    
    logging.debug(f"MARVIS DEBUG: Selected site_id: {site_id}")
    org_id = get_cached_or_prompted_org_id()
    logging.debug(f"MARVIS DEBUG: Using org_id: {org_id}")
    
    try:
        print(f"! Running Marvis AI network analysis...")
        print(f"   Analyzing site-level connectivity")
        print(f"   Site ID: {site_id}")
        
        logging.info(f"Starting Marvis network connectivity analysis for site: {site_id}")
        logging.debug(f"MARVIS DEBUG: About to call mistapi.api.v1.orgs.troubleshoot.troubleshootOrg with org_id={org_id}, site_id={site_id}")
        
        # Call Marvis troubleshoot endpoint for site
        response = mistapi.api.v1.orgs.troubleshoot.troubleshootOrg(
            apisession, org_id, 
            site_id=site_id
        )
        
        logging.debug(f"MARVIS DEBUG: API response received. Status: {response.status if hasattr(response, 'status') else 'unknown'}")
        logging.debug(f"MARVIS DEBUG: Response data type: {type(response.data)}")
        logging.debug(f"MARVIS DEBUG: Response data is None: {response.data is None}")
        
        if response.data:
            logging.debug(f"MARVIS DEBUG: Response data keys: {list(response.data.keys()) if isinstance(response.data, dict) else 'not a dict'}")
            logging.debug(f"MARVIS DEBUG: Response data length: {len(response.data) if hasattr(response.data, '__len__') else 'no length'}")
            logging.debug(f"MARVIS DEBUG: Full response data structure: {json.dumps(response.data, indent=2, default=str) if response.data else 'None'}")
            
            print(" Marvis AI network analysis completed!")
            
            # Save results to CSV with optimized formatting
            logging.debug("MARVIS DEBUG: About to format data for CSV")
            data = format_marvis_data_for_csv(response.data, "network")
            logging.debug(f"MARVIS DEBUG: Formatted data length: {len(data) if data else 0}")
            logging.debug(f"MARVIS DEBUG: Formatted data sample: {data[:1] if data else 'empty'}")
            
            filename = f"MarvisInsights_Network_{site_id}.csv"
            DataExporter.save_data_to_output(data, filename)
            print(f"! Results saved to {filename}")
            logging.debug(f"MARVIS DEBUG: Saved data to {filename}")
            
            # Display summary if available
            if isinstance(response.data, dict):
                logging.debug("MARVIS DEBUG: Response data is a dict, checking for results/insights")
                if 'results' in response.data:
                    results = response.data.get('results', [])
                    logging.debug(f"MARVIS DEBUG: Found 'results' key with {len(results)} items")
                    print("\n  Network Connectivity Analysis:")
                    for idx, result in enumerate(results):
                        logging.debug(f"MARVIS DEBUG: Processing result {idx}: {result}")
                        description = result.get('description', 'Analysis result') if isinstance(result, dict) else str(result)
                        print(f"  !? {description}")
                        if isinstance(result, dict) and result.get('action'):
                            print(f"    Recommended Action: {result['action']}")
                elif 'insights' in response.data:
                    insights = response.data.get('insights', [])
                    logging.debug(f"MARVIS DEBUG: Found 'insights' key with {len(insights)} items")
                    print("\n  Marvis Network Insights:")
                    for idx, insight in enumerate(insights):
                        logging.debug(f"MARVIS DEBUG: Processing insight {idx}: {insight}")
                        description = insight.get('description', insight) if isinstance(insight, dict) else str(insight)
                        print(f"  !? {description}")
                else:
                    logging.debug("MARVIS DEBUG: No 'results' or 'insights' keys found in response data")
                    logging.debug(f"MARVIS DEBUG: Available keys in response: {list(response.data.keys())}")
                    print(f"\n  Analysis Data: {len(data)} items processed")
                    if response.data:
                        print(f"! Raw response keys: {list(response.data.keys())}")
                        # Show some raw data for debugging
                        for key, value in list(response.data.items())[:5]:
                            print(f"   {key}: {str(value)[:100]}{'...' if len(str(value)) > 100 else ''}")
            else:
                logging.debug(f"MARVIS DEBUG: Response data is not a dict, type: {type(response.data)}")
                print(f"\n  Raw response: {str(response.data)[:200]}{'...' if len(str(response.data)) > 200 else ''}")
            
        else:
            logging.debug("MARVIS DEBUG: Response data is None or empty")
            print(" No network connectivity issues detected for this site.")
            print(" This indicates the network is operating within normal parameters.")
            
    except Exception as e:
        logging.error(f"MARVIS DEBUG: Exception in troubleshoot_network_connectivity: {e}")
        logging.error(f"MARVIS DEBUG: Exception type: {type(e)}")
        logging.error(f"MARVIS DEBUG: Exception traceback: ", exc_info=True)
        print(f"! Failed to troubleshoot network: {e}")
        print(" This may indicate:")
        print("   - Marvis (VNA) is not enabled for your organization")
        print("   - The site has no devices or insufficient data for analysis")
        print("   - Insufficient permissions for network troubleshooting")
    
    logging.debug("MARVIS DEBUG: Exiting troubleshoot_network_connectivity()")

def view_marvis_insights():
    """
    View available Marvis (VNA) insights and capabilities for the organization.
    This provides information about Marvis availability and organizational insights.
    """
    print("\n  Marvis (VNA) Insights & Capabilities")
    print("=" * 50)
    
    org_id = get_cached_or_prompted_org_id()
    
    try:
        print(" Checking Marvis availability and organizational insights...")
        
        # Try to get organization info to check Marvis capabilities
        org_response = mistapi.api.v1.orgs.orgs.getOrg(apisession, org_id)
        
        if org_response.data:
            org_info = org_response.data
            print(f"! Organization: {org_info.get('name', 'Unknown')}")
            
            # Check for Marvis-related features
            features = org_info.get('features', [])
            marvis_features = [f for f in features if any(keyword in f.lower() for keyword in ['marvis', 'vna', 'insight'])]
            
            if marvis_features:
                print("\n  Marvis/VNA Features Available:")
                for feature in marvis_features:
                    print(f"  !? {feature}")
            else:
                print("\n  No specific Marvis/VNA features detected in organization settings.")
            
            # Try to get organization-level insights if available
            try:
                print("\n Attempting to retrieve organization-level insights...")
                
                # Try different insight endpoints that might be available
                insight_endpoints = [
                    ("Organization Sites SLE", lambda: mistapi.api.v1.orgs.insights.getOrgSitesSle(apisession, org_id)),
                ]
                
                insights_found = False
                for endpoint_name, endpoint_func in insight_endpoints:
                    try:
                        logging.debug(f"MARVIS DEBUG: Testing endpoint: {endpoint_name}")
                        response = endpoint_func()
                        logging.debug(f"MARVIS DEBUG: {endpoint_name} response status: {response.status if hasattr(response, 'status') else 'unknown'}")
                        logging.debug(f"MARVIS DEBUG: {endpoint_name} response data type: {type(response.data)}")
                        logging.debug(f"MARVIS DEBUG: {endpoint_name} response data is None: {response.data is None}")
                        
                        if response.data:
                            insights_data = response.data if isinstance(response.data, list) else [response.data]
                            logging.debug(f"MARVIS DEBUG: {endpoint_name} insights data length: {len(insights_data)}")
                            logging.debug(f"MARVIS DEBUG: {endpoint_name} full response: {json.dumps(response.data, indent=2, default=str)[:1000]}...")
                            
                            if insights_data:
                                print(f"\n  {endpoint_name}:")
                                for insight in insights_data[:5]:  # Show first 5 insights
                                    description = insight.get('description', insight.get('type', insight.get('name', str(insight))))
                                    print(f"  !? {description}")
                                
                                if len(insights_data) > 5:
                                    print(f"  ... and {len(insights_data) - 5} more insights")
                                
                                # Save insights to CSV with optimized formatting
                                if "Sites SLE" in endpoint_name:
                                    # Use optimized formatting for Sites SLE data
                                    formatted_insights = format_marvis_data_for_csv(response.data, "sites")
                                else:
                                    # Use legacy formatting for other insight types
                                    formatted_insights = flatten_nested_fields_in_list(insights_data)
                                    formatted_insights = escape_multiline_strings_for_csv(formatted_insights)
                                
                                filename = f"MarvisInsights_{endpoint_name.replace(' ', '_')}.csv"
                                DataExporter.save_data_to_output(formatted_insights, filename)
                                print(f"  Full insights saved to {filename}")
                                insights_found = True
                    except Exception as e:
                        error_message = str(e)
                        if "404" in error_message:
                            logging.debug(f"Endpoint {endpoint_name} not available for this organization (404): {e}")
                        elif "403" in error_message:
                            logging.debug(f"Access denied to {endpoint_name} (403): {e}")
                        else:
                            logging.debug(f"Could not fetch {endpoint_name}: {e}")
                        continue
                
                if not insights_found:
                    print("\n  No organization-level insights currently available.")
                
            except Exception as e:
                logging.warning(f"Could not retrieve organization insights: {e}")
                print(f"! Could not retrieve insights: {e}")
            
            print("\n  Marvis (VNA - Virtual Network Assistant) Usage Guide:")
            print("   Targeted Troubleshooting:")
            print("     !? Use client troubleshooting for specific device connectivity issues")
            print("     !? Use device troubleshooting for AP, switch, or gateway performance")
            print("     !? Use network troubleshooting for site-wide connectivity analysis")
            print()
            print("   Requirements:")
            print("     !? Marvis must be enabled for your organization")
            print("     !? Devices must be actively managed and reporting data")
            print("     !? Sufficient data history for meaningful analysis")
            print()
            print("   Best Practices:")
            print("     !? Run troubleshooting when issues are actively occurring")
            print("     !? Provide specific timeframes when prompted")
            print("     !? Review saved CSV files for detailed analysis results")
            
        else:
            print(" Could not retrieve organization information.")
            
    except Exception as e:
        logging.error(f"Failed to get Marvis insights: {e}")
        print(f"! Failed to get Marvis insights: {e}")
        print(" This may indicate:")
        print("   - Marvis (VNA) is not enabled for your organization")
        print("   - Insufficient permissions to view organization details")
        print("   - API connectivity issues")
        print("   - Organization may not have Marvis licensing")
        print()
        print(" Contact your Mist administrator to:")
        print("   !? Verify Marvis/VNA licensing and enablement")
        print("   !? Confirm user permissions for AI troubleshooting")
        print("   !? Check organization feature settings")

def export_current_guest_users_to_csv():
    """
    Export all current guest users in the org to OrgCurrentGuests.csv
    """
    print("Current and Historical Guest Users:")
    logging.info("Exporting all current guest users in the org...")  # Log start of function
    org_id = get_cached_or_prompted_org_id()
    logging.debug(f"Using org_id: {org_id} for current guest export.")

    # Call the Mist API to get current guest authorizations
    response = mistapi.api.v1.orgs.guests.searchOrgGuestAuthorization(apisession, org_id, limit=1000)
    guests = mistapi.get_all(response=response, mist_session=apisession)
    logging.info(f"Fetched {len(guests)} current guest users from API.")

    # Flatten nested fields for CSV compatibility
    guests = flatten_nested_fields_in_list(guests)
    # Escape multiline strings for CSV compatibility
    guests = escape_multiline_strings_for_csv(guests)

    # Write the processed data to a CSV file
    DataExporter.save_data_to_output(guests, "OrgCurrentGuests.csv")
    print(f"! {len(guests)} current guest users exported to OrgCurrentGuests.csv")
    logging.info(" Current guests exported to OrgCurrentGuests.csv")  # Log completion

def export_historical_guest_users_to_csv():
    """
    Export all guest users from the last 7 days to OrgHistoricalGuests.csv
    """
    logging.info("Exporting all guest users from the last 7 days...")  # Log start of function
    org_id = get_cached_or_prompted_org_id()
    # Calculate epoch for 7 days ago
    end_time = int(time.time())
    start_time = end_time - 7 * 24 * 3600
    logging.debug(f"Fetching guest authorizations from {start_time} to {end_time} (epoch seconds).")
    # Call the Mist API to get guest authorizations in the last 7 days
    response = mistapi.api.v1.orgs.guests.searchOrgGuestAuthorization(
        apisession, org_id, limit=1000, start=start_time, end=end_time
    )
    guests = mistapi.get_all(response=response, mist_session=apisession)
    logging.info(f"Fetched {len(guests)} historical guest users from API.")
    # Flatten nested fields for CSV compatibility
    guests = flatten_nested_fields_in_list(guests)
    # Escape multiline strings for CSV compatibility
    guests = escape_multiline_strings_for_csv(guests)
    # Write the processed data to a CSV file
    DataExporter.save_data_to_output(guests, "OrgHistoricalGuests.csv")
    print(f"! {len(guests)} historical guest users exported to OrgHistoricalGuests.csv")
    logging.info(" Historical guests exported to OrgHistoricalGuests.csv")  # Log completion

def export_switch_vc_stats_to_csv():
    """
    Export virtual chassis stats (including stacking cable info) for all switches in the org.
    """
    print("Switch Virtual Chassis Statistics:")
    logging.info("Exporting all switch virtual chassis stats...")

    # Ensure OrgInventory.csv is fresh
    check_and_generate_csv("OrgInventory.csv", export_device_inventory_to_csv)

    # Load OrgInventory.csv and filter for switches that are virtual chassis (`vc_mac` present and not empty)
    inventory_path = get_csv_file_path("OrgInventory.csv")
    with open(inventory_path, mode="r", encoding="utf-8") as file:
        reader = csv.DictReader(file)
        switches = [row for row in reader if row.get("type") == "switch" and row.get("vc_mac", "").strip()]

    if not switches:
        logging.warning("No switches found in OrgInventory.csv.")
        return

    all_vc_stats = []

    for switch in tqdm(switches, desc="Switches", unit="switch"):
        site_id = switch.get("site_id")
        device_id = switch.get("id")
        name = switch.get("name", "")
        mac = switch.get("mac", "")
        model = switch.get("model", "")
        serial = switch.get("serial", "")

        # Log which switch is being processed
        logging.debug(f"Processing switch: name={name}, id={device_id}, site_id={site_id}, mac={mac}, model={model}, serial={serial}")

        if not site_id or not device_id:
            logging.warning(f"Skipping switch with missing site_id or device_id: name={name}, mac={mac}")
            continue

        try:
            # Get VC stats for this switch (returns a flat dict)
            vc_stats = mistapi.api.v1.sites.devices.getSiteDeviceVirtualChassis(apisession, site_id, device_id).data
            logging.debug(f"Fetched VC stats for switch {name} ({device_id}): {vc_stats}")
            # Merge all switch info and VC info into a single dictionary
            entry = {**switch, **vc_stats}
            all_vc_stats.append(entry)
        except Exception as e:
            logging.warning(f"Failed to fetch VC stats for switch {name} ({device_id}): {e}")

    # Flatten and write to CSV
    logging.info(f"Flattening and sanitizing {len(all_vc_stats)} VC stats entries for CSV export.")
    all_vc_stats = flatten_nested_fields_in_list(all_vc_stats)
    all_vc_stats = escape_multiline_strings_for_csv(all_vc_stats)
    DataExporter.save_data_to_output(all_vc_stats, "OrgSwitchVCStats.csv")
    print(f"! {len(all_vc_stats)} switch VC stats exported to OrgSwitchVCStats.csv")
    logging.info(f"! Switch VC stats exported to OrgSwitchVCStats.csv ({len(all_vc_stats)} records).")
    # Optionally log a preview of the data
    if all_vc_stats:
        logging.debug(f"Sample VC stats row: {all_vc_stats[0]}")
        # Display a summary PrettyTable for quick inspection
        table = PrettyTable()
        summary_fields = ["name", "mac", "model", "serial", "site_id", "vc_mac", "status", "members_0_vc_role", "members_1_vc_role"]
        table.field_names = [f for f in summary_fields if f in all_vc_stats[0]]

        for row in all_vc_stats:
            table.add_row([row.get(f, "") for f in table.field_names])
        logging.debug("\n" + table.get_string())  # Log the table output (debug mode only)

def prompt_select_site_and_device_ids(site_id=None, device_id=None):
    """
    Returns site_id and device_id, either from arguments or via interactive prompts.
    """
    if not site_id:
        site_id = prompt_select_site_id_from_csv()
        if not site_id:
            print(" No site selected.")
            return None, None

    if not device_id:
        device_id = prompt_select_device_id_from_inventory(site_id, device_type="all")
        if not device_id:
            print(" No device selected.")
            return None, None

    return site_id, device_id

def create_shell_session(site_id, device_id):
    """
    Creates a shell session and returns the WebSocket URL.
    """
    try:
        resp = mistapi.api.v1.sites.devices.createSiteDeviceShellSession(apisession, site_id, device_id)
        shell_data = resp.data

        return shell_data.get("url")
    except Exception as e:
        print(f"! Failed to create shell session: {e}")
        return None

def run_interactive_shell(shell_url, debug=False):
    if debug:
        websocket.enableTrace(True)

    print(" Connecting to WebSocket shell...")
    ws = websocket.create_connection(shell_url)
    print(" Connected.")

    screen = pyte.Screen(80, 40)
    stream = pyte.Stream(screen)

    def _resize():
        cols, rows = shutil.get_terminal_size()
        resize_msg = json.dumps({'resize': {'width': cols, 'height': rows}})
        if debug:
            print(f"[DEBUG] Sending resize: {resize_msg}")
        ws.send(resize_msg)

    def _ws_in():
        while ws.connected:
            try:
                data = ws.recv()
                if isinstance(data, bytes):
                    data = data.decode('utf-8', errors='ignore')
                if debug:
                    print(f"[DEBUG] Raw recv: {repr(data)}")
                if data:
                    stream.feed(data)
                    for row_index in sorted(screen.dirty):
                        sys.stdout.write(f"\x1b[{row_index+1};1H")  # Move cursor to line row_index+1
                        sys.stdout.write(screen.display[row_index] + "\x1b[K")  # Clear to end of line
                    sys.stdout.flush()
                    screen.dirty.clear()
            except Exception as e:
                print(f'\n## Connection lost: {e} ##')
                return
    def _ws_out(key):
        if ws.connected:
            keymap = {
                "enter": "\n", "space": " ", "tab": "\t",
                "up": "\x00\x1b[A", "down": "\x00\x1b[B",
                "left": "\x00\x1b[D", "right": "\x00\x1b[C",
                               "backspace": "\x08"
            }
            if key == "~":
                print('\n## Exit from shell ##')
                ws.sock.shutdown(2)
                ws.sock.close()
                stop_listening()
                return
            k = keymap.get(key, key)
            data = f"\00{k}"
            data_byte = bytearray(map(ord, data))
            if debug:
                print(f"[DEBUG] Sending: {repr(data)}")
            try:
                ws.send_binary(data_byte)
            except Exception as e:
                print(f'\n## Send failed: {e} ##')
                return

    _resize()
    threading.Thread(target=_ws_in).start()

    # Wake up Juniper SSR prompt
   
    time.sleep(1)
    ws.send_binary(bytearray(map(ord, "\00\n\n")))
    if debug:
        print("[DEBUG] Sent wakeup sequence to Juniper SSRs")

    listen_keyboard(on_release=_ws_out, delay_second_char=0, delay_other_chars=0, lower=False)


    _resize()
    threading.Thread(target=_ws_in).start()

    # Wake up Juniper SSR prompt
    time.sleep(1)
    ws.send_binary(bytearray(map(ord, "\00\n\n")))
    if debug:
        print("[DEBUG] Sent wakeup sequence to Juniper SSRs")

    listen_keyboard(on_release=_ws_out, delay_second_char=0, delay_other_chars=0, lower=False)

def launch_cli_shell(site_id=None, device_id=None, debug=False):
    site_id, device_id = prompt_select_site_and_device_ids(site_id, device_id)
    if not site_id or not device_id:
        return
    shell_url = create_shell_session(site_id, device_id)
    if shell_url:
        run_interactive_shell(shell_url, debug=debug)

def listen_for_command_output(mist_host, mist_apitoken, site_id, device_id, session_id, timeout=30, idle_timeout=3, debug=False):
    if debug:
        websocket.enableTrace(True)

    ws_url = f"wss://{mist_host}/api-ws/v1/stream"
    headers = [f"Authorization: Token {mist_apitoken}"]
    subscribe_msg = {
        "subscribe": f"/sites/{site_id}/devices/{device_id}/cmd"
    }

    output_lines = []
    buffer = ""
    last_message_time = time.time()

    def on_message(ws, message):
        nonlocal last_message_time, buffer, output_lines
        last_message_time, buffer = _handle_ws_message(message, session_id, buffer, output_lines, debug)

    def on_close(ws, *args):
        _handle_ws_close(output_lines, debug)

    def on_error(ws, error):
        logging.error(f"! WebSocket error: {error}")

    def on_open(ws):
        logging.info(" WebSocket opened. Subscribing...")
        ws.send(json.dumps(subscribe_msg))

    ws = websocket.WebSocketApp(
        ws_url,
        header=headers,
        on_message=on_message,
        on_error=on_error,
        on_close=on_close,
        on_open=on_open
    )

    def run_ws():
        ws.run_forever()

    ws_thread = threading.Thread(target=run_ws)
    ws_thread.start()

    start_time = time.time()
    while time.time() - start_time < timeout:
        time.sleep(1)
        if time.time() - last_message_time > idle_timeout and output_lines:
            logging.info(" Idle timeout reached. Closing WebSocket.")
            ws.close()
            break

    if ws.keep_running:
        logging.warning(" Timeout waiting for ARP output.")
        ws.close()

def _handle_ws_message(message, session_id, buffer, output_lines, debug=False):
    """Handle incoming WebSocket message with comprehensive error logging."""
    last_message_time = time.time()
    try:
        if debug:
            logging.debug(f"WebSocket raw message received: {message}")

        msg = json.loads(message)
        data_str = msg.get("data", "{}")
        data_obj = json.loads(data_str) if isinstance(data_str, str) else data_str
        inner_data = data_obj.get("data", {})
        if isinstance(inner_data, str):
            inner_data = json.loads(inner_data)

        if inner_data.get("session") == session_id:
            raw_output = inner_data.get("raw", "")
            buffer += raw_output
            while "\n" in buffer:
                line, buffer = buffer.split("\n", 1)
                output_lines.append(line)
            if debug:
                logging.debug(f"Processed WebSocket data: {len(raw_output)} chars, buffer size: {len(buffer)}")

    except json.JSONDecodeError as e:
        logging.error(f"WebSocket message JSON decode error: {e}")
        if debug:
            logging.debug(f"Invalid JSON content: {message}")
    except KeyError as e:
        logging.warning(f"WebSocket message missing expected key: {e}")
        if debug:
            logging.debug(f"Message structure: {message}")
    except Exception as e:
        logging.error(f"Unexpected error parsing WebSocket message: {e}")
        if debug:
            logging.exception("Full WebSocket message parsing error:")

    return last_message_time, buffer

def _handle_ws_close(output_lines, debug=False):
    logging.info(" WebSocket closed.")
    if output_lines:
        compiled_output = "\n".join(output_lines)
        _save_output_to_file(compiled_output)
        export_arp_output_to_csv("arp_output_raw.txt")


        print("\n  ARP Output Received:\n")
        rows = compiled_output.split("\n")
        parsed_rows = [row.split("\t") for row in rows if row.strip()]
        max_cols = max(len(row) for row in parsed_rows)
        for row in parsed_rows:
            while len(row) < max_cols:
                row.append("")
        table = PrettyTable()
        table.field_names = [f"Col {col_num+1}" for col_num in range(max_cols)]
        for row in parsed_rows:
            table.add_row(row)

        if debug:
            print(table)
            logging.info(f"! Compiled ARP Output:\n{compiled_output}")
            logging.debug("\n" + table.get_string())
        else:
            print(f"! ARP output received with {len(parsed_rows)} rows.")
    else:
        print(" No ARP output received for this session.")
        logging.warning(" No ARP output received for this session.")

def export_arp_output_to_csv(txt_filename="arp_output_raw.txt", csv1="arp_dataset1.csv", csv2="arp_dataset2.csv"):
    try:
        # SECURITY: Use proper file path handling for all file operations
        txt_file_path = get_csv_file_path(txt_filename)
        csv1_path = get_csv_file_path(csv1)
        csv2_path = get_csv_file_path(csv2)
        
        with open(txt_file_path, "r", encoding="utf-8") as f:
            raw_text = f.read()

        lines = raw_text.splitlines()
        dataset1 = []
        dataset2 = []
        current_dataset = dataset1

        for line in lines:
            if "Total" in line:
                current_dataset = dataset2
            else:
                # Split on tabs and clean each field
                columns = [col.strip() for col in line.split("\t") if col.strip()]
                if columns:
                    current_dataset.append(columns)

        with open(csv1_path, 'w', newline='', encoding='utf-8') as f1:
            writer = csv.writer(f1)
            writer.writerows(dataset1)

        with open(csv2_path, 'w', newline='', encoding='utf-8') as f2:
            writer = csv.writer(f2)
            writer.writerows(dataset2)

        print(f"! Saved {len(dataset1)} rows to {csv1_path}")
        print(f"! Saved {len(dataset2)} rows to {csv2_path}")

    except Exception as e:
        print(f"! Failed to export ARP output to CSV: {e}")

def _save_output_to_file(compiled_output, filename="arp_output_raw.txt"):
    try:
        # SECURITY: Use proper file path handling to ensure files go to data/ directory
        file_path = get_csv_file_path(filename)
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(compiled_output)
        logging.info(f"! ARP output saved to {file_path}")
    except Exception as e:
        logging.error(f"! Failed to save ARP output to file: {e}")

def trigger_arp_command(mist_host, mist_apitoken, site_id, device_id):
    url = f"https://{mist_host}/api/v1/sites/{site_id}/devices/{device_id}/arp"
    headers = {'Authorization': f'Token {mist_apitoken}'}
    response = requests.post(url, headers=headers, json={})

    if response.status_code == 200:
        session_id = response.json().get("session")
        print(f"! ARP command triggered. Session ID: {session_id}")
        return session_id
    else:
        print(f"! Failed to trigger ARP command: {response.status_code}")
        print(response.text)
        return None

def run_arp_via_websocket(site_id=None, device_id=None):
    if not site_id or not device_id:
        site_id, device_id = prompt_select_site_and_device_ids(site_id, device_id)
    if not site_id or not device_id:
        return

    # Retrieve mist_host and mist_apitoken from the apisession or environment
    mist_host = getattr(apisession, "host", None) or os.getenv("MIST_HOST")
    mist_apitoken = getattr(apisession, "apitoken", None) or os.getenv("MIST_APITOKEN")

    if not mist_host or not mist_apitoken:
        print(" Mist host or API token not found in session or environment.")
        return

    print(" Subscribing to WebSocket stream...")
    session_id = trigger_arp_command(mist_host, mist_apitoken, site_id, device_id)
    if session_id:
        listen_for_command_output(mist_host.replace("api.", "api-ws."), mist_apitoken, site_id, device_id, session_id)

def loop_refresh_core_datasets(delay=None, debug=False):
    """
    Continuously refreshes core datasets with optional dynamic delay based on API usage.
    If delay is None, it will be calculated dynamically to avoid exceeding API limits.
    Loop can be stopped gracefully by creating a file named 'stop_loop.txt'.
    """
    logging.info(" Starting continuous data refresh loop...")
    smoothed = None  # Initialize smoothed delay tracker
    get_cached_or_prompted_org_id()  # Ensure org_id is loaded from .env if not already

    try:
        while True:
            if os.path.exists("stop_loop.txt"):
                logging.info(" Stop signal detected (stop_loop.txt). Exiting loop.")
                break

            export_all_sites_to_csv()
            export_device_inventory_to_csv()
            export_device_stats_to_csv()
            export_device_port_stats_to_csv()
            export_vpn_peer_stats_to_csv()
            logging.info(" All datasets refreshed.")

            # Determine delay
            if delay is not None:
                actual_delay = delay
            else:
                smoothed, actual_delay = get_rate_limited_delay(smoothed)

            logging.info(f"! Sleeping for {actual_delay:.2f} seconds...")
            time.sleep(actual_delay)

    except KeyboardInterrupt:
        logging.info(" Loop interrupted by user (Ctrl+C). Exiting gracefully.")

def load_pid_tuning_data():
    """Load PID tuning data from file with comprehensive logging."""
    logging.debug(f"ENTRY: load_pid_tuning_data()")
    
    if os.path.exists(tuning_data_file):
        try:
            logging.debug(f"File I/O: Attempting to read PID tuning data from {tuning_data_file}")
            with open(tuning_data_file, 'r') as f:
                data = json.load(f)
            
            # Validate and clean error history
            if "error" in data and isinstance(data["error"], list):
                cleaned_errors = []
                for err in data["error"]:
                    if isinstance(err, (int, float)) and not (math.isnan(err) or math.isinf(err)):
                        cleaned_errors.append(float(err))
                data["error"] = cleaned_errors
            else:
                data["error"] = []
                
            logging.debug(f"File I/O: Successfully loaded PID tuning data from {tuning_data_file}")
            logging.debug(f"EXIT: load_pid_tuning_data - loaded from file")
            return data
        except json.JSONDecodeError as e:
            logging.error(f"File I/O: Failed to parse JSON in {tuning_data_file}: {e}. Using defaults.")
        except OSError as e:
            logging.error(f"File I/O: OS error reading {tuning_data_file}: {e}. Using defaults.")
        except Exception as e:
            logging.error(f"File I/O: Unexpected error reading {tuning_data_file}: {e}. Using defaults.")
    else:
        logging.debug(f"File I/O: {tuning_data_file} does not exist, using defaults")
        
    logging.debug(f"EXIT: load_pid_tuning_data - using defaults")
    return {"k_p": 0.1, "k_i": 0.0005, "error": [], "integral": 0.0}

def save_pid_tuning_data(data):
    """Save PID tuning data to file with comprehensive logging."""
    logging.debug(f"ENTRY: save_pid_tuning_data(data_keys={list(data.keys()) if data else []})")
    
    try:
        logging.debug(f"File I/O: Attempting to write PID tuning data to {tuning_data_file}")
        with open(tuning_data_file, 'w') as f:
            json.dump(data, f, indent=2)
        logging.debug(f"File I/O: Successfully wrote PID tuning data to {tuning_data_file}")
        logging.debug(f"EXIT: save_pid_tuning_data - success")
    except OSError as e:
        logging.error(f"File I/O: OS error writing to {tuning_data_file}: {e}")
        logging.debug(f"EXIT: save_pid_tuning_data - OS error")
        raise
    except Exception as e:
        logging.error(f"File I/O: Unexpected error writing to {tuning_data_file}: {e}")
        logging.debug(f"EXIT: save_pid_tuning_data - unexpected error")
        raise

def adjust_gains(data):
    """
    Adjusts PID gains based on the trend of recent errors.
    If error is increasing (positive trend), increase gains.
    If error is decreasing (negative trend), decrease gains.
    """
    recent_errors = data["error"][-10:]
    if not recent_errors:
        return

    error_trend = sum(recent_errors) / len(recent_errors)

    if error_trend > 0:
        data["k_p"] *= 1.05
        data["k_i"] *= 1.05
    elif error_trend < 0:
        data["k_p"] *= 0.95
        data["k_i"] *= 0.95

    # Clamp gains to prevent runaway values
    data["k_p"] = min(max(data["k_p"], 1e-6), 1.0)
    data["k_i"] = min(max(data["k_i"], 1e-8), 0.01)

def compute_dynamic_alpha(errors, min_alpha=0.1, max_alpha=0.9):
    """
    Computes a dynamic smoothing factor alpha based on the standard deviation of recent errors.
    """
    if len(errors) < 2:
        return 0.3  # default fallback
    
    try:
        # Ensure errors is a list of numbers and convert to numpy array safely
        recent_errors = errors[-10:]
        # Convert to float64 explicitly to avoid type conversion issues
        error_array = np.array(recent_errors, dtype=np.float64)
        std_dev = np.std(error_array)
        normalized = min(std_dev / 50, 1.0)  # adjust divisor to control sensitivity
        alpha = min_alpha + (max_alpha - min_alpha) * normalized
        return round(alpha, 3)
    except Exception as e:
        logging.warning(f"Failed to compute dynamic alpha: {e}. Using fallback value.")
        return 0.3

def show_route_via_websocket():
    """
    Launches a shell session, runs 'show route 0.0.0.0 | display json | no-more',
    and saves the output to ws.log.
    """
    logging.info("Launching shell to run 'show route 0.0.0.0'...")
    site_id, device_id = prompt_select_site_and_device_ids()
    if not site_id or not device_id:
        return

    shell_url = create_shell_session(site_id, device_id)
    if not shell_url:
        logging.error(" Could not create shell session.")
        return

    try:
        ws = websocket.create_connection(shell_url)
        print(" Connected to shell session.")
        time.sleep(1)
        command = "show route 0.0.0.0 | display json | no-more\n"
        ws.send_binary(bytearray(map(ord, f"\00{command}")))

        output_lines = []
        while True:
            data = ws.recv()
            if isinstance(data, bytes):
                data = data.decode("utf-8", errors="ignore")
            output_lines.append(data)
            print(data, end="")
            if "DONE!" in data or "mist@" in data:
                break
        ws.close()

        with open("ws.log", "w", encoding="utf-8") as f:
            f.write("".join(output_lines))
        print(" WebSocket output saved to ws.log")

    except Exception as e:
        print(f"! Error during shell session: {e}")

def export_gateway_templates_to_csv():
    """template_lookup = {t["id"]: t.get("name", "Unknown") for t in templates if "id" in t}
    Fetches and exports all gateway templates in the organization to OrgGatewayTemplates.csv.
    """
    logging.info("Starting export of gateway templates...")
    fetch_and_display_api_data(
        title="Org Gateway Templates:",
        api_call=mistapi.api.v1.orgs.gateway_templates.listOrgGatewayTemplates,
        filename="OrgGatewayTemplates.csv",
        sort_key="name",
        limit=1000
    )
    logging.info(" Gateway templates exported to OrgGatewayTemplates.csv.")

def export_gateway_management_ips_to_csv(fast=False):
    """
    Exports gateway management overlay IPs grouped by gateway template association.
    Creates a single CSV with gateway info, management IPs, status, and template names.
    
    This function:
    1. Gets current device inventory (calls existing function)
    2. Gets gateway template mappings (calls existing function) 
    3. Gets gateway configurations with management IPs (calls existing function)
    4. Outputs CSV with: Gateway Name, Gateway Template, Management IP, Online Status, Site Name
    
    Args:
        fast (bool): Enable fast mode for API calls
    """
    logging.info("Starting export of gateway management overlay IPs...")
    print("Gateway Management IP Export:")
    print("Collecting data from inventory, templates, and configurations...")
    
    org_id = get_cached_or_prompted_org_id()
    
    # Ensure required CSVs are fresh by calling existing functions
    print("  1. Ensuring site list with template mappings is current...")
    check_and_generate_csv("SiteList.csv", export_all_sites_to_csv)
    
    print("  2. Ensuring gateway templates are current...")
    check_and_generate_csv("OrgGatewayTemplates.csv", export_gateway_templates_to_csv)
    
    print("  3. Ensuring gateway device data with connection status is current...")
    check_and_generate_csv("GatewaysWithSiteInfo.csv", export_gateways_with_site_info_to_csv)
    
    print("  4. Ensuring gateway configurations with management IPs are current...")
    check_and_generate_csv("AllSiteGatewayConfigs.csv", lambda: export_gateway_device_configs_to_csv(fast=fast))
    
    print("  5. Processing and correlating data...")
    
    # Load required data
    try:
        # Load sites with gateway template associations
        with open(get_csv_file_path("SiteList.csv"), encoding="utf-8") as f:
            sites = list(csv.DictReader(f))
        
        # Load gateway templates for name lookups
        with open(get_csv_file_path("OrgGatewayTemplates.csv"), encoding="utf-8") as f:
            templates = list(csv.DictReader(f))
        
        # Load gateway device data with connection status
        with open(get_csv_file_path("GatewaysWithSiteInfo.csv"), encoding="utf-8") as f:
            gateway_devices = list(csv.DictReader(f))
        
        # Load gateway configurations with management IPs
        with open(get_csv_file_path("AllSiteGatewayConfigs.csv"), encoding="utf-8") as f:
            gateway_configs = list(csv.DictReader(f))
            
    except FileNotFoundError as e:
        logging.error(f"Required CSV file not found: {e}")
        print(f"! Error: Required CSV file not found: {e}")
        return
    
    # Create lookup dictionaries
    site_lookup = {site.get("id"): site for site in sites}
    template_lookup = {t.get("id"): t.get("name", "Unknown Template") for t in templates}
    
    # Create device lookup for connection status by device name
    device_lookup = {dev.get("name"): dev for dev in gateway_devices}
    
    # Create management IP lookup by device name
    mgmt_ip_lookup = {config.get("name"): config.get("gateway_mgmt_overlay_ip_ip", "") 
                      for config in gateway_configs}
    
    # Process gateway devices and correlate with template and management IP data
    results = []
    gateways_processed = 0
    gateways_with_mgmt_ip = 0
    
    for device in gateway_devices:
        gateway_name = device.get("name", "Unknown Gateway")
        site_id = device.get("site_id", "")
        site_name = device.get("site_name", "Unknown Site")
        connected_status = device.get("connected", "")
        
        # Get management IP from configs
        mgmt_ip = mgmt_ip_lookup.get(gateway_name, "")
        
        # Determine connection status - simple online/offline based on connected field
        connected_val = str(connected_status).strip().lower()
        
        if connected_val in ['true', '1', 'yes']:
            status = "Online"
        elif connected_val in ['false', '0', 'no']:
            status = "Offline"
        else:
            # Empty or unknown connection status
            status = "Unknown"
        
        # Get template information
        site_info = site_lookup.get(site_id, {})
        template_id = site_info.get("gatewaytemplate_id", "")
        template_name = template_lookup.get(template_id, "No Template") if template_id else "No Template"
        
        # Prepare result row
        result_row = {
            "gateway_name": gateway_name,
            "management_ip": mgmt_ip if mgmt_ip else "Not Configured",
            "status": status,
            "site_name": site_name,
            "gateway_template": template_name,
            "template_id": template_id if template_id else "None"
        }
        
        results.append(result_row)
        gateways_processed += 1
        
        if mgmt_ip:
            gateways_with_mgmt_ip += 1
            logging.debug(f"Gateway {gateway_name}: Management IP {mgmt_ip}, Status: {status} (Template: {template_name})")
        else:
            logging.debug(f"Gateway {gateway_name}: No management IP configured, Status: {status} (Template: {template_name})")
    
    # Sort results by template name, then gateway name
    results.sort(key=lambda x: (x["gateway_template"], x["gateway_name"]))
    
    # Create the final CSV with requested columns
    final_results = [
        {
            "Gateway Name": row["gateway_name"],
            "Gateway Template": row["gateway_template"],
            "Management IP": row["management_ip"],
            "Online Status": row["status"],
            "Site Name": row["site_name"]
        }
        for row in results
    ]
    
    # Write CSV file
    DataExporter.save_data_to_output(final_results, "GatewayManagementIPs.csv")
    
    # Summary output
    print(f"! Gateway management IP export completed:")
    print(f"  - Total gateways processed: {gateways_processed}")
    print(f"  - Gateways with management IPs: {gateways_with_mgmt_ip}")
    print(f"  - Gateways without management IPs: {gateways_processed - gateways_with_mgmt_ip}")
    print(f"  - Output CSV: GatewayManagementIPs.csv")
    
    logging.info(f"Gateway management IP export completed. {gateways_processed} gateways processed, {gateways_with_mgmt_ip} with management IPs.")

def ssh_runner_by_gateway_template(fast=False):
    """
    SSH runner that targets gateways by template name and online status.
    
    This function:
    1. Ensures gateway management IP data is current (calls Menu Option 4)
    2. Prompts user to select a gateway template name
    3. Filters for gateways with that template AND online status
    4. Extracts management IPs from matching gateways
    5. Executes SSH commands on those filtered hosts using the SSH runner
    
    Args:
        fast (bool): Enable fast mode for data collection
    """
    logging.info("Starting SSH runner targeting gateways by template and online status...")
    print("SSH Runner - Gateway Template Targeting:")
    print("=" * 60)
    
    # Step 1: Ensure gateway management IP data is current
    print("  1. Ensuring gateway management IP data is current...")
    check_and_generate_csv("GatewayManagementIPs.csv", lambda: export_gateway_management_ips_to_csv(fast=fast))
    
    # Step 2: Read the gateway data
    try:
        with open(get_csv_file_path("GatewayManagementIPs.csv"), encoding="utf-8") as f:
            gateways = list(csv.DictReader(f))
    except FileNotFoundError:
        print("! Error: Gateway management IP data not found. Please run Menu Option 4 first.")
        logging.error("GatewayManagementIPs.csv not found")
        return
    
    if not gateways:
        print("! No gateway data found.")
        return
    
    # Step 3: Get unique template names for user selection
    template_names = sorted(set(gw.get("Gateway Template", "Unknown") for gw in gateways))
    template_names = [t for t in template_names if t and t != "Unknown"]
    
    if not template_names:
        print("! No gateway templates found in the data.")
        return
    
    template_names.sort()
    
    print(f"\n  2. Available gateway templates:")
    for i, template_name in enumerate(template_names, 1):
        gateway_count = sum(1 for gw in gateways if gw.get("Gateway Template") == template_name)
        online_count = sum(1 for gw in gateways if gw.get("Gateway Template") == template_name and gw.get("Online Status") == "Online")
        print(f"     {i:2}. {template_name} ({gateway_count} total, {online_count} online)")
    
    # Step 4: User selects template
    try:
        selection = input(f"\n  Enter template number (1-{len(template_names)}) or template name: ").strip()
        
        # Try to parse as number first
        try:
            template_index = int(selection) - 1
            if 0 <= template_index < len(template_names):
                selected_template = template_names[template_index]
            else:
                print(f"! Invalid selection. Please choose 1-{len(template_names)}")
                return
        except ValueError:
            # Try to match by name (case insensitive)
            matching_templates = [t for t in template_names if selection.lower() in t.lower()]
            if len(matching_templates) == 1:
                selected_template = matching_templates[0]
            elif len(matching_templates) > 1:
                print(f"! Ambiguous template name. Matches: {', '.join(matching_templates)}")
                return
            else:
                print(f"! Template '{selection}' not found.")
                return
    
    except KeyboardInterrupt:
        print("\n! Operation cancelled by user.")
        return
    
    # Step 5: Filter gateways by template and online status
    filtered_gateways = [
        gw for gw in gateways 
        if gw.get("Gateway Template") == selected_template 
        and gw.get("Online Status") == "Online"
        and gw.get("Management IP") != "Not Configured"
        and gw.get("Management IP", "").strip()
    ]
    
    if not filtered_gateways:
        print(f"! No online gateways with configured management IPs found for template '{selected_template}'")
        return
    
    # Step 6: Extract management IPs
    management_ips = [gw.get("Management IP") for gw in filtered_gateways]
    
    print(f"\n  3. Found {len(filtered_gateways)} online gateways with management IPs for template '{selected_template}':")
    for gw in filtered_gateways:
        print(f"     - {gw.get('Gateway Name', 'Unknown'):15} | {gw.get('Management IP'):15} | {gw.get('Site Name', 'Unknown')}")
    
    # Step 7: Confirm before executing SSH
    try:
        confirm = input(f"\n  Execute SSH commands on these {len(management_ips)} gateways? (y/N): ").strip().lower()
        if confirm not in ['y', 'yes']:
            print("! Operation cancelled.")
            return
    except KeyboardInterrupt:
        print("\n! Operation cancelled by user.")
        return
    
    # Step 8: Load SSH configuration and execute
    print(f"\n  4. Loading SSH configuration and executing commands...")
    
    try:
        # Get SSH configuration from environment
        ssh_config = EnhancedSSHRunner.load_ssh_config_from_env()
        
        if not ssh_config.get('username') or not ssh_config.get('password'):
            print("! SSH credentials not found in .env file.")
            print("  Please set SSH_USER and SSH_PASSWORD in your .env file.")
            return
        
        # Get commands from configuration
        commands = ssh_config.get('commands', [])
        if not commands:
            # Try loading from CSV fallback
            commands = EnhancedSSHRunner.load_commands_from_csv()
            if not commands:
                print("! No SSH commands found in .env file or data/SSH_COMMANDS.CSV")
                print("  Please set SSH_COMMANDS in .env or add commands to data/SSH_COMMANDS.CSV")
                return
        
        print(f"  - Target hosts: {len(management_ips)} gateways")
        print(f"  - Commands to execute: {len(commands)}")
        print(f"  - Template filter: {selected_template}")
        
        # Execute SSH commands on filtered hosts
        results = EnhancedSSHRunner.run_ssh_commands_multi_host(
            hosts=management_ips,
            username=ssh_config['username'],
            password=ssh_config['password'],
            commands=commands,
            port=22,
            timeout=30,
            use_shell=True,
            max_threads=5
        )
        
        # Summary
        successful_count = results.get('successful', 0)
        print(f"\n! SSH execution completed:")
        print(f"  - Template: {selected_template}")
        print(f"  - Hosts targeted: {len(management_ips)}")
        print(f"  - Successful: {successful_count}")
        print(f"  - Failed: {results.get('failed', 0)}")
        print(f"  - Per-host logs: per-host-logs/ssh_output_<host>_<timestamp>.log")
        
        logging.info(f"SSH runner by template completed: {selected_template}, {successful_count}/{results.get('total', len(management_ips))} successful")
        
    except Exception as e:
        print(f"! Error during SSH execution: {e}")
        logging.error(f"SSH runner by template error: {e}", exc_info=True)

def show_dhcp_security_binding():
    """
    Launches a shell session and runs 'show dhcp-security binding' on the selected device.
    Saves output to ws_dhcp.log.
    """
    logging.info("Launching shell to run 'show dhcp-security binding'...")
    site_id, device_id = prompt_select_site_and_device_ids()
    if not site_id or not device_id:
        return

    shell_url = create_shell_session(site_id, device_id)
    if not shell_url:
        logging.error(" Could not create shell session.")
        return

    try:
        ws = websocket.create_connection(shell_url)
        print(" Connected to shell session.")
        time.sleep(1)
        command = "show dhcp-security binding | display json | no-more\nDONE!"
        ws.send_binary(bytearray(map(ord, f"\00{command}")))

        output_lines = []
        while True:
            data = ws.recv()
            if isinstance(data, bytes):
                data = data.decode("utf-8", errors="ignore")
            output_lines.append(data)
            print(data, end="")
            if "DONE!" in data:
                break
        ws.close()

        with open("ws_dhcp.log", "w", encoding="utf-8") as f:
            f.write("".join(output_lines))
        print(" DHCP security binding output saved to ws_dhcp.log")

    except Exception as e:
        print(f"! Error during shell session: {e}")

def show_vlans():
    """
    Launches a shell session and runs 'show vlans' on the selected device.
    Saves output to ws_vlans.log.
    """
    logging.info("Launching shell to run 'show vlans'...")
    site_id, device_id = prompt_select_site_and_device_ids()
    if not site_id or not device_id:
        return

    shell_url = create_shell_session(site_id, device_id)
    if not shell_url:
        logging.error("!! Could not create shell session.")
        return

    try:
        ws = websocket.create_connection(shell_url)
        print("Connected to shell session.")
        time.sleep(1)
        command = "show vlans | display json | no-more\nDONE!"
        ws.send_binary(bytearray(map(ord, f"\00{command}")))

        output_lines = []
        while True:
            data = ws.recv()
            if isinstance(data, bytes):
                data = data.decode("utf-8", errors="ignore")
            output_lines.append(data)
            print(data, end="")
            if "DONE!" in data:
                break
        ws.close()

        with open("ws_vlans.log", "w", encoding="utf-8") as f:
            f.write("".join(output_lines))
        print("VLANs output saved to ws_vlans.log")

    except Exception as e:
        print(f"Error during shell session: {e}")

def append_delay_metrics_log(delay_metrics, api_cache, tuning_data, filename="delay_metrics.json", max_entries=100):
    """
    Appends delay metrics, API cache, and tuning data to a JSON file.
    Each call writes a new line with a timestamped entry.
    Maintains only the last max_entries (default 100) to prevent unlimited file growth.
    """
    logging.debug(f"ENTRY: append_delay_metrics_log(filename={filename}, max_entries={max_entries})")
    
    # SECURITY: File path is forced into data/ directory unless caller provides an explicit path.
    # This prevents creating arbitrary files in the application root (permission errors in container) or unsafe paths.
    if filename == "delay_metrics.json":
        try:
            data_directory = "data"
            os.makedirs(data_directory, exist_ok=True)
            filename = os.path.join(data_directory, filename)
        except Exception as directory_creation_error:
            logging.error(f"File I/O: Failed to ensure data directory for delay metrics file: {directory_creation_error}")
            # Fall back to original filename; subsequent write may fail but we continue safely.

    log_entry = {
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "delay_metrics": delay_metrics,
        "api_cache": api_cache,
        "tuning_data": tuning_data
    }
    
    try:
        # Read existing entries if file exists
        existing_entries = []
        if os.path.exists(filename):
            try:
                with open(filename, "r", encoding="utf-8") as f:
                    for line in f:
                        line = line.strip()
                        if line:
                            existing_entries.append(json.loads(line))
                logging.debug(f"File I/O: Loaded {len(existing_entries)} existing entries from {filename}")
            except (json.JSONDecodeError, OSError) as e:
                logging.warning(f"File I/O: Failed to read existing entries from {filename}: {e}. Starting fresh.")
                existing_entries = []
        
        # Add new entry and keep only the last max_entries
        existing_entries.append(log_entry)
        if len(existing_entries) > max_entries:
            existing_entries = existing_entries[-max_entries:]
            logging.debug(f"File I/O: Trimmed to last {max_entries} entries")
        
        # Write all entries back to file
        logging.debug(f"File I/O: Writing {len(existing_entries)} entries to {filename}")
        with open(filename, "w", encoding="utf-8") as f:
            for entry in existing_entries:
                json.dump(entry, f)
                f.write("\n")
        
        logging.debug(f"File I/O: Successfully updated delay metrics in {filename}")
        logging.debug(f"EXIT: append_delay_metrics_log - success")
    except OSError as e:
        logging.error(f"File I/O: OS error writing delay metrics to {filename}: {e}")
        logging.debug(f"EXIT: append_delay_metrics_log - OS error")
    except Exception as e:
        logging.error(f"File I/O: Failed to write delay metrics to {filename}: {e}")
        logging.debug(f"EXIT: append_delay_metrics_log - error")

def export_gateway_device_configs_to_csv(debug=False, fast=False):
    """
    Fetches and exports configuration details for all gateway devices across all sites in the organization
    to AllSiteGatewayConfigs.csv. Also generates a filtered CSV with selected fields and port info.
    """
    logging.info("Starting export of all gateway device configurations...")
    org_id = get_cached_or_prompted_org_id()
    data = fetch_gateway_device_configs_from_api(apisession, org_id, fast=fast)
    if not data:
        logging.warning(" No device configs found.")
        return

    # Flatten and sanitize the data
    flattened = flatten_nested_fields_in_list(data)
    sanitized = escape_multiline_strings_for_csv(flattened)

    # Write full dataset to CSV
    DataExporter.save_data_to_output(sanitized, "AllSiteGatewayConfigs.csv")
    logging.info(" Device configs saved to AllSiteGatewayConfigs.csv")

    # Identify port config columns (excluding _vpn_paths_)
    base_columns = ["mac", "name"]
    port_columns = [
        col for col in sanitized[0].keys()
        if re.match(r"(?i)port_config_ge-0/0/\d+_.*", col) and "_vpn_paths_" not in col
    ]
    columns_to_keep = base_columns + port_columns

    # Filter rows where any port column has non-empty value
    filtered_rows = [
        {col: row.get(col, "") for col in columns_to_keep}
        for row in sanitized
        if any(row.get(col) not in [None, "", "null"] for col in port_columns)
    ]

    # Write filtered dataset to CSV
    if not filtered_rows:
        logging.warning(" No rows matched the port config filter. FilteredGatewayPortConfigs.csv will be empty.")
        filtered_csv_path = get_csv_file_path("FilteredGatewayPortConfigs.csv")
        with open(filtered_csv_path, "w", newline="", encoding="utf-8") as f:
            f.write("No matching data found.\n")
    else:
        if debug:
            logging.debug(f"Sample filtered row: {filtered_rows[0]}")
        DataExporter.save_data_to_output(filtered_rows, "FilteredGatewayPortConfigs.csv")
        logging.info(" Filtered gateway port configs saved to FilteredGatewayPortConfigs.csv")

def fetch_gateway_device_configs_from_api(apisession, org_id, fast=False, max_workers=None):
    """
    Fetches configuration details for all gateway devices in the org using org inventory.
    If `fast` is True, fetches each device config concurrently using connection pool management.
    
    Args:
        apisession: Authenticated Mist API session.
        org_id: Organization ID.
        fast (bool): If True, enables high-concurrency mode with connection pool management.
        max_workers (int): Optional override for number of concurrent threads.
    
    Returns:
        List of device configuration dictionaries.
    """
    logging.info("Fetching org inventory to find gateway devices...")
    try:
        response = mistapi.api.v1.orgs.inventory.getOrgInventory(apisession, org_id, limit=1000)
        inventory = mistapi.get_all(response=response, mist_session=apisession)
    except Exception as e:
        logging.error(f"! Failed to fetch org inventory: {e}")
        return []

    logging.info(f"Found {len(inventory)} total devices in org inventory.")

    # Load site names from SiteList.csv for enrichment
    site_name_lookup = {}
    try:
        site_list_path = get_csv_file_path("SiteList.csv")
        with open(site_list_path, mode="r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            site_name_lookup = {row.get("id"): row.get("name", "Unnamed Site") for row in reader}
    except Exception as e:
        logging.warning(f"! Failed to load SiteList.csv for site names: {e}")

    # Filter for gateway devices and build work list
    work_items = []
    for device in inventory:
        if device.get("type") == "gateway":
            site_id = device.get("site_id")
            device_id = device.get("id")
            if site_id and device_id:
                site_name = site_name_lookup.get(site_id, "Unknown")
                work_items.append((site_id, device_id, site_name))

    logging.info(f"Prepared {len(work_items)} gateway device config API calls.")

    def fetch_config(work_item, connection_semaphore):
        """Fetch configuration for a single device with retry logic."""
        site_id, device_id, site_name = work_item
        
        with connection_semaphore:  # Limit concurrent connections
            try:
                logging.debug(f"Fetching config for {device_id} ({site_name})")
                response = mistapi.api.v1.sites.devices.getSiteDevice(apisession, site_id, device_id)
                config = getattr(response, "data", {})
                if config:
                    # Add site metadata for enrichment
                    config["site_name"] = site_name
                    config["site_id"] = site_id
                    logging.debug(f"! Config fetched for {device_id}")
                    return config
                else:
                    logging.warning(f"! Empty config for device {device_id}")
            except Exception as e:
                logging.error(f"! Failed to fetch config for device {device_id}: {e}")
                return None

    def retry_fetch_config(failed_items, connection_semaphore):
        """Retry wrapper for device config fetching."""
        max_retries = int(os.getenv('FAST_MODE_SEQUENTIAL_MAX_RETRIES', '1'))
        retry_results = []
        
        for work_item in failed_items:
            site_id, device_id, site_name = work_item
            
            for attempt in range(max_retries + 1):
                result = fetch_config(work_item, connection_semaphore)
                if result is not None:
                    retry_results.append(result)
                    break
                if attempt < max_retries:
                    delay = 0.5 * (1.5 ** attempt)  # Exponential backoff
                    logging.debug(f"Retrying device {device_id} in {delay:.2f}s (attempt {attempt + 2}/{max_retries + 1})")
                    time.sleep(delay)
            else:
                logging.warning(f"! Failed to fetch config for device {device_id} after {max_retries + 1} attempts")
        
        return retry_results

    # Use connection pool management helper if fast mode is enabled
    if fast:
        successful_results, failed_items = execute_with_connection_pool_management(
            work_items=work_items,
            worker_function=fetch_config,
            batch_description="gateway device configs",
            retry_function=retry_fetch_config
        )
        all_device_configs = successful_results
    else:
        # Sequential processing for non-fast mode
        all_device_configs = []
        # Create a dummy semaphore for sequential processing
        dummy_semaphore = threading.Semaphore(1)
        
        for work_item in tqdm(work_items, desc="Fetching Configs", unit="device"):
            result = fetch_config(work_item, dummy_semaphore)
            if result is not None:
                all_device_configs.append(result)

    # Filter out None results
    all_device_configs = [config for config in all_device_configs if config is not None]
    
    logging.info(f"! Completed fetching {len(all_device_configs)} gateway device configs.")
    return all_device_configs

def get_rate_limited_delay(smoothed_delay=None):
    """
    Calculates an appropriate delay for API rate limiting using PID control.
    Includes comprehensive logging for tuning and backoff mechanisms.
    """
    logging.debug(f"ENTRY: get_rate_limited_delay(smoothed_delay={smoothed_delay})")
    
    global _api_usage_cache
    tuning_data = load_pid_tuning_data()
    logging.debug(f"Loaded PID tuning data: k_p={tuning_data.get('k_p')}, k_i={tuning_data.get('k_i')}, integral={tuning_data.get('integral')}")

    # Reset gains if out of bounds
    if tuning_data["k_p"] < 1e-6 or tuning_data["k_i"] < 1e-8 or tuning_data["k_p"] > 1.0 or tuning_data["k_i"] > 0.01:
        logging.warning(f"PID gains out of bounds, resetting: k_p={tuning_data['k_p']}, k_i={tuning_data['k_i']}")
        tuning_data["k_p"] = 0.1
        tuning_data["k_i"] = 0.001

    k_p = float(tuning_data["k_p"])
    k_i = float(tuning_data["k_i"])
    delay_integral = float(tuning_data.get("integral", 0.0))
    error_history = tuning_data.get("error", [])

    try:
        now = datetime.now(timezone.utc)
        current_time = time.time()
        elapsed = current_time - _api_usage_cache["last_updated"]
        previous_elapsed = float(_api_usage_cache.get("previous_elapsed", elapsed))

        # Hybrid refresh trigger: every 60s, every 100 requests, or top of the hour
        refresh_needed = (
            not _api_usage_cache["initialized"]
            or _api_usage_cache["perceived_requests"] >= 100
            or elapsed > 60
            or (now.minute == 0 and now.second < 5)
        )
        
        if refresh_needed:
            logging.debug(f"Refreshing API usage cache - elapsed: {elapsed:.1f}s, perceived_requests: {_api_usage_cache['perceived_requests']}")
            try:
                usage = mistapi.api.v1.self.usage.getSelfApiUsage(apisession).data
                _api_usage_cache["used"] = usage.get("requests", 0)
                _api_usage_cache["limit"] = usage.get("request_limit", 5000)
                _api_usage_cache["last_updated"] = current_time
                _api_usage_cache["perceived_requests"] = 0
                _api_usage_cache["initialized"] = True
                logging.debug(f"API usage refreshed: {_api_usage_cache['used']}/{_api_usage_cache['limit']} requests")
            except Exception as api_e:
                logging.warning(f"Failed to refresh API usage data: {api_e}. Using cached values.")
        else:
            estimated_growth = round((_api_usage_cache["limit"] / 3600) * elapsed)
            _api_usage_cache["used"] += estimated_growth
            _api_usage_cache["last_updated"] = current_time
            _api_usage_cache["perceived_requests"] += 1
            logging.debug(f"Using estimated API usage: {_api_usage_cache['used']}/{_api_usage_cache['limit']} requests")

        used = min(_api_usage_cache["used"], _api_usage_cache["limit"])
        limit = _api_usage_cache["limit"]

        seconds_elapsed = now.minute * 60 + now.second + now.microsecond / 1_000_000
        seconds_remaining = max(3600 - seconds_elapsed, 1)
        ideal_used = (seconds_elapsed / 3600) * limit
        error = used - ideal_used

        # Detect hour boundary and decay integral
        if seconds_elapsed < previous_elapsed:
            logging.info(" Hour boundary crossed. Resetting integral.")
            logging.debug(f"Before reset: delay_integral={delay_integral} (type: {type(delay_integral)})")
            delay_integral *= 0.5
            logging.debug(f"After reset: delay_integral={delay_integral} (type: {type(delay_integral)})")

        _api_usage_cache["previous_elapsed"] = seconds_elapsed

        remaining_requests = max(limit - used, 1)
        base_delay = min(seconds_remaining / remaining_requests, 10)

        unsat_delay = base_delay + k_p * error + k_i * delay_integral
        sat_delay = max(min(unsat_delay, 10), 0.01)

        # Log backoff calculation details
        if sat_delay > 2.0:
            logging.warning(f"High delay calculated: {sat_delay:.3f}s (base: {base_delay:.3f}s, error: {error:.1f}, used: {used}/{limit})")
        elif sat_delay > 1.0:
            logging.info(f"Moderate delay calculated: {sat_delay:.3f}s (used: {used}/{limit})")
        else:
            logging.debug(f"Normal delay calculated: {sat_delay:.3f}s (used: {used}/{limit})")

        # Adaptive back_calc_gain
        back_calc_gain = min(max(abs(sat_delay - unsat_delay) / 10, 0.01), 0.5)

        # Decaying integral update
        decay_factor = 0.98
        delay_integral = delay_integral * decay_factor + back_calc_gain * (sat_delay - unsat_delay)
        delay_integral = max(min(delay_integral, 1000), -1000)

        # Ensure error is a valid number before adding to history
        if isinstance(error, (int, float)) and not (math.isnan(error) or math.isinf(error)):
            error_history.append(float(error))
        else:
            logging.warning(f"Invalid error value: {error}. Skipping addition to error history.")
        
        # Clean error_history before computing alpha to ensure all values are numeric
        cleaned_error_history = []
        for err in error_history:
            try:
                # Try to convert to float
                if err is not None:
                    float_val = float(err)
                    # Check if it's a valid finite number
                    if not (math.isnan(float_val) or math.isinf(float_val)):
                        cleaned_error_history.append(float_val)
            except (ValueError, TypeError):
                # Skip values that can't be converted to float
                continue
        
        logging.debug(f"About to call compute_dynamic_alpha with cleaned_error_history={cleaned_error_history} (length: {len(cleaned_error_history)})")
        alpha = compute_dynamic_alpha(cleaned_error_history)
        logging.debug(f"compute_dynamic_alpha returned: {alpha} (type: {type(alpha)})")
        
        # Defensive type checking - ensure alpha is a valid float
        if not isinstance(alpha, (int, float)) or math.isnan(alpha) or math.isinf(alpha):
            logging.warning(f"Invalid alpha value: {alpha} (type: {type(alpha)}). Using fallback 0.3")
            alpha = 0.3

        smoothed_delay = sat_delay if smoothed_delay is None else alpha * sat_delay + (1 - alpha) * smoothed_delay
        delay_in_seconds = max(smoothed_delay, 0.01)

        logging.info(f"Rate limiting: sleeping for {delay_in_seconds:.3f} seconds")

        # Save updated tuning data using cleaned error history
        tuning_data["error"] = cleaned_error_history[-20:]  # Use cleaned history and keep only last 20 entries
        tuning_data["integral"] = delay_integral
        tuning_data["back_calc_gain"] = back_calc_gain
        adjust_gains(tuning_data)
        save_pid_tuning_data(tuning_data)

        delay_metrics = {
            "used": used,
            "limit": limit,
            "error": error,
            "base_delay": base_delay,
            "unsat_delay": unsat_delay,
            "final_delay": delay_in_seconds,
            "alpha": alpha
        }
        append_delay_metrics_log(delay_metrics, _api_usage_cache, tuning_data)

        logging.debug(f"EXIT: get_rate_limited_delay - delay: {delay_in_seconds:.3f}s")
        return smoothed_delay, delay_in_seconds

    except Exception as e:
        logging.error(f"Failed to calculate dynamic delay: {e}. Using default 500ms fallback delay.")
        logging.debug(f"EXIT: get_rate_limited_delay - error fallback")
        return smoothed_delay, 0.5

def export_combined_inventory_with_site_info():
    """
    Combines fresh AllDevicesWithSiteInfo data into multiple CSV files
    grouped by calendar week based on 'created_time' field.
    Also generates a summary report with device counts per week.
    
    Outputs:
        - Weekly CSV files: data/CombinedInventory_ByWeek/YYYY_Week_##.csv
        - Summary report: data/CombinedInventory_ByWeek/CombinedInventory_Summary.csv
        - Master CSV: data/CombinedInventory_ByWeek/CombinedInventory_Master.csv
          (with simplified headers: serial, model, Street Address, City, State, Zip)
    """
    print("Combined Inventory with Site Info by Calendar Week:")

    # Load environment variables
    load_dotenv()
    END_CUSTOMER_NAME = os.getenv("END_CUSTOMER_NAME")
    END_CUSTOMER_ACCOUNT_ID = os.getenv("END_CUSTOMER_ACCOUNT_ID")

    # Always regenerate fresh data
    export_devices_with_site_info_to_csv()

    # Load the enriched device + site info
    devices_with_site_info_path = get_csv_file_path("AllDevicesWithSiteInfo.csv")
    with open(devices_with_site_info_path, mode="r", encoding="utf-8") as f:
        site_configs = list(csv.DictReader(f))

    # Create a subfolder for weekly CSV files in the data directory
    output_folder = os.path.join("data", "CombinedInventory_ByWeek")
    os.makedirs(output_folder, exist_ok=True)

    # Initialize data structures for weekly grouping and summary
    weekly_data = defaultdict(list)
    summary_data = defaultdict(int)

    # Process each device entry
    for device in site_configs:
        try:
            created_time = int(device.get("created_time", 0))
            created_date = datetime.fromtimestamp(created_time, tz=timezone.utc)
            year, week, _ = created_date.isocalendar()
            week_key = f"{year}_Week_{week:02d}"

            weekly_data[week_key].append({
                "Full Site": device.get("site_name", ""),
                "System Serial Number": device.get("serial", ""),
                "System Model Number": device.get("model", ""),
                "End Customer Name": END_CUSTOMER_NAME,
                "Address Line 1": device.get("street", ""),
                "Address Line 2": "",
                "City": device.get("city", ""),
                "State": device.get("state", ""),
                "Country": device.get("country", "US"),
                "Zip Code / Postal Code": device.get("zip_code", ""),
                "End Customer Account ID": END_CUSTOMER_ACCOUNT_ID
            })

            summary_data[(year, week)] += 1
        except Exception as e:
            logging.warning(f"! Skipping device due to error: {e}")

    # Define output CSV columns
    fieldnames = [
        "Full Site", "System Serial Number", "System Model Number", "End Customer Name",
        "Address Line 1", "Address Line 2", "City", "State", "Country",
        "Zip Code / Postal Code", "End Customer Account ID"
    ]

    # Write weekly CSV files
    for week_key, rows in weekly_data.items():
        output_file = os.path.join(output_folder, f"{week_key}.csv")
        with open(output_file, mode="w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(rows)

    # Write summary report
    summary_file = os.path.join(output_folder, "CombinedInventory_Summary.csv")
    with open(summary_file, mode="w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["Year", "Week", "Device Count"])
        for (year, week), count in sorted(summary_data.items()):
            writer.writerow([year, week, count])

    # Export master CSV with simplified column headers
    master_csv_data = []
    for device in site_configs:
        master_csv_data.append({
            "serial": device.get("serial", ""),
            "model": device.get("model", ""),
            "Street Address": device.get("street", ""),
            "City": device.get("city", ""),
            "State": device.get("state", ""),
            "Zip": device.get("zip_code", "")
        })
    
    master_csv_file = os.path.join(output_folder, "CombinedInventory_Master.csv")
    master_csv_fieldnames = ["serial", "model", "Street Address", "City", "State", "Zip"]
    with open(master_csv_file, mode="w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=master_csv_fieldnames)
        writer.writeheader()
        writer.writerows(master_csv_data)

    # Count the total weekly files created
    total_weeks = len(weekly_data)
    total_devices = len(site_configs)
    print(f"! {total_weeks} weekly CSV files created in data/CombinedInventory_ByWeek/ folder ({total_devices} total devices processed)")
    print(f"! Summary report exported to data/CombinedInventory_ByWeek/CombinedInventory_Summary.csv")
    print(f"! Master inventory exported to data/CombinedInventory_ByWeek/CombinedInventory_Master.csv ({len(master_csv_data)} devices)")

def normalize_zip_code(zip_code):
    """
    Normalizes a zip code to compare only the first 5 digits:
    - Removes everything after and including a dash
    - If only 4 digits before dash, prepends a '0'
    - Returns first 5 digits only
    """
    if not zip_code:
        return ""
    
    # Convert to string and strip whitespace
    zip_str = str(zip_code).strip()
    
    # Remove everything after and including a dash
    if '-' in zip_str:
        zip_str = zip_str.split('-')[0]
    
    # Remove any non-digit characters
    zip_digits = ''.join(filter(str.isdigit, zip_str))
    
    # If only 4 digits, prepend a '0'
    if len(zip_digits) == 4:
        zip_digits = '0' + zip_digits
    
    # Return first 5 digits
    return zip_digits[:5]

def normalize_state_name(state_str):
    """
    Normalizes state names and abbreviations to a consistent format.
    Converts both full state names and abbreviations to lowercase abbreviations.
    """
    if not state_str:
        return ""
    
    # Convert to lowercase and strip
    state = state_str.lower().strip()
    
    # State name to abbreviation mapping
    state_mapping = {
        # Full names to abbreviations
        'alabama': 'al', 'alaska': 'ak', 'arizona': 'az', 'arkansas': 'ar', 'california': 'ca',
        'colorado': 'co', 'connecticut': 'ct', 'delaware': 'de', 'florida': 'fl', 'georgia': 'ga',
        'hawaii': 'hi', 'idaho': 'id', 'illinois': 'il', 'indiana': 'in', 'iowa': 'ia',
        'kansas': 'ks', 'kentucky': 'ky', 'louisiana': 'la', 'maine': 'me', 'maryland': 'md',
        'massachusetts': 'ma', 'michigan': 'mi', 'minnesota': 'mn', 'mississippi': 'ms', 'missouri': 'mo',
        'montana': 'mt', 'nebraska': 'ne', 'nevada': 'nv', 'new hampshire': 'nh', 'new jersey': 'nj',
        'new mexico': 'nm', 'new york': 'ny', 'north carolina': 'nc', 'north dakota': 'nd', 'ohio': 'oh',
        'oklahoma': 'ok', 'oregon': 'or', 'pennsylvania': 'pa', 'rhode island': 'ri', 'south carolina': 'sc',
        'south dakota': 'sd', 'tennessee': 'tn', 'texas': 'tx', 'utah': 'ut', 'vermont': 'vt',
        'virginia': 'va', 'washington': 'wa', 'west virginia': 'wv', 'wisconsin': 'wi', 'wyoming': 'wy',
        'district of columbia': 'dc',
        
        # Abbreviations to themselves (normalized to lowercase)
        'al': 'al', 'ak': 'ak', 'az': 'az', 'ar': 'ar', 'ca': 'ca', 'co': 'co', 'ct': 'ct',
        'de': 'de', 'fl': 'fl', 'ga': 'ga', 'hi': 'hi', 'id': 'id', 'il': 'il', 'in': 'in',
        'ia': 'ia', 'ks': 'ks', 'ky': 'ky', 'la': 'la', 'me': 'me', 'md': 'md', 'ma': 'ma',
        'mi': 'mi', 'mn': 'mn', 'ms': 'ms', 'mo': 'mo', 'mt': 'mt', 'ne': 'ne', 'nv': 'nv',
        'nh': 'nh', 'nj': 'nj', 'nm': 'nm', 'ny': 'ny', 'nc': 'nc', 'nd': 'nd', 'oh': 'oh',
        'ok': 'ok', 'or': 'or', 'pa': 'pa', 'ri': 'ri', 'sc': 'sc', 'sd': 'sd', 'tn': 'tn',
        'tx': 'tx', 'ut': 'ut', 'vt': 'vt', 'va': 'va', 'wa': 'wa', 'wv': 'wv', 'wi': 'wi',
        'wy': 'wy', 'dc': 'dc'
    }
    
    return state_mapping.get(state, state)

def validate_addresses_with_nominatim(mist_address, comparison_address, timeout=5, debug=False, skip_ssl_verify=False, org_name=None, mist_duplicates=None, ref_duplicates=None, site_name=None):
    """
    Validate both address sets against Nominatim (OpenStreetMap) geocoding API
    to determine which is more accurate/complete.
    
    Args:
        mist_address (dict): Mist address data
        comparison_address (dict): Comparison CSV address data  
        timeout (int): HTTP request timeout in seconds
        debug (bool): If True, enables detailed debug logging for API requests and responses
        org_name (str): Organization name for intelligent tiebreaking
        mist_duplicates (dict): Dictionary of duplicate Mist addresses {addr_key: [site_names]}
        ref_duplicates (dict): Dictionary of duplicate reference addresses {addr_key: [site_names]}
        site_name (str): Current site name being processed
        
    Returns:
        dict: {
            'mist_validation': {'valid': bool, 'confidence': float, 'lat': float, 'lon': float},
            'comparison_validation': {'valid': bool, 'confidence': float, 'lat': float, 'lon': float},
            'recommendation': str  # 'mist', 'comparison', or 'uncertain'
        }
    """
    
    # Suppress SSL warnings if skip_ssl_verify is enabled
    if skip_ssl_verify:
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        if debug:
            logging.warning("SSL certificate verification disabled - urllib3 warnings suppressed")
    
    if debug:
        logging.debug("ENTRY: validate_addresses_with_nominatim()")
        logging.debug(f"  mist_address: {mist_address}")
        logging.debug(f"  comparison_address: {comparison_address}")
        logging.debug(f"  timeout: {timeout}")
        logging.debug(f"  skip_ssl_verify: {skip_ssl_verify}")
    
    def geocode_address(address_dict, address_source="unknown"):
        """Geocode a single address using Nominatim"""
        try:
            # Build address string
            address_parts = []
            if address_dict.get('address'):
                address_parts.append(address_dict['address'])
            if address_dict.get('city'):
                address_parts.append(address_dict['city'])
            if address_dict.get('state'):
                address_parts.append(address_dict['state'])
            if address_dict.get('zip'):
                address_parts.append(address_dict['zip'])
                
            if not address_parts:
                if debug:
                    logging.debug(f"GEOCODE [{address_source}]: No address parts available")
                return {'valid': False, 'confidence': 0.0, 'lat': None, 'lon': None, 'error': 'Empty address'}
            
            address_string = ', '.join(address_parts)
            if debug:
                logging.debug(f"GEOCODE [{address_source}]: Built address string: '{address_string}'")
            
            # Nominatim API call (respect 1 req/sec limit)
            url = "https://nominatim.openstreetmap.org/search"
            params = {
                'format': 'json',
                'q': address_string,
                'limit': 1,
                'addressdetails': 1
            }
            headers = {
                'User-Agent': 'MistHelper/1.0 (address validation)'
            }
            
            if debug:
                logging.debug(f"GEOCODE [{address_source}]: Making API request to {url}")
                logging.debug(f"GEOCODE [{address_source}]: Request params: {params}")
                logging.debug(f"GEOCODE [{address_source}]: Request headers: {headers}")
                if skip_ssl_verify:
                    logging.warning(f"GEOCODE [{address_source}]: SSL certificate verification DISABLED for this request")
            
            # Configure SSL verification based on parameter
            verify_ssl = not skip_ssl_verify
            if skip_ssl_verify and debug:
                logging.debug(f"GEOCODE [{address_source}]: SSL verification bypassed (verify={verify_ssl})")
            
            # Retry logic for timeout errors
            max_retries = 2
            retry_delay = 2  # seconds
            
            for attempt in range(max_retries + 1):
                try:
                    # Increase timeout for better reliability
                    actual_timeout = timeout + (attempt * 5)  # Increase timeout on retries
                    if debug and attempt > 0:
                        logging.debug(f"GEOCODE [{address_source}]: Retry attempt {attempt} with timeout {actual_timeout}s")
                    
                    response = requests.get(url, params=params, headers=headers, timeout=actual_timeout, verify=verify_ssl)
                    
                    if debug:
                        logging.debug(f"GEOCODE [{address_source}]: Response status: {response.status_code}")
                        logging.debug(f"GEOCODE [{address_source}]: Response headers: {dict(response.headers)}")
                        logging.debug(f"GEOCODE [{address_source}]: Response content length: {len(response.content)} bytes")
                    
                    # If we get here, the request succeeded, break out of retry loop
                    break
                    
                except (requests.exceptions.ReadTimeout, requests.exceptions.ConnectTimeout) as timeout_error:
                    if attempt < max_retries:
                        if debug:
                            logging.debug(f"GEOCODE [{address_source}]: Timeout on attempt {attempt + 1}, retrying in {retry_delay}s...")
                        time.sleep(retry_delay)
                        continue
                    else:
                        # Final retry failed, raise the timeout error
                        if debug:
                            logging.debug(f"GEOCODE [{address_source}]: All retry attempts failed due to timeout")
                        raise timeout_error
                except Exception as e:
                    # Non-timeout errors should not be retried
                    raise e
            
            if response.status_code == 200:
                results = response.json()
                if debug:
                    logging.debug(f"GEOCODE [{address_source}]: Response JSON: {results}")
                
                if results:
                    result = results[0]
                    
                    # Calculate confidence from multiple Nominatim fields
                    confidence = 0.0
                    importance = float(result.get('importance', 0.0))
                    
                    if debug:
                        logging.debug(f"GEOCODE [{address_source}]: Raw importance from Nominatim: {importance}")
                        logging.debug(f"GEOCODE [{address_source}]: Available result fields: {list(result.keys())}")
                    
                    # Use importance if available and meaningful, otherwise calculate from match quality
                    if importance > 0.01:  # Only use importance if it's above a minimal threshold
                        confidence = min(1.0, importance * 2.0)  # Scale importance up as it's often small
                        if debug:
                            logging.debug(f"GEOCODE [{address_source}]: Using scaled importance: {confidence:.3f}")
                    else:
                        # Calculate confidence based on address components matched and result quality
                        display_name = result.get('display_name', '').lower()
                        address_string_lower = address_string.lower()
                        
                        if debug:
                            logging.debug(f"GEOCODE [{address_source}]: Calculating custom confidence")
                            logging.debug(f"GEOCODE [{address_source}]: Address string: '{address_string_lower}'")
                            logging.debug(f"GEOCODE [{address_source}]: Display name: '{display_name}'")
                        
                        # Count matching components with partial matching
                        match_score = 0.0
                        total_components = len(address_parts)
                        
                        for part in address_parts:
                            part_clean = part.lower().strip()
                            if len(part_clean) > 2:  # Only check meaningful parts
                                # Full match gets full point
                                if part_clean in display_name:
                                    match_score += 1.0
                                    if debug:
                                        logging.debug(f"GEOCODE [{address_source}]: Full match for '{part_clean}'")
                                # Partial match gets partial point
                                elif any(word in display_name for word in part_clean.split() if len(word) > 2):
                                    match_score += 0.5
                                    if debug:
                                        logging.debug(f"GEOCODE [{address_source}]: Partial match for '{part_clean}'")
                        
                        # Base confidence from component matching
                        component_conf = match_score / total_components if total_components > 0 else 0.0
                        
                        # Quality indicators from Nominatim result
                        quality_boost = 0.0
                        
                        # Place type quality boost
                        place_type = result.get('type', '').lower()
                        place_class = result.get('class', '').lower()
                        
                        if place_type in ['house', 'building', 'commercial', 'office', 'retail', 'shop']:
                            quality_boost += 0.3
                        elif place_type in ['residential', 'industrial', 'public']:
                            quality_boost += 0.2
                        elif place_class in ['building', 'place', 'amenity']:
                            quality_boost += 0.1
                            
                        # Address detail quality (more specific = higher confidence)
                        if result.get('address', {}):
                            address_details = result.get('address', {})
                            detail_count = len([v for v in address_details.values() if v])
                            if detail_count >= 5:  # Street, city, state, postcode, country
                                quality_boost += 0.2
                            elif detail_count >= 3:
                                quality_boost += 0.1
                        
                        # Combine component matching with quality indicators
                        confidence = min(1.0, component_conf + quality_boost)
                        
                        if debug:
                            logging.debug(f"GEOCODE [{address_source}]: Component confidence: {component_conf:.3f}")
                            logging.debug(f"GEOCODE [{address_source}]: Quality boost: {quality_boost:.3f}")
                            logging.debug(f"GEOCODE [{address_source}]: Place type: '{place_type}', Class: '{place_class}'")
                            logging.debug(f"GEOCODE [{address_source}]: Final confidence: {confidence:.3f}")
                    
                    geocode_result = {
                        'valid': True,
                        'confidence': confidence,
                        'lat': float(result['lat']),
                        'lon': float(result['lon']),
                        'display_name': result.get('display_name', ''),
                        'place_type': result.get('type', ''),
                        'place_class': result.get('class', ''),
                        'address_details': result.get('address', {}),
                        'error': None
                    }
                    if debug:
                        logging.debug(f"GEOCODE [{address_source}]: SUCCESS - confidence: {confidence:.3f}, lat: {result['lat']}, lon: {result['lon']}")
                        logging.debug(f"GEOCODE [{address_source}]: Display name: {result.get('display_name', '')}")
                        logging.debug(f"GEOCODE [{address_source}]: Place type: {result.get('type', '')}")
                    return geocode_result
                else:
                    if debug:
                        logging.debug(f"GEOCODE [{address_source}]: No results found in API response")
                    return {'valid': False, 'confidence': 0.0, 'lat': None, 'lon': None, 'error': 'No results found'}
            else:
                if debug:
                    logging.debug(f"GEOCODE [{address_source}]: HTTP error {response.status_code}")
                    logging.debug(f"GEOCODE [{address_source}]: Response content: {response.content[:200]}...")
                return {'valid': False, 'confidence': 0.0, 'lat': None, 'lon': None, 'error': f'HTTP {response.status_code}'}
                
        except Exception as e:
            if debug:
                logging.debug(f"GEOCODE [{address_source}]: Exception occurred: {str(e)}")
                logging.debug(f"GEOCODE [{address_source}]: Full traceback: {traceback.format_exc()}")
            return {'valid': False, 'confidence': 0.0, 'lat': None, 'lon': None, 'error': str(e)}
    
    # Validate both addresses (with rate limiting)
    if debug:
        logging.debug("Starting mist address validation...")
    mist_result = geocode_address(mist_address, "MIST")
    
    if debug:
        logging.debug("Applying rate limiting delay (1.1 seconds)...")
    time.sleep(1.1)  # Respect Nominatim 1 req/sec limit
    
    if debug:
        logging.debug("Starting comparison address validation...")
    comparison_result = geocode_address(comparison_address, "COMPARISON")
    
    if debug:
        logging.debug(f"Mist validation result: {mist_result}")
        logging.debug(f"Comparison validation result: {comparison_result}")
    
    # Determine recommendation based on validation results
    recommendation = 'uncertain'
    recommendation_reason = "Unknown"
    
    if mist_result['valid'] and not comparison_result['valid']:
        recommendation = 'mist'
        recommendation_reason = f"Only Mist address is valid (confidence: {mist_result['confidence']:.3f})"
        if debug:
            logging.debug("Recommendation: MIST (only mist address is valid)")
    elif comparison_result['valid'] and not mist_result['valid']:
        recommendation = 'comparison'
        recommendation_reason = f"Only reference address is valid (confidence: {comparison_result['confidence']:.3f})"
        if debug:
            logging.debug("Recommendation: COMPARISON (only comparison address is valid)")
    elif mist_result['valid'] and comparison_result['valid']:
        # Both addresses are valid - need intelligent tiebreaker logic
        if debug:
            logging.debug("TIEBREAKER: Both addresses validated successfully, applying tiebreaker logic")
        
        # Check for duplicate address disqualification first
        mist_is_duplicate = False
        ref_is_duplicate = False
        
        if mist_duplicates and site_name:
            # Create address key for Mist address
            mist_addr_key = f"{mist_address['address'].lower()}|{mist_address['city'].lower()}|{mist_address['state'].lower()}|{mist_address['zip']}"
            mist_is_duplicate = mist_addr_key in mist_duplicates
            if debug and mist_is_duplicate:
                logging.debug(f"TIEBREAKER: Mist address is duplicate - shared by sites: {mist_duplicates[mist_addr_key]}")
        
        if ref_duplicates and site_name:
            # Create address key for reference address
            ref_addr_key = f"{comparison_address['address'].lower()}|{comparison_address['city'].lower()}|{comparison_address['state'].lower()}|{comparison_address['zip']}"
            ref_is_duplicate = ref_addr_key in ref_duplicates
            if debug and ref_is_duplicate:
                logging.debug(f"TIEBREAKER: Reference address is duplicate - shared by sites: {ref_duplicates[ref_addr_key]}")
        
        # Apply duplicate disqualification logic
        if mist_is_duplicate and ref_is_duplicate:
            recommendation = 'uncertain' 
            recommendation_reason = "  Both addresses are duplicates (shared between multiple sites) - manual review required"
            if debug:
                logging.debug("TIEBREAKER: Both addresses are duplicates - flagging as uncertain")
        elif mist_is_duplicate and not ref_is_duplicate:
            recommendation = 'comparison'
            recommendation_reason = "Mist address is duplicate (shared between sites), using reference address"
            if debug:
                logging.debug("TIEBREAKER: Mist address is duplicate, recommending reference")
        elif ref_is_duplicate and not mist_is_duplicate:
            recommendation = 'mist'
            recommendation_reason = "Reference address is duplicate (shared between sites), using Mist address"
            if debug:
                logging.debug("TIEBREAKER: Reference address is duplicate, recommending Mist")
        else:
            # No duplicates detected, proceed with standard tiebreaker logic
            # Both valid - compare confidence scores
            if mist_result['confidence'] > comparison_result['confidence'] * 1.1:  # 10% threshold
                recommendation = 'mist'
                recommendation_reason = f"Mist address has higher confidence ({mist_result['confidence']:.3f} vs {comparison_result['confidence']:.3f})"
                if debug:
                    logging.debug(f"Recommendation: MIST (higher confidence: {mist_result['confidence']:.3f} vs {comparison_result['confidence']:.3f})")
            elif comparison_result['confidence'] > mist_result['confidence'] * 1.1:
                recommendation = 'comparison'
                recommendation_reason = f"Reference address has higher confidence ({comparison_result['confidence']:.3f} vs {mist_result['confidence']:.3f})"
                if debug:
                    logging.debug(f"Recommendation: COMPARISON (higher confidence: {comparison_result['confidence']:.3f} vs {mist_result['confidence']:.3f})")
            else:
                # Confidence scores are too close - use intelligent tiebreaker
                recommendation = 'uncertain'
                recommendation_reason = f"Both addresses have similar confidence ({mist_result['confidence']:.3f} vs {comparison_result['confidence']:.3f})"
                
                if org_name and debug:
                    logging.debug(f"Both addresses valid with similar confidence - applying organization name tiebreaker with org: '{org_name}'")
                
                # Intelligent tiebreaker using organization name and business context
                if org_name:
                    # Normalize organization name for comparison
                    normalized_org = NameNormalizationUtils.normalize_business_name(org_name)
                    
                    # Extract business names from validated address display names
                    mist_display = mist_result.get('display_name', '').lower()
                    comp_display = comparison_result.get('display_name', '').lower()
                    
                    # Calculate organization name similarity with each address
                    mist_org_similarity = NameNormalizationUtils.calculate_org_name_similarity(normalized_org, mist_display)
                    comp_org_similarity = NameNormalizationUtils.calculate_org_name_similarity(normalized_org, comp_display)
                    
                    if debug:
                        logging.debug(f"Organization name similarity scores: Mist={mist_org_similarity:.3f}, Comparison={comp_org_similarity:.3f}")
                    
                    # If there's a clear winner based on organization name similarity
                    if mist_org_similarity > comp_org_similarity + 0.1:  # 10% threshold
                        recommendation = 'mist'
                        recommendation_reason = f"Mist address better matches organization '{org_name}' (similarity: {mist_org_similarity:.3f} vs {comp_org_similarity:.3f})"
                        if debug:
                            logging.debug(f"Recommendation: MIST (organization name match: {mist_org_similarity:.3f} vs {comp_org_similarity:.3f})")
                    elif comp_org_similarity > mist_org_similarity + 0.1:
                        recommendation = 'comparison'
                        recommendation_reason = f"Reference address better matches organization '{org_name}' (similarity: {comp_org_similarity:.3f} vs {mist_org_similarity:.3f})"
                        if debug:
                            logging.debug(f"Recommendation: COMPARISON (organization name match: {comp_org_similarity:.3f} vs {mist_org_similarity:.3f})")
                    else:
                        # Still too close - apply business context rules
                        business_recommendation = apply_business_context_rules(mist_result, comparison_result, debug)
                        if business_recommendation == 'mist':
                            recommendation = 'mist'
                            recommendation_reason = f"Mist address appears more business-appropriate (type: {mist_result.get('place_type', 'unknown')})"
                        elif business_recommendation == 'comparison':
                            recommendation = 'comparison'
                            recommendation_reason = f"Reference address appears more business-appropriate (type: {comparison_result.get('place_type', 'unknown')})"
                        else:
                            recommendation = 'uncertain'
                            recommendation_reason = f"All tiebreakers inconclusive - similar confidence ({mist_result['confidence']:.3f} vs {comparison_result['confidence']:.3f})"
                            if debug:
                                logging.debug(f"Recommendation: UNCERTAIN (all tiebreakers inconclusive - confidence: {mist_result['confidence']:.3f} vs {comparison_result['confidence']:.3f})")
                else:
                    # No org name available - apply business context rules only
                    business_recommendation = apply_business_context_rules(mist_result, comparison_result, debug)
                    if business_recommendation == 'mist':
                        recommendation = 'mist'
                        recommendation_reason = f"Mist address appears more business-appropriate (type: {mist_result.get('place_type', 'unknown')})"
                    elif business_recommendation == 'comparison':
                        recommendation = 'comparison'
                        recommendation_reason = f"Reference address appears more business-appropriate (type: {comparison_result.get('place_type', 'unknown')})"
                    else:
                        recommendation = 'uncertain'
                        recommendation_reason = f"Confidence scores too close and no clear business preference ({mist_result['confidence']:.3f} vs {comparison_result['confidence']:.3f})"
                        if debug:
                            logging.debug(f"Recommendation: UNCERTAIN (confidence scores too close: {mist_result['confidence']:.3f} vs {comparison_result['confidence']:.3f})")
    else:
        recommendation = 'uncertain'
        recommendation_reason = "Both addresses failed validation"
        if debug:
            logging.debug("Recommendation: UNCERTAIN (both addresses failed validation)")
    
    final_result = {
        'mist_validation': mist_result,
        'comparison_validation': comparison_result,
        'recommendation': recommendation,
        'recommendation_reason': recommendation_reason
    }
    
    if debug:
        logging.debug(f"EXIT: validate_addresses_with_nominatim() - returning: {final_result}")
    
    return final_result

class NameNormalizationUtils:
    """General name & token normalization helpers (business, org, and generic strings).

    EVOLUTION:
        Derived from earlier AddressBusinessNameUtils which wrapped two former free functions.
        Generalized to support broader name cleaning (future: contact names, model families, etc.).

    FEATURES:
        * Business suffix stripping
        * Generic punctuation stripping & whitespace collapsing
        * Alias methods maintained for backward compatibility
        * Token extraction utility for future semantic similarity modules

    SECURITY: Pure string transformations; no I/O or external calls.
    """

    BUSINESS_SUFFIX_PATTERNS = [
        r'\binc\.?$', r'\bincorporated$', r'\bllc\.?$', r'\bcorp\.?$', r'\bcorporation$',
        r'\bltd\.?$', r'\blimited$', r'\bco\.?$', r'\bcompany$', r'\benterprise$',
        r'\benterprises$', r'\bgroup$', r'\bholdings$', r'\bassociates$', r'\bpartners$',
        r'\b& co\.?$', r'\b&co\.?$'
    ]

    @staticmethod
    def normalize_business_name(business_name: str) -> str:
        if not business_name:
            return ""
        normalized = business_name.lower().strip()
        for suffix in NameNormalizationUtils.BUSINESS_SUFFIX_PATTERNS:
            normalized = re.sub(suffix, '', normalized).strip()
        normalized = re.sub(r'[^\w\s]', ' ', normalized)
        normalized = re.sub(r'\s+', ' ', normalized).strip()
        return normalized

    @staticmethod
    def normalize_generic(name: str) -> str:
        """Lightweight generic normalization (lower, trim, collapse whitespace)."""
        if not name:
            return ""
        cleaned = unicodedata.normalize('NFKD', str(name)).casefold().strip()
        cleaned = re.sub(r'\s+', ' ', cleaned)
        return cleaned

    @staticmethod
    def extract_tokens(name: str) -> list:
        """Return lowercase alphanumeric tokens for fuzzy pipelines."""
        if not name:
            return []
        return re.findall(r'[a-z0-9]+', NameNormalizationUtils.normalize_generic(name))

    @staticmethod
    def calculate_org_name_similarity(org_name: str, address_display: str) -> float:
        if not org_name or not address_display:
            return 0.0
        org_words = set(org_name.split())
        address_words = set(re.findall(r'\b\w+\b', address_display.lower()))
        exact_matches = len(org_words.intersection(address_words))
        word_similarity = (exact_matches / len(org_words)) if org_words else 0.0
        string_similarity = SequenceMatcher(None, org_name, address_display).ratio()
        combined_similarity = (word_similarity * 0.7) + (string_similarity * 0.3)
        return min(1.0, combined_similarity)

# Backward-compatible alias (in case any external automation referenced the old class name)
AddressBusinessNameUtils = NameNormalizationUtils

def apply_business_context_rules(mist_result, comparison_result, debug=False):
    """
    Apply business context rules when both addresses are valid but confidence scores are similar.
    Returns 'mist', 'comparison', or 'uncertain'.
    """
    # Business address type preferences based on place_type
    business_place_types = ['commercial', 'office', 'retail', 'building', 'shop', 'store']
    residential_place_types = ['house', 'residential', 'apartment']
    
    mist_place = mist_result.get('place_type', '').lower()
    comp_place = comparison_result.get('place_type', '').lower()
    
    if debug:
        logging.debug(f"Business context analysis: Mist place_type='{mist_place}', Comparison place_type='{comp_place}'")
    
    # Prefer business/commercial addresses over residential
    mist_is_business = any(biz_type in mist_place for biz_type in business_place_types)
    comp_is_business = any(biz_type in comp_place for biz_type in business_place_types)
    
    mist_is_residential = any(res_type in mist_place for res_type in residential_place_types)
    comp_is_residential = any(res_type in comp_place for res_type in residential_place_types)
    
    if mist_is_business and comp_is_residential:
        if debug:
            logging.debug("Business context rule: Preferring Mist (business over residential)")
        return 'mist'
    elif comp_is_business and mist_is_residential:
        if debug:
            logging.debug("Business context rule: Preferring Comparison (business over residential)")
        return 'comparison'
    
    # If both are business or both are residential, check confidence again with lower threshold
    confidence_diff = abs(mist_result['confidence'] - comparison_result['confidence'])
    if confidence_diff > 0.05:  # 5% threshold for final decision
        if mist_result['confidence'] > comparison_result['confidence']:
            if debug:
                logging.debug(f"Business context rule: Preferring Mist (slightly higher confidence: {mist_result['confidence']:.3f})")
            return 'mist'
        else:
            if debug:
                logging.debug(f"Business context rule: Preferring Comparison (slightly higher confidence: {comparison_result['confidence']:.3f})")
            return 'comparison'
    
    if debug:
        logging.debug("Business context rules inconclusive")
    return 'uncertain'

def normalize_address_string(address_str):
    """
    Normalizes an address string for comparison by:
    - Converting to lowercase
    - Removing extra whitespace
    - Standardizing common abbreviations
    - Removing punctuation
    - Unicode normalization for diacritics
    """
    
    if not address_str:
        return ""
    
    # Unicode normalization (NFKD) and casefold for robust comparison
    normalized = unicodedata.normalize('NFKD', address_str)
    normalized = normalized.casefold().strip()
    
    # Remove extra whitespace and collapse multiple spaces
    normalized = re.sub(r'\s+', ' ', normalized)
    
    # Common address abbreviations standardization
    abbreviations = {
        r'\bstreet\b': 'st',
        r'\bst\b': 'st',
        r'\bavenue\b': 'ave',
        r'\bave\b': 'ave',
        r'\bboulevard\b': 'blvd',
        r'\bblvd\b': 'blvd',
        r'\bbuilding\b': 'bldg',
        r'\bbuilding\b': 'bldg',
        r'\bsuite\b': 'ste',
        r'\bsuite\b': 'ste',
        r'\bnorth\b': 'n',
        r'\bsouth\b': 's',
        r'\beast\b': 'e',
        r'\bwest\b': 'w',
        r'\bdrive\b': 'dr',
        r'\bdr\b': 'dr',
        r'\broad\b': 'rd',
        r'\brd\b': 'rd',
        r'\blane\b': 'ln',
        r'\bln\b': 'ln',
        r'\bcourt\b': 'ct',
        r'\bct\b': 'ct',
        r'\bplace\b': 'pl',
        r'\bpl\b': 'pl',
        r'\bparkway\b': 'pkwy',
        r'\bpkwy\b': 'pkwy',
        r'\bhighway\b': 'hwy',
        r'\bhwy\b': 'hwy',
    }
    
    for full_form, abbrev in abbreviations.items():
        normalized = re.sub(full_form, abbrev, normalized)
    
    # Remove punctuation and extra spaces
    normalized = re.sub(r'[^\w\s]', ' ', normalized)
    normalized = ' '.join(normalized.split())
    
    return normalized

def parse_address_components(address_string, debug=False):
    """
    Parse address components with defensive parsing and robust heuristics.
    
    Handles:
    - Defensive parsing with length checks
    - "Unknown" address detection  
    - Puerto Rico US territory mapping
    - Right-to-left parsing (country, zip, city, street)
    - Unicode normalization
    - Graceful error handling
    
    Args:
        address_string (str): Raw address string
        debug (bool): Enable debug logging
        
    Returns:
        dict: {
            'address': str,
            'city': str, 
            'state': str,
            'zip': str,
            'country': str,
            'is_parseable': bool,
            'parse_reason': str,
            'original': str
        }
    """
    
    if debug:
        logging.debug(f"PARSE_ADDRESS: Input: '{address_string}'")
    
    # Initialize default result
    result = {
        'address': None,
        'city': None,
        'state': None,
        'zip': None,
        'country': None,
        'is_parseable': False,
        'parse_reason': 'unparsed',
        'original': address_string or ""
    }
    
    # Handle empty or None input
    if not address_string or not str(address_string).strip():
        result['parse_reason'] = 'empty_input'
        if debug:
            logging.debug("PARSE_ADDRESS: Empty input")
        return result
    
    # Handle "Unknown" addresses
    cleaned_input = str(address_string).strip()
    if cleaned_input.lower() in ['unknown', 'n/a', 'na', 'none', 'null', '']:
        result['parse_reason'] = 'unknown_address'
        if debug:
            logging.debug("PARSE_ADDRESS: Unknown address detected")
        return result
    
    try:
        # Unicode normalization and defensive cleaning
        normalized = unicodedata.normalize('NFKD', cleaned_input)
        
        # Trim whitespace, collapse repeated commas, remove empty tokens
        parts = [part.strip() for part in normalized.split(',')]
        parts = [part for part in parts if part]  # Remove empty parts
        
        if not parts:
            result['parse_reason'] = 'no_parts_after_cleaning'
            if debug:
                logging.debug("PARSE_ADDRESS: No parts after cleaning")
            return result
        
        if debug:
            logging.debug(f"PARSE_ADDRESS: Cleaned parts: {parts}")
        
        # Parse from right to left (country, postal/ZIP, city, street)
        
        # Step 1: Detect country (last token if recognizable)
        country = None
        remaining_parts = parts[:]
        
        if len(remaining_parts) > 0:
            last_part = remaining_parts[-1].strip().lower()
            
            # Country detection patterns
            if last_part in ['usa', 'united states', 'united states of america', 'us']:
                country = 'US'
                remaining_parts = remaining_parts[:-1]
            elif last_part in ['puerto rico', 'pr']:
                country = 'US'  # Puerto Rico is US territory
                remaining_parts = remaining_parts[:-1]
            elif len(last_part) == 2 and last_part.isalpha():
                # Assume 2-letter country code
                country = last_part.upper()
                remaining_parts = remaining_parts[:-1]
        
        # Step 2: Detect ZIP/postal code (numeric patterns)
        zip_code = None
        if len(remaining_parts) > 0:
            last_part = remaining_parts[-1].strip()
            
            # US ZIP pattern: 5 digits or 5+4 format
            if re.match(r'^\d{5}(-?\d{4})?$', last_part):
                zip_code = last_part
                remaining_parts = remaining_parts[:-1]
                # If no country detected but ZIP found, assume US
                if not country:
                    country = 'US'
        
        # Step 3: Handle Puerto Rico special case
        state = None
        if len(remaining_parts) > 0:
            last_part = remaining_parts[-1].strip().lower()
            
            # Check for Puerto Rico in various positions
            if last_part == 'puerto rico':
                state = 'PR'
                country = 'US'
                remaining_parts = remaining_parts[:-1]
            elif country == 'US' and last_part == 'pr':
                state = 'PR'
                remaining_parts = remaining_parts[:-1]
            elif len(last_part) <= 2 and last_part.isalpha():
                # Assume state abbreviation
                state = last_part.upper()
                remaining_parts = remaining_parts[:-1]
            elif len(remaining_parts) > 1:
                # Check if it's a full state name
                state_normalized = normalize_state_name(last_part)
                if state_normalized:
                    state = state_normalized.upper()
                    remaining_parts = remaining_parts[:-1]
        
        # Step 4: Detect city (next remaining part from right)
        city = None
        if len(remaining_parts) > 0:
            city = remaining_parts[-1].strip()
            remaining_parts = remaining_parts[:-1]
        
        # Step 5: Everything else is street address
        address = None
        if remaining_parts:
            address = ', '.join(remaining_parts).strip()
        
        # Populate result
        result.update({
            'address': address,
            'city': city,
            'state': state,
            'zip': zip_code,
            'country': country,
            'is_parseable': True,
            'parse_reason': 'success'
        })
        
        if debug:
            logging.debug(f"PARSE_ADDRESS: Parsed result: {result}")
        
        return result
        
    except Exception as e:
        result['parse_reason'] = f'exception: {str(e)}'
        if debug:
            logging.warning(f"PARSE_ADDRESS: Exception during parsing: {e}")
        return result

def enhanced_usaddress_parse(address_string, debug=False):
    """
    Enhanced address parsing using usaddress-scourgify for US addresses.
    Falls back to heuristic parsing for non-US or failed cases.
    
    Args:
        address_string (str): Raw address string
        debug (bool): Enable debug logging
        
    Returns:
        dict: Same format as parse_address_components
    """
    try:
        
        if debug:
            logging.debug(f"USADDRESS_PARSE: Attempting usaddress parsing for: '{address_string}'")
        
        # Try usaddress-scourgify first
        try:
            parsed = normalize_address_record(address_string)
            
            result = {
                'address': parsed.get('address_line_1', ''),
                'city': parsed.get('city', ''),
                'state': parsed.get('state', ''),
                'zip': parsed.get('postal_code', ''),
                'country': 'US',  # usaddress is US-focused
                'is_parseable': True,
                'parse_reason': 'usaddress_success',
                'original': address_string or ""
            }
            
            # Handle address_line_2 if present
            if parsed.get('address_line_2'):
                # Combine address lines with space
                address_parts = [parsed.get('address_line_1', ''), parsed.get('address_line_2', '')]
                result['address'] = ' '.join(part for part in address_parts if part)
            
            if debug:
                logging.debug(f"USADDRESS_PARSE: Success: {result}")
            
            return result
            
        except Exception as usaddress_error:
            if debug:
                logging.debug(f"USADDRESS_PARSE: Failed with: {usaddress_error}")
            
            # Fall back to heuristic parsing
            return parse_address_components(address_string, debug=debug)
    
    except ImportError:
        if debug:
            logging.debug("USADDRESS_PARSE: usaddress-scourgify not available, using heuristic parsing")
        return parse_address_components(address_string, debug=debug)

def calculate_string_similarity(str1, str2):
    """
    Calculate similarity percentage between two strings using RapidFuzz for better performance.
    Falls back to difflib if RapidFuzz is not available.
    
    Returns a percentage from 0-100.
    """
    if not str1 and not str2:
        return 100.0  # Both empty, consider perfect match
    if not str1 or not str2:
        return 0.0    # One empty, one not, no match
    
    # Normalize both strings
    norm_str1 = normalize_address_string(str1)
    norm_str2 = normalize_address_string(str2)
    
    try:
        # Try RapidFuzz for better performance and accuracy
        
        # Use token-based similarity for better address matching
        similarity = fuzz.token_sort_ratio(norm_str1, norm_str2) / 100.0
        return similarity * 100
        
    except ImportError:
        # Fall back to difflib
        similarity = difflib.SequenceMatcher(None, norm_str1, norm_str2).ratio()
        return similarity * 100

def check_address_should_skip(comparison_address, skip_addresses, debug=False):
    """
    Check if a comparison address should be automatically skipped (treating Mist address as correct).
    
    Args:
        comparison_address (dict): Address from comparison CSV to check
        skip_addresses (list): List of addresses to skip from AddressSkip.csv
        debug (bool): Enable debug logging
        
    Returns:
        tuple: (should_skip: bool, skip_reason: str)
    """
    if not skip_addresses:
        return False, ""
    
    # Normalize comparison address fields for matching
    comp_addr = str(comparison_address.get('address', '')).strip().upper()
    comp_city = str(comparison_address.get('city', '')).strip().upper()
    comp_state = str(comparison_address.get('state', '')).strip().upper()
    comp_zip = str(comparison_address.get('zip', '')).strip().upper()
    
    for skip_entry in skip_addresses:
        skip_addr = str(skip_entry.get('Skip_Address', '')).strip().upper()
        skip_city = str(skip_entry.get('Skip_City', '')).strip().upper()
        skip_state = str(skip_entry.get('Skip_State', '')).strip().upper()
        skip_zip = str(skip_entry.get('Skip_Zip', '')).strip().upper()
        skip_reason = skip_entry.get('Reason', 'Address in skip list')
        
        # Check for exact matches (case-insensitive)
        if (comp_addr == skip_addr and 
            comp_city == skip_city and 
            comp_state == skip_state and 
            comp_zip == skip_zip):
            
            if debug:
                logging.debug(f"ADDRESS_SKIP: Found exact match - {comp_addr}, {comp_city}, {comp_state}, {comp_zip}")
            return True, skip_reason
            
        # Check for partial matches (any field matches and others are empty in skip list)
        partial_match = False
        matching_fields = 0
        
        if skip_addr and comp_addr == skip_addr:
            partial_match = True
            matching_fields += 1
        if skip_city and comp_city == skip_city:
            partial_match = True
            matching_fields += 1
        if skip_state and comp_state == skip_state:
            partial_match = True
            matching_fields += 1
        if skip_zip and comp_zip == skip_zip:
            partial_match = True
            matching_fields += 1
            
        # For wildcard patterns: require at least 3 empty fields AND only 1 matching field
        # For specific addresses: require at least 2 matching fields
        empty_fields = sum([1 for f in [skip_addr, skip_city, skip_state, skip_zip] if not f])
        populated_fields = 4 - empty_fields
        
        if partial_match:
            # Wildcard pattern (mostly empty fields): require exactly 1 match
            if empty_fields >= 3 and matching_fields == 1:
                if debug:
                    logging.debug(f"ADDRESS_SKIP: Found wildcard match - {comp_addr}, {comp_city}, {comp_state}, {comp_zip}")
                return True, skip_reason
            # Specific address pattern: require at least 50% field match
            elif populated_fields >= 2 and matching_fields >= max(2, populated_fields // 2):
                if debug:
                    logging.debug(f"ADDRESS_SKIP: Found specific address match - {comp_addr}, {comp_city}, {comp_state}, {comp_zip}")
                return True, skip_reason
    
    return False, ""

def enhanced_compare_addresses_with_threshold(mist_address, comparison_address, threshold, debug=False):
    """
    Enhanced address comparison with robust parsing and better similarity metrics.
    
    Args:
        mist_address (dict): Dictionary with keys: address, city, state, zip, country
        comparison_address (dict): Dictionary with keys: address, city, state, zip, country  
        threshold (float): Minimum similarity percentage required to be considered a match
        debug (bool): Enable debug logging
        
    Returns:
        dict: {
            'overall_similarity': float,
            'is_match': bool,
            'field_similarities': {
                'address': float,
                'city': float, 
                'state': float,
                'zip': float
            },
            'failed_fields': list,
            'parse_status': dict  # New: parsing status for both addresses
        }
    """
    # Field weights for overall similarity calculation
    field_weights = {
        'address': 0.4,    # Street address is most important
        'city': 0.3,       # City is very important
        'state': 0.2,      # State is important
        'zip': 0.1         # Zip is least weighted since we already have zip comparison
    }
    
    field_similarities = {}
    failed_fields = []
    parse_status = {
        'mist_parseable': True,
        'comparison_parseable': True,
        'mist_reason': 'valid',
        'comparison_reason': 'valid'
    }
    
    if debug:
        logging.debug(f"ENHANCED_COMPARE: Mist address: {mist_address}")
        logging.debug(f"ENHANCED_COMPARE: Comparison address: {comparison_address}")
    
    # Check for unparseable addresses
    for field in field_weights.keys():
        mist_value = mist_address.get(field, "")
        comp_value = comparison_address.get(field, "")
        
        # Check if either address appears to be unparseable
        if str(mist_value).strip().lower() in ['unknown', 'n/a', 'na', 'none', 'null', '']:
            parse_status['mist_parseable'] = False
            parse_status['mist_reason'] = 'unknown_address'
        
        if str(comp_value).strip().lower() in ['unknown', 'n/a', 'na', 'none', 'null', '']:
            parse_status['comparison_parseable'] = False
            parse_status['comparison_reason'] = 'unknown_address'
    
    # If either address is unparseable, return early with low similarity
    if not parse_status['mist_parseable'] or not parse_status['comparison_parseable']:
        if debug:
            logging.debug(f"ENHANCED_COMPARE: Unparseable address detected: {parse_status}")
        
        return {
            'overall_similarity': 0.0,
            'is_match': False,
            'field_similarities': {field: 0.0 for field in field_weights.keys()},
            'failed_fields': list(field_weights.keys()),
            'parse_status': parse_status
        }
    
    # Compare address fields using enhanced similarity
    for field, weight in field_weights.items():
        mist_value = str(mist_address.get(field, "")).strip()
        comp_value = str(comparison_address.get(field, "")).strip()
        
        if field == 'zip':
            # Use normalized zip comparison
            mist_norm = normalize_zip_code(mist_value)
            comp_norm = normalize_zip_code(comp_value)
            similarity = 100.0 if mist_norm == comp_norm and mist_norm else 0.0
        elif field == 'state':
            # Use normalized state comparison (handles abbreviations vs full names)
            mist_norm = normalize_state_name(mist_value)
            comp_norm = normalize_state_name(comp_value)
            similarity = 100.0 if mist_norm == comp_norm and mist_norm else 0.0
        else:
            # Use enhanced string similarity for address and city fields
            similarity = calculate_string_similarity(mist_value, comp_value)
        
        field_similarities[field] = similarity
        
        # Use a more forgiving threshold for individual fields (75% of the overall threshold)
        field_threshold = threshold * 0.75
        if similarity < field_threshold:
            failed_fields.append(field)
        
        if debug:
            logging.debug(f"ENHANCED_COMPARE: {field} similarity: {similarity:.1f}% (threshold: {field_threshold:.1f}%)")
    
    # Calculate weighted overall similarity
    overall_similarity = sum(field_similarities[field] * field_weights[field] 
                           for field in field_weights.keys())
    
    is_match = overall_similarity >= threshold
    
    result = {
        'overall_similarity': overall_similarity,
        'is_match': is_match,
        'field_similarities': field_similarities,
        'failed_fields': failed_fields,
        'parse_status': parse_status
    }
    
    if debug:
        logging.debug(f"ENHANCED_COMPARE: Result: {result}")
    
    return result

def compare_addresses_with_threshold(mist_address, comparison_address, threshold):
    """
    Compare two address dictionaries and return overall similarity percentage and field-by-field breakdown.
    
    Args:
        mist_address (dict): Dictionary with keys: address, city, state, zip, country
        comparison_address (dict): Dictionary with keys: address, city, state, zip, country  
        threshold (float): Minimum similarity percentage required to be considered a match
        
    Returns:
        dict: {
            'overall_similarity': float,
            'is_match': bool,
            'field_similarities': {
                'address': float,
                'city': float, 
                'state': float,
                'zip': float
            },
            'failed_fields': list
        }
    """
    # Field weights for overall similarity calculation
    field_weights = {
        'address': 0.4,    # Street address is most important
        'city': 0.3,       # City is very important
        'state': 0.2,      # State is important
        'zip': 0.1         # Zip is least weighted since we already have zip comparison
    }
    
    field_similarities = {}
    failed_fields = []
    
    # Compare address fields (ignore country as requested)
    for field, weight in field_weights.items():
        mist_value = mist_address.get(field, "").strip()
        comp_value = comparison_address.get(field, "").strip()
        
        if field == 'zip':
            # Use normalized zip comparison
            mist_norm = normalize_zip_code(mist_value)
            comp_norm = normalize_zip_code(comp_value)
            similarity = 100.0 if mist_norm == comp_norm else 0.0
        elif field == 'state':
            # Use normalized state comparison (handles abbreviations vs full names)
            mist_norm = normalize_state_name(mist_value)
            comp_norm = normalize_state_name(comp_value)
            similarity = 100.0 if mist_norm == comp_norm else 0.0
        else:
            # Use string similarity for address and city fields
            similarity = calculate_string_similarity(mist_value, comp_value)
        
        field_similarities[field] = similarity
        
        if similarity < threshold:
            failed_fields.append(field)
    
    # Calculate weighted overall similarity
    overall_similarity = sum(field_similarities[field] * field_weights[field] 
                           for field in field_weights.keys())
    
    is_match = overall_similarity >= threshold
    
    return {
        'overall_similarity': overall_similarity,
        'is_match': is_match,
        'field_similarities': field_similarities,
        'failed_fields': failed_fields
    }

class AddressComparisonCounters:
    """Track comprehensive metrics for address comparison operations."""
    
    def __init__(self):
        """Initialize all counter attributes and timing."""
        self.total_devices = 0
        self.devices_enriched = 0
        self.devices_skipped = 0
        self.perfect_matches = 0
        self.mismatches_found = 0
        self.auto_corrections = 0
        self.comparison_failures = 0
        self.parse_failures = 0
        self.parse_failure_reasons = {}
        self.start_time = None
        self.end_time = None
    
    def start_timing(self):
        """Start the timing counter for performance tracking."""
        import time
        self.start_time = time.time()
    
    def end_timing(self):
        """End the timing counter for performance tracking."""
        import time
        self.end_time = time.time()
    
    def get_duration(self):
        """Get the elapsed time in seconds between start and end timing."""
        if self.start_time is None or self.end_time is None:
            return 0
        return self.end_time - self.start_time
    
    def increment_parse_failure(self, reason):
        """
        Increment parse failure counter and track the specific reason.
        
        Args:
            reason (str): The specific reason for the parse failure
        """
        self.parse_failures += 1
        if reason in self.parse_failure_reasons:
            self.parse_failure_reasons[reason] += 1
        else:
            self.parse_failure_reasons[reason] = 1
    
    def log_summary(self):
        """Log a comprehensive summary of all counter metrics."""
        import logging
        logging.info("Address comparison operation completed successfully")
        logging.info(f"Total devices processed: {self.total_devices}")
        logging.info(f"Devices enriched: {self.devices_enriched}")
        logging.info(f"Devices skipped: {self.devices_skipped}")
        logging.info(f"Perfect matches: {self.perfect_matches}")
        logging.info(f"Mismatches found: {self.mismatches_found}")
        logging.info(f"Auto corrections: {self.auto_corrections}")
        logging.info(f"Comparison failures: {self.comparison_failures}")
        logging.info(f"Parse failures: {self.parse_failures}")
        if self.parse_failure_reasons:
            logging.info(f"Parse failure breakdown: {self.parse_failure_reasons}")
        logging.info(f"Processing duration: {self.get_duration():.2f} seconds")

def create_address_parse_failures_csv(parse_failures, filename="AddressParseFailures.csv"):
    """
    Create a CSV file documenting address parsing failures.
    
    Args:
        parse_failures (list): List of parse failure records
        filename (str): Output filename
    """
    if not parse_failures:
        logging.info("No address parsing failures to document.")
        return
    
    try:
        output_path = get_csv_file_path(filename)
        
        with open(output_path, "w", newline='', encoding="utf-8") as f:
            fieldnames = [
                'site_id', 'site_name', 'device_id', 'device_serial', 'device_name',
                'original_address', 'parsed_tokens', 'failure_reason', 'timestamp'
            ]
            
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            
            for failure in parse_failures:
                writer.writerow(failure)
        
        logging.info(f"Address parsing failures documented in: {filename} ({len(parse_failures)} records)")
        print(f"! Address parsing failures documented in: {filename} ({len(parse_failures)} records)")
        
    except Exception as e:
        logging.error(f"Failed to create address parse failures CSV: {e}")
        print(f"! Failed to create address parse failures CSV: {e}")

def get_device_identifier(device, warn_on_missing=False):
    """
    Get the best available identifier for a device with fallback logic.
    
    Args:
        device (dict): Device record
        warn_on_missing (bool): Whether to log a warning on first missing name
        
    Returns:
        str: Device identifier (name, serial, or device_id)
    """
    # Try name first
    name = device.get("name", "").strip()
    if name:
        return name
    
    # Fall back to serial
    serial = device.get("serial", "").strip()
    if serial:
        if warn_on_missing:
            logging.warning(f"Device {serial} missing name field, using serial as identifier")
        return serial
    
    # Fall back to device_id
    device_id = device.get("id", "").strip()
    if device_id:
        if warn_on_missing:
            logging.warning(f"Device {device_id} missing name and serial, using device_id as identifier")
        return device_id
    
    # Last resort
    if warn_on_missing:
        logging.warning("Device found with no name, serial, or id - using 'UNKNOWN'")
    return "UNKNOWN"

def compare_inventory_with_csv(fast=False, address_check=False, debug=False, skip_ssl_verify=True):
    """
    Compares combined inventory data with site info against a user-selected CSV file.
    Shows items where addresses don't meet the configured similarity threshold.
    Skips items that aren't in the comparison CSV file.
    Uses configurable ADDRESS_MATCH_THRESHOLD from .env file for fuzzy address matching.
    
    Enhanced with:
    - Hardened address parsing with defensive error handling
    - Comprehensive metrics and counters
    - Parse failure artifact generation
    - Improved logging and observability
    - Better handling of "Unknown" addresses
    - Unicode normalization and fuzzy matching improvements
    
    Args:
        fast (bool): If True, enables optimized data generation mode using cached data
                    and concurrent processing where applicable.
        address_check (bool): If True, enables external address validation using Nominatim API.
                             Overrides ENABLE_ADDRESS_VALIDATION setting from .env file.
        debug (bool): If True, enables detailed debug logging for API requests and responses.
        skip_ssl_verify (bool): If True, skips SSL certificate verification for external APIs.
    """
    # Initialize counters for comprehensive tracking
    counters = AddressComparisonCounters()
    counters.start_timing()
    parse_failures = []  # Track parsing failures for artifact generation

    # Load environment variables
    load_dotenv()
    END_CUSTOMER_NAME = os.getenv("END_CUSTOMER_NAME")
    END_CUSTOMER_ACCOUNT_ID = os.getenv("END_CUSTOMER_ACCOUNT_ID")
    
    # Get configurable address match threshold (default: 75%)
    ADDRESS_MATCH_THRESHOLD = float(os.getenv("ADDRESS_MATCH_THRESHOLD", "75"))

    print("* Data Integrity Analysis: Comparing Mist vs Comparison CSV addresses...")
    print(f"* Using address similarity threshold: {ADDRESS_MATCH_THRESHOLD}% (conflicts below this will be flagged)")
    print(f"* Enhanced features: defensive parsing, Unicode normalization, fuzzy matching")
    if fast:
        print("* Fast mode enabled: Using optimized data generation and caching")
    if debug:
        print(" Debug mode enabled: Detailed comparison logging active")
        logging.debug("ENTRY: compare_inventory_with_csv()")
        logging.debug(f"  Parameters: fast={fast}, address_check={address_check}, debug={debug}")
        logging.debug(f"  ADDRESS_MATCH_THRESHOLD={ADDRESS_MATCH_THRESHOLD}")
    
    # Check if address validation is enabled
    address_validation_enabled = address_check or os.getenv("ENABLE_ADDRESS_VALIDATION", "false").lower() == "true"
    
    if address_validation_enabled:
        source = "--address-check flag" if address_check else ".env file"
        print(f"! External address validation enabled via {source}")
        print(f"   Address conflicts will be validated using Nominatim API")
        if debug:
            logging.debug(f"Address validation enabled via {source}")
    else:
        print("  External address validation disabled")
        print("   Use --address-check flag or set ENABLE_ADDRESS_VALIDATION=true in .env to enable intelligent address recommendations")
        if debug:
            logging.debug("Address validation disabled")
    
    # Use efficient caching instead of always regenerating fresh data
    check_and_generate_csv("AllDevicesWithSiteInfo.csv", lambda: export_devices_with_site_info_to_csv(fast=fast))

    # Load the enriched device + site info
    devices_with_site_info_path = get_csv_file_path("AllDevicesWithSiteInfo.csv")
    with open(devices_with_site_info_path, mode="r", encoding="utf-8") as f:
        site_configs = list(csv.DictReader(f))

    # Find all CSV files in the data directory
    data_dir = "data"
    csv_files = glob.glob(os.path.join(data_dir, "*.csv"))
    # Remove path prefix and exclude our source file
    csv_files = [os.path.basename(f) for f in csv_files if os.path.basename(f) != "AllDevicesWithSiteInfo.csv"]
    
    if not csv_files:
        print(" No CSV files found in the data directory for comparison.")
        print(f"   Please place comparison CSV files in the '{data_dir}' folder.")
        logging.error("No CSV files found for comparison in data directory.")
        return

    # Present CSV files to user for selection
    print("\n  Available CSV files for comparison:")
    print("=" * 60)
    for idx, csv_file in enumerate(csv_files):
        print(f"[{idx}] {csv_file}")
    
    try:
        user_input = input(f"\nEnter the index (0-{len(csv_files)-1}) of the CSV file to compare against: ").strip()
        selected_index = int(user_input)
        
        if selected_index < 0 or selected_index >= len(csv_files):
            print(" Invalid index selected.")
            logging.error(f"Invalid CSV file index selected: {selected_index}")
            return
            
        comparison_file = csv_files[selected_index]
        print(f"! Selected comparison file: {comparison_file}")
        logging.info(f"User selected comparison file: {comparison_file}")
        
    except ValueError:
        print(" Invalid input. Please enter a numeric index.")
        logging.error("Invalid numeric input for CSV file selection.")
        return
    except KeyboardInterrupt:
        print("\n Operation cancelled by user.")
        logging.info("CSV comparison operation cancelled by user.")
        return

    # Load the comparison CSV file
    try:
        comparison_file_path = get_csv_file_path(comparison_file)
        with open(comparison_file_path, mode="r", encoding="utf-8") as f:
            comparison_data = list(csv.DictReader(f))
    except Exception as e:
        print(f"! Error reading comparison file {comparison_file}: {e}")
        logging.error(f"Error reading comparison file {comparison_file}: {e}")
        return

    print(f"! Loaded {len(site_configs)} devices from AllDevicesWithSiteInfo.csv")
    print(f"! Loaded {len(comparison_data)} records from {comparison_file}")

    # Load the address skip list for automatic corrections
    skip_addresses = []
    skip_file_path = get_csv_file_path("AddressSkip.csv")
    try:
        with open(skip_file_path, mode="r", encoding="utf-8") as f:
            skip_data = list(csv.DictReader(f))
            skip_addresses = skip_data
        print(f"! Loaded {len(skip_addresses)} skip addresses from AddressSkip.csv")
        if debug:
            logging.debug(f"Loaded {len(skip_addresses)} addresses to skip from AddressSkip.csv")
    except FileNotFoundError:
        print("  AddressSkip.csv not found - no addresses will be automatically skipped")
        if debug:
            logging.debug("AddressSkip.csv not found - continuing without skip list")
    except Exception as e:
        print(f"!  Error loading AddressSkip.csv: {e}")
        logging.warning(f"Error loading AddressSkip.csv: {e}")

    # Create lookup dictionaries for comparison data
    # Try common field names for serial number and zip code
    comparison_serials = {}
    comparison_zip_lookup = {}
    
    # Detect field names in comparison CSV
    if not comparison_data:
        print(" Comparison CSV file is empty.")
        return
        
    comparison_headers = comparison_data[0].keys()
    
    # Try to find serial number field
    serial_field = None
    for header in comparison_headers:
        if any(term in header.lower() for term in ['serial', 'sn', 'system serial']):
            serial_field = header
            break
    
    # Try to find zip code field  
    zip_field = None
    for header in comparison_headers:
        if any(term in header.lower() for term in ['zip', 'postal', 'zip code', 'postal code']):
            zip_field = header
            break

    # Try to find address fields
    address_field = None
    city_field = None
    state_field = None
    country_field = None
    
    for header in comparison_headers:
        if any(term in header.lower() for term in ['address', 'street', 'address line']):
            address_field = header
        elif any(term in header.lower() for term in ['city']):
            city_field = header
        elif any(term in header.lower() for term in ['state']):
            state_field = header
        elif any(term in header.lower() for term in ['country']):
            country_field = header

    if not serial_field:
        print(" Could not find serial number field in comparison CSV.")
        print("   Looked for fields containing: 'serial', 'sn', 'system serial'")
        print(f"   Available fields: {list(comparison_headers)}")
        logging.error(f"Serial field not found in {comparison_file}. Available fields: {list(comparison_headers)}")
        return
        
    if not zip_field:
        print(" Could not find zip code field in comparison CSV.")
        print("   Looked for fields containing: 'zip', 'postal', 'zip code', 'postal code'")
        print(f"   Available fields: {list(comparison_headers)}")
        logging.error(f"Zip field not found in {comparison_file}. Available fields: {list(comparison_headers)}")
        return

    print(f"! Using serial field: '{serial_field}'")
    print(f"! Using zip field: '{zip_field}'")
    if address_field:
        print(f"! Using address field: '{address_field}'")
    if city_field:
        print(f"! Using city field: '{city_field}'")
    if state_field:
        print(f"! Using state field: '{state_field}'")
    if country_field:
        print(f"! Using country field: '{country_field}'")

    # Build lookup dictionaries from comparison data
    comparison_address_lookup = {}  # serial -> full address info from comparison CSV
    for row in comparison_data:
        serial = row.get(serial_field, "").strip()
        zip_code = row.get(zip_field, "").strip()
        if serial:
            # Normalize zip code for comparison
            normalized_zip = normalize_zip_code(zip_code)
            comparison_serials[serial] = normalized_zip
            # Store full address info for diff report
            comparison_address_lookup[serial] = {
                "Address": row.get(address_field, "") if address_field else "",
                "City": row.get(city_field, "") if city_field else "",
                "State": row.get(state_field, "") if state_field else "",
                "Country": row.get(country_field, "") if country_field else "",
                "Zip": zip_code
            }

    print(f"! Built comparison lookup with {len(comparison_serials)} serial numbers")
    
    # Show validation count if address validation is enabled
    if address_validation_enabled:
        validation_count = len([d for d in site_configs if d.get('serial', '').strip() in comparison_serials])
        print(f"! Will validate {validation_count} address conflicts using Nominatim API")

    # Duplicate address detection between sites
    print("\n  Checking for duplicate addresses between sites...")
    
    # Get unique address per site for Mist data
    mist_site_addresses = {}  # site_name -> address_key
    for device in site_configs:
        site_name = device.get("site_name", "")
        if not site_name or site_name in mist_site_addresses:
            continue  # Skip if already processed this site
            
        mist_address = {
            'address': device.get("street", "").strip(),
            'city': device.get("city", "").strip(),
            'state': device.get("state", "").strip(),
            'zip': device.get("zip_code", "").strip()
        }
        
        # Skip empty addresses
        if not any([mist_address['address'], mist_address['city'], mist_address['state'], mist_address['zip']]):
            continue
        
        # Create normalized address key
        address_key = f"{mist_address['address'].lower()}|{mist_address['city'].lower()}|{mist_address['state'].lower()}|{mist_address['zip']}"
        mist_site_addresses[site_name] = {
            'address_key': address_key,
            'address': mist_address
        }
    
    # Find duplicate addresses between Mist sites
    mist_address_to_sites = {}  # address_key -> [list of site names]
    for site_name, addr_data in mist_site_addresses.items():
        address_key = addr_data['address_key']
        if address_key not in mist_address_to_sites:
            mist_address_to_sites[address_key] = []
        mist_address_to_sites[address_key].append(site_name)
    
    mist_duplicates = {addr_key: sites for addr_key, sites in mist_address_to_sites.items() if len(sites) > 1}
    
    # Get unique address per site for reference data
    ref_site_addresses = {}  # site_name -> address_key
    for device in site_configs:
        device_serial = device.get("serial", "").strip()
        site_name = device.get("site_name", "")
        
        if not site_name or site_name in ref_site_addresses or device_serial not in comparison_address_lookup:
            continue  # Skip if already processed this site or no reference data
            
        ref_data = comparison_address_lookup[device_serial]
        ref_address = {
            'address': ref_data.get("Address", "").strip(),
            'city': ref_data.get("City", "").strip(),
            'state': ref_data.get("State", "").strip(),
            'zip': ref_data.get("Zip", "").strip()
        }
        
        # Skip empty addresses
        if not any([ref_address['address'], ref_address['city'], ref_address['state'], ref_address['zip']]):
            continue
        
        # Create normalized address key
        address_key = f"{ref_address['address'].lower()}|{ref_address['city'].lower()}|{ref_address['state'].lower()}|{ref_address['zip']}"
        ref_site_addresses[site_name] = {
            'address_key': address_key,
            'address': ref_address
        }
    
    # Find duplicate addresses between reference sites
    ref_address_to_sites = {}  # address_key -> [list of site names]
    for site_name, addr_data in ref_site_addresses.items():
        address_key = addr_data['address_key']
        if address_key not in ref_address_to_sites:
            ref_address_to_sites[address_key] = []
        ref_address_to_sites[address_key].append(site_name)
    
    ref_duplicates = {addr_key: sites for addr_key, sites in ref_address_to_sites.items() if len(sites) > 1}
    
    # Report results
    if mist_duplicates:
        print("    Mist sites sharing the same address:")
        for addr_key, sites in mist_duplicates.items():
            # Get the actual address for display
            sample_site = sites[0]
            addr = mist_site_addresses[sample_site]['address']
            print(f"        Address: {addr['address']}, {addr['city']}, {addr['state']} {addr['zip']}")
            print(f"        Sites ({len(sites)}): {', '.join(sites)}")
    
    if ref_duplicates:
        print("    Reference sites sharing the same address:")
        for addr_key, sites in ref_duplicates.items():
            # Get the actual address for display
            sample_site = sites[0]
            addr = ref_site_addresses[sample_site]['address']
            print(f"        Address: {addr['address']}, {addr['city']}, {addr['state']} {addr['zip']}")
            print(f"        Sites ({len(sites)}): {', '.join(sites)}")
    
    # Summary
    if not mist_duplicates and not ref_duplicates:
        print("    No duplicate addresses found between sites")
    else:
        print(f"     Found {len(mist_duplicates)} Mist address duplications affecting {sum(len(sites) for sites in mist_duplicates.values())} sites")
        print(f"     Found {len(ref_duplicates)} reference address duplications affecting {sum(len(sites) for sites in ref_duplicates.values())} sites")
        if debug:
            logging.info(f"DUPLICATE_CHECK: Found {len(mist_duplicates)} Mist duplicates and {len(ref_duplicates)} reference duplicates between sites")

    # Process device data and find address mismatches using configurable threshold
    # IMPROVED ORDER OF OPERATIONS:
    # 1. Fix both addresses first (normalize, parse, clean up)  
    # 2. Remove duplicates (addresses that are the same after normalization)
    # 3. Remove addresses in skip file (known problematic addresses)
    # 4. Then validate remaining conflicts with external API
    
    mismatched_items = []
    diff_report_items = []
    skipped_count = 0
    validation_count = 0
    devices_needing_validation = []  # Will be populated after filtering
    
    print(f"\n  Processing {len(site_configs)} total devices with improved order of operations...")
    print(" Step 1: Parsing and normalizing all addresses...")
    
    # Step 1: Process all devices, fix addresses, and identify initial mismatches
    all_conflicts = []  # Store all conflicts before filtering
    counters.total_devices = len(site_configs)
    first_missing_name_warned = False
    
    for device in tqdm(site_configs, desc="Step 1: Parsing Addresses", unit="device"):
        device_serial = device.get("serial", "").strip()
        device_identifier = get_device_identifier(device, warn_on_missing=not first_missing_name_warned)
        
        if not first_missing_name_warned and device_identifier != device.get("name", "").strip():
            first_missing_name_warned = True
        
        # Skip if device serial not in comparison file
        if device_serial not in comparison_serials:
            counters.devices_skipped += 1
            if debug:
                logging.debug(f"DEVICE_SKIP [{device_serial}]: Not found in comparison CSV (available: {len(comparison_serials)} serials)")
            continue
        
        counters.devices_enriched += 1
        
        # Enhanced address parsing with error handling
        try:
            # Parse Mist address with enhanced parsing
            mist_address_raw = device.get("site_address", "").strip()
            if not mist_address_raw:
                # Fall back to component parsing if no combined address
                mist_address = {
                    'address': device.get("street", "").strip(),
                    'city': device.get("city", "").strip(),
                    'state': device.get("state", "").strip(),
                    'zip': device.get("zip_code", "").strip()
                }
            else:
                # Use enhanced parsing for combined address
                parsed_mist = enhanced_usaddress_parse(mist_address_raw, debug=debug)
                if not parsed_mist['is_parseable']:
                    # Document parsing failure
                    failure_record = {
                        'site_id': device.get("site_id", ""),
                        'site_name': device.get("site_name", ""),
                        'device_id': device.get("id", ""),
                        'device_serial': device_serial,
                        'device_name': device_identifier,
                        'original_address': mist_address_raw,
                        'parsed_tokens': str(mist_address_raw.split(',')),
                        'failure_reason': parsed_mist['parse_reason'],
                        'timestamp': datetime.now(timezone.utc).isoformat()
                    }
                    parse_failures.append(failure_record)
                    counters.increment_parse_failure(parsed_mist['parse_reason'])
                    
                    # Fall back to component parsing
                    mist_address = {
                        'address': device.get("street", "").strip(),
                        'city': device.get("city", "").strip(),
                        'state': device.get("state", "").strip(),
                        'zip': device.get("zip_code", "").strip()
                    }
                else:
                    mist_address = {
                        'address': parsed_mist.get('address') or "",
                        'city': parsed_mist.get('city') or "",
                        'state': parsed_mist.get('state') or "",
                        'zip': parsed_mist.get('zip') or ""
                    }
            
            # Parse comparison address
            comparison_address_data = comparison_address_lookup.get(device_serial, {})
            
            # Additional safety check: if no comparison data found, skip this device
            if not comparison_address_data or not any(comparison_address_data.values()):
                counters.devices_skipped += 1
                if debug:
                    logging.debug(f"DEVICE_SKIP [{device_serial}]: No comparison address data found")
                continue
            
            comparison_address_raw = f"{comparison_address_data.get('Address', '')}, {comparison_address_data.get('City', '')}, {comparison_address_data.get('State', '')}, {comparison_address_data.get('Zip', '')}".strip(", ")
            
            if comparison_address_raw and comparison_address_raw != "   ":
                parsed_comp = enhanced_usaddress_parse(comparison_address_raw, debug=debug)
                if not parsed_comp['is_parseable']:
                    # Document parsing failure for comparison address
                    failure_record = {
                        'site_id': 'COMPARISON_CSV',
                        'site_name': 'COMPARISON_CSV', 
                        'device_id': device_serial,
                        'device_serial': device_serial,
                        'device_name': device_identifier,
                        'original_address': comparison_address_raw,
                        'parsed_tokens': str(comparison_address_raw.split(',')),
                        'failure_reason': parsed_comp['parse_reason'],
                        'timestamp': datetime.now(timezone.utc).isoformat()
                    }
                    parse_failures.append(failure_record)
                    counters.increment_parse_failure(f"comparison_{parsed_comp['parse_reason']}")
            
            comparison_address = {
                'address': comparison_address_data.get("Address", "").strip(),
                'city': comparison_address_data.get("City", "").strip(),
                'state': comparison_address_data.get("State", "").strip(),
                'zip': comparison_address_data.get("Zip", "").strip()
            }
            
            # Validate that we have meaningful comparison data
            if not any([comparison_address['address'], comparison_address['city'], comparison_address['state'], comparison_address['zip']]):
                counters.devices_skipped += 1
                if debug:
                    logging.debug(f"DEVICE_SKIP [{device_serial}]: Empty comparison address data")
                continue
            
            if debug:
                logging.debug(f"DEVICE_COMPARISON [{device_serial}]: Mist address: {mist_address}")
                logging.debug(f"DEVICE_COMPARISON [{device_serial}]: Comparison address: {comparison_address}")
            
            # Enhanced address comparison with defensive parsing
            comparison_result = enhanced_compare_addresses_with_threshold(
                mist_address, comparison_address, ADDRESS_MATCH_THRESHOLD, debug=debug
            )
            
            if debug:
                logging.debug(f"DEVICE_COMPARISON [{device_serial}]: Enhanced similarity result: {comparison_result}")
            
            # Track match/mismatch statistics
            if comparison_result['is_match']:
                counters.perfect_matches += 1
            else:
                counters.mismatches_found += 1
                # Store conflict for further filtering
                all_conflicts.append({
                    'device': device,
                    'device_serial': device_serial,
                    'device_identifier': device_identifier,
                    'mist_address': mist_address,
                    'comparison_address': comparison_address,
                    'comparison_result': comparison_result
                })
                
        except Exception as device_error:
            logging.warning(f"! Error processing device {device_serial}: {device_error}")
            counters.comparison_failures += 1
            
            # Document this as a parse failure
            failure_record = {
                'site_id': device.get("site_id", ""),
                'site_name': device.get("site_name", ""),
                'device_id': device.get("id", ""),
                'device_serial': device_serial,
                'device_name': device_identifier if 'device_identifier' in locals() else device_serial,
                'original_address': str(device.get("site_address", "")),
                'parsed_tokens': 'N/A',
                'failure_reason': f'device_processing_error: {str(device_error)}',
                'timestamp': datetime.now(timezone.utc).isoformat()
            }
            parse_failures.append(failure_record)
            counters.increment_parse_failure('device_processing_error')

    print(f"! Step 1 Complete: Found {len(all_conflicts)} address conflicts from {counters.devices_enriched} analyzed devices")
    
    # Step 2: Remove duplicate addresses (after normalization) 
    print(" Step 2: Removing duplicate addresses...")
    unique_conflicts = []
    seen_addresses = set()
    
    for conflict in all_conflicts:
        # Create normalized address key for deduplication
        mist_addr = conflict['mist_address']
        comp_addr = conflict['comparison_address']
        address_key = f"{mist_addr['address'].lower().strip()}|{mist_addr['city'].lower().strip()}|{mist_addr['state'].lower().strip()}|{mist_addr['zip'].strip()}" + \
                     f"||{comp_addr['address'].lower().strip()}|{comp_addr['city'].lower().strip()}|{comp_addr['state'].lower().strip()}|{comp_addr['zip'].strip()}"
        
        if address_key not in seen_addresses:
            seen_addresses.add(address_key)
            unique_conflicts.append(conflict)
        else:
            if debug:
                logging.debug(f"DUPLICATE_REMOVED [{conflict['device_serial']}]: Address pair already seen")
    
    duplicates_removed = len(all_conflicts) - len(unique_conflicts)
    print(f"! Step 2 Complete: Removed {duplicates_removed} duplicate address pairs, {len(unique_conflicts)} unique conflicts remain")
    
    # Step 3: Remove addresses in skip file
    print(" Step 3: Applying address skip filters...")
    filtered_conflicts = []
    
    for conflict in unique_conflicts:
        comparison_address = conflict['comparison_address']
        device_serial = conflict['device_serial']
        
        # Check if comparison address should be automatically skipped
        should_skip, skip_reason = check_address_should_skip(comparison_address, skip_addresses, debug=debug)
        
        if should_skip:
            # Automatically treat as a match (Mist address is correct)
            counters.perfect_matches += 1
            counters.auto_corrections += 1
            
            if debug:
                logging.debug(f"ADDRESS_SKIP [{device_serial}]: Skipped comparison address due to: {skip_reason}")
            
            print(f"    Auto-corrected: {device_serial} (Skip reason: {skip_reason})")
        else:
            filtered_conflicts.append(conflict)
    
    skip_filtered = len(unique_conflicts) - len(filtered_conflicts)
    print(f"! Step 3 Complete: Removed {skip_filtered} addresses via skip filters, {len(filtered_conflicts)} conflicts require analysis")
    
    # Step 4: Prepare for external validation (only if enabled)
    if address_validation_enabled and filtered_conflicts:
        devices_needing_validation = [(c['device'], c['device_serial'], c['mist_address'], c['comparison_address']) for c in filtered_conflicts]
        total_validations = len(devices_needing_validation)
        print(f"\n  Step 4: External address validation enabled - {total_validations} remaining conflicts need validation")
        print(" This may take several minutes due to API rate limiting (1 request/second)...")
        if debug:
            logging.debug(f"ADDRESS_VALIDATION: {total_validations} devices require external validation after filtering")
    elif not address_validation_enabled and filtered_conflicts:
        # Process conflicts without validation
        print(f"\n  Step 4: Processing {len(filtered_conflicts)} conflicts without external validation...")
        for conflict in filtered_conflicts:
            device = conflict['device'] 
            device_serial = conflict['device_serial']
            comparison_result = conflict['comparison_result']
            mist_address = conflict['mist_address']
            comparison_address = conflict['comparison_address']
            
            # Generate mismatch records
            try:
                created_time = int(device.get("created_time", 0))
                created_date = datetime.fromtimestamp(created_time, tz=timezone.utc)
                year, week, _ = created_date.isocalendar()
                week_key = f"{year}_Week_{week:02d}"

                # Determine primary mismatch type based on failed fields
                failed_fields = comparison_result['failed_fields']
                if 'zip' in failed_fields and len(failed_fields) == 1:
                    mismatch_type = "Zip Code Mismatch"
                elif 'address' in failed_fields:
                    mismatch_type = "Address Mismatch"
                elif 'city' in failed_fields:
                    mismatch_type = "City Mismatch"
                elif 'state' in failed_fields:
                    mismatch_type = "State Mismatch"
                else:
                    mismatch_type = "Multi-field Address Mismatch"

                # Enhanced mismatch item with parse status
                mismatched_item = {
                    "Week": week_key,
                    "Full Site": device.get("site_name", ""),
                    "System Serial Number": device_serial,
                    "System Model Number": device.get("model", ""),
                    "End Customer Name": END_CUSTOMER_NAME,
                    "Address Line 1": mist_address['address'],
                    "Address Line 2": "",
                    "City": mist_address['city'],
                    "State": mist_address['state'],
                    "Current Zip Code": mist_address['zip'],
                    "Current Zip Normalized": normalize_zip_code(mist_address['zip']),
                    "Comparison Zip Code": comparison_address['zip'],
                    "End Customer Account ID": END_CUSTOMER_ACCOUNT_ID,
                    "Mismatch Type": mismatch_type,
                    "Overall Similarity": f"{comparison_result['overall_similarity']:.1f}%",
                    "Address Similarity": f"{comparison_result['field_similarities']['address']:.1f}%",
                    "City Similarity": f"{comparison_result['field_similarities']['city']:.1f}%",
                    "State Similarity": f"{comparison_result['field_similarities']['state']:.1f}%",
                    "Zip Similarity": f"{comparison_result['field_similarities']['zip']:.1f}%",
                    "Failed Fields": ', '.join(failed_fields),
                    # Enhanced fields
                    "Mist_Parse_Status": comparison_result['parse_status']['mist_parseable'],
                    "Comparison_Parse_Status": comparison_result['parse_status']['comparison_parseable'],
                    "Parse_Issues": f"Mist: {comparison_result['parse_status']['mist_reason']}, Comp: {comparison_result['parse_status']['comparison_reason']}",
                    # Address validation results (No validation in basic mode)
                    "Mist_Validation_Status": 'N/A',
                    "Mist_Confidence": 'N/A',
                    "Comparison_Validation_Status": 'N/A',
                    "Comparison_Confidence": 'N/A',
                    "Validation_Recommendation": 'N/A'
                }
                mismatched_items.append(mismatched_item)

                # Enhanced diff report item with parse status
                diff_item = {
                    "Week": week_key,
                    "Full Site": device.get("site_name", ""),
                    "System Serial Number": device_serial,
                    "System Model Number": device.get("model", ""),
                    "End Customer Name": END_CUSTOMER_NAME,
                    "Mist_Address_Line_1": mist_address['address'],
                    "Mist_City": mist_address['city'],
                    "Mist_State": mist_address['state'],
                    "Mist_Zip_Code": mist_address['zip'],
                    "Mist_Zip_Normalized": normalize_zip_code(mist_address['zip']),
                    "Comparison_Address": comparison_address['address'],
                    "Comparison_City": comparison_address['city'],
                    "Comparison_State": comparison_address['state'],
                    "Comparison_Zip_Code": comparison_address['zip'],
                    "Comparison_Zip_Normalized": normalize_zip_code(comparison_address['zip']),
                    "End Customer Account ID": END_CUSTOMER_ACCOUNT_ID,
                    "Mismatch Type": mismatch_type,
                    "Overall Similarity": f"{comparison_result['overall_similarity']:.1f}%",
                    "Address Similarity": f"{comparison_result['field_similarities']['address']:.1f}%",
                    "City Similarity": f"{comparison_result['field_similarities']['city']:.1f}%",
                    "State Similarity": f"{comparison_result['field_similarities']['state']:.1f}%",
                    "Zip Similarity": f"{comparison_result['field_similarities']['zip']:.1f}%",
                    "Failed Fields": ', '.join(failed_fields),
                    # Enhanced fields
                    "Mist_Parse_Status": comparison_result['parse_status']['mist_parseable'],
                    "Comparison_Parse_Status": comparison_result['parse_status']['comparison_parseable'],
                    "Parse_Issues": f"Mist: {comparison_result['parse_status']['mist_reason']}, Comp: {comparison_result['parse_status']['comparison_reason']}",
                    # Address validation results (No validation in basic mode)
                    "Mist_Validation_Status": 'N/A',
                    "Mist_Confidence": 'N/A',
                    "Comparison_Validation_Status": 'N/A',
                    "Comparison_Confidence": 'N/A',
                    "Validation_Recommendation": 'N/A'
                }
                diff_report_items.append(diff_item)
                
            except Exception as mismatch_error:
                logging.warning(f"! Error processing mismatch for device {device_serial}: {mismatch_error}")
                counters.comparison_failures += 1
    
    # Step 4 (continued): Process devices that need validation with proper progress bar
    if address_validation_enabled and devices_needing_validation:
        # Get organization name for intelligent tiebreaker logic
        org_name = None
        try:
            if debug:
                logging.debug("Fetching organization information for tiebreaker logic...")
            org_response = mistapi.api.v1.orgs.orgs.getOrg(apisession, org_id)
            if org_response.status_code == 200:
                org_data = org_response.data
                org_name = org_data.get('name', '').strip()
                if debug:
                    logging.debug(f"Organization name retrieved: '{org_name}'")
            else:
                if debug:
                    logging.warning(f"Failed to retrieve organization info: HTTP {org_response.status_code}")
        except Exception as e:
            if debug:
                logging.warning(f"Could not retrieve organization name for tiebreaker: {e}")
        
        validation_count = 0
        for device, device_serial, mist_address, comparison_address in tqdm(devices_needing_validation, desc="Step 4: Validating Addresses", unit="device"):
            validation_count += 1
            if debug:
                logging.debug(f"DEVICE_VALIDATION [{device_serial}]: Starting validation process")
                logging.debug(f"DEVICE_VALIDATION [{device_serial}]: Mist address: {mist_address}")
                logging.debug(f"DEVICE_VALIDATION [{device_serial}]: Comparison address: {comparison_address}")
            
            # Find the corresponding conflict for this device
            conflict = next((c for c in filtered_conflicts if c['device_serial'] == device_serial), None)
            if not conflict:
                logging.warning(f"Could not find conflict data for device {device_serial}")
                continue
                
            comparison_result = conflict['comparison_result']
            
            if debug:
                logging.debug(f"DEVICE_VALIDATION [{device_serial}]: Similarity result: {comparison_result}")
            
            # Perform external address validation
            validation_result = None
            ADDRESS_VALIDATION_TIMEOUT = int(os.getenv("ADDRESS_VALIDATION_TIMEOUT", "10"))
            
            try:
                # Create formatted address strings for logging
                mist_addr_str = f"{mist_address['address']}, {mist_address['city']}, {mist_address['state']} {mist_address['zip']}".replace(", , ", ", ").strip(", ")
                comp_addr_str = f"{comparison_address['address']}, {comparison_address['city']}, {comparison_address['state']} {comparison_address['zip']}".replace(", , ", ", ").strip(", ")
                
                print(f"! [{validation_count}/{total_validations}] Validating {device_serial}...")
                print(f"    Mist:       {mist_addr_str}")
                print(f"    Reference:  {comp_addr_str}")
                
                logging.info(f"ADDRESS_VALIDATION [{device_serial}]: Starting validation")
                logging.info(f"ADDRESS_VALIDATION [{device_serial}]: Mist address: {mist_addr_str}")
                logging.info(f"ADDRESS_VALIDATION [{device_serial}]: Comparison address: {comp_addr_str}")
                
                validation_result = validate_addresses_with_nominatim(
                    mist_address, comparison_address, ADDRESS_VALIDATION_TIMEOUT, debug=debug, skip_ssl_verify=skip_ssl_verify, org_name=org_name,
                    mist_duplicates=mist_duplicates, ref_duplicates=ref_duplicates, site_name=device.get("site_name", "")
                )
                
                # Format results for display
                mist_status = " Valid" if validation_result['mist_validation']['valid'] else " Invalid"
                comp_status = " Valid" if validation_result['comparison_validation']['valid'] else " Invalid"
                
                mist_conf = f"{validation_result['mist_validation']['confidence']:.3f}" if validation_result['mist_validation']['valid'] else "N/A"
                comp_conf = f"{validation_result['comparison_validation']['confidence']:.3f}" if validation_result['comparison_validation']['valid'] else "N/A"
                
                recommendation_icon = {"mist": " Mist", "comparison": " Reference", "uncertain": " Uncertain"}
                recommendation_display = recommendation_icon.get(validation_result['recommendation'], validation_result['recommendation'])
                recommendation_reason = validation_result.get('recommendation_reason', 'No reason provided')
                
                print(f"    Results:    Mist: {mist_status} (conf: {mist_conf}) | Reference: {comp_status} (conf: {comp_conf})")
                print(f"    Recommendation: {recommendation_display}")
                if validation_result['recommendation'] != 'uncertain' or 'inconclusive' not in recommendation_reason.lower():
                    print(f"    Reason: {recommendation_reason}")
                
                logging.info(f"ADDRESS_VALIDATION [{device_serial}]: Mist validation - valid: {validation_result['mist_validation']['valid']}, confidence: {mist_conf}")
                logging.info(f"ADDRESS_VALIDATION [{device_serial}]: Comparison validation - valid: {validation_result['comparison_validation']['valid']}, confidence: {comp_conf}")
                logging.info(f"ADDRESS_VALIDATION [{device_serial}]: Final recommendation: {validation_result['recommendation']}")
                logging.info(f"ADDRESS_VALIDATION [{device_serial}]: Recommendation reason: {recommendation_reason}")
                
            except Exception as e:
                print(f"    Validation failed: {str(e)}")
                logging.warning(f"ADDRESS_VALIDATION [{device_serial}]: Validation failed: {e}")
                if debug:
                    logging.debug(f"ADDRESS_VALIDATION [{device_serial}]: Full exception traceback: {traceback.format_exc()}")
                validation_result = None
            
            # Process mismatch logic for this device
            if not comparison_result['is_match']:
                try:
                    created_time = int(device.get("created_time", 0))
                    created_date = datetime.fromtimestamp(created_time, tz=timezone.utc)
                    year, week, _ = created_date.isocalendar()
                    week_key = f"{year}_Week_{week:02d}"

                    # Determine primary mismatch type based on failed fields
                    failed_fields = comparison_result['failed_fields']
                    if 'zip' in failed_fields and len(failed_fields) == 1:
                        mismatch_type = "Zip Code Mismatch"
                    elif 'address' in failed_fields:
                        mismatch_type = "Address Mismatch"
                    elif 'city' in failed_fields:
                        mismatch_type = "City Mismatch"
                    elif 'state' in failed_fields:
                        mismatch_type = "State Mismatch"
                    else:
                        mismatch_type = "Multi-field Address Mismatch"

                    # Standard mismatch item (enhanced format)
                    mismatched_item = {
                        "Week": week_key,
                        "Full Site": device.get("site_name", ""),
                        "System Serial Number": device_serial,
                        "System Model Number": device.get("model", ""),
                        "End Customer Name": END_CUSTOMER_NAME,
                        "Address Line 1": mist_address['address'],
                        "Address Line 2": "",
                        "City": mist_address['city'],
                        "State": mist_address['state'],
                        "Current Zip Code": mist_address['zip'],
                        "Current Zip Normalized": normalize_zip_code(mist_address['zip']),
                        "Comparison Zip Code": comparison_address['zip'],
                        "End Customer Account ID": END_CUSTOMER_ACCOUNT_ID,
                        "Mismatch Type": mismatch_type,
                        "Overall Similarity": f"{comparison_result['overall_similarity']:.1f}%",
                        "Address Similarity": f"{comparison_result['field_similarities']['address']:.1f}%",
                        "City Similarity": f"{comparison_result['field_similarities']['city']:.1f}%",
                        "State Similarity": f"{comparison_result['field_similarities']['state']:.1f}%",
                        "Zip Similarity": f"{comparison_result['field_similarities']['zip']:.1f}%",
                        "Failed Fields": ', '.join(failed_fields),
                        # Address validation results (if enabled)
                        "Mist_Validation_Status": validation_result['mist_validation']['valid'] if validation_result else 'N/A',
                        "Mist_Confidence": f"{validation_result['mist_validation']['confidence']:.3f}" if validation_result and validation_result['mist_validation']['valid'] else 'N/A',
                        "Comparison_Validation_Status": validation_result['comparison_validation']['valid'] if validation_result else 'N/A',
                        "Comparison_Confidence": f"{validation_result['comparison_validation']['confidence']:.3f}" if validation_result and validation_result['comparison_validation']['valid'] else 'N/A',
                        "Validation_Recommendation": validation_result['recommendation'] if validation_result else 'N/A'
                    }
                    mismatched_items.append(mismatched_item)

                    # Diff report item (showing both address sets with similarity scores)
                    diff_item = {
                        "Week": week_key,
                        "Full Site": device.get("site_name", ""),
                        "System Serial Number": device_serial,
                        "System Model Number": device.get("model", ""),
                        "End Customer Name": END_CUSTOMER_NAME,
                        "Mist_Address_Line_1": mist_address['address'],
                        "Mist_City": mist_address['city'],
                        "Mist_State": mist_address['state'],
                        "Mist_Zip_Code": mist_address['zip'],
                        "Mist_Zip_Normalized": normalize_zip_code(mist_address['zip']),
                        "Comparison_Address": comparison_address['address'],
                        "Comparison_City": comparison_address['city'],
                        "Comparison_State": comparison_address['state'],
                        "Comparison_Zip_Code": comparison_address['zip'],
                        "Comparison_Zip_Normalized": normalize_zip_code(comparison_address['zip']),
                        "End Customer Account ID": END_CUSTOMER_ACCOUNT_ID,
                        "Mismatch Type": mismatch_type,
                        "Overall Similarity": f"{comparison_result['overall_similarity']:.1f}%",
                        "Address Similarity": f"{comparison_result['field_similarities']['address']:.1f}%",
                        "City Similarity": f"{comparison_result['field_similarities']['city']:.1f}%",
                        "State Similarity": f"{comparison_result['field_similarities']['state']:.1f}%", 
                        "Zip Similarity": f"{comparison_result['field_similarities']['zip']:.1f}%",
                        "Failed Fields": ', '.join(failed_fields),
                        # Address validation results (if enabled)
                        "Mist_Validation_Status": validation_result['mist_validation']['valid'] if validation_result else 'N/A',
                        "Mist_Confidence": f"{validation_result['mist_validation']['confidence']:.3f}" if validation_result and validation_result['mist_validation']['valid'] else 'N/A',
                        "Comparison_Validation_Status": validation_result['comparison_validation']['valid'] if validation_result else 'N/A',
                        "Comparison_Confidence": f"{validation_result['comparison_validation']['confidence']:.3f}" if validation_result and validation_result['comparison_validation']['valid'] else 'N/A',
                        "Validation_Recommendation": validation_result['recommendation'] if validation_result else 'N/A'
                    }
                    diff_report_items.append(diff_item)
                except Exception as e:
                    logging.warning(f"! Skipping device due to error: {e}")
    
    else:
        # Skip external validation section since it's already handled above in the improved order of operations
        pass

    # End timing and generate artifacts
    counters.end_timing()
    
    # Create parse failures artifact if there were any
    if parse_failures:
        create_address_parse_failures_csv(parse_failures)
    
    # Enhanced results display
    print(f"\n  Data Integrity Analysis Results:")
    print(f"   Total devices analyzed: {counters.total_devices}")
    print(f"   Devices with comparison data: {counters.devices_enriched}")
    print(f"    Devices excluded (not in comparison CSV): {counters.devices_skipped}")
    print(f"   Address conflicts found: {counters.mismatches_found}")
    print(f"   Consistent addresses: {counters.perfect_matches}")
    print(f"   Auto-skipped addresses: {counters.auto_corrections}")
    print(f"   Parse failures: {counters.parse_failures}")
    
    if counters.mismatches_found > 0:
        conflict_rate = (counters.mismatches_found / counters.devices_enriched) * 100
        print(f"   Conflict rate: {conflict_rate:.1f}% of analyzed devices have address discrepancies")
    
    if counters.parse_failures > 0:
        print(f"   Parse failure breakdown:")
        for reason, count in counters.parse_failure_reasons.items():
            print(f"      - {reason}: {count}")
    
    processing_rate = counters.total_devices / counters.get_duration() if counters.get_duration() > 0 else 0
    print(f"    Processing rate: {processing_rate:.1f} devices/second")

    # Log comprehensive summary
    counters.log_summary()

    if mismatched_items:
        print(f"\n  Data Integrity Conflicts (address discrepancies requiring review):")
        print("=" * 130)
        for idx, item in enumerate(mismatched_items[:10]):  # Show first 10
            mist_addr = f"{item.get('Mist_Address_Line_1', '')}, {item.get('Mist_City', '')}, {item.get('Mist_State', '')}"
            comp_addr = f"{item.get('Comparison_Address', '')}, {item.get('Comparison_City', '')}, {item.get('Comparison_State', '')}"
            print(f"[{idx+1:2}] Serial: {item['System Serial Number']:<15}")
            print(f"     Mist:       {mist_addr}")
            print(f"     Reference:  {comp_addr}")
            print(f"     Similarity: {item['Overall Similarity']:<6} | Type: {item['Mismatch Type']}")
            if address_validation_enabled and item.get('Validation_Recommendation', 'N/A') != 'N/A':
                print(f"     Recommendation: {item['Validation_Recommendation']}")
            print()
        
        if len(mismatched_items) > 10:
            print(f"   ... and {len(mismatched_items) - 10} more conflicts (see CSV report for complete list)")
            
        # Always save to CSV (no prompting)
        if mismatched_items:
            base_filename = comparison_file.replace('.csv', '')
            
            # Save comprehensive address comparison report with both address sets
            output_file = f"AddressMismatches_vs_{base_filename}.csv"
            fieldnames = [
                "Week", "Full Site", "System Serial Number", "System Model Number", 
                "End Customer Name", "Mist_Address_Line_1", "Mist_City", "Mist_State",
                "Mist_Zip_Code", "Mist_Zip_Normalized", "Comparison_Address", "Comparison_City", 
                "Comparison_State", "Comparison_Zip_Code", "Comparison_Zip_Normalized",
                "End Customer Account ID", "Mismatch Type", "Overall Similarity",
                "Address Similarity", "City Similarity", "State Similarity", "Zip Similarity", "Failed Fields",
                "Mist_Parse_Status", "Comparison_Parse_Status", "Parse_Issues",
                "Mist_Validation_Status", "Mist_Confidence", "Comparison_Validation_Status", "Comparison_Confidence", 
                "Validation_Recommendation"
            ]
            
            with open(output_file, mode="w", newline="", encoding="utf-8") as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(diff_report_items)
            
            print(f"! Data integrity report saved to: {output_file}")
            print(f"! Location: {get_csv_file_path(output_file)}")
            print(f"\n  Data Integrity Summary:")
            print(f"   Found {len(diff_report_items)} address conflicts requiring review")
            if address_validation_enabled:
                print(f"   External validation recommendations included")
                print(f"   Check 'Validation_Recommendation' column for guidance")
            else:
                print(f"    No external validation performed")
                print(f"   Run with --address-check for intelligent recommendations")
            
            logging.info(f"Saved {len(diff_report_items)} address conflicts to {output_file}")
    else:
        total_good_addresses = counters.perfect_matches + counters.auto_corrections
        print(f"! Data integrity check complete! All {total_good_addresses} addresses are consistent.")
        print(f"   No conflicts found between Mist and comparison data")
        if counters.auto_corrections > 0:
            print(f"   {counters.auto_corrections} addresses auto-skipped via AddressSkip.csv")

def export_gateway_templates_to_csv():
    """
    Fetches all gateway templates for the organization and exports them to OrgGatewayTemplates.csv.
    """
    print("Gateway Templates:")
    logging.info("Exporting gateway templates for the organization...")
    org_id = get_cached_or_prompted_org_id()
    # Fetch gateway templates using the Mist API
    response = mistapi.api.v1.orgs.gatewaytemplates.listOrgGatewayTemplates(apisession, org_id)
    templates = getattr(response, "data", [])
    if not templates:
        logging.warning("No gateway templates found for this organization.")
        print("No gateway templates found for this organization.")
        return
    # Flatten and sanitize for CSV
    templates = flatten_nested_fields_in_list(templates)
    templates = escape_multiline_strings_for_csv(templates)
    DataExporter.save_data_to_output(templates, "OrgGatewayTemplates.csv")
    print(f"! {len(templates)} gateway templates exported to OrgGatewayTemplates.csv")
    logging.info(" Gateway templates exported to OrgGatewayTemplates.csv")

def export_gateways_with_wan_overrides_to_csv(fast=False):
    """
    Generates a CSV report of gateways with ports that are overridden from their template configuration.
    This helps identify outliers that need to be corrected back to template compliance.
    
    Report includes for OVERRIDDEN ports only:
    - Gateway Router Device Name  
    - Port descriptions/labels for ge-0/0/0, ge-0/0/1, ge-0/0/2, {{wan1_interface}}, {{wan2_interface}}, {{wan3_interface}}
    - Port status (up/down)
    - Port admin status (disabled/enabled)
    - Port gateway IP address
    - Port IP address
    - Port netmask
    - Port config type (DHCP or STATIC)
    - Port name/number
    - Whether port is overridden from template (always "Yes" for filtered results)
    
    Searches 6 total ports: 3 hardcoded (ge-0/0/0, ge-0/0/1, ge-0/0/2) + 3 variable-based 
    ({{wan1_interface}}, {{wan2_interface}}, {{wan3_interface}}) for comprehensive coverage.
    """
    print("Gateway Ports Overridden from Template (Compliance Outliers):")
    logging.info(" Identifying gateway ports with template overrides (outliers for compliance correction)...")

    # Ensure required CSVs are fresh
    check_and_generate_csv("AllSiteGatewayConfigs.csv", lambda: export_gateway_device_configs_to_csv(fast=fast))
    check_and_generate_csv("SiteList_ListAPI.csv", export_all_sites_list_to_csv)
    check_and_generate_csv("OrgGatewayTemplates.csv", export_gateway_templates_to_csv)

    # Load data
    with open(get_csv_file_path("AllSiteGatewayConfigs.csv"), encoding="utf-8") as f:
        configs = list(csv.DictReader(f))
    with open(get_csv_file_path("SiteList_ListAPI.csv"), encoding="utf-8") as f:
        sites = list(csv.DictReader(f))
    with open(get_csv_file_path("OrgGatewayTemplates.csv"), encoding="utf-8") as f:
        templates = list(csv.DictReader(f))

    # Create lookups for site and template names
    site_lookup = {site.get("id"): site.get("name", "Unknown Site") for site in sites}
    # Create site to gateway template ID mapping from SiteList
    site_to_template_id = {site.get("id"): site.get("gatewaytemplate_id", "") for site in sites}
    template_lookup = {t.get("id"): t.get("name", "Unknown Template") for t in templates}
    
    # Debug template lookup
    logging.debug(f"[DEBUG] Created template lookup with {len(template_lookup)} templates")
    for template_id, template_name in list(template_lookup.items())[:3]:  # Show first 3 for debugging
        logging.debug(f"[DEBUG] Template: {template_id} -> {template_name}")

    overridden_port_info = []
    # Target ports: original 3 hardcoded ports + 3 variable-based ports (6 total)
    target_ports = ["ge-0/0/0", "ge-0/0/1", "ge-0/0/2", "{{wan1_interface}}", "{{wan2_interface}}", "{{wan3_interface}}"]

    # OPTIMIZATION: First pass - identify devices with overrides without fetching stats
    logging.info(" First pass: Identifying devices with port overrides...")
    devices_with_overrides = {}  # device_id -> (device_info, overridden_port_names)
    
    for row in configs:
        device_name = row.get("name", "").strip()
        site_id = row.get("site_id", "").strip()
        device_id = row.get("id", "").strip()
        site_name = site_lookup.get(site_id, "Unknown Site")
        # Get template ID from site-level gateway template assignment
        template_id = site_to_template_id.get(site_id, "")
        template_name = template_lookup.get(template_id, "No Template") if template_id else "No Template"
        
        # Debug template lookup for this device
        if template_id:
            if template_id in template_lookup:
                logging.debug(f"[DEBUG] Device {device_name}: site_id='{site_id}' -> template_id='{template_id}' -> template_name='{template_name}'")
            else:
                logging.warning(f"[WARN] Device {device_name}: Template ID '{template_id}' not found in gateway templates (orphaned assignment)")
                template_name = f"Missing Template ({template_id[:8]}...)"
        else:
            logging.debug(f"[DEBUG] Device {device_name}: No gatewaytemplate_id found for site {site_id}")
        
        if not device_name or not site_id or not device_id:
            continue

        # Check each target port for overrides using CSV data only
        device_overridden_ports = []
        for port_name in target_ports:
            # Check if port is overridden from template by looking for port_config fields in the CSV
            # Need to check for both base port (port_config_{port}_*) and subinterfaces (port_config_{port}.*) 
            # to catch configurations like {{wan2_interface}}.70 or ge-0/0/1.100
            port_config_fields = [col for col in row if 
                col.startswith(f"port_config_{port_name}_") or 
                col.startswith(f"port_config_{port_name}.")]
            
            # Check for non-empty values (excluding vpn_paths which are template-inherited)
            override_fields = []
            for field in port_config_fields:
                value = row.get(field, "").strip().lower()
                if value not in ["", "null", "none"] and "_vpn_paths_" not in field:
                    override_fields.append(f"{field}={value}")
            
            is_overridden = len(override_fields) > 0
            if is_overridden:
                device_overridden_ports.append(port_name)
        
        # If this device has any overridden ports, mark it for API calls
        if device_overridden_ports:
            devices_with_overrides[device_id] = {
                "device_name": device_name,
                "site_id": site_id,
                "site_name": site_name,
                "template_id": template_id,
                "template_name": template_name,
                "row_data": row,
                "overridden_ports": device_overridden_ports
            }

    logging.info(f"! Found {len(devices_with_overrides)} devices with port overrides out of {len(configs)} total gateway devices")
    
    if not devices_with_overrides:
        logging.info(" No template overrides found - all gateways are compliant with their assigned templates!")
        # Still create empty CSV file with proper headers
        output_file = "GatewayOverriddenPorts.csv"
        fieldnames = [
            "gateway_device_name", "site_name", "template_name", "port_name", "recommended_variable",
            "port_description", "port_status", "port_admin_status", "port_gateway_ip", "port_ip_address",
            "port_netmask", "port_config_type", "port_usage", "overridden_from_template",
            "device_id", "site_id", "template_id"
        ]
        output_path = get_csv_file_path(output_file)
        with open(output_path, mode="w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
        print(f"! Gateway override report written to {output_file}")
        print(" No template overrides found - all gateways are compliant with their assigned templates!")
        return

    # OPTIMIZATION: Second pass - fetch device configs and stats only for devices with overrides
    logging.info(f"! Second pass: Fetching device configs and stats for {len(devices_with_overrides)} devices with overrides...")
    
    if fast and len(devices_with_overrides) > 5:  # Use connection pool management for fast mode with 5+ devices
        logging.info(" Using fast mode with connection pool management for device data fetching...")
        
        # Define worker function for fetching device configs and stats
        def fetch_device_data(device_info, connection_semaphore):
            """Worker function that fetches config and stats for a single device."""
            device_id = device_info[0]
            device_data = device_info[1]
            device_name = device_data["device_name"]
            site_id = device_data["site_id"]
            
            # Acquire connection semaphore before making API calls
            with connection_semaphore:
                port_configs = {}
                interface_stats = {}
                
                # Fetch live device info from getSiteDevice API for current config
                try:
                    resp = mistapi.api.v1.sites.devices.getSiteDevice(apisession, site_id, device_id)
                    device_config_data = getattr(resp, "data", {})
                    port_configs = device_config_data.get("port_config", {})
                except Exception as e:
                    logging.warning(f"[WARN] Could not fetch device config for {device_name} ({device_id}): {e}")
                    port_configs = {}

                # Fetch live device stats for current port status 
                try:
                    stats_resp = mistapi.api.v1.sites.stats.getSiteDeviceStats(apisession, site_id, device_id)
                    stats_data = getattr(stats_resp, "data", {})
                    interface_stats = stats_data.get("if_stat", {})
                except Exception as e:
                    # Handle 403 Forbidden and other errors gracefully
                    if "403" in str(e) or "Forbidden" in str(e):
                        logging.warning(f"[WARN] Insufficient permissions to fetch device stats for {device_name} ({device_id}): 403 Forbidden")
                    else:
                        logging.warning(f"[WARN] Could not fetch device stats for {device_name} ({device_id}): {e}")
                    interface_stats = {}

                return (device_id, port_configs, interface_stats)
        
        # Prepare work items for the helper
        work_items = list(devices_with_overrides.items())
        
        # Use the reusable connection pool management helper
        successful_results, failed_devices = execute_with_connection_pool_management(
            work_items=work_items,
            worker_function=fetch_device_data,
            batch_description="override devices",
            retry_function=None  # No retry for this use case
        )
        
        # Build device_data_cache from successful results
        device_data_cache = {}
        for device_id, port_configs, interface_stats in successful_results:
            device_data_cache[device_id] = (port_configs, interface_stats)
        
        # Handle failed devices (fallback to empty configs)
        for failed_item in failed_devices:
            device_id = failed_item[0]
            device_data_cache[device_id] = ({}, {})
        
        logging.info(f"! Fast mode: Fetched data for {len(successful_results)}/{len(work_items)} devices with connection pool protection")
        
    else:
        # Regular sequential processing for non-fast mode or small datasets
        device_data_cache = {}  # device_id -> (port_configs, interface_stats)
        
        for device_id, device_info in devices_with_overrides.items():
            device_name = device_info["device_name"]
            site_id = device_info["site_id"]
            
            # Fetch live device info from getSiteDevice API for current config
            try:
                resp = mistapi.api.v1.sites.devices.getSiteDevice(apisession, site_id, device_id)
                device_data = getattr(resp, "data", {})
                port_configs = device_data.get("port_config", {})
            except Exception as e:
                logging.warning(f"[WARN] Could not fetch device config for {device_name} ({device_id}): {e}")
                port_configs = {}

            # Fetch live device stats for current port status 
            try:
                stats_resp = mistapi.api.v1.sites.stats.getSiteDeviceStats(apisession, site_id, device_id)
                stats_data = getattr(stats_resp, "data", {})
                interface_stats = stats_data.get("if_stat", {})
            except Exception as e:
                # Handle 403 Forbidden and other errors gracefully
                if "403" in str(e) or "Forbidden" in str(e):
                    logging.warning(f"[WARN] Insufficient permissions to fetch device stats for {device_name} ({device_id}): 403 Forbidden")
                else:
                    logging.warning(f"[WARN] Could not fetch device stats for {device_name} ({device_id}): {e}")
                interface_stats = {}

            device_data_cache[device_id] = (port_configs, interface_stats)

    # Third pass: Process only the overridden ports with their stats
    logging.info(" Third pass: Processing overridden ports with live data...")
    for device_id, device_info in devices_with_overrides.items():
        device_name = device_info["device_name"]
        site_id = device_info["site_id"]
        site_name = device_info["site_name"]
        template_id = device_info["template_id"]
        template_name = device_info["template_name"]
        row = device_info["row_data"]
        overridden_ports = device_info["overridden_ports"]
        
        port_configs, interface_stats = device_data_cache.get(device_id, ({}, {}))

        # Process each overridden port
        for port_name in overridden_ports:
            port_config = port_configs.get(port_name, {})
            interface_stat = interface_stats.get(port_name, {})
            
            # Get port config fields from CSV for override details
            port_config_fields = [col for col in row if col.startswith(f"port_config_{port_name}_")]
            
            # Extract port configuration details
            ip_config = port_config.get("ip_config", {})
            usage = port_config.get("usage", "")
            description = port_config.get("description", "")
            disabled = port_config.get("disabled", False)
            
            # Extract IP configuration details
            port_ip = ip_config.get("ip", "")
            netmask = ip_config.get("netmask", "")
            gateway_ip = ip_config.get("gateway", "")
            config_type = ip_config.get("type", "")
            
            # Convert config type to human readable
            if config_type == "dhcp":
                config_type_display = "DHCP"
            elif config_type == "static":
                config_type_display = "STATIC"
            else:
                config_type_display = config_type.upper() if config_type else "UNKNOWN"
            
            # Extract port status from interface stats
            port_status = "down"
            if interface_stat:
                # Check the "up" field from if_stat which is the actual port status
                if interface_stat.get("up", False):
                    port_status = "up"
            
            # Admin status
            admin_status = "disabled" if disabled else "enabled"
            
            # Create detailed port entry for overridden port
            port_entry = {
                "gateway_device_name": device_name,
                "site_name": site_name,
                "template_name": template_name,
                "port_name": port_name,
                "port_description": description,
                "port_status": port_status,
                "port_admin_status": admin_status,
                "port_gateway_ip": gateway_ip,
                "port_ip_address": port_ip,
                "port_netmask": netmask,
                "port_config_type": config_type_display,
                "port_usage": usage,
                "overridden_from_template": "Yes",
                "device_id": device_id,
                "site_id": site_id,
                "template_id": template_id
            }
            
            # Add the overridden port to our results
            overridden_port_info.append(port_entry)

    # Write to CSV with only overridden port information
    output_file = "GatewayOverriddenPorts.csv"
    DataExporter.save_data_to_output(overridden_port_info, output_file)

    # Calculate summary statistics
    total_gateways_processed = len(configs)
    devices_with_overrides_count = len(devices_with_overrides) if 'devices_with_overrides' in locals() else 0
    if overridden_port_info:
        gateways_with_overrides = len(set(entry["device_id"] for entry in overridden_port_info))
    else:
        gateways_with_overrides = 0
    total_overridden_ports = len(overridden_port_info)

    logging.info(f"! Gateway override report written to {output_file} with {total_overridden_ports} overridden ports from {gateways_with_overrides} gateway devices.")
    logging.info(f"! API Optimization: Made device config/stats calls for only {devices_with_overrides_count} devices instead of all {total_gateways_processed} devices")
    print(f"! Gateway override report written to {output_file}")
    print(f"! Found {total_overridden_ports} overridden ports across {gateways_with_overrides} of {total_gateways_processed} gateway devices")
    print(f"! API Optimization: Only fetched live data for {devices_with_overrides_count} devices with overrides (saved {total_gateways_processed - devices_with_overrides_count} unnecessary API calls)")
    print(f"! Target ports analyzed: {', '.join(target_ports)}")
    print(f"! These are outliers that may need correction to match template configuration")
    
    if total_overridden_ports == 0:
        print(" No template overrides found - all gateways are compliant with their assigned templates!")

def set_wan2_interface_site_variable():
    """
    Menu #103: Set WAN2 Interface Site Variable
    
    Creates and sets the {{wan2_interface}} site variable to 'ge-0/0/1' across selected sites.
    This prepares sites for template-based WAN2 interface configuration migration.
    
    Workflow:
    1. Prompts for site selection (single or multiple)
    2. For each site, checks for existing WAN2 port overrides (ge-0/0/1 OR {{wan2_interface}})
    3. Sets site variable 'wan2_interface'='ge-0/0/1' via updateSiteSettings API
    4. Generates report showing:
       - Sites with variable successfully set
       - Sites with WAN2 port overrides (flagged for manual review)
       - Current WAN2 configuration details
    
    Override Detection: Checks BOTH hardcoded 'ge-0/0/1' AND variable-based '{{wan2_interface}}'
    port configurations to identify any device-level overrides requiring manual review.
    
    SECURITY: Read current settings before write to preserve other configurations.
    Safe operation - only modifies site variables, not device or template configs.
    """
    print("\n  Set WAN2 Interface Site Variable")
    print("=" * 70)
    print("  This operation will set the 'wan2_interface' site variable to 'ge-0/0/1'")
    print("  across selected sites, preparing them for template-based WAN migration.")
    print("=" * 70)
    
    logging.info("Menu #103: Set WAN2 Interface Site Variable operation started")
    
    org_id = get_cached_or_prompted_org_id()
    
    # Step 1: Ensure required CSVs are fresh
    print("\n  Preparing site and gateway configuration data...")
    check_and_generate_csv("SiteList.csv", export_all_sites_to_csv)
    check_and_generate_csv("AllSiteGatewayConfigs.csv", export_gateway_device_configs_to_csv)
    check_and_generate_csv("OrgGatewayTemplates.csv", export_gateway_templates_to_csv)
    
    # Step 2: Load site data
    site_list_path = get_csv_file_path("SiteList.csv")
    with open(site_list_path, encoding="utf-8") as f:
        sites = list(csv.DictReader(f))
    
    if not sites:
        print(" No sites found in organization.")
        logging.warning("No sites available for WAN2 variable assignment")
        return
    
    # Step 3: Site selection
    print(f"\n  Found {len(sites)} sites in organization")
    print("  Site Selection:")
    print("   1. Select individual sites")
    print("   2. All sites in organization")
    print("   3. Cancel")
    
    selection_choice = input("\n  Choose selection method (1-3): ").strip()
    
    sites_to_configure = []
    
    if selection_choice == "1":
        # Individual site selection
        print("\n  Available Sites:")
        for index, site in enumerate(sites, start=1):
            site_name = site.get("name", "Unnamed Site")
            site_id = site.get("id", "")
            print(f"   [{index}] {site_name} ({site_id})")
        
        print("\n  Enter site numbers to configure (comma-separated, e.g., 1,3,5):")
        site_indices_input = input("  Site numbers: ").strip()
        
        try:
            selected_indices = [int(idx.strip()) - 1 for idx in site_indices_input.split(",")]
            sites_to_configure = [sites[idx] for idx in selected_indices if 0 <= idx < len(sites)]
        except (ValueError, IndexError) as e:
            print(f" Invalid site selection: {e}")
            logging.error(f"Invalid site selection in Menu #103: {e}")
            return
            
    elif selection_choice == "2":
        # All sites
        sites_to_configure = sites
        
    else:
        print(" Operation cancelled.")
        logging.info("Menu #103 cancelled by user")
        return
    
    if not sites_to_configure:
        print(" No sites selected.")
        return
    
    # Filter out VRE sites (SECURITY: VRE sites excluded from WAN2 variable operations)
    original_count = len(sites_to_configure)
    sites_to_configure = [
        site for site in sites_to_configure 
        if not site.get("name", "").startswith("VRE")
    ]
    filtered_count = original_count - len(sites_to_configure)
    
    if filtered_count > 0:
        print(f"\n  !? SECURITY: Excluded {filtered_count} VRE sites from configuration")
        logging.info(f"Menu #103: Excluded {filtered_count} VRE sites from WAN2 variable operation")
    
    if not sites_to_configure:
        print(" No sites remaining after filtering VRE sites.")
        logging.warning("Menu #103: All selected sites were VRE sites - operation cancelled")
        return
    
    print(f"\n  Will configure {len(sites_to_configure)} sites with wan2_interface variable.")
    
    # Confirmation
    confirm = input("\n  Proceed with setting site variables? (yes/no): ").strip().lower()
    if confirm not in ['yes', 'y']:
        print(" Operation cancelled.")
        logging.info("Menu #103 cancelled by user at confirmation prompt")
        return
    
    # Step 4: Load gateway configs to check for ge-0/0/1 overrides
    gateway_configs_path = get_csv_file_path("AllSiteGatewayConfigs.csv")
    with open(gateway_configs_path, encoding="utf-8") as f:
        gateway_configs = list(csv.DictReader(f))
    
    # Step 4a: Load gateway templates to analyze IP configuration types for comparison
    # This enables intelligent DHCP vs Static IP conflict detection
    template_configs_path = get_csv_file_path("OrgGatewayTemplates.csv")
    with open(template_configs_path, encoding="utf-8") as f:
        template_data = list(csv.DictReader(f))
    
    # Step 4b: Build site-to-template mapping for IP type comparison
    # Extract gatewaytemplate_id from SiteList.csv to link sites to templates
    site_to_template_id = {}
    for site in sites:
        site_id = site.get("id", "").strip()
        template_id = site.get("gatewaytemplate_id", "").strip()
        if site_id and template_id:
            site_to_template_id[site_id] = template_id
    
    logging.info(f"Mapped {len(site_to_template_id)} sites to gateway templates for IP type analysis")
    
    # Step 4c: Extract IP config type from templates for ge-0/0/1 port
    # Enables DHCP vs Static conflict detection (CRITICAL vs INFO severity classification)
    template_port_configs = {}
    for template_row in template_data:
        template_id = template_row.get("id", "").strip()
        if not template_id:
            continue
        
        # Parse ip_config JSON from template CSV column (SECURITY: validate JSON structure)
        ip_config_raw = template_row.get("port_config_ge-0/0/1_ip_config", "").strip()
        ip_type = ""
        ip_address = ""
        netmask = ""
        gateway = ""
        
        if ip_config_raw:
            try:
                import json
                ip_config_data = json.loads(ip_config_raw) if ip_config_raw else {}
                ip_type = ip_config_data.get("type", "").lower()
                if ip_type == "static":
                    ip_address = ip_config_data.get("ip", "")
                    netmask = ip_config_data.get("netmask", "")
                    gateway = ip_config_data.get("gateway", "")
            except json.JSONDecodeError as json_error:
                logging.warning(f"Failed to parse template IP config for template {template_id}: {json_error}")
                ip_type = "parse_error"
        
        template_port_configs[template_id] = {
            "ip_type": ip_type if ip_type else "not_configured",
            "ip": ip_address,
            "netmask": netmask,
            "gateway": gateway
        }
    
    logging.info(f"Loaded port IP configs for {len(template_port_configs)} gateway templates")
    
    # Build override detection map: site_id -> list of devices with ge-0/0/1 or {{wan2_interface}} overrides
    site_overrides_map = {}
    for config_row in gateway_configs:
        site_id = config_row.get("site_id", "").strip()
        device_name = config_row.get("name", "").strip()
        
        # Check if ge-0/0/1 OR {{wan2_interface}} (with or without subinterfaces) has any port_config overrides
        # This catches ALL variations:
        # - port_config_ge-0/0/1_* (hardcoded base port: ge-0/0/1_usage, ge-0/0/1_ip_config, etc.)
        # - port_config_ge-0/0/1.* (hardcoded subinterface: ge-0/0/1.70_usage, ge-0/0/1.100_ip_config, etc.)
        # - port_config_{{wan2_interface}}_* (variable base port: {{wan2_interface}}_usage, {{wan2_interface}}_ip_config, etc.)
        # - port_config_{{wan2_interface}}.* (variable subinterface: {{wan2_interface}}.70_usage, {{wan2_interface}}.100_ip_config, etc.)
        # 
        # Note: Variable ports may or may not use subinterfaces depending on VLAN configuration
        ge_001_fields = [col for col in config_row if 
            col.startswith("port_config_ge-0/0/1_") or 
            col.startswith("port_config_ge-0/0/1.") or
            col.startswith("port_config_{{wan2_interface}}_") or
            col.startswith("port_config_{{wan2_interface}}.")]
        
        # Step 4d: Parse device IP config to compare with template (DHCP vs Static detection)
        # Handle BOTH base ports (JSON) and subinterfaces (flattened CSV columns)
        device_ip_type = ""
        device_static_ip = ""
        device_netmask = ""
        device_gateway = ""
        port_identifier = ""  # Track which port/subinterface had the config
        
        # First, try to find subinterface IP configs (e.g., ge-0/0/1.70_ip_config_type)
        # Subinterfaces are already flattened in CSV, no JSON parsing needed
        subinterface_ip_configs = []
        for col in config_row:
            # Match: port_config_ge-0/0/1.XX_ip_config_type or port_config_{{wan2_interface}}.XX_ip_config_type
            if (col.startswith("port_config_ge-0/0/1.") or col.startswith("port_config_{{wan2_interface}}.")) and col.endswith("_ip_config_type"):
                subif_ip_type = config_row.get(col, "").strip().lower()
                if subif_ip_type:
                    # Extract subinterface name from column (e.g., "ge-0/0/1.70" from "port_config_ge-0/0/1.70_ip_config_type")
                    subif_name = col.replace("port_config_", "").replace("_ip_config_type", "")
                    
                    # Get IP details from flattened columns
                    ip_col_base = f"port_config_{subif_name}_ip_config"
                    subif_ip = config_row.get(f"{ip_col_base}_ip", "").strip()
                    subif_netmask = config_row.get(f"{ip_col_base}_netmask", "").strip()
                    subif_gateway = config_row.get(f"{ip_col_base}_gateway", "").strip()
                    
                    subinterface_ip_configs.append({
                        "port": subif_name,
                        "type": subif_ip_type,
                        "ip": subif_ip,
                        "netmask": subif_netmask,
                        "gateway": subif_gateway
                    })
        
        # Use first subinterface config if found (most common case for WAN2 with VLANs)
        if subinterface_ip_configs:
            first_subif = subinterface_ip_configs[0]
            device_ip_type = first_subif["type"]
            device_static_ip = first_subif["ip"]
            device_netmask = first_subif["netmask"]
            device_gateway = first_subif["gateway"]
            port_identifier = first_subif["port"]
            logging.debug(f"Found subinterface IP config for {device_name}: {port_identifier} = {device_ip_type}")
        
        # Fallback: Try base port JSON config (e.g., port_config_ge-0/0/1_ip_config)
        if not device_ip_type:
            device_ip_config_raw = config_row.get("port_config_ge-0/0/1_ip_config", "").strip()
            if device_ip_config_raw:
                try:
                    import json
                    device_ip_data = json.loads(device_ip_config_raw) if device_ip_config_raw else {}
                    device_ip_type = device_ip_data.get("type", "").lower()
                    if device_ip_type == "static":
                        device_static_ip = device_ip_data.get("ip", "")
                        device_netmask = device_ip_data.get("netmask", "")
                        device_gateway = device_ip_data.get("gateway", "")
                    port_identifier = "ge-0/0/1"
                    logging.debug(f"Found base port IP config for {device_name}: {port_identifier} = {device_ip_type}")
                except json.JSONDecodeError as json_error:
                    logging.warning(f"Failed to parse device IP config JSON for {device_name} at site {site_id}: {json_error}")
                    device_ip_type = "parse_error"
                    port_identifier = "ge-0/0/1 (parse_error)"
        
        # Step 4e: Get template IP config for comparison (check both base and subinterface)
        template_id_for_site = site_to_template_id.get(site_id, "")
        template_config = template_port_configs.get(template_id_for_site, {})
        template_ip_type = template_config.get("ip_type", "unknown")
        
        # If device has subinterface, also check template for same subinterface
        if port_identifier and "." in port_identifier:
            # Look for template subinterface config in flattened columns
            template_subif_col = f"port_config_{port_identifier}_ip_config_type"
            for template_row in template_data:
                if template_row.get("id", "").strip() == template_id_for_site:
                    template_subif_type = template_row.get(template_subif_col, "").strip().lower()
                    if template_subif_type:
                        template_ip_type = template_subif_type
                        logging.debug(f"Found template subinterface IP config for {port_identifier}: {template_ip_type}")
                    break
        
        # Step 4f: Determine override severity based on IP type mismatch
        # CRITICAL: Template=DHCP but Device=Static (locally unique IPs that must be preserved)
        # WARNING: Template=Static but Device=DHCP (unusual configuration)
        # INFO: Same IP type but other fields overridden (description, usage, etc.)
        override_severity = "NONE"
        ip_type_conflict = False
        
        has_override = any(
            config_row.get(field, "").strip().lower() not in ["", "null", "none"]
            for field in ge_001_fields
            if "_vpn_paths_" not in field  # Exclude VPN paths (template-inherited)
        )
        
        if has_override:
            # Classify override severity for intelligent manual review prioritization
            if template_ip_type == "dhcp" and device_ip_type == "static":
                override_severity = "CRITICAL"
                ip_type_conflict = True
            elif template_ip_type == "static" and device_ip_type == "dhcp":
                override_severity = "WARNING"
                ip_type_conflict = True
            elif template_ip_type == device_ip_type and device_ip_type in ["dhcp", "static"]:
                override_severity = "INFO"
                ip_type_conflict = False
            else:
                override_severity = "UNKNOWN"
                ip_type_conflict = False
            
            # Store detailed override information for reporting
            if site_id not in site_overrides_map:
                site_overrides_map[site_id] = []
            
            # Build port display string (include subinterface if present)
            port_display = port_identifier if port_identifier else "ge-0/0/1"
            
            site_overrides_map[site_id].append({
                "device_name": device_name,
                "port_identifier": port_display,
                "template_ip_type": template_ip_type.upper(),
                "device_ip_type": device_ip_type.upper() if device_ip_type else "NOT_CONFIGURED",
                "device_static_ip": device_static_ip,
                "device_netmask": device_netmask,
                "device_gateway": device_gateway,
                "override_severity": override_severity,
                "ip_type_conflict": ip_type_conflict
            })
    
    # Step 5: Process each site
    results = []
    print("\n  Processing sites...")
    
    for site in tqdm(sites_to_configure, desc="Configuring sites", unit="site"):
        site_id = site.get("id", "")
        site_name = site.get("name", "Unnamed Site")
        
        result = {
            "site_id": site_id,
            "site_name": site_name,
            "variable_set": False,
            "has_overrides": False,
            "override_devices": [],
            "critical_override_count": 0,
            "warning_override_count": 0,
            "info_override_count": 0,
            "total_override_count": 0,
            "status": "",
            "error": ""
        }
        
        # Check for overrides with IP type conflict analysis
        if site_id in site_overrides_map:
            result["has_overrides"] = True
            override_details = site_overrides_map[site_id]
            
            # Extract device names for backward compatibility
            result["override_devices"] = [d["device_name"] for d in override_details]
            
            # Count overrides by severity level
            critical_overrides = [d for d in override_details if d["override_severity"] == "CRITICAL"]
            warning_overrides = [d for d in override_details if d["override_severity"] == "WARNING"]
            info_overrides = [d for d in override_details if d["override_severity"] == "INFO"]
            
            result["critical_override_count"] = len(critical_overrides)
            result["warning_override_count"] = len(warning_overrides)
            result["info_override_count"] = len(info_overrides)
            result["total_override_count"] = len(override_details)
            
            # Flatten override details for CSV export (one row per site with comma-separated details)
            # Format: device_name@port(severity:template_type->device_type:static_ip)
            override_summaries = []
            for override_detail in override_details:
                device_name = override_detail["device_name"]
                port_id = override_detail.get("port_identifier", "ge-0/0/1")
                severity = override_detail["override_severity"]
                template_ip = override_detail["template_ip_type"]
                device_ip = override_detail["device_ip_type"]
                static_ip = override_detail["device_static_ip"]
                netmask = override_detail["device_netmask"]
                
                if static_ip and netmask:
                    summary = f"{device_name}@{port_id}({severity}:{template_ip}->{device_ip}:{static_ip}{netmask})"
                elif static_ip:
                    summary = f"{device_name}@{port_id}({severity}:{template_ip}->{device_ip}:{static_ip})"
                else:
                    summary = f"{device_name}@{port_id}({severity}:{template_ip}->{device_ip})"
                override_summaries.append(summary)
            
            result["override_details"] = "; ".join(override_summaries)
        
        try:
            # Fetch current site settings
            logging.debug(f"Fetching current settings for site {site_name} ({site_id})")
            settings_resp = mistapi.api.v1.sites.setting.getSiteSetting(apisession, site_id)
            current_settings = settings_resp.data if hasattr(settings_resp, 'data') else {}
            
            if not isinstance(current_settings, dict):
                current_settings = {}
            
            # Get existing vars or create new dict
            site_vars = current_settings.get("vars", {})
            if not isinstance(site_vars, dict):
                site_vars = {}
            
            # Set the wan2_interface variable
            site_vars["wan2_interface"] = "ge-0/0/1"
            
            # Update settings with new vars
            current_settings["vars"] = site_vars
            
            # Write back to API
            logging.debug(f"Updating site settings for {site_name} with wan2_interface variable")
            update_resp = mistapi.api.v1.sites.setting.updateSiteSettings(
                apisession,
                site_id,
                body=current_settings
            )
            
            if update_resp.status_code == 200:
                result["variable_set"] = True
                result["status"] = "SUCCESS"
                logging.info(f"Successfully set wan2_interface variable for site {site_name}")
            else:
                result["status"] = "FAILED"
                result["error"] = f"API returned status {update_resp.status_code}"
                logging.error(f"Failed to set variable for site {site_name}: status {update_resp.status_code}")
        
        except Exception as e:
            result["status"] = "ERROR"
            result["error"] = str(e)
            logging.error(f"Error setting variable for site {site_name}: {e}")
            logging.error(traceback.format_exc())
        
        results.append(result)
    
    # Step 6: Generate report with IP type conflict analysis
    report_data = []
    for result in results:
        report_entry = {
            "site_name": result["site_name"],
            "site_id": result["site_id"],
            "wan2_variable_set": "Yes" if result["variable_set"] else "No",
            "status": result["status"],
            "has_wan2_overrides": "Yes" if result["has_overrides"] else "No",
            "total_override_count": result.get("total_override_count", 0),
            "critical_override_count": result.get("critical_override_count", 0),
            "warning_override_count": result.get("warning_override_count", 0),
            "info_override_count": result.get("info_override_count", 0),
            "override_devices": ", ".join(result["override_devices"]) if result["override_devices"] else "",
            "override_details": result.get("override_details", ""),
            "requires_manual_review": "CRITICAL" if result.get("critical_override_count", 0) > 0 else ("WARNING" if result.get("warning_override_count", 0) > 0 else ("INFO" if result.get("info_override_count", 0) > 0 else "No")),
            "error": result["error"]
        }
        report_data.append(report_entry)
    
    # Save report
    output_file = "WAN2_SiteVariable_Report.csv"
    DataExporter.save_data_to_output(report_data, output_file)
    
    # Print summary with IP type conflict breakdown
    success_count = sum(1 for r in results if r["variable_set"])
    override_count = sum(1 for r in results if r["has_overrides"])
    critical_sites = sum(1 for r in results if r.get("critical_override_count", 0) > 0)
    warning_sites = sum(1 for r in results if r.get("warning_override_count", 0) > 0)
    info_sites = sum(1 for r in results if r.get("has_overrides") and r.get("critical_override_count", 0) == 0 and r.get("warning_override_count", 0) == 0)
    
    print(f"\n  Configuration Complete!")
    print(f"=" * 70)
    print(f"  Sites Processed: {len(results)}")
    print(f"  Variables Set: {success_count}")
    print(f"  Sites with WAN2 Overrides: {override_count}")
    print(f"    -> CRITICAL (DHCP->Static IP conflicts): {critical_sites}")
    print(f"    -> WARNING (Static->DHCP conflicts): {warning_sites}")
    print(f"    -> INFO (Same IP type, other overrides): {info_sites}")
    print(f"\n  Report saved to: {output_file}")
    print(f"=" * 70)
    
    if critical_sites > 0:
        print(f"\n  !? CRITICAL ATTENTION: {critical_sites} sites have DHCP->Static IP conflicts")
        print(f"  Template specifies DHCP but devices use locally unique static IPs")
        print(f"  These MUST be manually reviewed before template migration (Menu #104)")
        print(f"  Static IPs will be lost if template DHCP is applied without device overrides")
        print(f"  Check 'override_details' column for device names and static IP addresses")
    
    if warning_sites > 0:
        print(f"\n  ! WARNING: {warning_sites} sites have Static->DHCP conflicts")
        print(f"  Template specifies Static IP but devices configured for DHCP")
        print(f"  Review recommended before template migration")
    
    if info_sites > 0:
        print(f"\n  INFO: {info_sites} sites have same-IP-type overrides (likely safe)")
        print(f"  Template and device use same IP configuration type (both DHCP or both Static)")
        print(f"  Overrides may be for description, usage, or other non-critical fields")
    
    logging.info(f"Menu #103 complete: {success_count}/{len(results)} sites configured")
    logging.info(f"Override breakdown - CRITICAL: {critical_sites}, WARNING: {warning_sites}, INFO: {info_sites}")

def update_gateway_templates_wan2_variable(fast: bool = False, dry_run: bool = False):
    """
    Menu #104: Update Gateway Templates for WAN2 Variable Migration (DESTRUCTIVE - Bidirectional)
    
    Bidirectional operation supporting both APPLY and REVERT modes:
    - APPLY: Replace hardcoded 'ge-0/0/1' port references with {{wan2_interface}} variable
    - REVERT: Replace {{wan2_interface}} variable with hardcoded 'ge-0/0/1' (undo migration)
    
    Both modes preserve device-level static IP overrides by migrating port_config keys.
    
    Args:
        fast: Enable parallel processing with connection pooling
        dry_run: Show what would be changed without making actual API calls
    
    Workflow:
    1. Lists all gateway templates with site assignment counts
    2. Allows selection of specific templates to update
    3. Prompts for operation direction (APPLY variable or REVERT to hardcoded)
    4. For each template:
       - Fetches current template configuration
       - Identifies port_config entries matching search pattern (including subinterfaces)
       - Replaces port names according to selected direction
       - Shows preview of changes
    5. Requires uppercase 'MIGRATE' confirmation before applying
    6. Updates templates via updateOrgGatewayTemplate API
    7. CRITICAL: Identifies devices with port overrides matching the pattern
    8. For each device with override:
       - Renames port_config keys to match template migration direction
       - Preserves static IP configurations (IP, netmask, gateway)
       - Updates device via updateSiteDevice API
    9. Generates audit reports for both template and device migrations
    
    SECURITY: DESTRUCTIVE operation requiring explicit confirmation.
    This modifies production gateway templates AND device configurations.
    
    CRITICAL SAFETY FEATURE: Without device override migration, sites with static IP
    overrides would LOSE their static IPs when template migration occurs.
    
    APPLY mode: Templates using {{wan2_interface}} will resolve to site variable values.
    REVERT mode: Useful for undoing variable migration or troubleshooting.
    """
    print("\n  DESTRUCTIVE: Update Gateway Templates for WAN2 Variable Migration")
    print("=" * 70)
    if dry_run:
        print("  >> DRY-RUN MODE: No changes will be made to templates or devices")
        print("  >> This will show what WOULD be changed without modifying anything")
    else:
        print("  !? WARNING: This operation modifies gateway templates")
        print("  !? All sites using affected templates will inherit the change")
        print("  !? Ensure sites have 'wan2_interface' variable set (Menu #103)")
    print("=" * 70)
    
    logging.warning("Menu #104 DESTRUCTIVE: Update Gateway Templates WAN2 Variable operation started")
    
    org_id = get_cached_or_prompted_org_id()
    
    # Step 1: Ensure required data is fresh
    print("\n  Loading gateway template data...")
    check_and_generate_csv("OrgGatewayTemplates.csv", export_gateway_templates_to_csv)
    check_and_generate_csv("SiteList.csv", export_all_sites_to_csv)
    
    # Load templates
    templates_path = get_csv_file_path("OrgGatewayTemplates.csv")
    with open(templates_path, encoding="utf-8") as f:
        template_rows = list(csv.DictReader(f))
    
    if not template_rows:
        print(" No gateway templates found.")
        logging.warning("No gateway templates available for modification")
        return
    
    # Load and EARLY FILTER sites (OPTIMIZATION: filter VRE sites immediately to reduce processing)
    sites_path = get_csv_file_path("SiteList.csv")
    with open(sites_path, encoding="utf-8") as f:
        all_sites = list(csv.DictReader(f))
    
    # OPTIMIZATION: Filter out VRE sites BEFORE any processing (not after)
    # This reduces memory usage and speeds up all subsequent operations
    original_site_count = len(all_sites)
    sites = [site for site in all_sites if not site.get("name", "").startswith("VRE")]
    vre_filtered_count = original_site_count - len(sites)
    
    if vre_filtered_count > 0:
        print(f"\n  !? SECURITY: Excluded {vre_filtered_count} VRE sites from template impact analysis (early filter)")
        logging.info(f"Menu #104: Excluded {vre_filtered_count} VRE sites from WAN2 template operation (early optimization)")
    
    logging.info(f"Processing {len(sites)} non-VRE sites for template assignment counts")
    
    template_site_counts = {}
    for site in sites:
        template_id = site.get("gatewaytemplate_id", "").strip()
        if template_id:
            template_site_counts[template_id] = template_site_counts.get(template_id, 0) + 1
    
    # Step 2: Display templates with site counts
    template_rows_sorted = sorted(template_rows, key=lambda t: t.get("name", "Unnamed Template").lower())
    
    print(f"\n  Available Gateway Templates ({len(template_rows_sorted)}):")
    template_list = []
    for idx, template in enumerate(template_rows_sorted, start=1):
        template_id = template.get("id", "")
        template_name = template.get("name", "Unnamed Template")
        site_count = template_site_counts.get(template_id, 0)
        template_list.append({
            "id": template_id,
            "name": template_name,
            "site_count": site_count
        })
        print(f"   [{idx}] {template_name} ({site_count} sites)")
    
    # Step 3: Template selection
    print("\n  Template Selection:")
    print("   Enter template numbers to modify (comma-separated, e.g., 1,3,5)")
    print("   Or 'all' to modify all templates")
    print("   Or 'cancel' to abort")
    
    selection_input = input("\n  Selection: ").strip().lower()
    
    if selection_input == "cancel":
        print(" Operation cancelled.")
        logging.info("Menu #104 cancelled by user at template selection")
        return
    
    templates_to_modify = []
    if selection_input == "all":
        templates_to_modify = template_list
    else:
        try:
            selected_indices = [int(idx.strip()) - 1 for idx in selection_input.split(",")]
            templates_to_modify = [template_list[idx] for idx in selected_indices if 0 <= idx < len(template_list)]
        except (ValueError, IndexError) as e:
            print(f" Invalid selection: {e}")
            logging.error(f"Invalid template selection in Menu #104: {e}")
            return
    
    if not templates_to_modify:
        print(" No templates selected.")
        return
    
    print(f"\n  Selected {len(templates_to_modify)} templates for modification:")
    total_affected_sites = sum(t["site_count"] for t in templates_to_modify)
    for template in templates_to_modify:
        print(f"   - {template['name']} ({template['site_count']} sites)")
    print(f"\n  Total sites affected: {total_affected_sites}")
    
    # Step 3.5: Ask user for operation direction (apply variable or revert to hardcoded)
    print("\n  Operation Direction:")
    print("   [1] Replace hardcoded ports with {{wan2_interface}} variable (standard migration)")
    print("   [2] Replace {{wan2_interface}} variable with hardcoded 'ge-0/0/1' (revert/undo)")
    print("   [cancel] Abort operation")
    
    direction_input = input("\n  Select operation [1/2/cancel]: ").strip().lower()
    
    if direction_input == "cancel":
        print(" Operation cancelled.")
        logging.info("Menu #104 cancelled by user at operation direction selection")
        return
    
    if direction_input == "2":
        operation_mode = "revert"
        search_pattern = "{{wan2_interface}}"
        replacement_value = "ge-0/0/1"
        print("\n  !? REVERT MODE: Will replace {{wan2_interface}} with hardcoded 'ge-0/0/1'")
        logging.info("Menu #104: User selected REVERT mode (variable -> hardcoded)")
    elif direction_input == "1":
        operation_mode = "apply"
        search_pattern = "ge-0/0/1"
        replacement_value = "{{wan2_interface}}"
        print("\n  APPLY MODE: Will replace hardcoded 'ge-0/0/1' with {{wan2_interface}} variable")
        logging.info("Menu #104: User selected APPLY mode (hardcoded -> variable)")
    else:
        print(" Invalid selection. Operation cancelled.")
        logging.info("Menu #104 cancelled - invalid operation direction")
        return
    
    # Step 4: Fetch and analyze templates for changes
    print(f"\n  Analyzing templates for {search_pattern} port configurations...")
    templates_with_changes = []
    
    # OPTIMIZATION: Parallelize template configuration fetches to reduce wall-clock time
    # For large template sets, this reduces sequential API waits significantly
    def fetch_template_config(template_info):
        """Worker function to fetch and analyze a single template configuration"""
        template_id = template_info["id"]
        template_name = template_info["name"]
        
        try:
            # Fetch full template configuration
            logging.debug(f"Fetching template configuration for {template_name}")
            template_resp = mistapi.api.v1.orgs.gatewaytemplates.getOrgGatewayTemplate(
                apisession,
                org_id,
                template_id
            )
            template_config = template_resp.data if hasattr(template_resp, 'data') else {}
            
            if not isinstance(template_config, dict):
                logging.warning(f"Template {template_name} returned invalid data structure")
                return None
            
            # Check for port_config with search pattern
            port_config = template_config.get("port_config", {})
            if not isinstance(port_config, dict):
                logging.debug(f"Template {template_name} has no port_config")
                return None
            
            # Find entries matching search pattern (including subinterfaces)
            changes_needed = False
            ports_to_replace = []  # List of (original_key, new_key) tuples
            
            # Check all port keys for search pattern references
            for port_key in port_config.keys():
                if port_key == search_pattern:
                    # Exact match - simple replacement
                    changes_needed = True
                    ports_to_replace.append((port_key, replacement_value))
                elif port_key.startswith(f"{search_pattern}."):
                    # Subinterface (e.g., ge-0/0/1.70 or {{wan2_interface}}.70) - replace prefix only
                    suffix = port_key[len(search_pattern):]  # Extract ".70" or similar
                    new_key = f"{replacement_value}{suffix}"
                    changes_needed = True
                    ports_to_replace.append((port_key, new_key))
                    logging.info(f"Found subinterface in template {template_name}: {port_key} -> {new_key}")
                elif search_pattern in port_key:
                    # Complex pattern that contains search string but doesn't start with it
                    # Log warning but don't attempt replacement (might be port range like "ge-0/0/0-2")
                    logging.warning(f"Found complex port pattern in template {template_name}: {port_key}")
                    print(f"\n  !? Template '{template_name}' uses complex port pattern: '{port_key}'")
                    print(f"     This requires manual review - cannot automatically replace")
            
            if changes_needed:
                return {
                    "id": template_id,
                    "name": template_name,
                    "site_count": template_info["site_count"],
                    "config": template_config,
                    "ports_to_replace": ports_to_replace  # List of (old_key, new_key) tuples
                }
            return None
        
        except Exception as e:
            logging.error(f"Error analyzing template {template_name}: {e}")
            logging.error(traceback.format_exc())
            print(f"\n  !? Error analyzing template '{template_name}': {e}")
            return None
    
    # Use ThreadPoolExecutor for parallel template fetches
    max_workers = min(10, len(templates_to_modify))  # Limit concurrent API calls
    logging.info(f"Fetching {len(templates_to_modify)} template configurations in parallel (max {max_workers} workers)")
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all template fetch tasks
        future_to_template = {
            executor.submit(fetch_template_config, template_info): template_info
            for template_info in templates_to_modify
        }
        
        # Collect results with progress bar
        # CRITICAL FIX: Use fully qualified concurrent.futures.as_completed to avoid tqdm parameter conflicts
        import concurrent.futures
        for future in tqdm(concurrent.futures.as_completed(future_to_template), 
                          total=len(templates_to_modify),
                          desc="Analyzing templates", 
                          unit="template"):
            template_result = future.result()
            if template_result:
                templates_with_changes.append(template_result)
    
    if not templates_with_changes:
        print(f"\n  No templates found with {search_pattern} port configurations.")
        print("  No changes needed.")
        logging.info(f"Menu #104: No templates require modification (searched for {search_pattern})")
        return
    
    # Step 5: Show preview and confirm
    print(f"\n  Preview of Changes ({operation_mode.upper()} mode):")
    print(f"  {len(templates_with_changes)} templates will be modified:")
    for template in templates_with_changes:
        print(f"\n   Template: {template['name']}")
        print(f"   Sites Affected: {template['site_count']}")
        print(f"   Changes:")
        for old_key, new_key in template['ports_to_replace']:
            print(f"     Port key '{old_key}' -> '{new_key}'")
    
    print(f"\n  {'=' * 70}")
    if dry_run:
        print(f"  >> DRY-RUN: Would modify {len(templates_with_changes)} templates")
        print(f"  >> affecting {sum(t['site_count'] for t in templates_with_changes)} sites")
        print(f"  >> No confirmation needed in dry-run mode - proceeding with preview")
    else:
        print(f"  !? CRITICAL: This operation will modify {len(templates_with_changes)} templates")
        print(f"  !? affecting {sum(t['site_count'] for t in templates_with_changes)} sites")
        print(f"  !? Type 'MIGRATE' (all caps) to proceed or anything else to cancel")
    print(f"  {'=' * 70}")
    
    if not dry_run:
        confirmation = input("\n  Confirmation: ").strip()
        if confirmation != "MIGRATE":
            print(" Operation cancelled.")
            logging.info("Menu #104 cancelled by user at final confirmation")
            return
    
    # Step 6: Apply changes
    print("\n  Applying template modifications...")
    results = []
    
    for template in tqdm(templates_with_changes, desc="Updating templates", unit="template"):
        template_id = template["id"]
        template_name = template["name"]
        template_config = template["config"]
        
        result = {
            "template_name": template_name,
            "template_id": template_id,
            "site_count": template["site_count"],
            "status": "",
            "changes_made": "",
            "error": ""
        }
        
        try:
            # Modify port_config: replace all ge-0/0/1 references with {{wan2_interface}}
            port_config = template_config.get("port_config", {})
            changes_list = []
            
            # Process each port replacement
            for old_key, new_key in template["ports_to_replace"]:
                if old_key in port_config:
                    # Save the port configuration
                    port_config_data = port_config[old_key]
                    
                    # Remove old key
                    del port_config[old_key]
                    
                    # Add new key with variable
                    port_config[new_key] = port_config_data
                    
                    changes_list.append(f"'{old_key}' -> '{new_key}'")
                    logging.debug(f"Template {template_name}: Replaced {old_key} with {new_key}")
            
            if changes_list:
                # Update template config
                template_config["port_config"] = port_config
                
                result["changes_made"] = "; ".join(changes_list)
                
                # Push update to API (skip in dry-run mode)
                if dry_run:
                    result["status"] = "DRY-RUN"
                    logging.info(f"DRY-RUN: Would update template {template_name} with changes: {result['changes_made']}")
                else:
                    logging.debug(f"Updating template {template_name} via API")
                    update_resp = mistapi.api.v1.orgs.gatewaytemplates.updateOrgGatewayTemplate(
                        apisession,
                        org_id,
                        template_id,
                        body=template_config
                    )
                    
                    if update_resp.status_code == 200:
                        result["status"] = "SUCCESS"
                        logging.info(f"Successfully updated template {template_name}")
                    else:
                        result["status"] = "FAILED"
                        result["error"] = f"API returned status {update_resp.status_code}"
                        logging.error(f"Failed to update template {template_name}: status {update_resp.status_code}")
            else:
                result["status"] = "SKIPPED"
                result["error"] = "No matching ports found in configuration"
        
        except Exception as e:
            result["status"] = "ERROR"
            result["error"] = str(e)
            logging.error(f"Error updating template {template_name}: {e}")
            logging.error(traceback.format_exc())
        
        results.append(result)
    
    # Step 7: Migrate device-level overrides to preserve static IPs
    # CRITICAL: Devices with port overrides need port_config keys updated to match template changes
    # Without this, static IP overrides would be lost when template changes take effect
    print(f"\n  Step 7: Migrating device-level port overrides ({operation_mode.upper()} mode)...")
    if operation_mode == "apply":
        print(f"  !? CRITICAL: Preserving static IP configurations on devices")
        print(f"  !? Renaming device overrides from '{search_pattern}' to '{replacement_value}'")
    else:
        print(f"  !? REVERT: Updating device overrides to match template reversion")
        print(f"  !? Renaming device overrides from '{search_pattern}' to '{replacement_value}'")
    
    # Build set of template IDs that were successfully migrated
    migrated_template_ids = {r["template_id"] for r in results if r["status"] == "SUCCESS"}
    
    # OPTIMIZATION: Build site-to-template mapping and filter to only sites using migrated templates
    # This reduces API calls from 3300+ sites to only affected sites (typically <100)
    # Also filters out VRE sites from device migration scope (already filtered from sites list earlier)
    site_to_template = {}
    affected_site_ids = set()
    for site in sites:
        site_id = site.get("id", "").strip()
        site_name = site.get("name", "").strip()
        template_id = site.get("gatewaytemplate_id", "").strip()
        
        # Skip VRE sites (redundant safety check - already filtered from sites list)
        if site_name.startswith("VRE"):
            logging.debug(f"Skipping VRE site {site_name} from device migration scope")
            continue
            
        if site_id and template_id:
            site_to_template[site_id] = template_id
            # Only track sites using successfully migrated templates
            if template_id in migrated_template_ids:
                affected_site_ids.add(site_id)
    
    logging.info(f"Device migration scope: {len(affected_site_ids)} sites using migrated templates (out of {len(sites)} total sites)")
    print(f"  >> Optimization: Checking only {len(affected_site_ids)} affected sites (not all {len(sites)} sites)")
    
    # OPTIMIZATION: Fetch device configs ONLY for affected sites instead of entire org
    # This dramatically reduces API calls for large orgs (3300 sites -> ~50-200 sites)
    devices_needing_migration = []
    
    if affected_site_ids:
        print(f"  >> Fetching gateway device configurations for {len(affected_site_ids)} affected sites...")
        
        for site_id in tqdm(affected_site_ids, desc="Checking site devices", unit="site"):
            try:
                # Fetch devices for this specific site (type='gateway' to filter)
                device_resp = mistapi.api.v1.sites.devices.listSiteDevices(
                    apisession,
                    site_id,
                    type='gateway'
                )
                site_devices = mistapi.get_all(response=device_resp, mist_session=apisession)
                
                # For each gateway device, check for port overrides matching search pattern
                for device in site_devices:
                    device_id = device.get("id", "").strip()
                    device_name = device.get("name", "").strip()
                    
                    # Fetch full device config to check port_config
                    device_config_resp = mistapi.api.v1.sites.devices.getSiteDevice(
                        apisession,
                        site_id,
                        device_id
                    )
                    device_config = getattr(device_config_resp, "data", {})
                    
                    # Check for port overrides matching search pattern
                    port_config = device_config.get("port_config", {})
                    if not isinstance(port_config, dict):
                        continue
                    
                    # Check if any port key matches search pattern
                    has_override = any(
                        port_key == search_pattern or port_key.startswith(f"{search_pattern}.")
                        for port_key in port_config.keys()
                    )
                    
                    if has_override:
                        devices_needing_migration.append({
                            "site_id": site_id,
                            "device_id": device_id,
                            "device_name": device_name,
                            "template_id": site_to_template.get(site_id)
                        })
                        logging.info(f"Found device '{device_name}' with {search_pattern} override at site {site_id}")
                        
            except Exception as e:
                logging.error(f"Error checking devices at site {site_id}: {e}")
                continue
    
    logging.info(f"Found {len(devices_needing_migration)} devices with ge-0/0/1 overrides needing migration")
    
    if devices_needing_migration:
        print(f"\n  Found {len(devices_needing_migration)} devices with port overrides to migrate")
        print(f"  These devices will have port_config keys renamed from 'ge-0/0/1' to '{{{{wan2_interface}}}}'")
        print(f"  This preserves static IP configurations after template migration")
        
        # Worker function for parallel device migration
        def migrate_single_device_override(device_info, connection_semaphore):
            """Worker function for parallel device override migration with connection pooling"""
            device_id = device_info["device_id"]
            device_name = device_info["device_name"]
            site_id = device_info["site_id"]
            
            device_result = {
                "device_name": device_name,
                "device_id": device_id,
                "site_id": site_id,
                "template_id": device_info["template_id"],
                "status": "",
                "ports_migrated": "",
                "error": ""
            }
            
            try:
                with connection_semaphore:  # Acquire connection slot
                    # Fetch current device configuration
                    logging.debug(f"Fetching device config for {device_name} ({device_id})")
                    device_resp = mistapi.api.v1.sites.devices.getSiteDevice(apisession, site_id, device_id)
                    device_config = getattr(device_resp, "data", {})
                    
                    if not isinstance(device_config, dict):
                        device_result["status"] = "SKIPPED"
                        device_result["error"] = "Invalid device config structure"
                        return device_result
                    
                    # Get port_config section
                    port_config = device_config.get("port_config", {})
                    if not isinstance(port_config, dict):
                        device_result["status"] = "SKIPPED"
                        device_result["error"] = "No port_config found"
                        return device_result
                    
                    # Find and rename port keys matching search pattern
                    ports_renamed = []
                    for port_key in list(port_config.keys()):  # list() to allow modification during iteration
                        new_key = None
                        
                        if port_key == search_pattern:
                            new_key = replacement_value
                        elif port_key.startswith(f"{search_pattern}."):
                            # Subinterface (e.g., ge-0/0/1.70 or {{wan2_interface}}.70)
                            suffix = port_key[len(search_pattern):]
                            new_key = f"{replacement_value}{suffix}"
                        
                        if new_key:
                            # Preserve port configuration under new key
                            port_config[new_key] = port_config[port_key]
                            del port_config[port_key]
                            ports_renamed.append(f"{port_key}->{new_key}")
                            logging.debug(f"Device {device_name}: Renamed {port_key} to {new_key}")
                    
                    if ports_renamed:
                        # Update device config
                        device_config["port_config"] = port_config
                        device_result["ports_migrated"] = "; ".join(ports_renamed)
                        
                        # Push update to API (skip in dry-run mode)
                        # Access dry_run from enclosing scope (closure)
                        is_dry_run = globals().get('args', type('obj', (), {'dry_run': False})()).dry_run if hasattr(globals().get('args', type('obj', (), {'dry_run': False})()), 'dry_run') else False
                        
                        if is_dry_run:
                            device_result["status"] = "DRY-RUN"
                            logging.info(f"DRY-RUN: Would migrate port overrides for device {device_name}: {device_result['ports_migrated']}")
                        else:
                            logging.debug(f"Updating device {device_name} via API")
                            update_resp = mistapi.api.v1.sites.devices.updateSiteDevice(
                                apisession,
                                site_id,
                                device_id,
                                body=device_config
                            )
                            
                            if update_resp.status_code == 200:
                                device_result["status"] = "SUCCESS"
                                logging.info(f"Successfully migrated port overrides for device {device_name}")
                            else:
                                device_result["status"] = "FAILED"
                                device_result["error"] = f"API returned status {update_resp.status_code}"
                                logging.error(f"Failed to update device {device_name}: status {update_resp.status_code}")
                    else:
                        device_result["status"] = "SKIPPED"
                        device_result["error"] = f"No {search_pattern} ports found in config"
                
            except Exception as e:
                device_result["status"] = "ERROR"
                device_result["error"] = str(e)
                logging.error(f"Error migrating device {device_name}: {e}")
                logging.error(traceback.format_exc())
            
            return device_result
        
        # Determine processing mode: fast mode with connection pooling or sequential
        use_fast_mode = fast and len(devices_needing_migration) > 5
        
        if use_fast_mode:
            print(f"\n  !? Fast mode enabled: Processing {len(devices_needing_migration)} devices with connection pooling")
            logging.info(f"Fast mode: Using connection pool for {len(devices_needing_migration)} device migrations")
            
            # Use connection pool management for parallel processing
            device_migration_results, failed_devices = execute_with_connection_pool_management(
                work_items=devices_needing_migration,
                worker_function=migrate_single_device_override,
                batch_description="devices"
            )
            
            if failed_devices:
                logging.warning(f"Fast mode: {len(failed_devices)} device migrations failed")
        else:
            if fast and len(devices_needing_migration) <= 5:
                print(f"\n  Sequential mode: Processing {len(devices_needing_migration)} devices (fast mode requires >5 devices)")
            elif not fast:
                print(f"\n  Sequential mode: Processing {len(devices_needing_migration)} devices")
            
            logging.info(f"Sequential mode: Processing {len(devices_needing_migration)} devices one at a time")
            device_migration_results = []
            
            # Sequential mode fallback: process devices one at a time with progress bar
            # Create dummy semaphore for sequential processing (no actual limiting)
            dummy_semaphore = threading.Semaphore(1)
            
            for device_info in tqdm(devices_needing_migration, desc="Migrating device overrides", unit="device"):
                result = migrate_single_device_override(device_info, dummy_semaphore)
                device_migration_results.append(result)
        
        # Generate device migration report
        device_output_file = "GatewayDevice_WAN2_Override_Migration.csv"
        DataExporter.save_data_to_output(device_migration_results, device_output_file)
        
        # Summary for device migrations
        device_success_count = sum(1 for r in device_migration_results if r["status"] == "SUCCESS")
        device_failure_count = len(device_migration_results) - device_success_count
        
        print(f"\n  Device Override Migration Complete!")
        print(f"  Devices Processed: {len(device_migration_results)}")
        print(f"  Successfully Migrated: {device_success_count}")
        print(f"  Failed: {device_failure_count}")
        print(f"  Device migration report: {device_output_file}")
        
        logging.info(f"Device override migration: {device_success_count} successful, {device_failure_count} failed")
    else:
        print(f"\n  No devices with ge-0/0/1 overrides found - no device migrations needed")
        logging.info("No device-level override migrations required")
    
    # Step 8: Generate audit report
    output_file = "GatewayTemplate_WAN2_Migration_Audit.csv"
    DataExporter.save_data_to_output(results, output_file)
    
    # Print summary
    if dry_run:
        dry_run_count = sum(1 for r in results if r["status"] == "DRY-RUN")
        print(f"\n  WAN2 Variable Migration DRY-RUN Complete!")
        print(f"=" * 70)
        print(f"  >> DRY-RUN MODE: No actual changes were made")
        print(f"  TEMPLATE MIGRATION PREVIEW:")
        print(f"    Templates Analyzed: {len(results)}")
        print(f"    Would Be Updated: {dry_run_count}")
        print(f"    Skipped: {len(results) - dry_run_count}")
    else:
        success_count = sum(1 for r in results if r["status"] == "SUCCESS")
        failure_count = len(results) - success_count
        
        print(f"\n  WAN2 Variable Migration Complete!")
        print(f"=" * 70)
        print(f"  TEMPLATE MIGRATION:")
        print(f"    Templates Processed: {len(results)}")
        print(f"    Successfully Updated: {success_count}")
        print(f"    Failed: {failure_count}")
    
    if devices_needing_migration:
        if dry_run:
            dry_run_device_count = sum(1 for r in device_migration_results if r["status"] == "DRY-RUN")
            print(f"\n  DEVICE OVERRIDE MIGRATION PREVIEW:")
            print(f"    Devices Analyzed: {len(device_migration_results)}")
            print(f"    Would Preserve Static IPs: {dry_run_device_count}")
            print(f"    Skipped: {len(device_migration_results) - dry_run_device_count}")
        else:
            device_success = sum(1 for r in device_migration_results if r["status"] == "SUCCESS")
            device_failed = len(device_migration_results) - device_success
            print(f"\n  DEVICE OVERRIDE MIGRATION:")
            print(f"    Devices Processed: {len(device_migration_results)}")
            print(f"    Static IPs Preserved: {device_success}")
            print(f"    Failed: {device_failed}")
    
    print(f"\n  REPORTS:")
    print(f"    Template audit: {output_file}")
    if devices_needing_migration:
        print(f"    Device migration: GatewayDevice_WAN2_Override_Migration.csv")
    print(f"=" * 70)
    
    if dry_run:
        dry_run_count = sum(1 for r in results if r["status"] == "DRY-RUN")
        if dry_run_count > 0:
            print(f"\n  >> DRY-RUN: {dry_run_count} templates WOULD use {{{{wan2_interface}}}} variable")
            if devices_needing_migration:
                dry_run_device_count = sum(1 for r in device_migration_results if r["status"] == "DRY-RUN")
                print(f"  >> DRY-RUN: {dry_run_device_count} devices WOULD have static IP overrides preserved")
                print(f"  >> DRY-RUN: Port configs WOULD migrate from 'ge-0/0/1' to '{{{{wan2_interface}}}}'")
            print(f"\n  >> To apply these changes, run without --dry-run flag")
            print(f"  >> Ensure all affected sites have 'wan2_interface' variable set (Menu #103)")
    else:
        success_count = sum(1 for r in results if r["status"] == "SUCCESS")
        if success_count > 0:
            print(f"\n  !? {success_count} templates now use {{{{wan2_interface}}}} variable")
            if devices_needing_migration:
                device_success = sum(1 for r in device_migration_results if r["status"] == "SUCCESS")
                print(f"  !? {device_success} devices had static IP overrides preserved")
                print(f"  !? Port configs migrated from 'ge-0/0/1' to '{{{{wan2_interface}}}}'")
            print(f"  !? Ensure all affected sites have 'wan2_interface' variable set (Menu #103)")
            print(f"  !? Sites without the variable may experience gateway connectivity issues")
    
    if failure_count > 0:
        print(f"\n  !? {failure_count} templates failed to update - check audit report")
    
    if devices_needing_migration:
        device_failed = len(device_migration_results) - sum(1 for r in device_migration_results if r["status"] == "SUCCESS")
        if device_failed > 0:
            print(f"\n  !? WARNING: {device_failed} devices failed override migration")
            print(f"  !? These devices may lose static IP configurations")
            print(f"  !? Check GatewayDevice_WAN2_Override_Migration.csv for details")
    
    logging.warning(f"Menu #104 DESTRUCTIVE operation complete ({operation_mode.upper()} mode): {success_count} templates updated, {failure_count} failed")
    if devices_needing_migration:
        logging.warning(f"Device override migration ({operation_mode.upper()} mode): {device_success} successful, {device_failed} failed")

def convert_virtual_chassis_to_virtual_mac():
    """
    Presents a list of sites first, then shows switches that are virtual chassis at the selected site,
    lets the user select one, and calls the Mist API to convert the device to a virtual MAC.
    """
    print("\n  DESTRUCTIVE: Virtual Chassis to Virtual MAC Conversion")
    print("=" * 60)
    
    # First, prompt for site selection
    site_id = prompt_site_selection()
    if not site_id:
        print(" No site selected.")
        return
    
    # Get site name for display
    site_name = "Unknown Site"
    try:
        org_id = get_cached_or_prompted_org_id()
        site_response = mistapi.api.v1.sites.getSite(apisession, site_id)
        if site_response.data:
            site_name = site_response.data.get('name', site_id)
    except Exception as e:
        logging.warning(f"Could not fetch site name for {site_id}: {e}")
    
    print(f"\n  Selected Site: {site_name} ({site_id})")
    
    # Ensure OrgInventory.csv is fresh
    check_and_generate_csv("OrgInventory.csv", export_device_inventory_to_csv)

    # Load OrgInventory.csv and filter for switches at the selected site with a non-empty id 
    inventory_path = get_csv_file_path("OrgInventory.csv")
    with open(inventory_path, mode="r", encoding="utf-8") as file:
        reader = list(csv.DictReader(file))
        switches = [
            row for row in reader
            if (row.get("type") == "switch" 
                and row.get("id", "").strip()
                and row.get("site_id") == site_id)
        ]

    if not switches:
        print(f"! No virtual chassis switches found at site '{site_name}'.")
        print(" Virtual chassis switches must have a device ID assigned.")
        logging.warning(f"No virtual chassis switches found at site {site_id}.")
        return

    # Display indexed list to user
    print(f"\n  Available Virtual Chassis Switches at '{site_name}':")
    print("-" * 80)
    index_to_device = {}
    name_to_device = {}
    for idx, sw in enumerate(switches):
        print(f"[{idx}] {sw.get('name', ''):20} MAC: {sw.get('mac', ''):17} Model: {sw.get('model', ''):10} Serial: {sw.get('serial', ''):15} ID: {sw.get('id', '')}")
        index_to_device[idx] = sw
        name_to_device[sw.get("name", "")] = sw

    user_input = input(f"\nEnter the index or switch name to convert to virtual MAC [0-{len(switches)-1}]: ").strip()

    # Resolve user input
    selected = None
    if user_input.isdigit():
        idx = int(user_input)
        selected = index_to_device.get(idx)
    else:
        selected = name_to_device.get(user_input)

    if not selected:
        print(" Switch not found by index or name.")
        logging.warning(f"Switch not found: {user_input}")
        return

    device_id = selected.get("id")
    if not device_id:
        print(" Missing device_id for selected switch.")
        logging.warning("Missing device_id for selected switch.")
        return

    # Confirmation prompt for destructive operation
    print(f"\n   DESTRUCTIVE OPERATION WARNING ")
    print(f"You are about to convert switch '{selected.get('name', '')}' to virtual MAC.")
    print(f"Site: {site_name}")
    print(f"Device ID: {device_id}")
    print(f"MAC: {selected.get('mac', '')}")
    print(f"This operation cannot be undone!")
    
    confirm = safe_input("\nType 'CONVERT' to proceed or anything else to cancel: ", "", True, "virtual MAC conversion confirmation")
    if confirm is None or confirm != "CONVERT":
        print(" Operation cancelled.")
        return

    print(f"! Converting switch '{selected.get('name', '')}' (device_id: {device_id}) at site '{site_name}' to virtual MAC...")
    try:
        # Call the Mist API to convert to virtual MAC
        resp = mistapi.api.v1.sites.devices.convertSiteVirtualChassisToVirtualMac(apisession, site_id, device_id)
        # Show the result to the user, including error details if present
        if hasattr(resp, "status_code") and resp.status_code >= 400:
            print(f"! Conversion failed (HTTP {resp.status_code}): {getattr(resp, 'data', '')}")
            logging.error(f"Conversion to virtual MAC failed for device {device_id} at site {site_id}. Response: {getattr(resp, 'data', '')}")
        elif isinstance(getattr(resp, "data", None), dict) and "detail" in resp.data:
            print(f"! Conversion failed: {resp.data['detail']}")
            logging.error(f"Conversion to virtual MAC failed for device {device_id} at site_id {site_id}. Detail: {resp.data['detail']}")
        else:
            print(" Conversion to virtual MAC triggered successfully!")
            print(" Check the device status in the Mist UI to monitor progress.")
            logging.info(f"Conversion to virtual MAC triggered for device {device_id} at site {site_id}. Response: {getattr(resp, 'data', '')}")
    except Exception as e:
        print(f"! Failed to convert to virtual MAC: {e}")
        logging.error(f"Failed to convert to virtual MAC: {e}")

def convert_virtual_chassis_by_site_list():
    """
    Reads site names from VCConvert.CSV (no header), finds all virtual chassis switches 
    in those sites, displays them to the user for confirmation, and converts them all 
    if the user confirms.
    """
    logging.info("Starting bulk virtual chassis to virtual MAC conversion by site list...")
    
    # Check if VCConvert.CSV exists
    csv_file = "VCConvert.CSV"
    csv_file_path = get_csv_file_path(csv_file)
    if not os.path.exists(csv_file_path):
        print(f"! File '{csv_file}' not found.")
        print(f"   Please create this file at: {csv_file_path}")
        print("   This file should contain site names (one per line, no header).")
        
        # Offer to create a basic file
        user_input = input("   Would you like to create an empty file to get started? (y/n): ").strip().lower()
        if user_input in ['y', 'yes']:
            try:
                template_path = create_missing_csv_template("VCConvert.CSV")
                print(f"! Empty file created at: {template_path}")
                print("   Please edit the file to add your site names and run the script again.")
            except Exception as e:
                print(f"! Failed to create file: {e}")
        
        logging.error(f"VCConvert.CSV file not found.")
        return

    # Read site names from CSV (no header)
    site_names = []
    try:
        with open(csv_file_path, mode="r", encoding="utf-8") as f:
            reader = csv.reader(f)
            for row in reader:
                if row and row[0].strip():  # Skip empty rows
                    site_names.append(row[0].strip())
    except Exception as e:
        print(f"! Error reading {csv_file}: {e}")
        logging.error(f"Error reading VCConvert.CSV: {e}")
        return

    if not site_names:
        print(f"! No site names found in {csv_file}.")
        logging.warning("No site names found in VCConvert.CSV.")
        return

    print(f"! Loaded {len(site_names)} site names from {csv_file}:")
    for idx, site_name in enumerate(site_names):
        print(f"  [{idx+1}] {site_name}")

    # Ensure required CSVs are fresh
    check_and_generate_csv("OrgInventory.csv", export_device_inventory_to_csv)
    check_and_generate_csv("SiteList.csv", export_all_sites_to_csv)

    # Load site list to get site IDs
    site_name_to_id = {}
    try:
        site_list_path = get_csv_file_path("SiteList.csv")
        with open(site_list_path, mode="r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                site_name_to_id[row.get("name", "")] = row.get("id", "")
    except Exception as e:
        print(f"! Error reading SiteList.csv: {e}")
        logging.error(f"Error reading SiteList.csv: {e}")
        return

    # Find site IDs for the specified site names
    target_site_ids = []
    missing_sites = []
    for site_name in site_names:
        site_id = site_name_to_id.get(site_name)
        if site_id:
            target_site_ids.append(site_id)
        else:
            missing_sites.append(site_name)

    if missing_sites:
        print(f"! Warning: The following sites were not found in the organization:")
        for site in missing_sites:
            print(f"   - {site}")

    if not target_site_ids:
        print(" No valid sites found. Exiting.")
        logging.error("No valid sites found for VC conversion.")
        return

    # Load inventory and filter for virtual chassis switches in target sites
    switches_to_convert = []
    try:
        inventory_path = get_csv_file_path("OrgInventory.csv")
        with open(inventory_path, mode="r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                if (row.get("type") == "switch" and 
                    row.get("site_id") in target_site_ids and 
                    row.get("id", "").strip()):
                    
                    # Get site name for display
                    site_name = next((name for name, id_ in site_name_to_id.items() if id_ == row.get("site_id")), "Unknown Site")
                    row["site_name"] = site_name
                    switches_to_convert.append(row)
    except Exception as e:
        print(f"! Error reading OrgInventory.csv: {e}")
        logging.error(f"Error reading OrgInventory.csv: {e}")
        return

    if not switches_to_convert:
        print(" No virtual chassis switches found in the specified sites.")
        logging.warning("No virtual chassis switches found in target sites.")
        return

    # Display switches that will be converted
    print(f"\n  Found {len(switches_to_convert)} virtual chassis switches to convert:")
    print("=" * 100)
    for idx, switch in enumerate(switches_to_convert):
        print(f"[{idx+1:2}] Site: {switch.get('site_name', ''):25} | "
              f"Name: {switch.get('name', ''):20} | "
              f"MAC: {switch.get('mac', ''):17} | "
              f"Model: {switch.get('model', ''):12} | "
              f"Serial: {switch.get('serial', '')}")

    # Ask for user confirmation
    print(f"\n  This will convert {len(switches_to_convert)} virtual chassis switches to virtual MAC.")
    print(" This operation cannot be undone easily.")
    
    confirm = input("\n  Do you want to proceed with the conversion? (yes/no): ").strip().lower()
    
    if confirm not in ['yes', 'y']:
        print(" Conversion cancelled by user.")
        logging.info("Virtual chassis conversion cancelled by user.")
        return

    # Proceed with conversions
    print(f"\n  Starting conversion of {len(switches_to_convert)} switches...")
    successful_conversions = 0
    failed_conversions = 0

    for idx, switch in enumerate(switches_to_convert):
        site_id = switch.get("site_id")
        device_id = switch.get("id")
        switch_name = switch.get("name", "")
        site_name = switch.get("site_name", "")
        
        print(f"\n[{idx+1}/{len(switches_to_convert)}] Converting '{switch_name}' at site '{site_name}'...")
        
        try:
            # Call the Mist API to convert to virtual MAC
            resp = mistapi.api.v1.sites.devices.convertSiteVirtualChassisToVirtualMac(apisession, site_id, device_id)
            
            # Check the response
            if hasattr(resp, "status_code") and resp.status_code >= 400:
                print(f"! Conversion failed (HTTP {resp.status_code}): {getattr(resp, 'data', '')}")
                logging.error(f"Conversion failed for {switch_name} at {site_name}. HTTP {resp.status_code}: {getattr(resp, 'data', '')}")
                failed_conversions += 1
            elif isinstance(getattr(resp, "data", None), dict) and "detail" in resp.data:
                print(f"! Conversion failed: {resp.data['detail']}")
                logging.error(f"Conversion failed for {switch_name} at {site_name}. Detail: {resp.data['detail']}")
                failed_conversions += 1
            else:
                print(f"! Conversion triggered successfully.")
                logging.info(f"Conversion triggered for {switch_name} at {site_name}. Response: {getattr(resp, 'data', '')}")
                successful_conversions += 1
                
        except Exception as e:
            print(f"! Exception during conversion: {e}")
            logging.error(f"Exception during conversion of {switch_name} at {site_name}: {e}")
            failed_conversions += 1

    # Summary
    print(f"\n  Conversion Summary:")
    print(f"   Successful conversions: {successful_conversions}")
    print(f"   Failed conversions: {failed_conversions}")
    print(f"   Total switches processed: {len(switches_to_convert)}")
    
    if successful_conversions > 0:
        print(f"\n  Note: Successful conversions may take a few minutes to complete.")
        print(f"   Monitor the devices in the Mist portal to confirm the conversion status.")
    
    logging.info(f"Bulk VC conversion completed: {successful_conversions} successful, {failed_conversions} failed")

def check_virtual_chassis_conversion_status():
    """
    Check all switches in the organization to determine if they have been converted to virtual MAC addresses.
    Virtual chassis switches that have been converted to virtual MAC will have vc_mac starting with "020003".
    Non-converted virtual chassis switches will have different vc_mac prefixes.
    
    This function:
    1. Uses cached OrgInventory.csv or generates fresh data
    2. Filters for switches with vc_mac (virtual chassis candidates)
    3. Checks vc_mac prefix to determine conversion status
    4. Exports results to VirtualChassisConversionStatus.csv
    5. Displays summary statistics
    """
    print("\n  Virtual Chassis to Virtual MAC Conversion Status Check")
    print("=" * 70)
    print(" Checking all switches for virtual chassis conversion status...")
    print(" Converted switches have vc_mac starting with '020003'")
    
    logging.info("Starting virtual chassis conversion status check...")
    
    # Ensure OrgInventory.csv is fresh
    check_and_generate_csv("OrgInventory.csv", export_device_inventory_to_csv)
    
    # Load inventory and filter for switches with vc_mac (virtual chassis switches)
    switches_with_vc_mac = []
    try:
        inventory_path = get_csv_file_path("OrgInventory.csv")
        with open(inventory_path, mode="r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                if (row.get("type") == "switch" and 
                    row.get("vc_mac", "").strip()):
                    switches_with_vc_mac.append(row)
                    
    except Exception as e:
        print(f"! Error reading OrgInventory.csv: {e}")
        logging.error(f"Error reading OrgInventory.csv: {e}")
        return
    
    if not switches_with_vc_mac:
        print(" No switches with vc_mac found in the organization.")
        print(" Only virtual chassis switches have vc_mac assigned.")
        logging.warning("No switches with vc_mac found.")
        return
    
    # Load site information for display
    site_id_to_name = {}
    try:
        check_and_generate_csv("SiteList.csv", export_all_sites_to_csv)
        site_list_path = get_csv_file_path("SiteList.csv")
        with open(site_list_path, mode="r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                site_id_to_name[row.get("id", "")] = row.get("name", "Unknown Site")
    except Exception as e:
        logging.warning(f"Could not load site names: {e}")
    
    # Analyze conversion status
    converted_switches = []
    not_converted_switches = []
    
    for switch in switches_with_vc_mac:
        vc_mac = switch.get("vc_mac", "")
        site_id = switch.get("site_id", "")
        site_name = site_id_to_name.get(site_id, "Unknown Site")
        
        # Create enhanced switch record with analysis
        enhanced_switch = switch.copy()
        enhanced_switch["site_name"] = site_name
        
        # Check if vc_mac starts with "020003" (converted to virtual MAC)
        if vc_mac.startswith("020003"):
            enhanced_switch["conversion_status"] = "CONVERTED"
            enhanced_switch["conversion_notes"] = "vc_mac starts with 020003 - converted to virtual MAC"
            converted_switches.append(enhanced_switch)
        else:
            enhanced_switch["conversion_status"] = "NOT_CONVERTED"
            enhanced_switch["conversion_notes"] = f"vc_mac starts with {vc_mac[:6]} - not converted to virtual MAC"
            not_converted_switches.append(enhanced_switch)
    
    # Combine all switches for export
    all_switches = converted_switches + not_converted_switches
    
    # Display summary
    total_switches = len(all_switches)
    converted_count = len(converted_switches)
    not_converted_count = len(not_converted_switches)
    
    print(f"\n  Virtual Chassis Conversion Status Summary:")
    print(f"   Total virtual chassis switches: {total_switches}")
    print(f"   Converted to virtual MAC: {converted_count}")
    print(f"   Not converted: {not_converted_count}")
    
    if converted_count > 0:
        print(f"\n Converted Switches (vc_mac starts with '020003'):")
        for switch in converted_switches[:10]:  # Show first 10
            print(f"   !? {switch.get('name', 'Unnamed'):20} | Site: {switch.get('site_name', ''):25} | vc_mac: {switch.get('vc_mac', '')[:8]}...")
        if len(converted_switches) > 10:
            print(f"   ... and {len(converted_switches) - 10} more")
    
    if not_converted_count > 0:
        print(f"\n Not Converted Switches (vc_mac does NOT start with '020003'):")
        for switch in not_converted_switches[:10]:  # Show first 10
            print(f"   !? {switch.get('name', 'Unnamed'):20} | Site: {switch.get('site_name', ''):25} | vc_mac: {switch.get('vc_mac', '')[:8]}...")
        if len(not_converted_switches) > 10:
            print(f"   ... and {len(not_converted_switches) - 10} more")
    
    # Export to CSV
    try:
        # Flatten any nested fields for CSV export
        flattened_switches = flatten_nested_fields_in_list(all_switches)
        sanitized_switches = escape_multiline_strings_for_csv(flattened_switches)
        
        # Save to CSV
        filename = "VirtualChassisConversionStatus.csv"
        DataExporter.save_data_to_output(sanitized_switches, filename)
        
        print(f"\n  Results exported to: {filename}")
        print(f"   Location: {get_csv_file_path(filename)}")
        
        # Log results
        logging.info(f"Virtual chassis conversion status check completed:")
        logging.info(f"  Total switches: {total_switches}")
        logging.info(f"  Converted: {converted_count}")
        logging.info(f"  Not converted: {not_converted_count}")
        logging.info(f"  Results exported to {filename}")
        
    except Exception as e:
        print(f"! Error exporting results: {e}")
        logging.error(f"Error exporting conversion status results: {e}")
    
    print(f"\n  Usage Notes:")
    print(f"   !? Use option 92 to convert individual switches")
    print(f"   !? Use option 93 for bulk conversion by site list")
    print(f"   !? Virtual chassis switches without '020003' vc_mac prefix can be converted")

def export_site_wifi_clients_to_csv(site_id=None):
    """
    Exports all currently connected WiFi clients and their session data for a selected site to SiteWiFiClients.CSV.
    Fetches both wireless client data and wireless client session data, then merges them based on MAC address.
    If site_id is not provided, prompts user to select from site list.
    
    The merged data includes:
    - Current client information (if available)
    - Session data for each client (prefixed with 'session_')
    - Session count for clients with multiple sessions
    - Sessions without corresponding current clients (marked as 'session_only')
    """
    print("Export Site WiFi Clients:")
    logging.info("Starting export of site WiFi clients...")
    
    # Ensure required CSVs are fresh
    check_and_generate_csv("SiteList.csv", export_all_sites_to_csv)
    
    # Get site_id if not provided
    if not site_id:
        site_id = prompt_select_site_id_from_csv("SiteList.csv")
        if not site_id:
            logging.error(" No site selected.")
            print(" No site selected.")
            return
    
    # Get site name for display
    site_name = "Unknown Site"
    try:
        site_list_path = get_csv_file_path("SiteList.csv")
        with open(site_list_path, mode="r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                if row.get("id") == site_id:
                    site_name = row.get("name", "Unknown Site")
                    break
    except Exception as e:
        logging.warning(f"! Failed to load site name from SiteList.csv: {e}")
    
    logging.info(f"Fetching WiFi clients for site: {site_name} (ID: {site_id})")
    print(f"! Fetching WiFi clients for site: {site_name}")
    
    try:
        # Call the Mist API to search for wireless clients at the site
        logging.info("Fetching wireless clients data...")
        client_response = mistapi.api.v1.sites.clients.searchSiteWirelessClients(apisession, site_id, limit=1000)
        clients = mistapi.get_all(response=client_response, mist_session=apisession)
        
        # Call the Mist API to search for wireless client sessions at the site
        logging.info("Fetching wireless client sessions data...")
        session_response = mistapi.api.v1.sites.clients.searchSiteWirelessClientSessions(apisession, site_id, limit=1000)
        sessions = mistapi.get_all(response=session_response, mist_session=apisession)
        
        if not clients and not sessions:
            logging.warning(" No WiFi clients or sessions found at this site.")
            print(" No WiFi clients or sessions found at this site.")
            # Create empty CSV with headers
            wifi_clients_path = get_csv_file_path("SiteWiFiClients.CSV")
            with open(wifi_clients_path, "w", newline="", encoding="utf-8") as f:
                writer = csv.writer(f)
                writer.writerow(["site_id", "site_name", "message"])
                writer.writerow([site_id, site_name, "No WiFi clients or sessions found"])
            return
        
        # Create a dictionary to store session data by MAC address for easy lookup
        sessions_by_mac = {}
        if sessions:
            for session in sessions:
                mac = session.get("mac")
                if mac:
                    # If multiple sessions exist for the same MAC, store them in a list
                    if mac in sessions_by_mac:
                        if not isinstance(sessions_by_mac[mac], list):
                            sessions_by_mac[mac] = [sessions_by_mac[mac]]
                        sessions_by_mac[mac].append(session)
                    else:
                        sessions_by_mac[mac] = session
        
        # Merge client data with session data based on MAC address
        enriched_clients = []
        processed_macs = set()
        
        # Process clients and merge with matching sessions
        if clients:
            for client in clients:
                client_mac = client.get("mac")
                # Add site information
                client["site_id"] = site_id
                client["site_name"] = site_name
                client["data_source"] = "client"
                
                # Merge with session data if available
                if client_mac and client_mac in sessions_by_mac:
                    session_data = sessions_by_mac[client_mac]
                    if isinstance(session_data, list):
                        # Multiple sessions - merge with the most recent one
                        latest_session = max(session_data, key=lambda x: x.get("start_time", 0))
                        for key, value in latest_session.items():
                            if key not in client:  # Don't overwrite client data
                                client[f"session_{key}"] = value
                        client["session_count"] = len(session_data)
                    else:
                        # Single session
                        for key, value in session_data.items():
                            if key not in client:  # Don't overwrite client data
                                client[f"session_{key}"] = value
                        client["session_count"] = 1
                    processed_macs.add(client_mac)
                else:
                    client["session_count"] = 0
                
                enriched_clients.append(client)
        
        # Add any sessions that don't have corresponding client data
        if sessions:
            for session in sessions:
                session_mac = session.get("mac")
                if session_mac and session_mac not in processed_macs:
                    # This is a session without a corresponding current client
                    session["site_id"] = site_id
                    session["site_name"] = site_name
                    session["data_source"] = "session_only"
                    session["session_count"] = 1
                    # Prefix session-specific fields to avoid conflicts
                    session_data = {}
                    for key, value in session.items():
                        if key not in ["site_id", "site_name", "data_source", "session_count"]:
                            session_data[f"session_{key}"] = value
                        else:
                            session_data[key] = value
                    enriched_clients.append(session_data)
        
        if not enriched_clients:
            logging.warning(" No data to export after processing.")
            print(" No data to export after processing.")
            return
        
        # Flatten and sanitize the data for CSV
        flattened = flatten_nested_fields_in_list(enriched_clients)
        sanitized = escape_multiline_strings_for_csv(flattened)
        
        # Write to CSV
        DataExporter.save_data_to_output(sanitized, "SiteWiFiClients.CSV")
        
        client_count = len(clients) if clients else 0
        session_count = len(sessions) if sessions else 0
        total_records = len(enriched_clients)
        
        logging.info(f"! WiFi data exported to SiteWiFiClients.CSV ({client_count} clients, {session_count} sessions, {total_records} total records)")
        print(f"! WiFi data exported to SiteWiFiClients.CSV")
        print(f"   {client_count} current clients, {session_count} sessions, {total_records} total records from {site_name}")
        
    except Exception as e:
        logging.error(f"! Failed to fetch WiFi data for site {site_id}: {e}")
        print(f"! Failed to fetch WiFi data: {e}")

def reboot_devices_by_gateway_template_list():
    """
    Reboots all devices associated with branch templates listed in GatewayTemplateRebootList.CSV.
    Logs the result of each reboot command to GatewayTemplateRebootResults.CSV.
    """

    logging.info("[46] Starting reboot_devices_by_gateway_template_list")

    # Step 1: Check if the reboot list file exists
    reboot_list_path = get_csv_file_path("GatewayTemplateRebootList.CSV")
    if not os.path.exists(reboot_list_path):
        logging.error(" GatewayTemplateRebootList.CSV not found.")
        print(" GatewayTemplateRebootList.CSV not found.")
        print(f"   Please create this file at: {reboot_list_path}")
        print("   This file should contain template names to reboot, one per line.")
        
        # Offer to create a basic file
        user_input = input("   Would you like to create an empty file to get started? (y/n): ").strip().lower()
        if user_input in ['y', 'yes']:
            try:
                template_path = create_missing_csv_template("GatewayTemplateRebootList.CSV")
                print(f"! Empty file created at: {template_path}")
                print("   Please edit the file to add your template names and run the script again.")
            except Exception as e:
                print(f"! Failed to create file: {e}")
        return

    # Step 2: Ensure required CSVs are fresh
    check_and_generate_csv("OrgDevices.csv", export_all_devices_to_csv)
    check_and_generate_csv("SiteList.csv", export_all_sites_to_csv)
    check_and_generate_csv("OrgGatewayTemplates.csv", export_gateway_templates_to_csv)
    check_and_generate_csv("AllSiteGatewayConfigs.csv", lambda: export_gateway_device_configs_to_csv(fast=True))

    # Step 3: Load template name to ID mapping from OrgGatewayTemplates.csv
    template_name_to_id = {}
    try:
        gateway_templates_path = get_csv_file_path("OrgGatewayTemplates.csv")
        with open(gateway_templates_path, encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                name = row.get("name", "").strip()
                tid = row.get("id", "").strip()
                if name and tid:
                    template_name_to_id[name] = tid
        logging.info(f"Loaded {len(template_name_to_id)} gateway templates from OrgGatewayTemplates.csv")
    except Exception as e:
        logging.error(f"! Failed to load gateway templates: {e}")
        print(f"! Failed to load gateway templates: {e}")
        return

    if not template_name_to_id:
        logging.warning(" No gateway templates found in OrgGatewayTemplates.csv")
        print(" No gateway templates found in OrgGatewayTemplates.csv")
        return

    # Step 4: Load reboot list of template names
    reboot_template_names = set()
    try:
        reboot_list_path = get_csv_file_path("GatewayTemplateRebootList.CSV")
        with open(reboot_list_path, encoding="utf-8") as f:
            reader = csv.reader(f)
            for row in reader:
                if row and row[0].strip():
                    reboot_template_names.add(row[0].strip())
        logging.info(f"Loaded {len(reboot_template_names)} template names from reboot list: {reboot_template_names}")
    except Exception as e:
        logging.error(f"! Failed to load reboot template list: {e}")
        print(f"! Failed to load reboot template list: {e}")
        return

    # Step 5: Map template names to IDs and show matches/mismatches
    reboot_template_ids = set()
    for name in reboot_template_names:
        if name in template_name_to_id:
            reboot_template_ids.add(template_name_to_id[name])
            logging.info(f"! Found template '{name}' with ID '{template_name_to_id[name]}'")
        else:
            logging.warning(f"! Template '{name}' not found in OrgGatewayTemplates.csv")
            print(f"! Template '{name}' not found in available templates")

    if not reboot_template_ids:
        logging.error(" No matching template IDs found for reboot")
        print(" No matching template IDs found for reboot")
        print("Available templates:")
        for name, tid in template_name_to_id.items():
            print(f"  - {name} ({tid})")
        return

    logging.info(f"Proceeding with {len(reboot_template_ids)} template IDs: {reboot_template_ids}")

    # Step 6: First find sites that use the target gateway template IDs and create site-to-template mapping
    sites_using_templates = set()
    site_to_template_mapping = {}  # Maps site_id to (template_id, template_name, site_name)
    template_id_to_name = {tid: name for name, tid in template_name_to_id.items()}  # Reverse lookup
    
    try:
        site_list_path = get_csv_file_path("SiteList.csv")
        with open(site_list_path, encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                gateway_template_id = row.get("gatewaytemplate_id", "").strip()
                if gateway_template_id in reboot_template_ids:
                    site_id = row.get("id", "").strip()
                    site_name = row.get("name", "").strip()
                    template_name = template_id_to_name.get(gateway_template_id, "Unknown Template")
                    sites_using_templates.add(site_id)
                    site_to_template_mapping[site_id] = (gateway_template_id, template_name, site_name)
                    logging.info(f"Found site '{site_name}' (ID: {site_id}) using gateway template '{template_name}' (ID: {gateway_template_id})")
    except Exception as e:
        logging.error(f"! Failed to load site list: {e}")
        print(f"! Failed to load site list: {e}")
        return

    if not sites_using_templates:
        logging.warning(" No sites found using the specified gateway templates")
        print(" No sites found using the specified gateway templates")
        return

    logging.info(f"Found {len(sites_using_templates)} sites using target templates: {sites_using_templates}")

    # Step 7: Load AllSiteGatewayConfigs and filter gateway devices by site_id
    reboot_targets = []
    try:
        gateway_configs_path = get_csv_file_path("AllSiteGatewayConfigs.csv")
        with open(gateway_configs_path, encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                device_site_id = row.get("site_id", "").strip()
                device_type = row.get("type", "").strip()
                device_id = row.get("id", "").strip()
                device_name = row.get("name", "").strip()
                
                # Debug logging for the first few devices
                logging.debug(f"Checking device '{device_name}' (ID: {device_id}) in site '{device_site_id}' of type '{device_type}'")
                
                # Only target gateway devices in sites that use our target templates
                if device_site_id in sites_using_templates and device_type == "gateway":
                    template_id, template_name, site_name = site_to_template_mapping.get(device_site_id, ("unknown", "Unknown Template", "Unknown Site"))
                    reboot_targets.append({
                        "device_id": device_id,
                        "device_name": device_name,
                        "site_id": device_site_id,
                        "site_name": site_name,
                        "template_id": template_id,
                        "template_name": template_name
                    })
                    logging.info(f"Found gateway device '{device_name}' (ID: {device_id}) in site '{site_name}' (ID: {device_site_id}) using template '{template_name}' (ID: {template_id})")
    except Exception as e:
        logging.error(f"! Failed to load gateway configs: {e}")
        print(f"! Failed to load gateway configs: {e}")
        return

    if not reboot_targets:
        logging.warning(" No gateway devices found in sites using the specified templates")
        print(" No gateway devices found in sites using the specified templates")
        
        # Provide debug information
        logging.info(f"Sites using target templates: {sites_using_templates}")
        print(f"Debug: Sites using target templates: {list(sites_using_templates)}")
        
        # Count devices by type in target sites
        device_counts = {}
        try:
            with open(get_csv_file_path("AllSiteGatewayConfigs.csv"), encoding="utf-8") as f:
                reader = csv.DictReader(f)
                for row in reader:
                    device_site_id = row.get("site_id", "").strip()
                    device_type = row.get("type", "").strip()
                    if device_site_id in sites_using_templates:
                        device_counts[device_type] = device_counts.get(device_type, 0) + 1
            
            if device_counts:
                logging.info(f"Device types found in target sites: {device_counts}")
                print(f"Debug: Device types found in target sites: {device_counts}")
            else:
                logging.warning("No devices found in any of the target sites")
                print("Debug: No devices found in any of the target sites")
        except Exception as e:
            logging.error(f"Failed to analyze devices in target sites: {e}")
        
        return

    logging.info(f"Found {len(reboot_targets)} gateway devices to reboot")
    
    # Step 8: Display devices and get user confirmation
    print("\n" + "=" * 100)
    print(" DEVICE REBOOT CONFIRMATION REQUIRED ")
    print("=" * 100)
    print(f"\n  The following {len(reboot_targets)} gateway devices will be REBOOTED:")
    print("-" * 100)
    
    # Group devices by template for better display
    devices_by_template = {}
    for target in reboot_targets:
        template_name = target['template_name']
        if template_name not in devices_by_template:
            devices_by_template[template_name] = []
        devices_by_template[template_name].append(target)
    
    # Display devices grouped by template
    for template_name, devices in devices_by_template.items():
        print(f"\n  Template: {template_name}")
        print(f"   {len(devices)} devices affected:")
        for device in devices:
            print(f"      !? {device['device_name']} (ID: {device['device_id']}) at site '{device['site_name']}'")
    
    # Display critical warnings in a cleaner format
    warning_lines = [
        " CRITICAL WARNING - READ CAREFULLY:",
        "!? This action will REBOOT network gateway devices",
        "!? Network connectivity will be TEMPORARILY LOST during reboot",
        "!? Users may experience service interruptions",
        "!? Remote sites may become inaccessible during reboot",
        "!? This is a DISRUPTIVE network operation",
        "!? Ensure you have alternative access methods if needed",
        "!? The script owner bears NO LIABILITY for any consequences",
        "!? Proceed only if you understand and accept these risks"
    ]
    
    print("\n" + "??" * 50)
    for line in warning_lines:
        print(line)
    print("??" * 50)
    
    print(f"\n  Summary:")
    print(f"   !? Total devices to reboot: {len(reboot_targets)}")
    print(f"   !? Templates involved: {len(devices_by_template)}")
    print(f"   !? Sites affected: {len(set(target['site_name'] for target in reboot_targets))}")
    
    # Get user confirmation with liability waiver
    print(f"\n  Do you want to proceed with rebooting {len(reboot_targets)} gateway devices?")
    print("   Type 'REBOOT' (all caps) to confirm, or anything else to cancel:")
    print("   By typing 'REBOOT', you acknowledge and accept all risks and liability.")
    
    try:
        user_input = input(">>> ").strip()
        if user_input != "REBOOT":
            print(" Reboot operation cancelled by user.")
            logging.info("Gateway reboot operation cancelled by user input")
            return
        else:
            print(" User confirmed reboot operation. Proceeding...")
            logging.info(f"! LIABILITY WAIVER ACCEPTED: User confirmed gateway reboot operation for {len(reboot_targets)} devices")
            logging.info(f"User input: '{user_input}' - User accepts full responsibility and liability for network disruption")
            # Log detailed device list for audit trail
            device_list = [f"{d['device_name']} ({d['device_id']}) at {d['site_name']}" for d in reboot_targets]
            logging.info(f"Devices to be rebooted: {device_list}")
    except KeyboardInterrupt:
        print("\n Reboot operation cancelled by user (Ctrl+C).")
        logging.info("Gateway reboot operation cancelled by user interrupt")
        return
    except Exception as e:
        print(f"! Error getting user input: {e}")
        logging.error(f"Error getting user input for reboot confirmation: {e}")
        return

    print("\n  Starting device reboot operations...")
    print("=" * 50)

    # Step 9: Reboot each device and log results
    results = []
    for device in reboot_targets:
        status = ""
        try:
            logging.info(f"Rebooting device '{device['device_name']}' (ID: {device['device_id']})")
            print(f"! Rebooting {device['device_name']} at {device['site_name']}...")
            resp = mistapi.api.v1.sites.devices.restartSiteDevice(
                apisession,
                device["site_id"],
                device["device_id"],
                body={"timestamp": datetime.now(timezone.utc).isoformat()}
            )
            # Handle different possible response formats
            if hasattr(resp, "data") and resp.data:
                if isinstance(resp.data, dict):
                    status = resp.data.get("status", f"SUCCESS - Response: {resp.data}")
                else:
                    status = f"SUCCESS - Data: {resp.data}"
            elif hasattr(resp, "status_code"):
                status = f"SUCCESS - HTTP {resp.status_code}"
            else:
                status = f"SUCCESS - Response: {str(resp)}"
            print(f"   Reboot command sent successfully")
            logging.info(f"! Reboot command sent for '{device['device_name']}': {status}")
        except Exception as e:
            status = f"ERROR: {e}"
            print(f"   Failed to send reboot command: {e}")
            logging.error(f"! Failed to reboot '{device['device_name']}': {e}")

        results.append({
            "Template ID": device["template_id"],
            "Template Name": device["template_name"],
            "Device ID": device["device_id"],
            "Device Name": device["device_name"],
            "Site ID": device["site_id"],
            "Site Name": device["site_name"],
            "Status": status
        })

    # Step 10: Write results to CSV
    try:
        results_csv_path = get_csv_file_path("GatewayTemplateRebootResults.CSV")
        with open(results_csv_path, "w", newline='', encoding="utf-8") as f:
            fieldnames = ["Template ID", "Template Name", "Device ID", "Device Name", "Site ID", "Site Name", "Status"]
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(results)
        
        print(f"\n  Operation completed!")
        print(f"   Reboot commands sent to {len(results)} devices")
        print(f"   Results logged to GatewayTemplateRebootResults.CSV")
        logging.info(f"! Reboot results written to GatewayTemplateRebootResults.CSV ({len(results)} entries)")
    except Exception as e:
        logging.error(f"! Failed to write results to CSV: {e}")
        print(f"! Failed to write results to CSV: {e}")


class MapsManager:
    """
    Comprehensive Maps Management System for Mist Sites
    
    Provides interactive management of site floor plans and maps including:
    - Map inventory and export operations
    - Image download and upload capabilities
    - Map creation and configuration
    - Device placement and auto-placement operations
    - Analytics and reporting
    """
    
    def __init__(self, api_session, organization_id):
        """Initialize MapsManager with API session and org context"""
        self.apisession = api_session
        self.org_id = organization_id
        self.current_site_id = None
        self.current_site_name = None
        logging.info(f"MapsManager initialized for organization: {self.org_id}")
    
    def select_site(self):
        """Prompt user to select a site and cache the selection"""
        site_id = prompt_site_selection()
        if site_id:
            # Get site name for display
            sites = fetch_all_sites_with_limit(self.org_id)
            site_name = next((s.get('name', 'Unknown') for s in sites if s['id'] == site_id), 'Unknown')
            self.current_site_id = site_id
            self.current_site_name = site_name
            print(f"\n   Site selected: {site_name}")
            logging.info(f"MapsManager site selection: {site_name} ({site_id})")
            return True
        else:
            print("\n! No site selected")
            return False
    
    def get_current_site(self):
        """Get current site selection, prompting if not set"""
        if not self.current_site_id:
            print("\n! No site currently selected. Please select a site first.")
            if not self.select_site():
                return None, None
        return self.current_site_id, self.current_site_name
    
    def _select_map_from_site(self, site_id, site_name):
        """Helper method to select a map from a site - returns map_id or None"""
        try:
            # Fetch maps for the site
            maps_response = mistapi.api.v1.sites.maps.listSiteMaps(
                self.apisession,
                site_id=site_id
            )
            
            if maps_response.status_code != 200:
                print(f"\n! Failed to fetch maps: HTTP {maps_response.status_code}")
                return None
            
            maps = maps_response.data
            if not maps:
                print(f"\n! No maps found for site: {site_name}")
                return None
            
            # Display map selection
            print(f"\nMaps for site: {site_name}")
            print(f"{'-' * 80}")
            for idx, map_item in enumerate(maps, 1):
                map_name = map_item.get('name', 'Unnamed')
                map_type = map_item.get('type', 'N/A')
                has_image = "with image" if 'url' in map_item else "no image"
                print(f"  {idx}. {map_name} ({map_type}) - {has_image}")
            print(f"{'-' * 80}")
            
            selection = input("\nSelect map number (or 0 to cancel): ").strip()
            try:
                map_idx = int(selection) - 1
                if map_idx < 0:
                    return None
                if map_idx >= len(maps):
                    print("\n! Invalid selection")
                    return None
                
                selected_map = maps[map_idx]
                return selected_map.get('id')
                
            except ValueError:
                print("\n! Invalid input - please enter a number")
                return None
                
        except EOFError:
            logging.info("EOF detected during map selection")
            return None
        except Exception as e:
            logging.error(f"Error selecting map: {e}", exc_info=True)
            print(f"\n! Error selecting map: {e}")
            return None
    
    def run_interactive_menu(self):
        """Main interactive menu loop for Maps Manager"""
        # Initial site selection
        print("\n" + "=" * 80)
        print("MAPS MANAGER - Initial Site Selection")
        print("=" * 80)
        print("\nPlease select a site to work with:")
        if not self.select_site():
            print("\n! Site selection required. Returning to main menu.")
            return
        
        while True:
            print("\n" + "=" * 80)
            print("MAPS MANAGER - Site Floorplan & Map Operations")
            if self.current_site_name:
                print(f"Current Site: {self.current_site_name}")
            print("=" * 80)
            print("\nSite Selection:")
            print("  S. Select different site")
            print("\nMap Inventory & Export:")
            print("  1. List maps for current site")
            print("  2. Export maps for current site to CSV/SQLite")
            print("  3. View detailed map information")
            print("\nMap Creation & Modification:")
            print("  4. Create new site map")
            print("  5. Update map properties")
            print("  6. Delete site map")
            print("  7. Upload/replace map image")
            print("  12. Clone/duplicate map")
            print("\nDevice Placement:")
            print("  8. View devices on map")
            print("  9. Auto-place APs on map")
            print("  10. Auto-orient APs on map")
            print("  11. Set AP/device location manually")
            print("\nBulk Operations (All Sites):")
            print("  20. List all site maps across organization")
            print("  21. Export all site maps to CSV/SQLite")
            print("  22. Export maps with image metadata")
            print("  23. Download all org map images")
            print("  24. Backup all maps (metadata + images)")
            print("  25. Maps without images report")
            print("\nAnalytics & Reporting:")
            print("  30. Map coverage analytics")
            print("  31. Device density by map")
            print("  32. Map usage statistics")
            print("\nVisualization & Editing:")
            print("  40. Interactive map viewer (view/edit devices, walls, zones)")
            print("\n  0. Return to main menu")
            print("=" * 80)
            
            try:
                choice = input("\nEnter your selection number now: ").strip().upper()
            except EOFError:
                logging.info("EOF detected in MapsManager menu - session disconnected")
                return
            
            if choice == "0":
                logging.info("Exiting Maps Manager")
                return
            elif choice == "S":
                self.select_site()
            elif choice == "1":
                self.list_site_maps()
            elif choice == "2":
                self.export_site_maps()
            elif choice == "3":
                self.view_map_details()
            elif choice == "4":
                self.create_site_map()
            elif choice == "5":
                self.update_map_properties()
            elif choice == "6":
                self.delete_site_map()
            elif choice == "7":
                self.upload_map_image()
            elif choice == "8":
                self.view_devices_on_map()
            elif choice == "9":
                self.auto_place_aps()
            elif choice == "10":
                self.auto_orient_aps()
            elif choice == "11":
                self.set_device_location()
            elif choice == "12":
                self.clone_map()
            elif choice == "20":
                self.list_all_org_maps()
            elif choice == "21":
                self.export_all_site_maps()
            elif choice == "22":
                self.export_maps_with_images()
            elif choice == "23":
                self.bulk_download_org_images()
            elif choice == "24":
                self.backup_all_maps()
            elif choice == "25":
                self.maps_without_images_report()
            elif choice == "30":
                self.map_coverage_analytics()
            elif choice == "31":
                self.device_density_analytics()
            elif choice == "32":
                self.map_usage_statistics()
            elif choice == "40":
                self.interactive_map_viewer()
            else:
                print(f"\n! Invalid selection: '{choice}'. Please enter a valid option.")
                logging.warning(f"Invalid Maps Manager menu selection: {choice}")
    
    def list_site_maps(self):
        """Display list of maps for currently selected site"""
        print("\n" + "-" * 80)
        print("LIST SITE MAPS - Current Site")
        print("-" * 80)
        
        site_id, site_name = self.get_current_site()
        if not site_id:
            return
        
        try:
            print(f"\nFetching maps for site: {site_name}")
            maps_response = mistapi.api.v1.sites.maps.listSiteMaps(
                self.apisession,
                site_id=site_id
            )
            
            if maps_response.status_code != 200:
                print(f"\n! Failed to fetch maps: HTTP {maps_response.status_code}")
                return
            
            maps = maps_response.data
            if not maps:
                print(f"\n! No maps found for site: {site_name}")
                return
            
            # Display summary
            print(f"\n{'-' * 80}")
            print(f"Total Maps Found: {len(maps)}")
            print(f"{'-' * 80}")
            print(f"{'Map Name':<35} {'Type':<15} {'Dimensions':<20} {'Image':<8}")
            print(f"{'-' * 80}")
            
            for map_item in maps:
                map_name = map_item.get('name', 'Unnamed')[:34]
                map_type = map_item.get('type', 'N/A')[:14]
                width = map_item.get('width', 0)
                height = map_item.get('height', 0)
                dimensions = f"{width}x{height}" if width and height else "N/A"
                has_image = "Yes" if 'url' in map_item else "No"
                print(f"{map_name:<35} {map_type:<15} {dimensions:<20} {has_image:<8}")
            
            print(f"{'-' * 80}")
            logging.info(f"Listed {len(maps)} maps for site {site_name}")
            
        except Exception as e:
            logging.error(f"Error listing site maps: {e}", exc_info=True)
            print(f"\n! Error listing maps: {e}")
    
    def list_all_org_maps(self):
        """Display summary list of all maps across organization sites"""
        print("\n" + "-" * 80)
        print("LIST ALL ORGANIZATION MAPS - All Sites")
        print("-" * 80)
        
        try:
            # Fetch all sites
            sites = fetch_all_sites_with_limit(self.org_id)
            if not sites:
                print("\n! No sites found in organization")
                return
            
            print(f"\nFetching maps from {len(sites)} sites...")
            all_maps = []
            
            for site in tqdm(sites, desc="Scanning sites", unit="site"):
                try:
                    maps_response = mistapi.api.v1.sites.maps.listSiteMaps(
                        self.apisession,
                        site_id=site['id']
                    )
                    
                    if maps_response.status_code == 200:
                        maps = maps_response.data
                        for map_item in maps:
                            all_maps.append({
                                'site_id': site['id'],
                                'site_name': site.get('name', 'Unknown'),
                                'map_id': map_item.get('id', 'N/A'),
                                'map_name': map_item.get('name', 'Unnamed'),
                                'type': map_item.get('type', 'N/A'),
                                'width': map_item.get('width', 0),
                                'height': map_item.get('height', 0),
                                'has_image': 'url' in map_item
                            })
                except Exception as e:
                    logging.debug(f"Error fetching maps for site {site['id']}: {e}")
                    continue
            
            if not all_maps:
                print("\n! No maps found across all sites")
                return
            
            # Display summary
            print(f"\n{'-' * 80}")
            print(f"Total Maps Found: {len(all_maps)}")
            print(f"{'-' * 80}")
            print(f"{'Site Name':<30} {'Map Name':<25} {'Type':<15} {'Image':<8}")
            print(f"{'-' * 80}")
            
            for map_item in all_maps:
                site_name = map_item['site_name'][:29]
                map_name = map_item['map_name'][:24]
                map_type = map_item['type'][:14]
                has_image = "Yes" if map_item['has_image'] else "No"
                print(f"{site_name:<30} {map_name:<25} {map_type:<15} {has_image:<8}")
            
            print(f"{'-' * 80}")
            logging.info(f"Listed {len(all_maps)} maps from {len(sites)} sites")
            
        except Exception as e:
            logging.error(f"Error listing site maps: {e}", exc_info=True)
            print(f"\n! Error listing maps: {e}")
    
    def export_site_maps(self):
        """Export maps for currently selected site to CSV/SQLite"""
        print("\n" + "-" * 80)
        print("EXPORT SITE MAPS - Current Site")
        print("-" * 80)
        
        site_id, site_name = self.get_current_site()
        if not site_id:
            return
        
        try:
            print(f"\nExporting maps for site: {site_name}")
            maps_response = mistapi.api.v1.sites.maps.listSiteMaps(
                self.apisession,
                site_id=site_id
            )
            
            if maps_response.status_code != 200:
                print(f"\n! Failed to fetch maps: HTTP {maps_response.status_code}")
                return
            
            maps = maps_response.data
            if not maps:
                print(f"\n! No maps found for site: {site_name}")
                return
            
            # Flatten and prepare data
            maps_data = []
            for map_item in maps:
                flattened = flatten_dict_recursively(map_item)
                flattened['site_id'] = site_id
                flattened['site_name'] = site_name
                flattened['org_id'] = self.org_id
                maps_data.append(flattened)
            
            # Write to dual output format
            safe_site_name = EnhancedSSHRunner.sanitize_filename(site_name)
            filename = f"SiteMaps_{safe_site_name}"
            write_data_with_format_selection(
                maps_data,
                filename,
                api_function_name='listSiteMaps'
            )
            
            print(f"\n{'-' * 80}")
            print(f"Export completed: {len(maps_data)} maps exported")
            print(f"{'-' * 80}")
            logging.info(f"Exported {len(maps_data)} maps from site {site_name}")
            
        except Exception as e:
            logging.error(f"Error exporting site maps: {e}", exc_info=True)
            print(f"\n! Error during export: {e}")
    
    def export_all_site_maps(self):
        """Export all site maps across organization to CSV/SQLite with full metadata"""
        print("\n" + "-" * 80)
        print("EXPORT ALL ORGANIZATION MAPS - All Sites")
        print("-" * 80)
        
        try:
            sites = fetch_all_sites_with_limit(self.org_id)
            if not sites:
                print("\n! No sites found in organization")
                return
            
            print(f"\nExporting maps from {len(sites)} sites...")
            all_maps_data = []
            
            for site in tqdm(sites, desc="Exporting maps", unit="site"):
                try:
                    maps_response = mistapi.api.v1.sites.maps.listSiteMaps(
                        self.apisession,
                        site_id=site['id']
                    )
                    
                    if maps_response.status_code == 200:
                        maps = maps_response.data
                        for map_item in maps:
                            # Flatten nested structures
                            flattened = flatten_dict_recursively(map_item)
                            flattened['site_id'] = site['id']
                            flattened['site_name'] = site.get('name', 'Unknown')
                            flattened['org_id'] = self.org_id
                            all_maps_data.append(flattened)
                except Exception as e:
                    logging.debug(f"Error exporting maps for site {site['id']}: {e}")
                    continue
            
            if not all_maps_data:
                print("\n! No maps found to export")
                return
            
            # Write to dual output format
            filename = "SiteMaps_Export"
            write_data_with_format_selection(
                all_maps_data,
                filename,
                api_function_name='listSiteMaps'
            )
            
            print(f"\n{'-' * 80}")
            print(f"Export completed: {len(all_maps_data)} maps exported")
            print(f"{'-' * 80}")
            logging.info(f"Exported {len(all_maps_data)} maps from {len(sites)} sites")
            
        except Exception as e:
            logging.error(f"Error exporting site maps: {e}", exc_info=True)
            print(f"\n! Error during export: {e}")
    
    def export_maps_with_images(self):
        """Export maps metadata focusing on image information"""
        print("\n" + "-" * 80)
        print("EXPORT MAPS WITH IMAGE METADATA")
        print("-" * 80)
        
        try:
            sites = fetch_all_sites_with_limit(self.org_id)
            if not sites:
                print("\n! No sites found in organization")
                return
            
            print(f"\nScanning {len(sites)} sites for maps with images...")
            maps_with_images = []
            
            for site in tqdm(sites, desc="Scanning for images", unit="site"):
                try:
                    maps_response = mistapi.api.v1.sites.maps.listSiteMaps(
                        self.apisession,
                        site_id=site['id']
                    )
                    
                    if maps_response.status_code == 200:
                        maps = maps_response.data
                        for map_item in maps:
                            if 'url' in map_item or 'thumbnail_url' in map_item:
                                flattened = flatten_dict_recursively(map_item)
                                flattened['site_id'] = site['id']
                                flattened['site_name'] = site.get('name', 'Unknown')
                                flattened['org_id'] = self.org_id
                                maps_with_images.append(flattened)
                except Exception as e:
                    logging.debug(f"Error scanning site {site['id']}: {e}")
                    continue
            
            if not maps_with_images:
                print("\n! No maps with images found")
                return
            
            filename = "SiteMaps_WithImages"
            write_data_with_format_selection(
                maps_with_images,
                filename,
                api_function_name='listSiteMaps'
            )
            
            print(f"\n{'-' * 80}")
            print(f"Export completed: {len(maps_with_images)} maps with images")
            print(f"{'-' * 80}")
            logging.info(f"Exported {len(maps_with_images)} maps with images")
            
        except Exception as e:
            logging.error(f"Error exporting maps with images: {e}", exc_info=True)
            print(f"\n! Error during export: {e}")
    
    def download_site_map_images(self):
        """Download map images to local disk"""
        print("\n" + "-" * 80)
        print("DOWNLOAD SITE MAP IMAGES")
        print("-" * 80)
        
        try:
            # Prompt for site selection
            site_id = prompt_site_selection()
            if not site_id:
                print("\n! No site selected")
                return
            
            # Get site name for display
            sites = fetch_all_sites_with_limit(self.org_id)
            site_name = next((s.get('name', 'Unknown') for s in sites if s['id'] == site_id), 'Unknown')
            
            print(f"\nFetching maps for site: {site_name}")
            maps_response = mistapi.api.v1.sites.maps.listSiteMaps(
                self.apisession,
                site_id=site_id
            )
            
            if maps_response.status_code != 200:
                print(f"\n! Failed to fetch maps: {maps_response.status_code}")
                return
            
            maps = maps_response.data
            maps_with_images = [m for m in maps if 'url' in m]
            
            if not maps_with_images:
                print(f"\n! No maps with images found for site: {site_name}")
                return
            
            print(f"\nFound {len(maps_with_images)} maps with images")
            
            # Create download directory
            import os
            download_dir = os.path.join("data", "map_images", EnhancedSSHRunner.sanitize_filename(site_name))
            os.makedirs(download_dir, exist_ok=True)
            
            print(f"Downloading to: {download_dir}")
            
            import requests
            downloaded = 0
            
            for map_item in tqdm(maps_with_images, desc="Downloading", unit="image"):
                try:
                    map_name = map_item.get('name', 'unnamed')
                    map_id = map_item.get('id', 'unknown')
                    image_url = map_item.get('url')
                    
                    if not image_url:
                        continue
                    
                    # Determine file extension from URL or default to .png
                    file_ext = '.png'
                    if '.' in image_url:
                        url_ext = image_url.rsplit('.', 1)[-1].split('?')[0]
                        if url_ext.lower() in ['png', 'jpg', 'jpeg', 'gif', 'svg']:
                            file_ext = f'.{url_ext.lower()}'
                    
                    filename = f"{EnhancedSSHRunner.sanitize_filename(map_name)}_{map_id[:8]}{file_ext}"
                    filepath = os.path.join(download_dir, filename)
                    
                    response = requests.get(image_url, timeout=30)
                    if response.status_code == 200:
                        with open(filepath, 'wb') as f:
                            f.write(response.content)
                        downloaded += 1
                    else:
                        logging.warning(f"Failed to download {map_name}: HTTP {response.status_code}")
                        
                except Exception as e:
                    logging.error(f"Error downloading map image {map_item.get('id')}: {e}")
                    continue
            
            print(f"\n{'-' * 80}")
            print(f"Downloaded {downloaded} of {len(maps_with_images)} images")
            print(f"Location: {download_dir}")
            print(f"{'-' * 80}")
            logging.info(f"Downloaded {downloaded} map images to {download_dir}")
            
        except Exception as e:
            logging.error(f"Error downloading map images: {e}", exc_info=True)
            print(f"\n! Error downloading images: {e}")
    
    def view_map_details(self):
        """View detailed information for a specific map"""
        print("\n" + "-" * 80)
        print("VIEW MAP DETAILS")
        print("-" * 80)
        
        site_id, site_name = self.get_current_site()
        if not site_id:
            return
        
        try:
            
            # Fetch maps for the site
            maps_response = mistapi.api.v1.sites.maps.listSiteMaps(
                self.apisession,
                site_id=site_id
            )
            
            if maps_response.status_code != 200:
                print(f"\n! Failed to fetch maps: {maps_response.status_code}")
                return
            
            maps = maps_response.data
            if not maps:
                print(f"\n! No maps found for site: {site_name}")
                return
            
            # Display map selection
            print(f"\nMaps for site: {site_name}")
            print(f"{'-' * 80}")
            for idx, map_item in enumerate(maps, 1):
                map_name = map_item.get('name', 'Unnamed')
                map_type = map_item.get('type', 'N/A')
                print(f"  {idx}. {map_name} ({map_type})")
            print(f"{'-' * 80}")
            
            try:
                selection = input("\nSelect map number (or 0 to cancel): ").strip()
                map_idx = int(selection) - 1
                
                if map_idx < 0 or map_idx >= len(maps):
                    print("\n! Invalid selection")
                    return
                
                selected_map = maps[map_idx]
                map_id = selected_map.get('id')
                
                # Fetch detailed map info
                detail_response = mistapi.api.v1.sites.maps.getSiteMap(
                    self.apisession,
                    site_id=site_id,
                    map_id=map_id
                )
                
                if detail_response.status_code != 200:
                    print(f"\n! Failed to fetch map details: {detail_response.status_code}")
                    return
                
                map_details = detail_response.data
                
                # Display details
                print(f"\n{'-' * 80}")
                print(f"MAP DETAILS: {map_details.get('name', 'Unnamed')}")
                print(f"{'-' * 80}")
                print(f"Map ID: {map_details.get('id', 'N/A')}")
                print(f"Type: {map_details.get('type', 'N/A')}")
                print(f"Width: {map_details.get('width', 0)} pixels")
                print(f"Height: {map_details.get('height', 0)} pixels")
                print(f"PPM (Pixels per meter): {map_details.get('ppm', 'N/A')}")
                print(f"Orientation: {map_details.get('orientation', 0)} degrees")
                print(f"Has Image: {'Yes' if 'url' in map_details else 'No'}")
                
                if 'url' in map_details:
                    print(f"Image URL: {map_details['url'][:80]}...")
                
                if 'latlng' in map_details:
                    latlng = map_details['latlng']
                    print(f"Coordinates: {latlng.get('lat')}, {latlng.get('lng')}")
                
                if 'wayfinding' in map_details:
                    print(f"Wayfinding Enabled: Yes")
                
                print(f"{'-' * 80}")
                logging.info(f"Viewed details for map {map_id}")
                
            except ValueError:
                print("\n! Invalid input - please enter a number")
            except EOFError:
                logging.info("EOF detected during map selection")
                return
                
        except Exception as e:
            logging.error(f"Error viewing map details: {e}", exc_info=True)
            print(f"\n! Error viewing map details: {e}")
    
    def create_site_map(self):
        """Create a new site map with basic configuration"""
        print("\n" + "-" * 80)
        print("CREATE NEW SITE MAP")
        print("-" * 80)
        print("\n! Note: This creates a map placeholder. Upload image separately (Menu 7)")
        
        site_id, site_name = self.get_current_site()
        if not site_id:
            return
        
        try:
            print(f"\nCreating map for site: {site_name}")
            print(f"{'-' * 80}")
            
            # Gather map configuration
            try:
                map_name = input("Enter map name: ").strip()
                if not map_name:
                    print("\n! Map name is required")
                    return
                
                print("\nMap type options:")
                print("  1. image (standard floor plan)")
                print("  2. google (Google Maps integration)")
                print("  3. baidu (Baidu Maps integration)")
                map_type_choice = input("Select type (1-3, default=1): ").strip() or "1"
                
                type_map = {"1": "image", "2": "google", "3": "baidu"}
                map_type = type_map.get(map_type_choice, "image")
                
                # Optional: dimensions (only for image type)
                width = None
                height = None
                ppm = None
                
                if map_type == "image":
                    width_input = input("Enter width in pixels (default=1024): ").strip()
                    height_input = input("Enter height in pixels (default=768): ").strip()
                    ppm_input = input("Enter pixels per meter (default=10): ").strip()
                    
                    width = int(width_input) if width_input else 1024
                    height = int(height_input) if height_input else 768
                    ppm = float(ppm_input) if ppm_input else 10.0
                
                # Build map payload
                map_payload = {
                    "name": map_name,
                    "type": map_type
                }
                
                if width:
                    map_payload["width"] = width
                if height:
                    map_payload["height"] = height
                if ppm:
                    map_payload["ppm"] = ppm
                
                # Create the map
                print(f"\nCreating map '{map_name}'...")
                create_response = mistapi.api.v1.sites.maps.createSiteMap(
                    self.apisession,
                    site_id=site_id,
                    body=map_payload
                )
                
                if create_response.status_code in [200, 201]:
                    created_map = create_response.data
                    print(f"\n{'-' * 80}")
                    print(f"Map created successfully!")
                    print(f"Map ID: {created_map.get('id')}")
                    print(f"Name: {created_map.get('name')}")
                    print(f"Type: {created_map.get('type')}")
                    print(f"{'-' * 80}")
                    logging.info(f"Created map {created_map.get('id')} for site {site_id}")
                else:
                    print(f"\n! Failed to create map: HTTP {create_response.status_code}")
                    logging.error(f"Map creation failed: {create_response.status_code} - {create_response.data}")
                    
            except ValueError as ve:
                print(f"\n! Invalid input: {ve}")
            except EOFError:
                logging.info("EOF detected during map creation")
                return
                
        except Exception as e:
            logging.error(f"Error creating site map: {e}", exc_info=True)
            print(f"\n! Error creating map: {e}")
    
    def clone_map(self):
        """Clone/duplicate an existing map at the current site including image, walls, paths, and zones"""
        logging.info("clone_map operation initiated")
        print("\n" + "-" * 80)
        print("CLONE/DUPLICATE MAP")
        print("-" * 80)
        print("! This will clone ALL map data: image, walls, paths, zones, wayfinding, etc.")
        
        site_id, site_name = self.get_current_site()
        if not site_id:
            logging.warning("clone_map aborted: No site selected")
            return
        
        logging.debug(f"clone_map - Site: {site_name} (ID: {site_id})")
        
        try:
            import os
            import requests
            import tempfile
            
            # Get source map selection
            print("\nSelect the map to clone:")
            source_map_id = self._select_map_from_site(site_id, site_name)
            if not source_map_id:
                logging.info("clone_map aborted: No source map selected")
                return
            
            logging.info(f"Cloning map - source_map_id: {source_map_id}")
            
            # Fetch complete source map details
            print("\nFetching source map details...")
            logging.debug(f"Calling getSiteMap API - site_id: {site_id}, map_id: {source_map_id}")
            source_response = mistapi.api.v1.sites.maps.getSiteMap(
                self.apisession,
                site_id=site_id,
                map_id=source_map_id
            )
            
            logging.debug(f"getSiteMap response: HTTP {source_response.status_code}")
            if source_response.status_code != 200:
                logging.error(f"Failed to fetch source map - HTTP {source_response.status_code}")
                print(f"\n! Failed to fetch source map: HTTP {source_response.status_code}")
                return
            
            source_map = source_response.data
            
            # Display source map info with all cloneable attributes
            print(f"\n{'-' * 80}")
            print(f"Source Map: {source_map.get('name', 'Unnamed')}")
            print(f"Type: {source_map.get('type', 'N/A')}")
            print(f"Dimensions: {source_map.get('width', 'N/A')}x{source_map.get('height', 'N/A')}")
            print(f"PPM: {source_map.get('ppm', 'N/A')}")
            print(f"Has Image: {'Yes' if 'url' in source_map else 'No'}")
            print(f"Has Walls: {'Yes' if 'wall_path' in source_map else 'No'}")
            print(f"Has Wayfinding: {'Yes' if 'wayfinding_path' in source_map else 'No'}")
            print(f"{'-' * 80}")
            
            # Prompt for new map name
            default_name = f"{source_map.get('name', 'Map')} (Copy)"
            new_name = input(f"\nEnter name for cloned map [{default_name}]: ").strip()
            if not new_name:
                new_name = default_name
            
            logging.info(f"Creating clone with new name: {new_name}")
            
            # Build complete clone payload - copy ALL relevant properties
            clone_payload = {
                "name": new_name,
                "type": source_map.get('type', 'image')
            }
            logging.debug(f"Base clone payload: {clone_payload}")
            
            # Copy dimensional properties
            if 'width' in source_map:
                clone_payload['width'] = source_map['width']
            if 'height' in source_map:
                clone_payload['height'] = source_map['height']
            if 'height_m' in source_map:
                clone_payload['height_m'] = source_map['height_m']
            if 'ppm' in source_map:
                clone_payload['ppm'] = source_map['ppm']
            if 'orientation' in source_map:
                clone_payload['orientation'] = source_map['orientation']
            
            # Copy location data
            if 'latlng' in source_map:
                clone_payload['latlng'] = source_map['latlng']
            if 'latlng_br' in source_map:
                clone_payload['latlng_br'] = source_map['latlng_br']
            if 'origin_x' in source_map:
                clone_payload['origin_x'] = source_map['origin_x']
            if 'origin_y' in source_map:
                clone_payload['origin_y'] = source_map['origin_y']
            
            # Copy wayfinding configuration
            if 'wayfinding' in source_map:
                clone_payload['wayfinding'] = source_map['wayfinding']
            if 'wayfinding_path' in source_map:
                clone_payload['wayfinding_path'] = source_map['wayfinding_path']
            
            # Copy wall paths (critical for RF modeling)
            if 'wall_path' in source_map:
                clone_payload['wall_path'] = source_map['wall_path']
            
            # Copy site survey paths
            if 'sitesurvey_path' in source_map:
                clone_payload['sitesurvey_path'] = source_map['sitesurvey_path']
            
            # Copy other map-specific settings
            if 'occupancy_limit' in source_map:
                clone_payload['occupancy_limit'] = source_map['occupancy_limit']
            if 'locked' in source_map:
                clone_payload['locked'] = source_map['locked']
            if 'view' in source_map:
                clone_payload['view'] = source_map['view']
            
            # Display clone plan
            print(f"\n{'-' * 80}")
            print("Clone Plan:")
            print(f"  New name: {new_name}")
            print(f"  Will copy: dimensions, orientation, location data, wayfinding, walls")
            print(f"  Image: {'Yes - will download and re-upload' if 'url' in source_map else 'No image to copy'}")
            print(f"{'-' * 80}")
            
            confirm = input("\nProceed with full clone? (yes/no): ").strip().lower()
            if confirm not in ['yes', 'y']:
                print("\n! Clone cancelled")
                return
            
            # Download image to temporary file if present
            image_temp_path = None
            if 'url' in source_map:
                try:
                    print("\nDownloading map image...")
                    image_url = source_map['url']
                    
                    # Determine file extension
                    file_ext = '.png'
                    if '.' in image_url:
                        url_ext = image_url.rsplit('.', 1)[-1].split('?')[0]
                        if url_ext.lower() in ['png', 'jpg', 'jpeg', 'gif', 'svg']:
                            file_ext = f'.{url_ext.lower()}'
                    
                    # Create temporary file
                    temp_fd, image_temp_path = tempfile.mkstemp(suffix=file_ext)
                    os.close(temp_fd)
                    
                    # Download image
                    response = requests.get(image_url, timeout=60)
                    if response.status_code == 200:
                        with open(image_temp_path, 'wb') as f:
                            f.write(response.content)
                        print(f"Downloaded image ({len(response.content) / 1024:.1f} KB)")
                    else:
                        print(f"! Warning: Failed to download image (HTTP {response.status_code})")
                        if image_temp_path and os.path.exists(image_temp_path):
                            os.remove(image_temp_path)
                        image_temp_path = None
                        
                except Exception as e:
                    logging.error(f"Error downloading map image: {e}")
                    print(f"! Warning: Could not download image: {e}")
                    if image_temp_path and os.path.exists(image_temp_path):
                        os.remove(image_temp_path)
                    image_temp_path = None
            
            # Create the cloned map
            print("\nCreating cloned map...")
            clone_response = mistapi.api.v1.sites.maps.createSiteMap(
                self.apisession,
                site_id=site_id,
                body=clone_payload
            )
            
            if clone_response.status_code not in [200, 201]:
                print(f"\n! Failed to clone map: HTTP {clone_response.status_code}")
                logging.error(f"Map clone failed: {clone_response.status_code} - {clone_response.data}")
                # Clean up temp file
                if image_temp_path and os.path.exists(image_temp_path):
                    os.remove(image_temp_path)
                return
            
            cloned_map = clone_response.data
            cloned_map_id = cloned_map.get('id')
            
            print(f"\n{'-' * 80}")
            print("Map structure cloned successfully!")
            print(f"Cloned Map ID: {cloned_map_id}")
            print(f"Name: {cloned_map.get('name')}")
            print(f"{'-' * 80}")
            
            # Upload image to cloned map if we have one
            if image_temp_path and os.path.exists(image_temp_path):
                try:
                    print("\nUploading image to cloned map...")
                    upload_response = mistapi.api.v1.sites.maps.addSiteMapImageFile(
                        self.apisession,
                        site_id=site_id,
                        map_id=cloned_map_id,
                        file_path=image_temp_path
                    )
                    
                    if upload_response.status_code in [200, 201]:
                        print("Image uploaded successfully!")
                        logging.info(f"Image uploaded to cloned map {cloned_map_id}")
                    else:
                        print(f"! Warning: Failed to upload image: HTTP {upload_response.status_code}")
                        logging.error(f"Image upload to cloned map failed: {upload_response.status_code}")
                        
                except Exception as e:
                    logging.error(f"Error uploading image to cloned map: {e}")
                    print(f"! Warning: Could not upload image to cloned map: {e}")
                finally:
                    # Clean up temporary file
                    if os.path.exists(image_temp_path):
                        os.remove(image_temp_path)
            
            # Display final summary
            print(f"\n{'-' * 80}")
            print("CLONE COMPLETE")
            print(f"{'-' * 80}")
            print(f"Original Map: {source_map.get('name')}")
            print(f"Cloned Map: {new_name}")
            print(f"Cloned Map ID: {cloned_map_id}")
            print("\nCloned elements:")
            print(f"  -> Dimensions: {clone_payload.get('width', 'N/A')}x{clone_payload.get('height', 'N/A')}")
            print(f"  -> PPM: {clone_payload.get('ppm', 'N/A')}")
            print(f"  -> Walls: {'Yes' if 'wall_path' in clone_payload else 'No'}")
            print(f"  -> Wayfinding: {'Yes' if 'wayfinding_path' in clone_payload else 'No'}")
            print(f"  -> Image: {'Yes' if image_temp_path else 'No'}")
            print(f"{'-' * 80}")
            print("\n! Note: Zones are site-level objects, not map objects.")
            print("! If you need to clone zones, use the Zones API separately.")
            
            logging.info(f"Successfully cloned map {source_map_id} to {cloned_map_id} at site {site_id}")
                
        except EOFError:
            logging.info("EOF detected during map clone")
            return
        except Exception as e:
            logging.error(f"Error cloning map: {e}", exc_info=True)
            print(f"\n! Error cloning map: {e}")
    
    def maps_without_images_report(self):
        """Generate report of maps that don't have uploaded images"""
        print("\n" + "-" * 80)
        print("MAPS WITHOUT IMAGES REPORT")
        print("-" * 80)
        
        try:
            sites = fetch_all_sites_with_limit(self.org_id)
            if not sites:
                print("\n! No sites found in organization")
                return
            
            print(f"\nScanning {len(sites)} sites for maps without images...")
            maps_without_images = []
            total_maps_scanned = 0
            
            for site in tqdm(sites, desc="Scanning sites", unit="site"):
                try:
                    maps_response = mistapi.api.v1.sites.maps.listSiteMaps(
                        self.apisession,
                        site_id=site['id']
                    )
                    
                    if maps_response.status_code == 200:
                        maps = maps_response.data
                        total_maps_scanned += len(maps)
                        for map_item in maps:
                            if 'url' not in map_item:
                                maps_without_images.append({
                                    'site_id': site['id'],
                                    'site_name': site.get('name', 'Unknown'),
                                    'map_id': map_item.get('id'),
                                    'map_name': map_item.get('name', 'Unnamed'),
                                    'type': map_item.get('type', 'N/A'),
                                    'width': map_item.get('width', 0),
                                    'height': map_item.get('height', 0),
                                    'org_id': self.org_id
                                })
                except Exception as e:
                    logging.debug(f"Error scanning site {site['id']}: {e}")
                    continue
            
            print(f"\nTotal maps scanned: {total_maps_scanned}")
            
            if not maps_without_images:
                print("\n" + "-" * 80)
                print(f"All {total_maps_scanned} maps have images uploaded!")
                print("-" * 80)
                return
            
            # Display report
            print(f"\n{'-' * 80}")
            print(f"MAPS WITHOUT IMAGES: {len(maps_without_images)} found")
            print(f"{'-' * 80}")
            print(f"{'Site Name':<30} {'Map Name':<30} {'Type':<15}")
            print(f"{'-' * 80}")
            
            for map_item in maps_without_images:
                site_name = map_item['site_name'][:29]
                map_name = map_item['map_name'][:29]
                map_type = map_item['type'][:14]
                print(f"{site_name:<30} {map_name:<30} {map_type:<15}")
            
            print(f"{'-' * 80}")
            
            # Export to CSV/SQLite
            filename = "MapsWithoutImages_Report"
            write_data_with_format_selection(
                maps_without_images,
                filename,
                api_function_name='listSiteMaps'
            )
            
            logging.info(f"Generated report: {len(maps_without_images)} maps without images")
            
        except Exception as e:
            logging.error(f"Error generating maps report: {e}", exc_info=True)
            print(f"\n! Error generating report: {e}")
    
    # Placeholder methods for future implementation
    def update_map_properties(self):
        """Update existing map properties (name, dimensions, orientation, etc.)"""
        print("\n" + "-" * 80)
        print("UPDATE MAP PROPERTIES")
        print("-" * 80)
        
        site_id, site_name = self.get_current_site()
        if not site_id:
            return
        
        try:
            # Get map selection
            map_id = self._select_map_from_site(site_id, site_name)
            if not map_id:
                return
            
            # Fetch current map details
            map_response = mistapi.api.v1.sites.maps.getSiteMap(
                self.apisession,
                site_id=site_id,
                map_id=map_id
            )
            
            if map_response.status_code != 200:
                print(f"\n! Failed to fetch map details: HTTP {map_response.status_code}")
                return
            
            current_map = map_response.data
            
            # Display current properties
            print(f"\nCurrent Map Properties:")
            print(f"{'-' * 80}")
            print(f"Name: {current_map.get('name', 'N/A')}")
            print(f"Type: {current_map.get('type', 'N/A')}")
            print(f"Width: {current_map.get('width', 'N/A')} pixels")
            print(f"Height: {current_map.get('height', 'N/A')} pixels")
            print(f"PPM (Pixels per meter): {current_map.get('ppm', 'N/A')}")
            print(f"Orientation: {current_map.get('orientation', 0)} degrees")
            print(f"{'-' * 80}")
            
            # Build update payload
            update_payload = {}
            
            print("\nEnter new values (press Enter to keep current value):")
            
            # Map name
            new_name = input(f"Map name [{current_map.get('name', '')}]: ").strip()
            if new_name:
                update_payload['name'] = new_name
            
            # Width
            new_width = input(f"Width in pixels [{current_map.get('width', '')}]: ").strip()
            if new_width:
                try:
                    update_payload['width'] = int(new_width)
                except ValueError:
                    print("! Invalid width, skipping")
            
            # Height
            new_height = input(f"Height in pixels [{current_map.get('height', '')}]: ").strip()
            if new_height:
                try:
                    update_payload['height'] = int(new_height)
                except ValueError:
                    print("! Invalid height, skipping")
            
            # PPM
            new_ppm = input(f"Pixels per meter [{current_map.get('ppm', '')}]: ").strip()
            if new_ppm:
                try:
                    update_payload['ppm'] = float(new_ppm)
                except ValueError:
                    print("! Invalid PPM, skipping")
            
            # Orientation
            new_orientation = input(f"Orientation in degrees [{current_map.get('orientation', 0)}]: ").strip()
            if new_orientation:
                try:
                    update_payload['orientation'] = int(new_orientation)
                except ValueError:
                    print("! Invalid orientation, skipping")
            
            if not update_payload:
                print("\n! No changes specified")
                return
            
            # Confirm update
            print(f"\n{'-' * 80}")
            print("Changes to apply:")
            for key, value in update_payload.items():
                print(f"  {key}: {value}")
            print(f"{'-' * 80}")
            
            confirm = input("\nApply these changes? (yes/no): ").strip().lower()
            if confirm not in ['yes', 'y']:
                print("\n! Update cancelled")
                return
            
            # Apply update
            print("\nApplying changes...")
            update_response = mistapi.api.v1.sites.maps.updateSiteMap(
                self.apisession,
                site_id=site_id,
                map_id=map_id,
                body=update_payload
            )
            
            if update_response.status_code in [200, 201]:
                print(f"\n{'-' * 80}")
                print("Map updated successfully!")
                print(f"{'-' * 80}")
                logging.info(f"Updated map {map_id} for site {site_id}")
            else:
                print(f"\n! Failed to update map: HTTP {update_response.status_code}")
                logging.error(f"Map update failed: {update_response.status_code} - {update_response.data}")
                
        except EOFError:
            logging.info("EOF detected during map update")
            return
        except Exception as e:
            logging.error(f"Error updating map properties: {e}", exc_info=True)
            print(f"\n! Error updating map: {e}")
    
    def delete_site_map(self):
        """Delete a site map with confirmation"""
        print("\n" + "-" * 80)
        print("DELETE SITE MAP")
        print("-" * 80)
        print("\n! WARNING: This action cannot be undone!")
        
        site_id, site_name = self.get_current_site()
        if not site_id:
            return
        
        try:
            # Get map selection
            map_id = self._select_map_from_site(site_id, site_name)
            if not map_id:
                return
            
            # Fetch current map details for display
            map_response = mistapi.api.v1.sites.maps.getSiteMap(
                self.apisession,
                site_id=site_id,
                map_id=map_id
            )
            
            if map_response.status_code != 200:
                print(f"\n! Failed to fetch map details: HTTP {map_response.status_code}")
                return
            
            current_map = map_response.data
            
            # Display map details before deletion
            print(f"\n{'-' * 80}")
            print("Map to be deleted:")
            print(f"  Name: {current_map.get('name', 'N/A')}")
            print(f"  Type: {current_map.get('type', 'N/A')}")
            print(f"  ID: {map_id}")
            print(f"{'-' * 80}")
            
            # Safety confirmation
            print("\nType 'DELETE' in uppercase to confirm deletion:")
            confirmation = input("Confirmation: ").strip()
            
            if confirmation != "DELETE":
                print("\n! Deletion cancelled")
                logging.info(f"Map deletion cancelled by user for map {map_id}")
                return
            
            # Perform deletion
            print("\nDeleting map...")
            delete_response = mistapi.api.v1.sites.maps.deleteSiteMap(
                self.apisession,
                site_id=site_id,
                map_id=map_id
            )
            
            if delete_response.status_code in [200, 204]:
                print(f"\n{'-' * 80}")
                print("Map deleted successfully!")
                print(f"{'-' * 80}")
                logging.info(f"Deleted map {map_id} from site {site_id}")
            else:
                print(f"\n! Failed to delete map: HTTP {delete_response.status_code}")
                logging.error(f"Map deletion failed: {delete_response.status_code} - {delete_response.data}")
                
        except EOFError:
            logging.info("EOF detected during map deletion")
            return
        except Exception as e:
            logging.error(f"Error deleting site map: {e}", exc_info=True)
            print(f"\n! Error deleting map: {e}")
    
    def upload_map_image(self):
        """Upload or replace map image file (multipart upload)"""
        logging.info("upload_map_image operation initiated")
        print("\n" + "-" * 80)
        print("UPLOAD/REPLACE MAP IMAGE")
        print("-" * 80)
        
        site_id, site_name = self.get_current_site()
        if not site_id:
            logging.warning("upload_map_image aborted: No site selected")
            return
        
        logging.debug(f"upload_map_image - Site: {site_name} (ID: {site_id})")
        
        try:
            # Get map selection
            map_id = self._select_map_from_site(site_id, site_name)
            if not map_id:
                return
            
            # Prompt for image file path
            import os
            print("\nEnter the path to the image file:")
            print("Supported formats: PNG, JPG, JPEG, GIF, SVG")
            file_path = input("File path: ").strip()
            
            # Remove quotes if user pasted path with quotes
            file_path = file_path.strip('"').strip("'")
            
            if not file_path:
                print("\n! No file path provided")
                return
            
            if not os.path.exists(file_path):
                print(f"\n! File not found: {file_path}")
                return
            
            if not os.path.isfile(file_path):
                print(f"\n! Path is not a file: {file_path}")
                return
            
            # Validate file extension
            valid_extensions = ['.png', '.jpg', '.jpeg', '.gif', '.svg']
            file_ext = os.path.splitext(file_path)[1].lower()
            if file_ext not in valid_extensions:
                print(f"\n! Invalid file type: {file_ext}")
                print(f"Supported types: {', '.join(valid_extensions)}")
                return
            
            # Check file size (warn if > 10MB)
            file_size = os.path.getsize(file_path)
            file_size_mb = file_size / (1024 * 1024)
            if file_size_mb > 10:
                print(f"\n! Warning: File size is {file_size_mb:.2f}MB")
                confirm = input("Continue with upload? (yes/no): ").strip().lower()
                if confirm not in ['yes', 'y']:
                    print("\n! Upload cancelled")
                    return
            
            print(f"\nFile: {os.path.basename(file_path)}")
            print(f"Size: {file_size_mb:.2f}MB")
            
            # Confirm upload
            confirm = input("\nUpload this image to the selected map? (yes/no): ").strip().lower()
            if confirm not in ['yes', 'y']:
                print("\n! Upload cancelled")
                return
            
            # Perform upload using mistapi
            print("\nUploading image...")
            
            # Use mistapi's addSiteMapImageFile method
            with open(file_path, 'rb') as image_file:
                upload_response = mistapi.api.v1.sites.maps.addSiteMapImageFile(
                    self.apisession,
                    site_id=site_id,
                    map_id=map_id,
                    file_path=file_path
                )
            
            if upload_response.status_code in [200, 201]:
                print(f"\n{'-' * 80}")
                print("Image uploaded successfully!")
                print(f"{'-' * 80}")
                logging.info(f"Uploaded image to map {map_id} for site {site_id}")
            else:
                print(f"\n! Failed to upload image: HTTP {upload_response.status_code}")
                logging.error(f"Image upload failed: {upload_response.status_code} - {upload_response.data}")
                
        except EOFError:
            logging.info("EOF detected during image upload")
            return
        except Exception as e:
            logging.error(f"Error uploading map image: {e}", exc_info=True)
            print(f"\n! Error uploading image: {e}")
    
    def view_devices_on_map(self):
        """Display all devices placed on a specific map"""
        print("\n" + "-" * 80)
        print("VIEW DEVICES ON MAP")
        print("-" * 80)
        
        site_id, site_name = self.get_current_site()
        if not site_id:
            return
        
        try:
            # Get map selection
            map_id = self._select_map_from_site(site_id, site_name)
            if not map_id:
                return
            
            # Fetch devices for the site
            print(f"\nFetching devices for site: {site_name}")
            devices_response = mistapi.api.v1.sites.devices.listSiteDevices(
                self.apisession,
                site_id=site_id,
                type="all"
            )
            
            if devices_response.status_code != 200:
                print(f"\n! Failed to fetch devices: HTTP {devices_response.status_code}")
                return
            
            all_devices = devices_response.data
            
            # Filter devices that are on this specific map
            devices_on_map = []
            for device in all_devices:
                if device.get('map_id') == map_id:
                    devices_on_map.append(device)
            
            if not devices_on_map:
                print(f"\n! No devices placed on this map")
                return
            
            # Display devices
            print(f"\n{'-' * 80}")
            print(f"Devices on Map: {len(devices_on_map)} found")
            print(f"{'-' * 80}")
            print(f"{'Device Name':<30} {'Type':<10} {'Model':<20} {'X,Y Coordinates':<20}")
            print(f"{'-' * 80}")
            
            for device in devices_on_map:
                device_name = device.get('name', 'Unnamed')[:29]
                device_type = device.get('type', 'N/A')[:9]
                device_model = device.get('model', 'N/A')[:19]
                x_coord = device.get('x', 'N/A')
                y_coord = device.get('y', 'N/A')
                coordinates = f"{x_coord},{y_coord}"
                print(f"{device_name:<30} {device_type:<10} {device_model:<20} {coordinates:<20}")
            
            print(f"{'-' * 80}")
            
            # Optional: Export to CSV
            export_choice = input("\nExport to CSV? (yes/no): ").strip().lower()
            if export_choice in ['yes', 'y']:
                devices_data = []
                for device in devices_on_map:
                    flattened = flatten_dict_recursively(device)
                    flattened['site_id'] = site_id
                    flattened['site_name'] = site_name
                    devices_data.append(flattened)
                
                filename = f"MapDevices_{EnhancedSSHRunner.sanitize_filename(site_name)}"
                write_data_with_format_selection(
                    devices_data,
                    filename,
                    api_function_name='listSiteDevices'
                )
                print(f"\n   Exported {len(devices_data)} devices")
            
            logging.info(f"Viewed {len(devices_on_map)} devices on map {map_id}")
            
        except EOFError:
            logging.info("EOF detected during view devices")
            return
        except Exception as e:
            logging.error(f"Error viewing devices on map: {e}", exc_info=True)
            print(f"\n! Error viewing devices: {e}")
    
    def auto_place_aps(self):
        """Automatically place APs on map using Mist auto-placement"""
        print("\n! Feature coming soon: Auto-place APs")
        logging.info("auto_place_aps called (placeholder)")
    
    def auto_orient_aps(self):
        """Automatically orient APs on map"""
        print("\n! Feature coming soon: Auto-orient APs")
        logging.info("auto_orient_aps called (placeholder)")
    
    def set_device_location(self):
        """Manually set AP/device coordinates on map"""
        print("\n! Feature coming soon: Set device location")
        logging.info("set_device_location called (placeholder)")
    
    def bulk_download_org_images(self):
        """Download all map images across entire organization"""
        print("\n" + "-" * 80)
        print("BULK DOWNLOAD ORG MAP IMAGES")
        print("-" * 80)
        
        try:
            sites = fetch_all_sites_with_limit(self.org_id)
            if not sites:
                print("\n! No sites found in organization")
                return
            
            print(f"\nScanning {len(sites)} sites for maps with images...")
            
            import os
            import requests
            
            # Create base download directory
            base_dir = os.path.join("data", "map_images_org_backup")
            os.makedirs(base_dir, exist_ok=True)
            
            total_maps = 0
            total_downloaded = 0
            
            for site in tqdm(sites, desc="Processing sites", unit="site"):
                try:
                    site_id = site['id']
                    site_name = site.get('name', 'Unknown')
                    
                    maps_response = mistapi.api.v1.sites.maps.listSiteMaps(
                        self.apisession,
                        site_id=site_id
                    )
                    
                    if maps_response.status_code != 200:
                        continue
                    
                    maps = maps_response.data
                    maps_with_images = [m for m in maps if 'url' in m]
                    
                    if not maps_with_images:
                        continue
                    
                    # Create site-specific directory
                    site_dir = os.path.join(base_dir, EnhancedSSHRunner.sanitize_filename(site_name))
                    os.makedirs(site_dir, exist_ok=True)
                    
                    for map_item in maps_with_images:
                        total_maps += 1
                        try:
                            map_name = map_item.get('name', 'unnamed')
                            map_id = map_item.get('id', 'unknown')
                            image_url = map_item.get('url')
                            
                            if not image_url:
                                continue
                            
                            # Determine file extension
                            file_ext = '.png'
                            if '.' in image_url:
                                url_ext = image_url.rsplit('.', 1)[-1].split('?')[0]
                                if url_ext.lower() in ['png', 'jpg', 'jpeg', 'gif', 'svg']:
                                    file_ext = f'.{url_ext.lower()}'
                            
                            filename = f"{EnhancedSSHRunner.sanitize_filename(map_name)}_{map_id[:8]}{file_ext}"
                            filepath = os.path.join(site_dir, filename)
                            
                            # Skip if already downloaded
                            if os.path.exists(filepath):
                                total_downloaded += 1
                                continue
                            
                            response = requests.get(image_url, timeout=30)
                            if response.status_code == 200:
                                with open(filepath, 'wb') as f:
                                    f.write(response.content)
                                total_downloaded += 1
                            else:
                                logging.warning(f"Failed to download {site_name}/{map_name}: HTTP {response.status_code}")
                                
                        except Exception as e:
                            logging.error(f"Error downloading map image {map_item.get('id')}: {e}")
                            continue
                            
                except Exception as e:
                    logging.debug(f"Error processing site {site['id']}: {e}")
                    continue
            
            print(f"\n{'-' * 80}")
            print(f"Download completed!")
            print(f"Total maps found: {total_maps}")
            print(f"Successfully downloaded: {total_downloaded}")
            print(f"Location: {base_dir}")
            print(f"{'-' * 80}")
            logging.info(f"Bulk downloaded {total_downloaded} of {total_maps} map images to {base_dir}")
            
        except Exception as e:
            logging.error(f"Error bulk downloading map images: {e}", exc_info=True)
            print(f"\n! Error during bulk download: {e}")
    
    def backup_all_maps(self):
        """Complete backup of all maps (metadata + images)"""
        print("\n! Feature coming soon: Backup all maps")
        logging.info("backup_all_maps called (placeholder)")
    
    def map_coverage_analytics(self):
        """Analyze RF coverage patterns by map"""
        print("\n! Feature coming soon: Map coverage analytics")
        logging.info("map_coverage_analytics called (placeholder)")
    
    def device_density_analytics(self):
        """Analyze device density and distribution by map"""
        print("\n! Feature coming soon: Device density analytics")
        logging.info("device_density_analytics called (placeholder)")
    
    def map_usage_statistics(self):
        """Generate usage statistics for maps"""
        print("\n! Feature coming soon: Map usage statistics")
        logging.info("map_usage_statistics called (placeholder)")
    
    def interactive_map_viewer(self):
        """
        Interactive map viewer with Plotly/Dash for viewing and editing:
        - Floor plan image display
        - Toggleable overlays: walls, zones, wayfinding paths
        - Device visualization: APs, switches, gateways with orientation indicators
        - Click-to-edit device locations
        - Save changes back to Mist Cloud
        """
        logging.info("Interactive map viewer initiated")
        print("\n" + "-" * 80)
        print("INTERACTIVE MAP VIEWER")
        print("-" * 80)
        
        site_id, site_name = self.get_current_site()
        if not site_id:
            logging.warning("Interactive map viewer aborted: No site selected")
            return
        
        logging.debug(f"Interactive map viewer - Site: {site_name} (ID: {site_id})")
        
        try:
            # Explicitly check and install required visualization packages
            print("\nChecking visualization dependencies...")
            logging.info("Starting visualization dependency check")
            required_packages = {'plotly': 'plotly>=5.14.0', 'dash': 'dash>=2.9.0'}
            optional_viz_packages = {'kaleido': 'kaleido>=0.2.1', 'matplotlib': 'matplotlib>=3.5.0'}
            
            # Trigger installation check through global import_manager instance
            # Access the global import_manager variable created at module initialization
            global import_manager
            for package_name, package_spec in required_packages.items():
                logging.debug(f"Checking required package: {package_name} ({package_spec})")
                import_manager.import_module_safely(
                    package_name, 
                    package_spec=package_spec,
                    required=False,  # Don't fail if can't install
                    skip_deps=False,  # Allow installation
                    skip_upgrade=True  # Don't check for upgrades
                )
                logging.debug(f"Package {package_name} check completed")
            
            # Optional packages (best-effort)
            for package_name, package_spec in optional_viz_packages.items():
                try:
                    logging.debug(f"Checking optional package: {package_name} ({package_spec})")
                    import_manager.import_module_safely(
                        package_name,
                        package_spec=package_spec,
                        required=False,
                        skip_deps=False,
                        skip_upgrade=True
                    )
                    logging.debug(f"Optional package {package_name} installed/verified")
                except Exception as e:
                    logging.debug(f"Optional package {package_name} unavailable: {e}")
                    pass  # Optional - continue without
            
            # Now attempt imports
            try:
                logging.debug("Attempting to import plotly modules")
                import plotly.graph_objects as go
                from plotly.subplots import make_subplots
                import plotly.express as px
                logging.info("Successfully imported plotly modules")
            except ImportError as e:
                logging.error(f"Failed to import plotly: {e}", exc_info=True)
                print(f"\n! Missing required package: {e}")
                print("! Install with: pip install plotly dash")
                confirm = input("\nWould you like to continue without interactive features? (yes/no): ").strip().lower()
                if confirm not in ['yes', 'y']:
                    logging.info("User declined matplotlib fallback")
                    return
                # Fallback to basic matplotlib if available
                try:
                    logging.debug("Attempting matplotlib fallback import")
                    import matplotlib.pyplot as plt
                    import matplotlib.patches as patches
                    from matplotlib.patches import FancyArrow
                    print("\n! Using matplotlib fallback (view-only mode)")
                    logging.info("Successfully imported matplotlib for fallback mode")
                    use_plotly = False
                except ImportError as matplotlib_error:
                    logging.error(f"Failed to import matplotlib fallback: {matplotlib_error}", exc_info=True)
                    print("\n! No visualization libraries available")
                    print("! Install plotly: pip install plotly dash")
                    print("! Or matplotlib: pip install matplotlib")
                    return
            else:
                use_plotly = True
                logging.debug("Using Plotly/Dash mode for interactive viewer")
            
            # Select map to view
            logging.debug(f"Prompting user to select map from site {site_name}")
            map_id = self._select_map_from_site(site_id, site_name)
            if not map_id:
                logging.info("Map viewer aborted: No map selected")
                return
            
            logging.debug(f"Selected map_id: {map_id}")
            
            # Fetch map details
            print("\nLoading map data...")
            logging.info(f"Fetching map details - site_id: {site_id}, map_id: {map_id}")
            map_response = mistapi.api.v1.sites.maps.getSiteMap(
                self.apisession,
                site_id=site_id,
                map_id=map_id
            )
            
            logging.debug(f"getSiteMap API response: HTTP {map_response.status_code}")
            if map_response.status_code != 200:
                logging.error(f"Failed to fetch map details - HTTP {map_response.status_code}, Response: {map_response.data if hasattr(map_response, 'data') else 'No data'}")
                print(f"\n! Failed to fetch map: HTTP {map_response.status_code}")
                return
            
            map_data = map_response.data
            map_name = map_data.get('name', 'Unnamed')
            map_width = map_data.get('width', 1000)
            map_height = map_data.get('height', 1000)
            
            logging.info(f"Map loaded: {map_name} (ID: {map_id})")
            logging.debug(f"Map dimensions: {map_width}x{map_height}px, PPM: {map_data.get('ppm', 'N/A')}, Orientation: {map_data.get('orientation', 0)}")
            logging.debug(f"Map has image: {'url' in map_data}, Has walls: {'wall_path' in map_data}, Has wayfinding: {'wayfinding_path' in map_data}")
            
            print(f"\nMap: {map_name}")
            print(f"Dimensions: {map_width}x{map_height} pixels")
            
            # Fetch devices on this map (use stats API for status information)
            print("Loading devices...")
            logging.info(f"Fetching device stats for site {site_id} (type=all)")
            devices_response = mistapi.api.v1.sites.stats.listSiteDevicesStats(
                self.apisession,
                site_id=site_id,
                limit=1000
            )
            
            logging.debug(f"listSiteDevicesStats API response: HTTP {devices_response.status_code}")
            if devices_response.status_code != 200:
                logging.error(f"Failed to fetch devices - HTTP {devices_response.status_code}")
                print(f"\n! Failed to fetch devices: HTTP {devices_response.status_code}")
                devices_on_map = []
            else:
                all_devices = devices_response.data
                logging.debug(f"Total devices at site: {len(all_devices)}")
                devices_on_map = [d for d in all_devices if d.get('map_id') == map_id]
                logging.info(f"Devices on selected map: {len(devices_on_map)}")
                
                # Log device type breakdown
                device_type_counts = {}
                for device in devices_on_map:
                    device_type = device.get('type', 'unknown')
                    device_type_counts[device_type] = device_type_counts.get(device_type, 0) + 1
                logging.debug(f"Device breakdown on map: {device_type_counts}")
            
            print(f"Devices on map: {len(devices_on_map)}")
            
            # Fetch zones for this site
            logging.info(f"Fetching zones for site {site_id}")
            try:
                zones_response = mistapi.api.v1.sites.zones.listSiteZones(
                    self.apisession,
                    site_id=site_id
                )
                
                if zones_response.status_code == 200:
                    all_zones = zones_response.data
                    # Filter zones that are on this specific map
                    zones_on_map = [z for z in all_zones if z.get('map_id') == map_id]
                    logging.info(f"Total zones at site: {len(all_zones)}, Zones on this map: {len(zones_on_map)}")
                    logging.debug(f"Zones on map: {zones_on_map}")
                else:
                    logging.warning(f"Failed to fetch zones - HTTP {zones_response.status_code}")
                    zones_on_map = []
            except Exception as zone_error:
                logging.error(f"Error fetching zones: {zone_error}", exc_info=True)
                zones_on_map = []
            
            print(f"Zones on map: {len(zones_on_map)}")
            
            # Fetch connected clients for the site to display on map
            clients_on_map = []
            try:
                logging.info(f"Fetching connected wireless client stats for site {site_id}")
                # Use stats API which includes location data (x, y, map_id)
                clients_response = mistapi.api.v1.sites.stats.listSiteWirelessClientsStats(
                    self.apisession,
                    site_id=site_id,
                    limit=1000
                )
                
                if clients_response.status_code == 200:
                    # Use get_all to handle pagination
                    all_clients = mistapi.get_all(response=clients_response, mist_session=self.apisession)
                    logging.info(f"Total wireless clients retrieved: {len(all_clients)}")
                    
                    # Log all unique map_ids to see what we have
                    client_map_ids = set(c.get('map_id') for c in all_clients if c.get('map_id'))
                    logging.info(f"Client map_ids found: {client_map_ids}")
                    logging.info(f"Looking for map_id: {map_id}")
                    
                    # Filter clients that have map location data matching this map
                    clients_on_map = [c for c in all_clients if c.get('map_id') == map_id and c.get('x') is not None and c.get('y') is not None]
                    logging.info(f"Clients on this map (after filtering): {len(clients_on_map)}")
                    
                    if clients_on_map:
                        logging.info(f"Sample client data: {clients_on_map[0]}")
                    elif all_clients:
                        logging.warning(f"No clients matched map_id {map_id}. Sample of all clients: {all_clients[0] if all_clients else 'none'}")
                else:
                    logging.warning(f"Failed to fetch client stats - HTTP {clients_response.status_code}")
            except Exception as client_error:
                logging.error(f"Error fetching client stats: {client_error}", exc_info=True)
            
            print(f"Connected clients on map: {len(clients_on_map)}")
            
            # Fetch RF coverage data from Mist API
            coverage_data = None
            try:
                logging.info(f"Fetching RF coverage data for map {map_id}")
                coverage_url = f"/api/v1/sites/{site_id}/location/coverage"
                coverage_params = {
                    'resolution': 'fine',
                    'duration': '1d',
                    'map_id': map_id,
                    'type': 'client',
                    'from_apollo': 'true'  # Undocumented: forces Apollo backend instead of PostgreSQL
                }
                
                coverage_response = self.apisession.mist_get(coverage_url, query=coverage_params)
                
                if coverage_response.status_code == 200:
                    coverage_data = coverage_response.data
                    
                    # Check for error response structure
                    if isinstance(coverage_data, dict) and 'exception' in coverage_data:
                        exception_str = str(coverage_data.get('exception', ''))
                        
                        if 'psycopg2' in exception_str or 'database' in exception_str.lower():
                            logging.warning(f"RF Coverage temporarily unavailable: Mist backend database connectivity issue")
                            logging.debug(f"Coverage API backend error: {exception_str}")
                        else:
                            logging.error(f"Coverage API returned error response (first 500 chars): {exception_str[:500]}")
                            logging.debug(f"Coverage API full error response: {exception_str}")
                            logging.debug(f"Error details - Query: {coverage_data.get('query')}, URI: {coverage_data.get('uri')}")
                        
                        coverage_data = None
                        print("  Note: RF Coverage heatmap unavailable (Mist backend issue) - continuing without it")
                    else:
                        result_count = len(coverage_data.get('results', [])) if coverage_data else 0
                        logging.info(f"RF coverage data retrieved: {result_count} grid points")
                else:
                    logging.warning(f"Failed to fetch RF coverage data - HTTP {coverage_response.status_code}")
                    coverage_data = None
            except Exception as coverage_error:
                logging.error(f"Error fetching RF coverage data: {coverage_error}", exc_info=True)
                coverage_data = None
            
            if use_plotly:
                logging.info(f"Launching Plotly/Dash viewer for map {map_name}")
                self._launch_plotly_viewer(map_data, devices_on_map, zones_on_map, clients_on_map, site_id, map_id, coverage_data)
            else:
                logging.info(f"Launching matplotlib fallback viewer for map {map_name}")
                self._launch_matplotlib_viewer(map_data, devices_on_map)
                
        except EOFError:
            logging.info("EOF detected during interactive map viewer")
            return
        except Exception as e:
            logging.error(f"Error in interactive map viewer: {e}", exc_info=True)
            print(f"\n! Error launching map viewer: {e}")
    
    def _launch_plotly_viewer(self, map_data, devices, zones, clients, site_id, map_id, coverage_data=None):
        """Launch interactive Plotly/Dash map viewer with edit capabilities, client display, and RF coverage heatmap"""
        coverage_count = len(coverage_data.get('results', [])) if coverage_data else 0
        logging.info(f"_launch_plotly_viewer called - map_id: {map_id}, devices: {len(devices)}, zones: {len(zones)}, clients: {len(clients)}, coverage: {coverage_count}")
        import plotly.graph_objects as go
        from math import cos, sin, radians
        import webbrowser
        import os
        
        try:
            logging.debug("Importing Dash modules for interactive viewer")
            from dash import Dash, html, dcc, Input, Output, State, callback_context
            import dash
            logging.info(f"Dash version: {dash.__version__}")
        except ImportError as e:
            logging.error(f"Failed to import Dash, falling back to static view: {e}", exc_info=True)
            print("\n! Dash not available - using static Plotly view only")
            print("! Install with: pip install dash")
            self._create_static_plotly_map(map_data, devices)
            return
        
        print("\n" + "-" * 80)
        print("LAUNCHING INTERACTIVE MAP VIEWER")
        print("-" * 80)
        print("! Opening web browser with interactive map...")
        print("! Features:")
        print("!   - Toggle layers (walls, zones, wayfinding, devices, clients)")
        print("!   - Ruler tool - Draw lines to measure distances")
        print("!   - Connected client visualization (green dots)")
        print("!   - Click devices/clients to see details")
        print("!   - Drag devices to new positions (future: save to cloud)")
        print("!   - Pan and zoom")
        print("! Press Ctrl+C in terminal to stop server")
        print("-" * 80)
        
        # Create Dash app with dark theme
        logging.debug("Creating Dash application instance")
        app = Dash(__name__)
        
        # Inject custom CSS for dark mode and responsive design
        app.index_string = '''
        <!DOCTYPE html>
        <html>
            <head>
                {%metas%}
                <title>{%title%}</title>
                {%favicon%}
                {%css%}
                <style>
                    body {
                        margin: 0;
                        padding: 0;
                        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
                        background-color: #1a1a1a;
                        color: #e0e0e0;
                    }
                    #react-entry-point {
                        height: 100vh;
                        display: flex;
                        flex-direction: column;
                    }
                    .main-container {
                        flex: 1;
                        display: flex;
                        overflow: hidden;
                    }
                    .map-container {
                        flex: 1;
                        display: flex;
                        flex-direction: column;
                        padding: 15px;
                        overflow: hidden;
                    }
                    .sidebar {
                        width: 280px;
                        background-color: #2d2d2d;
                        padding: 20px;
                        overflow-y: auto;
                        border-left: 1px solid #444;
                        box-shadow: -2px 0 10px rgba(0,0,0,0.3);
                    }
                    h1 {
                        margin: 0;
                        padding: 20px;
                        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                        color: white;
                        font-size: 24px;
                        font-weight: 600;
                        box-shadow: 0 2px 10px rgba(0,0,0,0.3);
                    }
                    h3 {
                        color: #a0a0ff;
                        font-size: 16px;
                        margin-top: 0;
                        margin-bottom: 15px;
                        border-bottom: 2px solid #444;
                        padding-bottom: 8px;
                    }
                    .sidebar p {
                        margin: 8px 0;
                        color: #b0b0b0;
                        font-size: 14px;
                    }
                    .sidebar hr {
                        border: none;
                        border-top: 1px solid #444;
                        margin: 20px 0;
                    }
                    /* Custom checkbox styling */
                    .sidebar label {
                        color: #d0d0d0 !important;
                        cursor: pointer;
                        transition: color 0.2s;
                    }
                    .sidebar label:hover {
                        color: #ffffff !important;
                    }
                    /* Graph container */
                    #map-display {
                        height: 100% !important;
                        width: 100% !important;
                    }
                    .js-plotly-plot {
                        height: 100% !important;
                    }
                    /* Info badges */
                    .info-badge {
                        display: inline-block;
                        padding: 4px 12px;
                        background-color: #3d3d3d;
                        border-radius: 12px;
                        margin: 4px 0;
                        font-size: 13px;
                        color: #a0a0ff;
                    }
                    .device-detail {
                        background-color: #3d3d3d;
                        padding: 12px;
                        border-radius: 8px;
                        margin: 8px 0;
                        border-left: 3px solid #667eea;
                    }
                    .device-detail strong {
                        color: #a0a0ff;
                    }
                    /* NOTE: CSS text-shadow doesn't work on Plotly SVG text elements.
                       Text labels use annotations with bgcolor/bordercolor instead. */
                </style>
            </head>
            <body>
                {%app_entry%}
                <footer>
                    {%config%}
                    {%scripts%}
                    {%renderer%}
                </footer>
            </body>
        </html>
        '''
        
        # Build figure
        logging.debug("Building Plotly figure")
        fig = go.Figure()
        
        # Set map dimensions and get PPM for unit conversions
        map_width = map_data.get('width', 1000)
        map_height = map_data.get('height', 1000)
        ppm = map_data.get('ppm', 10)  # pixels per meter, default to 10 if not set
        logging.debug(f"Map canvas dimensions: {map_width}x{map_height}, PPM: {ppm}")
        
        # Add map image if available
        # Note: Plotly uses bottom-left origin, but we keep Mist's coordinate system (top-left origin)
        # by inverting the Y-axis in the layout
        if 'url' in map_data:
            logging.debug(f"Adding map background image: {map_data.get('url')[:100]}...")
            fig.add_layout_image(
                source=map_data['url'],
                x=0, y=0,
                sizex=map_width, sizey=map_height,
                xref="x", yref="y",
                sizing="stretch",
                layer="below"
            )
        else:
            logging.warning("Map has no background image URL")
        
        # Add walls if present
        if 'wall_path' in map_data and map_data['wall_path']:
            wall_path = map_data['wall_path']
            logging.debug(f"Wall path data structure: {wall_path}")
            
            if 'nodes' in wall_path:
                logging.info(f"Processing {len(wall_path['nodes'])} wall path nodes")
                
                # Wall paths are SEGMENTS, not a continuous line
                # Each node has 'edges' that define which other nodes it connects to
                # We need to draw individual line segments based on edges
                
                # First, build a lookup of nodes by name
                node_lookup = {}
                for node in wall_path['nodes']:
                    node_name = node.get('name', '')
                    pos = node.get('position', {})
                    if node_name and pos:
                        node_lookup[node_name] = pos
                        logging.debug(f"Wall node '{node_name}': x={pos.get('x')}, y={pos.get('y')}, edges={node.get('edges', {})}")
                
                # Now draw segments based on edges
                for node in wall_path['nodes']:
                    node_name = node.get('name', '')
                    node_pos = node.get('position', {})
                    edges = node.get('edges', {})
                    
                    if not node_pos or not edges:
                        continue
                    
                    # Draw a line from this node to each connected node
                    for edge_name in edges.keys():
                        if edge_name in node_lookup:
                            target_pos = node_lookup[edge_name]
                            
                            # Draw segment
                            fig.add_trace(go.Scatter(
                                x=[node_pos.get('x', 0), target_pos.get('x', 0)],
                                y=[node_pos.get('y', 0), target_pos.get('y', 0)],
                                mode='lines',
                                name='Walls',
                                line=dict(color='#ff3333', width=4),
                                visible=True,
                                showlegend=False,
                                hoverinfo='skip'
                            ))
                
                # Add one invisible trace just for the legend
                fig.add_trace(go.Scatter(
                    x=[None], y=[None],
                    mode='lines',
                    name='Walls',
                    line=dict(color='#ff3333', width=4),
                    visible=True,
                    showlegend=True
                ))
        
        # Add wayfinding paths if present
        if 'wayfinding_path' in map_data and map_data['wayfinding_path']:
            wf_path = map_data['wayfinding_path']
            logging.debug(f"Wayfinding path data structure: {wf_path}")
            
            if 'nodes' in wf_path:
                logging.info(f"Processing {len(wf_path['nodes'])} wayfinding path nodes")
                
                # Wayfinding paths also use edge-based segments like walls
                # Build node lookup
                node_lookup = {}
                for node in wf_path['nodes']:
                    node_name = node.get('name', '')
                    pos = node.get('position', {})
                    if node_name and pos:
                        node_lookup[node_name] = pos
                        logging.debug(f"Wayfinding node '{node_name}': x={pos.get('x')}, y={pos.get('y')}, edges={node.get('edges', {})}")
                
                # Draw segments based on edges
                for node in wf_path['nodes']:
                    node_name = node.get('name', '')
                    node_pos = node.get('position', {})
                    edges = node.get('edges', {})
                    
                    if not node_pos or not edges:
                        continue
                    
                    # Draw a line from this node to each connected node
                    for edge_name in edges.keys():
                        if edge_name in node_lookup:
                            target_pos = node_lookup[edge_name]
                            
                            # Draw segment
                            fig.add_trace(go.Scatter(
                                x=[node_pos.get('x', 0), target_pos.get('x', 0)],
                                y=[node_pos.get('y', 0), target_pos.get('y', 0)],
                                mode='lines+markers',
                                name='Wayfinding',
                                line=dict(color='#4488ff', width=3, dash='dash'),
                                marker=dict(size=8, color='#4488ff'),
                                visible=True,
                                showlegend=False,
                                hoverinfo='skip'
                            ))
                
                # Add one invisible trace just for the legend
                fig.add_trace(go.Scatter(
                    x=[None], y=[None],
                    mode='lines+markers',
                    name='Wayfinding',
                    line=dict(color='#4488ff', width=3, dash='dash'),
                    marker=dict(size=8, color='#4488ff'),
                    visible=True,
                    showlegend=True
                ))
        
        # Add zones if present
        if zones and len(zones) > 0:
            logging.info(f"Processing {len(zones)} zones on this map")
            zone_colors = ['rgba(255,165,0,0.2)', 'rgba(0,255,255,0.2)', 'rgba(255,0,255,0.2)', 
                          'rgba(255,255,0,0.2)', 'rgba(0,255,0,0.2)', 'rgba(128,0,255,0.2)']
            
            for idx, zone in enumerate(zones):
                zone_name = zone.get('name', f'Zone {idx+1}')
                vertices = zone.get('vertices', [])
                
                logging.debug(f"Zone '{zone_name}': {len(vertices)} vertices - {vertices}")
                
                if vertices and len(vertices) >= 3:
                    # Extract x,y coordinates from vertices
                    zone_x = [v.get('x', 0) for v in vertices]
                    zone_y = [v.get('y', 0) for v in vertices]
                    # Close the polygon
                    zone_x.append(zone_x[0])
                    zone_y.append(zone_y[0])
                    
                    color = zone_colors[idx % len(zone_colors)]
                    border_color = color.replace('0.2', '0.8')  # More opaque border
                    
                    logging.debug(f"Drawing zone '{zone_name}' with {len(zone_x)} points")
                    
                    fig.add_trace(go.Scatter(
                        x=zone_x, y=zone_y,
                        mode='lines',
                        name=f'Zone: {zone_name}',
                        line=dict(color=border_color, width=2, dash='dot'),
                        fill='toself',
                        fillcolor=color,
                        opacity=1.0,  # Don't apply additional opacity - it's in the color
                        visible=True,
                        showlegend=True,
                        hovertext=f"Zone: {zone_name}",
                        hoverinfo='text'
                    ))
                    
                    # Add zone name label at upper-left corner
                    min_x = min(zone_x)
                    min_y = min(zone_y)
                    fig.add_annotation(
                        x=min_x + 10,  # Small offset from corner
                        y=min_y + 10,
                        text=f"<b>{zone_name}</b>",
                        showarrow=False,
                        font=dict(size=14, color='white', family='Arial Black'),
                        bgcolor=border_color.replace('0.8', '0.9'),
                        bordercolor='white',
                        borderwidth=2,
                        borderpad=4,
                        xanchor='left',
                        yanchor='top',
                        name='Zone Label'  # For toggle control
                    )
                else:
                    logging.warning(f"Zone '{zone_name}' has insufficient vertices: {len(vertices)}")
        else:
            logging.info("No zones found on this map")
        
        # Add validation paths (site survey paths) if present
        if 'sitesurvey_path' in map_data and map_data['sitesurvey_path']:
            sitesurvey_paths = map_data['sitesurvey_path']
            logging.info(f"Processing {len(sitesurvey_paths)} validation paths")
            
            for path_idx, path in enumerate(sitesurvey_paths):
                path_name = path.get('name', f'Path {path_idx + 1}')
                path_coords = path.get('coordinate', [])
                
                if path_coords and len(path_coords) >= 2:
                    # Extract coordinates
                    path_x = [coord.get('x', 0) for coord in path_coords]
                    path_y = [coord.get('y', 0) for coord in path_coords]
                    
                    # Draw validation path as connected line with markers
                    fig.add_trace(go.Scatter(
                        x=path_x,
                        y=path_y,
                        mode='lines+markers',
                        name=f'Validation: {path_name}',
                        line=dict(color='#ff00ff', width=3, dash='dot'),
                        marker=dict(size=10, color='#ff00ff', symbol='diamond', line=dict(color='white', width=2)),
                        visible=True,
                        showlegend=True,
                        hovertext=f"Validation Path: {path_name}<br>{len(path_coords)} points",
                        hoverinfo='text'
                    ))
                    
                    # Add path name label at start point
                    fig.add_annotation(
                        x=path_x[0],
                        y=path_y[0] - 20,
                        text=f"<b>{path_name}</b>",
                        showarrow=False,
                        font=dict(size=11, color='white', family='Arial Black'),
                        bgcolor='rgba(255,0,255,0.9)',
                        bordercolor='white',
                        borderwidth=2,
                        borderpad=3,
                        xanchor='center',
                        yanchor='bottom'
                    )
                    
                    logging.debug(f"Added validation path '{path_name}' with {len(path_coords)} points")
                else:
                    logging.warning(f"Validation path '{path_name}' has insufficient coordinates: {len(path_coords)}")
        else:
            logging.info("No validation paths found on this map")
        
        # Add connected clients if present
        if clients and len(clients) > 0:
            logging.info(f"Processing {len(clients)} connected clients on this map")
            logging.debug(f"Client sample data: {clients[0] if clients else 'None'}")
            client_x = []
            client_y = []
            client_hover = []
            client_names = []
            
            for client in clients:
                x = client.get('x')
                y = client.get('y')
                client_mac = client.get('mac', 'unknown')
                client_map_id = client.get('map_id', 'none')
                logging.debug(f"Client {client_mac}: x={x}, y={y}, map_id={client_map_id} (looking for map_id={map_id})")
                if x is not None and y is not None:
                    client_x.append(x)
                    client_y.append(y)
                    
                    # Use hostname or MAC for label
                    hostname = client.get('hostname', '')
                    label = hostname if hostname else client_mac[-8:]
                    client_names.append(label)
                    
                    # Build hover text with client details
                    hover = f"<b>Client</b><br>"
                    hover += f"MAC: {client.get('mac', 'N/A')}<br>"
                    hover += f"Hostname: {client.get('hostname', 'N/A')}<br>"
                    hover += f"SSID: {client.get('ssid', 'N/A')}<br>"
                    hover += f"AP: {client.get('ap_name', 'N/A')}<br>"
                    hover += f"Band: {client.get('band', 'N/A')}<br>"
                    hover += f"Signal: {client.get('rssi', 'N/A')} dBm<br>"
                    hover += f"Position: ({x}, {y})"
                    client_hover.append(hover)
            
            if client_x:
                # Add client markers
                fig.add_trace(go.Scatter(
                    x=client_x, y=client_y,
                    mode='markers',
                    name='Clients',
                    marker=dict(
                        symbol='circle',
                        size=12,
                        color='#00ff00',  # Bright green
                        line=dict(color='white', width=2),
                        opacity=0.9
                    ),
                    hovertext=client_hover,
                    hoverinfo='text',
                    visible=True,
                    showlegend=True
                ))
                
                # Add client name labels with shadow effect using annotations
                for i, (x, y, name) in enumerate(zip(client_x, client_y, client_names)):
                    fig.add_annotation(
                        x=x,
                        y=y - 10,  # Position above marker
                        text=f"<b>{name}</b>",
                        showarrow=False,
                        font=dict(size=9, color='white', family='Arial'),
                        bgcolor='rgba(0,128,0,0.9)',
                        bordercolor='white',
                        borderwidth=1,
                        borderpad=2,
                        xanchor='center',
                        yanchor='bottom',
                        name='Clients Label'  # For toggle control
                    )
                logging.info(f"Added {len(client_x)} clients to map visualization (out of {len(clients)} total clients)")
            else:
                logging.warning(f"Found {len(clients)} clients but none have x,y coordinates")
        else:
            logging.info("No connected clients found on this map")
        
        # Add devices by type with LARGER, more visible markers
        device_types = {'ap': [], 'switch': [], 'gateway': []}
        for device in devices:
            device_type = device.get('type', 'unknown')
            if device_type in device_types and 'x' in device and 'y' in device:
                device_types[device_type].append(device)
        
        # Enhanced colors and symbols for device types - with status-based coloring
        # Status colors: connected (green), disconnected (red), upgrading (orange/amber)
        type_config = {
            'ap': {
                'symbol': 'triangle-up',
                'name': 'Access Points',
                'size': 20,
                'colors': {
                    'connected': '#00ff00',      # Bright green
                    'disconnected': '#ff0000',   # Bright red
                    'upgrading': '#ff8800'       # Orange/amber
                }
            },
            'switch': {
                'symbol': 'square',
                'name': 'Switches',
                'size': 18,
                'colors': {
                    'connected': '#00ccff',      # Cyan
                    'disconnected': '#ff0000',   # Bright red
                    'upgrading': '#ff8800'       # Orange/amber
                }
            },
            'gateway': {
                'symbol': 'diamond',
                'name': 'Gateways',
                'size': 20,
                'colors': {
                    'connected': '#ff00ff',      # Magenta
                    'disconnected': '#ff0000',   # Bright red
                    'upgrading': '#ff8800'       # Orange/amber
                }
            }
        }
        
        for device_type, config in type_config.items():
            type_devices = device_types[device_type]
            if type_devices:
                x_coords = [d['x'] for d in type_devices]
                y_coords = [d['y'] for d in type_devices]  # Keep Mist Y-coordinates as-is
                names = [d.get('name', d.get('mac', 'Unknown')) for d in type_devices]
                orientations = [d.get('orientation', 0) for d in type_devices]
                
                # Debug log device orientations
                for d in type_devices:
                    device_name = d.get('name', 'Unnamed')
                    device_orientation = d.get('orientation', 0)
                    logging.debug(f"Device '{device_name}': orientation={device_orientation}")
                
                # Determine status and color for each device
                colors = []
                statuses = []
                for d in type_devices:
                    # Check device status
                    # Status can be: 'connected', 'disconnected', or check for upgrade in progress
                    status = d.get('status', 'disconnected')
                    
                    # Check if upgrading (upgrade_status field or checking for active upgrade)
                    if d.get('upgrade_status') or d.get('fwupdate', {}).get('progress') is not None:
                        device_status = 'upgrading'
                    elif status == 'connected':
                        device_status = 'connected'
                    else:
                        device_status = 'disconnected'
                    
                    statuses.append(device_status)
                    colors.append(config['colors'][device_status])
                
                hover_text = []
                for d, device_status in zip(type_devices, statuses):
                    text = f"<b>{d.get('name', 'Unnamed')}</b><br>"
                    text += f"Type: {d.get('type', 'N/A')}<br>"
                    text += f"Model: {d.get('model', 'N/A')}<br>"
                    text += f"MAC: {d.get('mac', 'N/A')}<br>"
                    text += f"Status: <b>{device_status.upper()}</b><br>"
                    if device_status == 'upgrading':
                        progress = d.get('fwupdate', {}).get('progress', 'N/A')
                        text += f"Upgrade Progress: {progress}%<br>" if progress != 'N/A' else ""
                    text += f"Position: ({d.get('x', 'N/A')}, {d.get('y', 'N/A')})<br>"
                    text += f"Orientation: {d.get('orientation', 0)}"
                    hover_text.append(text)
                
                # Add device markers with status-based colors
                fig.add_trace(go.Scatter(
                    x=x_coords, y=y_coords,
                    mode='markers',
                    name=config['name'],
                    marker=dict(
                        symbol=config['symbol'],
                        size=config['size'],
                        color=colors,  # Status-based color array
                        line=dict(color='white', width=2),
                        opacity=0.9
                    ),
                    hovertext=hover_text,
                    hoverinfo='text',
                    visible=True,
                    showlegend=True
                ))
                
                # Add device name labels with shadow effect using annotations
                for i, (x, y, name, device_color) in enumerate(zip(x_coords, y_coords, names, colors)):
                    fig.add_annotation(
                        x=x,
                        y=y - 15,  # Position above marker
                        text=f"<b>{name}</b>",
                        showarrow=False,
                        font=dict(size=11, color='white', family='Arial Black'),
                        bgcolor='rgba(0,0,0,0.85)',
                        bordercolor=device_color,  # Match device status color
                        borderwidth=2,
                        borderpad=3,
                        xanchor='center',
                        yanchor='bottom',
                        name=f"{config['name']} Label"  # For toggle control
                    )
                
                # Add mesh links for APs if mesh topology exists
                if device_type == 'ap':
                    mesh_links_added = 0
                    for i, device in enumerate(type_devices):
                        # Check if this AP has mesh info
                        mesh_uplink = device.get('mesh_uplink')
                        if mesh_uplink:
                            # Find the uplink AP
                            for uplink_device in type_devices:
                                if uplink_device.get('mac') == mesh_uplink:
                                    # Draw mesh link
                                    fig.add_trace(go.Scatter(
                                        x=[device['x'], uplink_device['x']],
                                        y=[device['y'], uplink_device['y']],
                                        mode='lines',
                                        line=dict(color='rgba(255,0,255,0.4)', width=2, dash='dash'),
                                        name='Mesh Link',
                                        showlegend=(mesh_links_added == 0),  # Only show in legend once
                                        hoverinfo='skip'
                                    ))
                                    mesh_links_added += 1
                                    break
                    if mesh_links_added > 0:
                        logging.info(f"Added {mesh_links_added} mesh links between APs")
                
                # Add Mist-style orientation indicators: crosshair + directional dot
                # Use status-based colors for crosshair and orientation dot
                for i, (x, y, angle, device, device_color, device_status) in enumerate(zip(x_coords, y_coords, orientations, type_devices, colors, statuses)):
                    # Crosshair at device location (always visible) - LARGER SIZE with status color
                    crosshair_size = 40  # Increased from 25 to 40
                    
                    # Horizontal line
                    fig.add_trace(go.Scatter(
                        x=[x - crosshair_size, x + crosshair_size],
                        y=[y, y],
                        mode='lines',
                        line=dict(color=device_color, width=3),  # Status-based color
                        name=f"{config['name']} Orientation",  # Name for toggle control
                        showlegend=False,
                        hoverinfo='skip'
                    ))
                    
                    # Vertical line
                    fig.add_trace(go.Scatter(
                        x=[x, x],
                        y=[y - crosshair_size, y + crosshair_size],
                        mode='lines',
                        line=dict(color=device_color, width=3),  # Status-based color
                        name=f"{config['name']} Orientation",  # Name for toggle control
                        showlegend=False,
                        hoverinfo='skip'
                    ))
                    
                    # Directional dot showing orientation (always visible for clarity)
                    dot_distance = 50  # Increased from 35 to 50
                    
                    # Convert Mist orientation to standard cartesian coordinates:
                    # - Mist: 0 = up (north), 90 = right (east), 180 = down, 270 = left
                    # - Math: 0 = right (east), 90 = up (north), counter-clockwise
                    # - Y-axis: Mist uses top-left origin with Y increasing downward
                    # Conversion: math_angle = 90 - mist_angle, then flip Y component
                    math_angle = 90 - angle
                    dot_x = x + dot_distance * cos(radians(math_angle))
                    dot_y = y - dot_distance * sin(radians(math_angle))  # Subtract because Y increases downward
                    
                    fig.add_trace(go.Scatter(
                        x=[dot_x],
                        y=[dot_y],
                        mode='markers',
                        marker=dict(
                            size=16,  # Increased from 10 to 16
                            color=device_color,  # Status-based color
                            line=dict(color='white', width=2)
                        ),
                        name=f"{config['name']} Orientation",  # Name for toggle control
                        showlegend=False,
                        hovertext=f"Orientation: {angle}",
                        hoverinfo='text'
                    ))
        
        # Add beacons (vBeacons and BLE beacons) if present in map data
        if 'vbeacons' in map_data and map_data['vbeacons']:
            vbeacons = map_data['vbeacons']
            logging.info(f"Processing {len(vbeacons)} virtual beacons")
            
            beacon_x = []
            beacon_y = []
            beacon_hover = []
            beacon_names = []
            
            for beacon in vbeacons:
                x = beacon.get('x')
                y = beacon.get('y')
                if x is not None and y is not None:
                    beacon_x.append(x)
                    beacon_y.append(y)
                    
                    name = beacon.get('name', 'Unnamed Beacon')
                    beacon_names.append(name)
                    
                    hover = f"<b>Virtual Beacon: {name}</b><br>"
                    hover += f"UUID: {beacon.get('uuid', 'N/A')}<br>"
                    hover += f"Major: {beacon.get('major', 'N/A')}<br>"
                    hover += f"Minor: {beacon.get('minor', 'N/A')}<br>"
                    hover += f"Power: {beacon.get('power', 'N/A')}<br>"
                    hover += f"Position: ({x}, {y})"
                    beacon_hover.append(hover)
            
            if beacon_x:
                # Add virtual beacon markers
                fig.add_trace(go.Scatter(
                    x=beacon_x,
                    y=beacon_y,
                    mode='markers',
                    name='Virtual Beacons',
                    marker=dict(
                        symbol='circle',
                        size=14,
                        color='#00ff00',  # Green for virtual beacons
                        line=dict(color='white', width=2),
                        opacity=0.9
                    ),
                    hovertext=beacon_hover,
                    hoverinfo='text',
                    visible=True,
                    showlegend=True
                ))
                
                # Add beacon name labels
                for i, (x, y, name) in enumerate(zip(beacon_x, beacon_y, beacon_names)):
                    fig.add_annotation(
                        x=x,
                        y=y - 12,
                        text=f"<b>{name}</b>",
                        showarrow=False,
                        font=dict(size=9, color='white', family='Arial'),
                        bgcolor='rgba(0,200,0,0.9)',
                        bordercolor='white',
                        borderwidth=1,
                        borderpad=2,
                        xanchor='center',
                        yanchor='bottom',
                        name='Virtual Beacons Label'  # For toggle control
                    )
                
                # Add coverage circles for vBeacons based on power
                for beacon in vbeacons:
                    x = beacon.get('x')
                    y = beacon.get('y')
                    power = beacon.get('power', 0)  # Power in dBm
                    
                    if x is not None and y is not None:
                        # Estimate coverage radius based on power (rough approximation)
                        # Higher power = larger radius
                        # Typical range: -12 to +4 dBm
                        base_radius = 50  # Base radius in pixels
                        power_factor = (power + 12) / 16  # Normalize -12 to +4 range
                        radius = base_radius + (power_factor * 100)
                        
                        # Create circle using parametric plot
                        theta = [i * 2 * pi / 50 for i in range(51)]
                        circle_x = [x + radius * cos(t) for t in theta]
                        circle_y = [y + radius * sin(t) for t in theta]
                        
                        fig.add_trace(go.Scatter(
                            x=circle_x,
                            y=circle_y,
                            mode='lines',
                            line=dict(color='rgba(0,255,0,0.3)', width=1, dash='dash'),
                            fill='toself',
                            fillcolor='rgba(0,255,0,0.05)',
                            name='vBeacon Coverage',
                            showlegend=False,
                            hoverinfo='skip'
                        ))
                
                logging.info(f"Added {len(beacon_x)} virtual beacons to map")
        else:
            logging.info("No virtual beacons found on this map")
        
        # Add BLE beacons if present
        if 'beacons' in map_data and map_data['beacons']:
            ble_beacons = map_data['beacons']
            logging.info(f"Processing {len(ble_beacons)} BLE beacons")
            
            ble_x = []
            ble_y = []
            ble_hover = []
            ble_names = []
            
            for beacon in ble_beacons:
                x = beacon.get('x')
                y = beacon.get('y')
                if x is not None and y is not None:
                    ble_x.append(x)
                    ble_y.append(y)
                    
                    name = beacon.get('name', beacon.get('mac', 'Unnamed'))
                    ble_names.append(name)
                    
                    hover = f"<b>BLE Beacon: {name}</b><br>"
                    hover += f"MAC: {beacon.get('mac', 'N/A')}<br>"
                    hover += f"Type: {beacon.get('type', 'N/A')}<br>"
                    hover += f"Power: {beacon.get('power', 'N/A')}<br>"
                    hover += f"Position: ({x}, {y})"
                    ble_hover.append(hover)
            
            if ble_x:
                # Add BLE beacon markers
                fig.add_trace(go.Scatter(
                    x=ble_x,
                    y=ble_y,
                    mode='markers',
                    name='BLE Beacons',
                    marker=dict(
                        symbol='circle',
                        size=14,
                        color='#00bfff',  # Cyan for BLE beacons
                        line=dict(color='white', width=2),
                        opacity=0.9
                    ),
                    hovertext=ble_hover,
                    hoverinfo='text',
                    visible=True,
                    showlegend=True
                ))
                
                # Add BLE beacon name labels
                for i, (x, y, name) in enumerate(zip(ble_x, ble_y, ble_names)):
                    fig.add_annotation(
                        x=x,
                        y=y - 12,
                        text=f"<b>{name}</b>",
                        showarrow=False,
                        font=dict(size=9, color='white', family='Arial'),
                        bgcolor='rgba(0,191,255,0.9)',
                        bordercolor='white',
                        borderwidth=1,
                        borderpad=2,
                        xanchor='center',
                        yanchor='bottom',
                        name='BLE Beacons Label'  # For toggle control
                    )
                
                logging.info(f"Added {len(ble_x)} BLE beacons to map")
        else:
            logging.info("No BLE beacons found on this map")
        
        # Add RF Coverage Heatmap from Mist API data
        if coverage_data and 'results' in coverage_data and len(coverage_data.get('results', [])) > 0:
            logging.info(f"Processing RF coverage data - {len(coverage_data.get('results', []))} grid points")
            
            # API returns coordinates in METERS - must convert to pixels using PPM
            result_def = coverage_data.get('result_def', [])
            results = coverage_data.get('results', [])
            gridsize_meters = coverage_data.get('gridsize', 1)
            
            logging.debug(f"Coverage result_def: {result_def}")
            logging.debug(f"Coverage gridsize: {gridsize_meters} meters, PPM: {ppm}")
            
            # Find indices for data fields
            try:
                x_idx = result_def.index('x')
                y_idx = result_def.index('y')
                max_rssi_idx = result_def.index('max_rssi')
                avg_rssi_idx = result_def.index('avg_rssi')
            except ValueError as e:
                logging.error(f"Coverage data missing expected fields: {e}")
                x_idx, y_idx, max_rssi_idx, avg_rssi_idx = 0, 1, 4, 5
            
            # Build grid data structure for heatmap
            grid_data = {}
            for result in results:
                if len(result) <= max(x_idx, y_idx, max_rssi_idx, avg_rssi_idx):
                    continue
                
                x_meters = result[x_idx]
                y_meters = result[y_idx]
                pixel_x = x_meters * ppm
                pixel_y = y_meters * ppm
                max_rssi = result[max_rssi_idx]
                
                grid_data[(pixel_x, pixel_y)] = max_rssi
            
            if grid_data:
                # Auto-scale color range based on actual data
                all_rssi_values = [v for v in grid_data.values() if v is not None]
                if all_rssi_values:
                    min_rssi = min(all_rssi_values)  # Most negative (weakest)
                    max_rssi = max(all_rssi_values)  # Closest to zero (strongest)
                    logging.info(f"RF Coverage RSSI range: {min_rssi} dBm (weakest) to {max_rssi} dBm (strongest)")
                else:
                    min_rssi = -100
                    max_rssi = -40
                
                # Convert to regular grid for Heatmap trace
                unique_x = sorted(set(x for x, y in grid_data.keys()))
                unique_y = sorted(set(y for x, y in grid_data.keys()))
                
                # Diagnostic logging for coordinate alignment debugging
                logging.info(f"HEATMAP DEBUG - Map dimensions: {map_width}x{map_height} pixels, PPM: {ppm}")
                logging.info(f"HEATMAP DEBUG - Coverage X range: {min(unique_x):.1f} to {max(unique_x):.1f} pixels (from {min(unique_x)/ppm:.1f}m to {max(unique_x)/ppm:.1f}m)")
                logging.info(f"HEATMAP DEBUG - Coverage Y range: {min(unique_y):.1f} to {max(unique_y):.1f} pixels (from {min(unique_y)/ppm:.1f}m to {max(unique_y)/ppm:.1f}m)")
                logging.info(f"HEATMAP DEBUG - Grid size: {len(unique_x)} x {len(unique_y)} = {len(grid_data)} data points")
                
                # Create Z matrix for heatmap - use None for missing data points
                # This prevents artificial values from being interpolated
                z_matrix = []
                for y_val in unique_y:
                    row = []
                    for x_val in unique_x:
                        rssi = grid_data.get((x_val, y_val), None)  # None for missing - no fake data
                        row.append(rssi)
                    z_matrix.append(row)
                
                # Custom colorscale: red (strongest/closest to 0) -> blue (weakest/most negative)
                colorscale = [
                    [0.0, 'rgb(0, 0, 255)'],      # Blue (weakest/most negative)
                    [0.33, 'rgb(0, 255, 0)'],     # Green
                    [0.50, 'rgb(255, 255, 0)'],   # Yellow
                    [0.67, 'rgb(255, 165, 0)'],   # Orange
                    [1.0, 'rgb(255, 0, 0)']       # Red (strongest/closest to 0)
                ]
                
                fig.add_trace(go.Heatmap(
                    x=unique_x,
                    y=unique_y,
                    z=z_matrix,
                    colorscale=colorscale,
                    zmin=min_rssi,  # Auto-scale to actual data range
                    zmax=max_rssi,
                    opacity=0.5,
                    name='RF Coverage',
                    hovertemplate='X: %{x}<br>Y: %{y}<br>RSSI: %{z} dBm<extra></extra>',
                    visible=False,
                    showscale=True,  # Show color scale legend
                    colorbar=dict(
                        title=dict(
                            text="RSSI (dBm)",
                            side="right",
                            font=dict(size=12, color='white')
                        ),
                        thickness=20,
                        len=0.5,
                        y=0.95,
                        yanchor='top',
                        x=1.02,
                        tickfont=dict(size=10, color='white'),
                        tickmode='linear',
                        tick0=min_rssi,
                        dtick=(max_rssi - min_rssi) / 5,  # Show 6 tick marks
                        outlinewidth=1,
                        outlinecolor='white'
                    ),
                    connectgaps=True,  # Interpolate across gaps for smooth coverage
                    zsmooth='best'  # Smooth interpolation between data points
                ))
                
                logging.info(f"Added RF Coverage heatmap: {len(grid_data)} cells ({gridsize_meters}m grid) with auto-scaled colors ({min_rssi} to {max_rssi} dBm)")
            else:
                logging.warning("No valid coverage grid data to visualize")
        elif coverage_data:
            logging.warning(f"Coverage data received but no results")
        else:
            logging.info("No RF coverage data available")
        
        # Add map origin marker (coordinate reference point)
        origin = map_data.get('origin', {}) or {}
        origin_x = origin.get('x', 0)
        origin_y = origin.get('y', 0)
        
        fig.add_trace(go.Scatter(
            x=[origin_x],
            y=[origin_y],
            mode='markers+text',
            name='Map Origin',
            marker=dict(
                symbol='x',
                size=20,
                color='yellow',
                line=dict(width=3, color='black')
            ),
            text=['Origin (0,0)'],
            textposition='top center',
            textfont=dict(size=12, color='yellow'),
            visible=False,
            showlegend=True
        ))
        
        # Update layout with dark theme and responsive sizing
        fig.update_layout(
            title={
                'text': f"Map: {map_data.get('name', 'Unnamed')}",
                'font': {'size': 20, 'color': '#e0e0e0'}
            },
            xaxis=dict(
                range=[-50, map_width + 50],  # Add margins to show full map
                visible=True, 
                title="X (pixels)",
                gridcolor='#444',
                zerolinecolor='#666',
                color='#b0b0b0',
                constrain='domain'  # Keep zoom within bounds
            ),
            yaxis=dict(
                range=[map_height + 50, -50],  # Inverted range with margins: Mist uses top-left origin
                visible=True, 
                title="Y (pixels)", 
                scaleanchor="x", 
                scaleratio=1,
                gridcolor='#444',
                zerolinecolor='#666',
                color='#b0b0b0',
                constrain='domain'  # Keep zoom within bounds
            ),
            autosize=True,
            hovermode='closest',
            showlegend=True,
            uirevision='constant',  # Prevent auto-ranging to data - maintain user's view
            legend=dict(
                x=0.02, 
                y=0.98, 
                bgcolor='rgba(45,45,45,0.9)',
                bordercolor='#667eea',
                borderwidth=2,
                font=dict(color='#e0e0e0', size=12)
            ),
            plot_bgcolor='#1a1a1a',
            paper_bgcolor='#1a1a1a',
            margin=dict(l=50, r=50, t=80, b=50),
            dragmode='zoom',  # Default to zoom, users can select drawing tools
            newshape=dict(
                line=dict(color='cyan', width=3),
                fillcolor='rgba(0,255,255,0.2)',
                opacity=0.8
            ),
            # Store PPM for unit conversions in annotations
            meta={'ppm': ppm, 'origin_x': map_data.get('origin_x', 0), 'origin_y': map_data.get('origin_y', 0)}
        )
        
        # Add origin crosshair marker (blue crosshair at origin point)
        origin_x = map_data.get('origin_x', 0)
        origin_y = map_data.get('origin_y', 0)
        crosshair_size = 40
        
        # Horizontal line of origin crosshair
        fig.add_trace(go.Scatter(
            x=[origin_x - crosshair_size, origin_x + crosshair_size],
            y=[origin_y, origin_y],
            mode='lines',
            line=dict(color='#00bfff', width=3),  # Deep sky blue
            name='Origin',
            showlegend=True,
            hovertext=f"Origin: ({origin_x}, {origin_y})",
            hoverinfo='text'
        ))
        
        # Vertical line of origin crosshair
        fig.add_trace(go.Scatter(
            x=[origin_x, origin_x],
            y=[origin_y - crosshair_size, origin_y + crosshair_size],
            mode='lines',
            line=dict(color='#00bfff', width=3),  # Deep sky blue
            showlegend=False,
            hovertext=f"Origin: ({origin_x}, {origin_y})",
            hoverinfo='text'
        ))
        
        # Center dot of origin crosshair
        fig.add_trace(go.Scatter(
            x=[origin_x],
            y=[origin_y],
            mode='markers',
            marker=dict(size=12, color='#00bfff', line=dict(color='white', width=2)),
            name='Origin Point',
            showlegend=False,
            hovertext=f"Origin: ({origin_x}, {origin_y})",
            hoverinfo='text'
        ))
        
        # Create responsive Dash layout with dark theme
        app.layout = html.Div([
            # Header with title and utilities buttons
            html.Div([
                html.H1(f"MistHelper Map Viewer - {map_data.get('name', 'Map')}", 
                       style={'display': 'inline-block', 'marginRight': '30px', 'marginBottom': '0'}),
                html.Div([
                    html.Button(' Auto-Zone', id='auto-zone-btn', n_clicks=0,
                               style={'marginRight': '10px', 'padding': '8px 15px', 'backgroundColor': '#667eea', 
                                      'color': 'white', 'border': 'none', 'borderRadius': '4px', 'cursor': 'pointer', 'fontWeight': 'bold'}),
                    html.Button(' Add vBeacon', id='add-vbeacon-btn', n_clicks=0,
                               style={'marginRight': '10px', 'padding': '8px 15px', 'backgroundColor': '#3d3d3d', 
                                      'color': '#00ff00', 'border': '1px solid #00ff00', 'borderRadius': '4px', 'cursor': 'pointer'}),
                    html.Button(' Add Beacon', id='add-beacon-btn', n_clicks=0,
                               style={'marginRight': '10px', 'padding': '8px 15px', 'backgroundColor': '#3d3d3d', 
                                      'color': '#00bfff', 'border': '1px solid #00bfff', 'borderRadius': '4px', 'cursor': 'pointer'}),
                    html.Button(' Change Image', id='change-image-btn', n_clicks=0, 
                               style={'marginRight': '10px', 'padding': '8px 15px', 'backgroundColor': '#3d3d3d', 
                                      'color': '#e0e0e0', 'border': '1px solid #667eea', 'borderRadius': '4px', 'cursor': 'pointer'}),
                    html.Button(' Remove Image', id='remove-image-btn', n_clicks=0,
                               style={'marginRight': '10px', 'padding': '8px 15px', 'backgroundColor': '#3d3d3d', 
                                      'color': '#e0e0e0', 'border': '1px solid #667eea', 'borderRadius': '4px', 'cursor': 'pointer'}),
                    html.Button(' Rename', id='rename-btn', n_clicks=0,
                               style={'marginRight': '10px', 'padding': '8px 15px', 'backgroundColor': '#3d3d3d', 
                                      'color': '#e0e0e0', 'border': '1px solid #667eea', 'borderRadius': '4px', 'cursor': 'pointer'}),
                    html.Button(' Delete', id='delete-btn', n_clicks=0,
                               style={'padding': '8px 15px', 'backgroundColor': '#3d3d3d', 
                                      'color': '#ff4444', 'border': '1px solid #ff4444', 'borderRadius': '4px', 'cursor': 'pointer'}),
                    html.Div(id='utilities-status', style={'display': 'inline-block', 'marginLeft': '20px', 'color': '#a0a0ff', 'fontSize': '13px'})
                ], style={'display': 'inline-block', 'float': 'right'})
            ], style={'padding': '15px 20px', 'borderBottom': '2px solid #667eea', 'backgroundColor': '#2a2a2a'}),
            
            html.Div([
                # Map container - responsive
                html.Div([
                    dcc.Graph(
                        id='map-display', 
                        figure=fig, 
                        config={
                            'displayModeBar': True,
                            'displaylogo': False,
                            'modeBarButtonsToAdd': [
                                'drawline',
                                'drawopenpath',
                                'drawclosedpath',
                                'drawcircle',
                                'drawrect',
                                'eraseshape'
                            ],
                            'scrollZoom': True,
                            'editable': True,
                            'edits': {
                                'shapePosition': True,
                                'annotationPosition': True
                            },
                            'toImageButtonOptions': {
                                'format': 'png',
                                'filename': f"map_{map_data.get('name', 'export')}",
                                'height': 1080,
                                'width': 1920,
                                'scale': 2
                            }
                        },
                        style={'height': '100%', 'width': '100%'}
                    )
                ], className='map-container'),
                
                # Sidebar
                html.Div([
                    html.H3(" Layer Controls"),
                    html.H4("Infrastructure", style={'fontSize': '13px', 'color': '#667eea', 'marginTop': '10px', 'marginBottom': '5px'}),
                    dcc.Checklist(
                        id='layer-toggle',
                        options=[
                            {'label': '  Walls', 'value': 'walls'},
                            {'label': '   Wayfinding', 'value': 'wayfinding'},
                            {'label': '  Location Zones', 'value': 'zones'},
                            {'label': '  Proximity Zones', 'value': 'proximity_zones'},
                            {'label': '  Validation Paths', 'value': 'validation'},
                            {'label': '  RF Diagnostics Heatmap', 'value': 'rf_heatmap'},
                            {'label': '  Map Origin', 'value': 'origin'},
                        ],
                        value=['walls', 'wayfinding', 'zones', 'validation'],
                        labelStyle={'display': 'block', 'margin': '8px 0', 'fontSize': '13px'},
                        style={'marginBottom': '10px'}
                    ),
                    html.H4("Beacons & Positioning", style={'fontSize': '13px', 'color': '#667eea', 'marginBottom': '5px'}),
                    dcc.Checklist(
                        id='beacon-toggle',
                        options=[
                            {'label': '  Virtual Beacons', 'value': 'vbeacons'},
                            {'label': '  vBeacon Coverage', 'value': 'vbeacon_coverage'},
                            {'label': '  3rd Party Beacons', 'value': 'ble_beacons'},
                        ],
                        value=['vbeacons', 'ble_beacons'],
                        labelStyle={'display': 'block', 'margin': '8px 0', 'fontSize': '13px'},
                        style={'marginBottom': '10px'}
                    ),
                    html.H4("Clients", style={'fontSize': '13px', 'color': '#667eea', 'marginBottom': '5px'}),
                    dcc.Checklist(
                        id='client-toggle',
                        options=[
                            {'label': '  WiFi Clients', 'value': 'wifi_clients'},
                            {'label': '  Wired Clients', 'value': 'wired_clients'},
                            {'label': '  Excluded Clients', 'value': 'excluded_clients'},
                            {'label': '  Show Associated AP', 'value': 'show_client_ap'},
                        ],
                        value=['wifi_clients', 'wired_clients', 'show_client_ap'],
                        labelStyle={'display': 'block', 'margin': '8px 0', 'fontSize': '13px'},
                        style={'marginBottom': '10px'}
                    ),
                    html.H4("Devices", style={'fontSize': '13px', 'color': '#667eea', 'marginBottom': '5px'}),
                    dcc.Checklist(
                        id='device-toggle',
                        options=[
                            {'label': '  Access Points', 'value': 'aps'},
                            {'label': '  Switches', 'value': 'switches'},
                            {'label': '  Gateways', 'value': 'gateways'},
                            {'label': '  Mesh Associations', 'value': 'mesh_links'},
                        ],
                        value=['aps', 'switches', 'gateways'],
                        labelStyle={'display': 'block', 'margin': '8px 0', 'fontSize': '13px'},
                        style={'marginBottom': '10px'}
                    ),
                    html.H4("Filters", style={'fontSize': '13px', 'color': '#667eea', 'marginBottom': '5px'}),
                    dcc.Checklist(
                        id='filter-toggle',
                        options=[
                            {'label': '  Hide Inactive Items', 'value': 'hide_inactive'},
                        ],
                        value=[],
                        labelStyle={'display': 'block', 'margin': '8px 0', 'fontSize': '13px'},
                        style={'marginBottom': '10px'}
                    ),
                    html.Hr(),
                    html.H3(" Drawing Tools"),
                    html.P("Quick actions for map elements:", style={'fontSize': '12px', 'color': '#888', 'marginBottom': '10px'}),
                    html.Div([
                        html.Button(' Insert Path', id='insert-path-btn', n_clicks=0,
                                   style={'width': '100%', 'marginBottom': '8px', 'padding': '8px', 'backgroundColor': '#3d3d3d',
                                          'color': '#ff00ff', 'border': '1px solid #ff00ff', 'borderRadius': '4px', 'cursor': 'pointer', 'fontSize': '13px'}),
                        html.Button(' Insert Rectangle', id='insert-rect-btn', n_clicks=0,
                                   style={'width': '100%', 'marginBottom': '8px', 'padding': '8px', 'backgroundColor': '#3d3d3d',
                                          'color': '#00bfff', 'border': '1px solid #00bfff', 'borderRadius': '4px', 'cursor': 'pointer', 'fontSize': '13px'}),
                        html.Button(' Insert Wall', id='insert-wall-btn', n_clicks=0,
                                   style={'width': '100%', 'marginBottom': '8px', 'padding': '8px', 'backgroundColor': '#3d3d3d',
                                          'color': '#ff8800', 'border': '1px solid #ff8800', 'borderRadius': '4px', 'cursor': 'pointer', 'fontSize': '13px'}),
                        html.Button(' Delete all Paths', id='delete-paths-btn', n_clicks=0,
                                   style={'width': '100%', 'marginBottom': '8px', 'padding': '8px', 'backgroundColor': '#3d3d3d',
                                          'color': '#ff4444', 'border': '1px solid #ff4444', 'borderRadius': '4px', 'cursor': 'pointer', 'fontSize': '13px'}),
                        html.Button(' Delete all Walls', id='delete-walls-btn', n_clicks=0,
                                   style={'width': '100%', 'marginBottom': '8px', 'padding': '8px', 'backgroundColor': '#3d3d3d',
                                          'color': '#ff4444', 'border': '1px solid #ff4444', 'borderRadius': '4px', 'cursor': 'pointer', 'fontSize': '13px'}),
                        html.Div(id='drawing-tool-status', style={'fontSize': '11px', 'color': '#a0a0ff', 'marginTop': '8px'})
                    ]),
                    html.Hr(),
                    html.H3(" Measurement Tools"),
                    html.P("Use the toolbar above the map:", style={'fontSize': '12px', 'color': '#888'}),
                    html.P(" Draw Line - Measure distances", style={'fontSize': '11px', 'marginLeft': '10px', 'color': '#999'}),
                    html.P(" Draw Path - Create routes", style={'fontSize': '11px', 'marginLeft': '10px', 'color': '#999'}),
                    html.P(" Draw Circle - Mark areas", style={'fontSize': '11px', 'marginLeft': '10px', 'color': '#999'}),
                    html.P(" Erase - Remove drawings", style={'fontSize': '11px', 'marginLeft': '10px', 'color': '#999'}),
                    html.Hr(),
                    html.H3(" Set Scale"),
                    html.P("1. Draw a line of known length", style={'fontSize': '11px', 'color': '#888'}),
                    html.P("2. Enter actual length below", style={'fontSize': '11px', 'color': '#888'}),
                    html.Div([
                        dcc.Input(
                            id='scale-length-input',
                            type='number',
                            placeholder='Length in meters',
                            style={
                                'width': '100%',
                                'padding': '8px',
                                'marginBottom': '8px',
                                'backgroundColor': '#3d3d3d',
                                'color': '#e0e0e0',
                                'border': '1px solid #667eea',
                                'borderRadius': '4px'
                            }
                        ),
                        html.Button(
                            'Set Scale from Last Line',
                            id='set-scale-button',
                            style={
                                'width': '100%',
                                'padding': '8px',
                                'backgroundColor': '#667eea',
                                'color': 'white',
                                'border': 'none',
                                'borderRadius': '4px',
                                'cursor': 'pointer',
                                'fontWeight': 'bold'
                            }
                        ),
                        html.Div(id='scale-status', style={'marginTop': '8px', 'fontSize': '11px', 'color': '#a0a0ff'})
                    ]),
                    html.Hr(),
                    html.H3(" Set Origin"),
                    html.P("Click map to set coordinate origin", style={'fontSize': '11px', 'color': '#888'}),
                    html.Div([
                        html.Button(
                            'Enable Origin Setting Mode',
                            id='origin-mode-button',
                            n_clicks=0,
                            style={
                                'width': '100%',
                                'padding': '8px',
                                'marginBottom': '8px',
                                'backgroundColor': '#3d3d3d',
                                'color': 'white',
                                'border': '1px solid #667eea',
                                'borderRadius': '4px',
                                'cursor': 'pointer',
                                'fontWeight': 'bold'
                            }
                        ),
                        html.Div(id='origin-status', children=[
                            html.P(f"Current: ({map_data.get('origin_x', 0)}, {map_data.get('origin_y', 0)})", 
                                   style={'fontSize': '11px', 'color': '#888', 'margin': '4px 0'})
                        ])
                    ]),
                    html.Hr(),
                    html.H3(" Location Zones"),
                    html.Div([
                        dcc.Checklist(
                            id='zone-toggle',
                            options=[
                                {'label': f" {zone.get('name', f'Zone {i+1}')}", 'value': zone.get('id', f'zone_{i}')}
                                for i, zone in enumerate(zones)
                            ],
                            value=[zone.get('id', f'zone_{i}') for i, zone in enumerate(zones)],
                            labelStyle={'display': 'block', 'margin': '8px 0', 'fontSize': '13px', 'color': '#e0e0e0'},
                            style={'marginBottom': '15px'}
                        ) if zones else html.P("No zones on this map", style={'color': '#888', 'fontSize': '12px', 'fontStyle': 'italic'}),
                        html.Div(id='selected-zone-info', children=[
                            html.P("Click a zone for details", style={'fontSize': '11px', 'color': '#888', 'fontStyle': 'italic'})
                        ], style={'padding': '10px', 'backgroundColor': '#3d3d3d', 'borderRadius': '4px', 'marginTop': '10px'}),
                        html.Div([
                            html.Button(' Edit Zone', id='edit-zone-btn', n_clicks=0,
                                       style={'width': '48%', 'marginRight': '4%', 'padding': '6px', 'backgroundColor': '#667eea',
                                              'color': 'white', 'border': 'none', 'borderRadius': '4px', 'cursor': 'pointer', 'fontSize': '12px'}),
                            html.Button(' Remove Zone', id='remove-zone-btn', n_clicks=0,
                                       style={'width': '48%', 'padding': '6px', 'backgroundColor': '#ff4444',
                                              'color': 'white', 'border': 'none', 'borderRadius': '4px', 'cursor': 'pointer', 'fontSize': '12px'})
                        ], style={'marginTop': '10px', 'display': 'flex'}) if zones else None
                    ]),
                    html.Hr(),
                    html.H3(" Map Info"),
                    html.Div(id='map-info', children=[
                        html.P([html.Span("Dimensions: ", className='info-badge'), f"{map_width}  {map_height} px"]),
                        html.P([html.Span("PPM: ", className='info-badge'), f"{map_data.get('ppm', 'N/A')}"]),
                        html.P([html.Span("Orientation: ", className='info-badge'), f"{map_data.get('orientation', 0)}"]),
                        html.P([html.Span("Devices: ", className='info-badge'), f"{len(devices)}"]),
                        html.P([html.Span("Clients: ", className='info-badge'), f"{len(clients)}"]),
                        html.P([html.Span("Zones: ", className='info-badge'), f"{len(zones)}"]),
                        html.P([html.Span("vBeacons: ", className='info-badge'), f"{len(map_data.get('vbeacons', []))}"]),
                        html.P([html.Span("BLE Beacons: ", className='info-badge'), f"{len(map_data.get('beacons', []))}"]),
                        html.P([html.Span("Validation Paths: ", className='info-badge'), 
                               f"{len(map_data.get('sitesurvey_path', []))}"])
                    ]),
                    html.Hr(),
                    html.Div(id='click-data', children=[
                        html.H3(" Device Info"),
                        html.P("Click a device for details", style={'color': '#888', 'fontStyle': 'italic'})
                    ])
                ], className='sidebar')
            ], className='main-container')
        ], style={'height': '100vh', 'display': 'flex', 'flexDirection': 'column'})
        
        # Callback for layer toggle
        @app.callback(
            Output('map-display', 'figure'),
            [Input('layer-toggle', 'value'),
             Input('beacon-toggle', 'value'),
             Input('client-toggle', 'value'),
             Input('device-toggle', 'value'),
             Input('filter-toggle', 'value')],
            State('map-display', 'figure')
        )
        def toggle_layers(infra_layers, beacon_layers, client_layers, device_layers, filter_layers, current_fig):
            # Combine all layer selections
            all_layers = (infra_layers or []) + (beacon_layers or []) + (client_layers or []) + (device_layers or []) + (filter_layers or [])
            
            # Toggle traces (markers, lines, shapes)
            for trace in current_fig['data']:
                trace_name = trace.get('name', '').lower()
                
                # Infrastructure
                if 'wall' in trace_name:
                    trace['visible'] = 'walls' in all_layers
                elif 'wayfinding' in trace_name:
                    trace['visible'] = 'wayfinding' in all_layers
                elif 'zone' in trace_name:
                    trace['visible'] = 'zones' in all_layers
                elif 'validation' in trace_name:
                    trace['visible'] = 'validation' in all_layers
                elif 'rf coverage' in trace_name:
                    trace['visible'] = 'rf_heatmap' in all_layers
                elif 'map origin' in trace_name:
                    trace['visible'] = 'origin' in all_layers
                    
                # Beacons
                elif 'vbeacon' in trace_name or 'virtual beacon' in trace_name:
                    trace['visible'] = 'vbeacons' in all_layers
                elif 'ble beacon' in trace_name or trace_name.startswith('beacon '):
                    trace['visible'] = 'ble_beacons' in all_layers
                    
                # Clients (with WiFi/Wired filtering)
                elif 'wifi client' in trace_name:
                    trace['visible'] = 'wifi_clients' in all_layers
                elif 'wired client' in trace_name:
                    trace['visible'] = 'wired_clients' in all_layers
                elif 'client' in trace_name:  # Generic clients (fallback)
                    trace['visible'] = ('wifi_clients' in all_layers or 'wired_clients' in all_layers)
                elif 'client-ap link' in trace_name:
                    trace['visible'] = 'show_client_ap' in all_layers
                    
                # Mesh links
                elif 'mesh link' in trace_name:
                    trace['visible'] = 'mesh_links' in all_layers
                    
                # Beacon coverage
                elif 'vbeacon coverage' in trace_name:
                    trace['visible'] = 'vbeacon_coverage' in all_layers
                    
                # Devices and their orientation indicators
                elif 'ap' in trace_name or 'access point' in trace_name:
                    trace['visible'] = 'aps' in all_layers
                elif 'switch' in trace_name:
                    trace['visible'] = 'switches' in all_layers
                elif 'gateway' in trace_name:
                    trace['visible'] = 'gateways' in all_layers
            
            # Toggle annotations (text labels)
            for annotation in current_fig.get('layout', {}).get('annotations', []):
                annotation_name = annotation.get('name', '').lower()
                
                # Zone labels
                if 'zone label' in annotation_name:
                    annotation['visible'] = 'zones' in all_layers
                    
                # Device labels
                elif 'access points label' in annotation_name:
                    annotation['visible'] = 'aps' in all_layers
                elif 'switches label' in annotation_name:
                    annotation['visible'] = 'switches' in all_layers
                elif 'gateways label' in annotation_name:
                    annotation['visible'] = 'gateways' in all_layers
                    
                # Client labels
                elif 'wifi clients label' in annotation_name:
                    annotation['visible'] = 'wifi_clients' in all_layers
                elif 'wired clients label' in annotation_name:
                    annotation['visible'] = 'wired_clients' in all_layers
                elif 'clients label' in annotation_name:  # Generic clients (fallback)
                    annotation['visible'] = ('wifi_clients' in all_layers or 'wired_clients' in all_layers)
                    
                # Beacon labels
                elif 'virtual beacons label' in annotation_name:
                    annotation['visible'] = 'vbeacons' in all_layers
                elif 'ble beacons label' in annotation_name:
                    annotation['visible'] = 'ble_beacons' in all_layers
            
            return current_fig
        
        # Callback for click events - enhanced device details display
        @app.callback(
            Output('click-data', 'children'),
            Input('map-display', 'clickData')
        )
        def display_click_data(clickData):
            if clickData is None:
                return [
                    html.H3(" Device Info"),
                    html.P("Click a device for details", style={'color': '#888', 'fontStyle': 'italic'})
                ]
            
            point = clickData['points'][0]
            hover_text = point.get('hovertext', '')
            
            # Parse hover text to extract device info
            details = []
            if hover_text:
                lines = hover_text.split('<br>')
                for line in lines:
                    if line.strip():
                        details.append(html.P(line.replace('<b>', '').replace('</b>', ''), 
                                            className='device-detail' if 'Type:' in line else None))
            
            return [
                html.H3(" Device Details"),
                html.Div(details if details else [html.P("No device data available")])
            ]
        
        # Callback to add multi-unit labels to drawn shapes
        @app.callback(
            Output('map-display', 'figure', allow_duplicate=True),
            Input('map-display', 'relayoutData'),
            State('map-display', 'figure'),
            prevent_initial_call=True
        )
        def update_shape_labels(relayoutData, current_fig):
            """Add multi-unit measurement labels to drawn shapes"""
            if not relayoutData:
                return current_fig
            
            # Get current PPM from figure metadata (may have been updated by user)
            current_ppm = current_fig.get('layout', {}).get('meta', {}).get('ppm', ppm)
            
            # Check if a new shape was added
            shapes = current_fig.get('layout', {}).get('shapes', [])
            if shapes and len(shapes) > 0:
                # Get the last shape (newly drawn)
                for idx, shape in enumerate(shapes):
                    if shape.get('type') == 'line':
                        # Calculate length in pixels
                        x0, y0 = shape.get('x0', 0), shape.get('y0', 0)
                        x1, y1 = shape.get('x1', 0), shape.get('y1', 0)
                        length_px = ((x1 - x0)**2 + (y1 - y0)**2)**0.5
                        
                        # Convert to meters and feet using current PPM
                        length_m = length_px / current_ppm if current_ppm > 0 else 0
                        length_ft = length_m * 3.28084
                        
                        # Create annotation with multi-unit label
                        annotation = dict(
                            x=(x0 + x1) / 2,
                            y=(y0 + y1) / 2,
                            text=f"<b>{length_px:.1f} px</b><br>{length_ft:.2f} ft<br>{length_m:.2f} m",
                            showarrow=False,
                            font=dict(size=12, color='cyan', family='Arial Black'),
                            bgcolor='rgba(0,0,0,0.7)',
                            bordercolor='cyan',
                            borderwidth=2,
                            borderpad=4
                        )
                        
                        # Add to annotations
                        if 'annotations' not in current_fig['layout']:
                            current_fig['layout']['annotations'] = []
                        current_fig['layout']['annotations'].append(annotation)
            
            return current_fig
        
        # Callback to set scale from user input
        @app.callback(
            [Output('scale-status', 'children'),
             Output('map-display', 'figure', allow_duplicate=True)],
            Input('set-scale-button', 'n_clicks'),
            [State('scale-length-input', 'value'),
             State('map-display', 'figure')],
            prevent_initial_call=True
        )
        def set_scale(n_clicks, actual_length_m, current_fig):
            """Calculate and update PPM based on drawn line and known length"""
            if not n_clicks or not actual_length_m or actual_length_m <= 0:
                return " Please enter a valid length in meters", current_fig
            
            # Find the last line shape
            shapes = current_fig.get('layout', {}).get('shapes', [])
            last_line = None
            for shape in reversed(shapes):
                if shape.get('type') == 'line':
                    last_line = shape
                    break
            
            if not last_line:
                return " Please draw a line first using the ruler tool", current_fig
            
            # Calculate line length in pixels
            x0, y0 = last_line.get('x0', 0), last_line.get('y0', 0)
            x1, y1 = last_line.get('x1', 0), last_line.get('y1', 0)
            length_px = ((x1 - x0)**2 + (y1 - y0)**2)**0.5
            
            # Calculate new PPM
            new_ppm = length_px / actual_length_m
            
            # Update PPM in figure metadata
            if 'meta' not in current_fig['layout']:
                current_fig['layout']['meta'] = {}
            current_fig['layout']['meta']['ppm'] = new_ppm
            
            # Update all existing measurement annotations with new PPM
            if 'annotations' in current_fig['layout']:
                for ann_idx, annotation in enumerate(current_fig['layout']['annotations']):
                    # Check if this is a measurement annotation (has px/ft/m format)
                    if 'px' in annotation.get('text', ''):
                        # Find corresponding shape
                        for shape_idx, shape in enumerate(shapes):
                            if shape.get('type') == 'line':
                                sx0, sy0 = shape.get('x0', 0), shape.get('y0', 0)
                                sx1, sy1 = shape.get('x1', 0), shape.get('y1', 0)
                                shape_px = ((sx1 - sx0)**2 + (sy1 - sy0)**2)**0.5
                                shape_m = shape_px / new_ppm
                                shape_ft = shape_m * 3.28084
                                
                                # Update annotation text
                                current_fig['layout']['annotations'][ann_idx]['text'] = (
                                    f"<b>{shape_px:.1f} px</b><br>{shape_ft:.2f} ft<br>{shape_m:.2f} m"
                                )
                                break
            
            status_msg = f" Scale set! New PPM: {new_ppm:.2f} ({actual_length_m:.2f}m = {length_px:.1f}px)"
            logging.info(f"Map scale updated: PPM {ppm}  {new_ppm:.2f} (user calibration: {actual_length_m}m)")
            
            return status_msg, current_fig
        
        # Callback to handle origin setting mode
        @app.callback(
            Output('origin-mode-button', 'style'),
            Input('origin-mode-button', 'n_clicks'),
            State('origin-mode-button', 'style'),
            prevent_initial_call=True
        )
        def toggle_origin_mode(n_clicks, current_style):
            """Toggle origin setting mode on/off with visual feedback"""
            if n_clicks % 2 == 1:  # Odd clicks = mode active
                current_style['backgroundColor'] = '#667eea'
                current_style['border'] = '2px solid #00bfff'
                return current_style
            else:  # Even clicks = mode inactive
                current_style['backgroundColor'] = '#3d3d3d'
                current_style['border'] = '1px solid #667eea'
                return current_style
        
        # Callback to set origin from map click
        @app.callback(
            [Output('origin-status', 'children'),
             Output('map-display', 'figure', allow_duplicate=True)],
            Input('map-display', 'clickData'),
            [State('origin-mode-button', 'n_clicks'),
             State('map-display', 'figure')],
            prevent_initial_call=True
        )
        def set_origin_from_click(clickData, mode_clicks, current_fig):
            """Set origin point when map is clicked in origin-setting mode"""
            # Check if origin mode is active (odd number of clicks)
            if not mode_clicks or mode_clicks % 2 == 0:
                # Mode not active - return current status
                current_origin_x = current_fig.get('layout', {}).get('meta', {}).get('origin_x', 0)
                current_origin_y = current_fig.get('layout', {}).get('meta', {}).get('origin_y', 0)
                return [html.P(f"Current: ({current_origin_x}, {current_origin_y})", 
                              style={'fontSize': '11px', 'color': '#888', 'margin': '4px 0'})], current_fig
            
            if not clickData:
                return [html.P("Click map to set origin", 
                              style={'fontSize': '11px', 'color': '#ff8800', 'margin': '4px 0'})], current_fig
            
            # Get clicked coordinates
            point = clickData['points'][0]
            new_origin_x = point['x']
            new_origin_y = point['y']
            
            # Update origin in figure metadata
            if 'meta' not in current_fig['layout']:
                current_fig['layout']['meta'] = {}
            current_fig['layout']['meta']['origin_x'] = new_origin_x
            current_fig['layout']['meta']['origin_y'] = new_origin_y
            
            # Find and update origin crosshair traces
            crosshair_size = 40
            for trace in current_fig['data']:
                if trace.get('name') == 'Origin':
                    # Update horizontal line
                    trace['x'] = [new_origin_x - crosshair_size, new_origin_x + crosshair_size]
                    trace['y'] = [new_origin_y, new_origin_y]
                    trace['hovertext'] = f"Origin: ({new_origin_x:.1f}, {new_origin_y:.1f})"
                elif trace.get('name') == 'Origin Point':
                    # Update center dot
                    trace['x'] = [new_origin_x]
                    trace['y'] = [new_origin_y]
                    trace['hovertext'] = f"Origin: ({new_origin_x:.1f}, {new_origin_y:.1f})"
                elif 'hovertext' in trace and 'Origin:' in str(trace.get('hovertext', '')):
                    # Update vertical line (no name but has Origin hovertext)
                    if trace.get('mode') == 'lines' and trace.get('showlegend') == False:
                        trace['x'] = [new_origin_x, new_origin_x]
                        trace['y'] = [new_origin_y - crosshair_size, new_origin_y + crosshair_size]
                        trace['hovertext'] = f"Origin: ({new_origin_x:.1f}, {new_origin_y:.1f})"
            
            status = [
                html.P(f" Origin set: ({new_origin_x:.1f}, {new_origin_y:.1f})", 
                      style={'fontSize': '11px', 'color': '#00ff00', 'margin': '4px 0'}),
                html.P("Click button again to exit mode", 
                      style={'fontSize': '10px', 'color': '#888', 'margin': '4px 0'})
            ]
            
            logging.info(f"Map origin updated to ({new_origin_x:.1f}, {new_origin_y:.1f})")
            return status, current_fig
        
        # Callback to handle drawing tool button actions
        @app.callback(
            Output('drawing-tool-status', 'children'),
            [Input('insert-path-btn', 'n_clicks'),
             Input('insert-rect-btn', 'n_clicks'),
             Input('insert-wall-btn', 'n_clicks'),
             Input('delete-paths-btn', 'n_clicks'),
             Input('delete-walls-btn', 'n_clicks')],
            prevent_initial_call=True
        )
        def handle_drawing_tools(path_clicks, rect_clicks, wall_clicks, del_path_clicks, del_wall_clicks):
            """Handle drawing tool button clicks"""
            ctx = dash.callback_context
            if not ctx.triggered:
                return ""
            
            button_id = ctx.triggered[0]['prop_id'].split('.')[0]
            
            if button_id == 'insert-path-btn':
                msg = " Use 'Draw Path' tool in toolbar above map to create validation paths"
                logging.info(f"Drawing tool: Insert path requested for map {map_id}")
                return html.Span(msg, style={'color': '#ff00ff'})
            
            elif button_id == 'insert-rect-btn':
                msg = " Use 'Draw Rectangle' tool in toolbar above map to create zones"
                logging.info(f"Drawing tool: Insert rectangle requested for map {map_id}")
                return html.Span(msg, style={'color': '#00bfff'})
            
            elif button_id == 'insert-wall-btn':
                msg = " Use 'Draw Path' tool in toolbar to create wall segments"
                logging.info(f"Drawing tool: Insert wall requested for map {map_id}")
                return html.Span(msg, style={'color': '#ff8800'})
            
            elif button_id == 'delete-paths-btn':
                msg = " Delete all paths: Use Mist API updateSiteMap - removes sitesurvey_path array"
                logging.warning(f"Drawing tool: Delete all paths requested for map {map_id}")
                return html.Span(msg, style={'color': '#ff4444', 'fontWeight': 'bold'})
            
            elif button_id == 'delete-walls-btn':
                msg = " Delete all walls: Use Mist API updateSiteMap - removes wall_path data"
                logging.warning(f"Drawing tool: Delete all walls requested for map {map_id}")
                return html.Span(msg, style={'color': '#ff4444', 'fontWeight': 'bold'})
            
            return ""
        
        # Callback to handle utilities button actions
        @app.callback(
            Output('utilities-status', 'children'),
            [Input('auto-zone-btn', 'n_clicks'),
             Input('change-image-btn', 'n_clicks'),
             Input('remove-image-btn', 'n_clicks'),
             Input('rename-btn', 'n_clicks'),
             Input('delete-btn', 'n_clicks')],
            prevent_initial_call=True
        )
        def handle_utilities(auto_zone_clicks, change_clicks, remove_clicks, rename_clicks, delete_clicks):
            """Handle utilities button clicks"""
            ctx = dash.callback_context
            if not ctx.triggered:
                return ""
            
            button_id = ctx.triggered[0]['prop_id'].split('.')[0]
            
            if button_id == 'auto-zone-btn':
                msg = " Auto-Zone: AI-powered zone detection - analyzes walls and creates location zones automatically"
                logging.info(f"Utilities: Auto-Zone requested for map {map_id}")
                return html.Span(msg, style={'color': '#667eea', 'fontWeight': 'bold'})
            
            elif button_id == 'change-image-btn':
                msg = " Change Image: Use Mist API updateSiteMapImage - feature requires file upload"
                logging.info(f"Utilities: Change Image requested for map {map_id}")
                return html.Span(msg, style={'color': '#ff8800'})
            
            elif button_id == 'remove-image-btn':
                msg = " Remove Image: Use Mist API deleteSiteMapImage - DESTRUCTIVE operation"
                logging.warning(f"Utilities: Remove Image requested for map {map_id}")
                return html.Span(msg, style={'color': '#ff4444'})
            
            elif button_id == 'rename-btn':
                msg = " Rename: Use Mist API updateSiteMap with new name - requires text input"
                logging.info(f"Utilities: Rename requested for map {map_id}")
                return html.Span(msg, style={'color': '#ff8800'})
            
            elif button_id == 'delete-btn':
                msg = " Delete Floorplan: Use Mist API deleteSiteMap - DESTRUCTIVE! Requires confirmation"
                logging.warning(f"Utilities: Delete requested for map {map_id}")
                return html.Span(msg, style={'color': '#ff0000', 'fontWeight': 'bold'})
            
            return ""
        
        # Callback to handle zone-specific toggles
        @app.callback(
            Output('map-display', 'figure', allow_duplicate=True),
            Input('zone-toggle', 'value'),
            State('map-display', 'figure'),
            prevent_initial_call=True
        )
        def toggle_individual_zones(selected_zone_ids, current_fig):
            """Show/hide individual zones based on checklist"""
            if not zones:
                return current_fig
            
            # Create set of selected IDs for fast lookup
            selected_set = set(selected_zone_ids) if selected_zone_ids else set()
            
            # Update visibility for each zone trace
            for trace in current_fig['data']:
                trace_name = trace.get('name', '')
                if trace_name.startswith('Zone:'):
                    # Extract zone name from trace name
                    zone_name = trace_name.replace('Zone: ', '')
                    # Find matching zone
                    for i, zone in enumerate(zones):
                        if zone.get('name') == zone_name:
                            zone_id = zone.get('id', f'zone_{i}')
                            trace['visible'] = zone_id in selected_set
                            break
            
            return current_fig
        
        # Callback for zone edit/remove buttons
        @app.callback(
            Output('selected-zone-info', 'children'),
            [Input('edit-zone-btn', 'n_clicks'),
             Input('remove-zone-btn', 'n_clicks'),
             Input('map-display', 'clickData')],
            prevent_initial_call=True
        )
        def handle_zone_actions(edit_clicks, remove_clicks, clickData):
            """Handle zone edit/remove and display selected zone info"""
            ctx = dash.callback_context
            if not ctx.triggered:
                return html.P("Click a zone for details", style={'fontSize': '11px', 'color': '#888', 'fontStyle': 'italic'})
            
            trigger_id = ctx.triggered[0]['prop_id'].split('.')[0]
            
            if trigger_id == 'edit-zone-btn':
                logging.info(f"Zone management: Edit zone requested for map {map_id}")
                return html.Div([
                    html.P(" Edit Zone: Use Mist API updateSiteMap", 
                          style={'fontSize': '11px', 'color': '#667eea', 'fontWeight': 'bold'}),
                    html.P("Modify zone vertices via API call", 
                          style={'fontSize': '10px', 'color': '#888'})
                ])
            
            elif trigger_id == 'remove-zone-btn':
                logging.warning(f"Zone management: Remove zone requested for map {map_id}")
                return html.Div([
                    html.P(" Remove Zone: DESTRUCTIVE operation", 
                          style={'fontSize': '11px', 'color': '#ff4444', 'fontWeight': 'bold'}),
                    html.P("Deletes zone via Mist API updateSiteMap", 
                          style={'fontSize': '10px', 'color': '#888'})
                ])
            
            elif trigger_id == 'map-display' and clickData:
                # Check if clicked on a zone
                point = clickData['points'][0]
                hover_text = point.get('hovertext', '')
                
                if 'Zone:' in hover_text:
                    zone_name = hover_text.split('Zone: ')[1] if 'Zone: ' in hover_text else 'Unknown'
                    return html.Div([
                        html.P(f" Selected: {zone_name}", 
                              style={'fontSize': '12px', 'color': '#00ff00', 'fontWeight': 'bold', 'marginBottom': '5px'}),
                        html.P(f"Clients: None", 
                              style={'fontSize': '10px', 'color': '#888'})
                    ])
            
            return html.P("Click a zone for details", style={'fontSize': '11px', 'color': '#888', 'fontStyle': 'italic'})
        
        # Determine host binding - use 0.0.0.0 in containers for external access
        dash_host = '0.0.0.0' if is_running_in_container() else '127.0.0.1'
        dash_port = int(os.getenv('DASH_PORT', '8050'))
        
        print("\nStarting Dash server...")
        if is_running_in_container():
            print(f"! Map viewer available at http://<container-ip>:{dash_port}")
            print("! Access from host: http://localhost:8050 (if port is mapped)")
        else:
            print("! Map viewer will open in your default browser")
        print("! Press Ctrl+C to stop the server\n")
        
        logging.info(f"Starting Dash server on http://{dash_host}:{dash_port}")
        
        # Open browser automatically (skip in container - no display)
        if not is_running_in_container():
            import webbrowser
            import threading
            import time
            
            def open_browser():
                """Wait for server to start, then open browser"""
                time.sleep(1.5)  # Wait for Dash server to initialize
                webbrowser.open(f'http://127.0.0.1:{dash_port}')
                logging.debug(f"Browser opened to http://127.0.0.1:{dash_port}")
            
            # Start browser opening in background thread
            threading.Thread(target=open_browser, daemon=True).start()
        
        try:
            app.run(debug=False, port=dash_port, host=dash_host)
        except KeyboardInterrupt:
            print("\n\nMap viewer stopped by user")
            logging.info("Interactive map viewer stopped by user (Ctrl+C)")
        except Exception as e:
            logging.error(f"Error running Dash server: {e}", exc_info=True)
            print(f"\n! Error running map viewer: {e}")
    
    def _create_static_plotly_map(self, map_data, devices):
        """Create static Plotly HTML map when Dash is not available"""
        import plotly.graph_objects as go
        from math import cos, sin, radians
        import webbrowser
        import os
        import tempfile
        
        print("\n! Creating static HTML map...")
        
        # Similar to _launch_plotly_viewer but save to HTML file
        fig = go.Figure()
        
        map_width = map_data.get('width', 1000)
        map_height = map_data.get('height', 1000)
        
        if 'url' in map_data:
            fig.add_layout_image(
                source=map_data['url'],
                x=0, y=map_height,
                sizex=map_width, sizey=map_height,
                xref="x", yref="y",
                sizing="stretch",
                layer="below"
            )
        
        # Add devices (simplified version)
        if devices:
            x_coords = [d.get('x', 0) for d in devices if 'x' in d]
            y_coords = [map_height - d.get('y', 0) for d in devices if 'y' in d]
            names = [d.get('name', d.get('mac', 'Unknown')) for d in devices if 'x' in d]
            
            fig.add_trace(go.Scatter(
                x=x_coords, y=y_coords,
                mode='markers+text',
                name='Devices',
                marker=dict(size=10, color='green'),
                text=names,
                textposition='top center'
            ))
        
        fig.update_layout(
            title=f"Map: {map_data.get('name', 'Unnamed')}",
            xaxis=dict(range=[0, map_width]),
            yaxis=dict(range=[0, map_height], scaleanchor="x", scaleratio=1),
            height=800
        )
        
        # Save to temp HTML file
        temp_html = os.path.join(tempfile.gettempdir(), f"mist_map_{map_data.get('id', 'unknown')[:8]}.html")
        logging.debug(f"Saving static map to: {temp_html}")
        fig.write_html(temp_html)
        
        print(f"\n! Map saved to: {temp_html}")
        print("! Opening in browser...")
        logging.info(f"Static HTML map created: {temp_html}")
        webbrowser.open(f"file://{temp_html}")
        logging.debug("Browser launched with static map")
    
    def _launch_matplotlib_viewer(self, map_data, devices):
        """Fallback matplotlib viewer (view-only)"""
        logging.info(f"_launch_matplotlib_viewer called - basic fallback mode")
        import matplotlib.pyplot as plt
        import matplotlib.patches as patches
        from matplotlib.patches import FancyArrow
        from math import cos, sin, radians
        
        print("\n! Using matplotlib viewer (view-only, no interactivity)")
        logging.debug("Creating matplotlib figure for basic visualization")
        
        fig, ax = plt.subplots(figsize=(12, 10))
        
        map_width = map_data.get('width', 1000)
        map_height = map_data.get('height', 1000)
        
        ax.set_xlim(0, map_width)
        ax.set_ylim(0, map_height)
        ax.set_aspect('equal')
        ax.set_title(f"Map: {map_data.get('name', 'Unnamed')}")
        ax.set_xlabel("X (pixels)")
        ax.set_ylabel("Y (pixels)")
        
        # Plot devices
        for device in devices:
            if 'x' not in device or 'y' not in device:
                continue
            
            x, y = device['x'], device['y']
            device_type = device.get('type', 'unknown')
            name = device.get('name', device.get('mac', 'Unknown'))
            orientation = device.get('orientation', 0)
            
            # Color by type
            color = {'ap': 'green', 'switch': 'orange', 'gateway': 'purple'}.get(device_type, 'gray')
            
            # Plot device
            ax.plot(x, y, marker='o', markersize=10, color=color, label=device_type if device_type not in ax.get_legend_handles_labels()[1] else "")
            ax.text(x, y + 20, name, fontsize=8, ha='center')
            
            # Add orientation arrow
            if orientation != 0:
                arrow_length = 30
                dx = arrow_length * cos(radians(orientation))
                dy = arrow_length * sin(radians(orientation))
                ax.arrow(x, y, dx, dy, head_width=10, head_length=10, fc=color, ec=color, alpha=0.7)
        
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        print("\n! Displaying map... Close window to return to menu")
        logging.info("Displaying matplotlib figure (blocking until window closed)")
        plt.show()
        logging.info("Matplotlib map viewer closed by user")


class FirmwareManager:
    """
    Advanced Firmware Management System for Mist Access Points
    
    This class provides comprehensive firmware upgrade capabilities including:
    1. Firmware status monitoring and reporting
    2. Site-based bulk firmware upgrades  
    3. Gateway template-based firmware upgrades
    4. Automatic site auto-upgrade configuration
    5. Multi-strategy upgrade orchestration (big_bang, canary, rrm, serial)
    6. Progress monitoring and audit logging
    
    Follows NASA/JPL coding standards for safety-critical operations with:
    - Comprehensive validation and error handling
    - Explicit user confirmation for destructive operations
    - Complete audit trails and logging
    - Rollback and recovery capabilities
    """
    
    def __init__(self, apisession, org_id):
        """
        Initialize FirmwareManager with API session and organization context.
        
        Args:
            apisession: Authenticated Mist API session
            org_id: Organization ID for operations
        """
        self.apisession = apisession
        self.org_id = org_id
        logging.info("FirmwareManager initialized for org_id: {}".format(org_id))
    
    def _is_firmware_downgrade(self, current_version, target_version):
        """
        Check if the target version is a downgrade from the current version.
        
        This method performs a basic version comparison to detect potential downgrades.
        SSR firmware versions typically follow patterns like: 6.3.4-7.r2, 6.3.5-37.sts
        
        Args:
            current_version (str): Current firmware version
            target_version (str): Target firmware version
            
        Returns:
            bool: True if target_version appears to be older than current_version
        """
        try:
            # Handle empty versions
            if not current_version or not target_version:
                return False
            
            # Extract major.minor.patch from versions like "6.3.4-7.r2" or "6.3.5-37.sts"
            current_parts = current_version.split('-')[0].split('.')
            target_parts = target_version.split('-')[0].split('.')
            
            # Pad shorter version to same length
            max_len = max(len(current_parts), len(target_parts))
            while len(current_parts) < max_len:
                current_parts.append('0')
            while len(target_parts) < max_len:
                target_parts.append('0')
            
            # Compare version parts numerically
            for current_part, target_part in zip(current_parts, target_parts):
                try:
                    current_num = int(current_part)
                    target_num = int(target_part)
                    
                    if target_num < current_num:
                        return True  # Downgrade detected
                    elif target_num > current_num:
                        return False  # Upgrade
                    # If equal, continue to next part
                except ValueError:
                    # Non-numeric parts, fall back to string comparison
                    if target_part < current_part:
                        return True
                    elif target_part > current_part:
                        return False
            
            # Versions are equal
            return False
            
        except Exception as e:
            logging.warning(f"Could not compare versions {current_version} vs {target_version}: {e}")
            # If we can't compare, err on the side of caution and allow the upgrade
            return False
    
    def check_firmware_upgrade_status(self, scope_choice=None, site_filter=None):
        """
        Check current firmware upgrade status across the organization.
        
        This method provides comprehensive upgrade status monitoring with:
        1. Device-level firmware status from device statistics (fwupdate field)
        2. Site-level upgrade operations and history
        3. Organization-wide upgrade tracking
        4. Current version vs. available version comparison
        5. Upgrade progress monitoring for active operations
        6. Failed upgrade identification and retry status
        7. Bulk status export to CSV for analysis
        8. Interactive site/device filtering options
        
        Args:
            scope_choice: Optional pre-selected scope (1-4)
            site_filter: Optional pre-selected site ID
            
        Reports include:
        - Current firmware versions and upgrade status per device
        - Active upgrade operations with progress tracking
        - Failed upgrades with error details and retry information
        - Upgrade history and completion statistics
        - Version mismatch identification across sites
        """
        logging.info("Starting firmware upgrade status check...")
        logging.debug("FirmwareManager.check_firmware_upgrade_status() initiated")
        
        print(" Firmware Upgrade Status Check")
        print("=" * 60)
        
        # Step 1: Choose scope (organization-wide or specific site) if not provided
        if scope_choice is None:
            print("\n  Select status check scope:")
            print("   [1] Organization-wide status (all sites and devices)")
            print("   [2] Specific site status")
            print("   [3] Active upgrade operations only")
            print("   [4] Failed upgrades only")
            print("   [5] Continuous monitoring mode (auto-refresh until complete)")
            
            while True:
                try:
                    scope_choice = input("Select scope (1-5): ").strip()
                    if scope_choice in ['1', '2', '3', '4', '5']:
                        logging.debug(f"User selected scope: {scope_choice}")
                        break
                    else:
                        print(" Invalid selection. Please choose 1-5.")
                        logging.debug(f"Invalid scope selection: {scope_choice}")
                except KeyboardInterrupt:
                    print("\n Operation cancelled by user.")
                    return
        
        if scope_choice == '2' and site_filter is None:
            # Get specific site selection
            logging.debug("User selected specific site mode")
            site_filter = prompt_site_selection()
            if not site_filter:
                print(" No site selected. Exiting.")
                logging.warning("No site selected in specific site mode")
                return
            logging.debug(f"Selected site filter: {site_filter}")
        
        # Handle monitoring mode (option 5)
        if scope_choice == '5':
            logging.info("Entering continuous monitoring mode")
            return self._continuous_monitoring_mode(site_filter)
        
        # Continue with the existing implementation...
        return self._execute_status_check(scope_choice, site_filter)
    
    def _continuous_monitoring_mode(self, site_filter=None):
        """
        Continuous monitoring mode that auto-refreshes upgrade status until complete or cancelled.
        
        Features:
        - Auto-refresh every 7 seconds with full device scan each iteration
        - Clear screen between refreshes
        - Show only devices actively upgrading
        - Detects new devices that start upgrading after monitoring begins
        - Exit automatically when all upgrades complete
        - Press Ctrl+C to exit at any time
        
        Note: Each refresh queries ALL devices (not just initial set), so new upgrades
        started after monitoring begins will be detected and displayed.
        
        Args:
            site_filter: Optional site ID to filter monitoring
        """
        import os
        import platform
        
        print("\n  Continuous Monitoring Mode")
        print("=" * 70)
        print("   Monitoring active firmware upgrades...")
        print("   Press Ctrl+C to exit at any time")
        print("   Auto-refreshing every 7 seconds")
        print("   NOTE: Each refresh scans ALL devices for active upgrades")
        print("=" * 70)
        
        logging.info("Starting continuous monitoring mode with 7-second refresh interval")
        iteration = 0
        
        try:
            while True:
                iteration += 1
                
                # Clear screen for cleaner display (platform-specific)
                if platform.system() == "Windows":
                    os.system('cls')
                else:
                    os.system('clear')
                
                # Display header
                print("\n  Firmware Upgrade Monitoring - Live View")
                print("=" * 70)
                print(f"   Refresh #{iteration} | Press Ctrl+C to exit")
                print(f"   Scanning all devices for active upgrades...")
                print("=" * 70)
                
                # Execute status check for active upgrades only
                # NOTE: This queries ALL devices each time, not just initial set
                # New devices that start upgrading will be detected automatically
                result = self._execute_monitoring_check(site_filter)
                
                if result is None:
                    print("\n   Error fetching upgrade status. Retrying...")
                    logging.warning(f"Monitoring iteration {iteration} failed")
                elif result == 0:
                    # No active upgrades found
                    print("\n  All upgrades completed!")
                    print("   No active firmware upgrades detected.")
                    print("   Exiting monitoring mode.")
                    logging.info("Monitoring mode exiting - all upgrades complete")
                    break
                else:
                    print(f"\n   Found {result} device(s) actively upgrading")
                    print("   Next refresh in 7 seconds...")
                
                # Wait 7 seconds before next refresh
                time.sleep(7)
                
        except KeyboardInterrupt:
            print("\n\n  Monitoring mode cancelled by user.")
            logging.info("Continuous monitoring mode cancelled by user")
            return
    
    def _execute_monitoring_check(self, site_filter=None):
        """
        Execute a single monitoring check iteration.
        
        This method performs a FULL fresh query of all devices on each call.
        It does NOT track specific devices from the first iteration - instead,
        it queries the API for ALL devices and checks their current upgrade status.
        
        This means:
        - New devices that start upgrading will be detected
        - Devices that complete will drop off automatically
        - Progress percentages are always current/live
        
        Returns:
            int: Number of devices actively upgrading, or None if error
        """
        try:
            # Fetch FRESH device statistics from API (not cached from previous iteration)
            all_device_stats = []
            
            if site_filter:
                # Query specific site for current device stats
                stats_resp = mistapi.api.v1.sites.stats.listSiteDevicesStats(
                    self.apisession, 
                    site_filter,
                    type="all",
                    limit=1000
                )
                site_stats = mistapi.get_all(response=stats_resp, mist_session=self.apisession)
                all_device_stats.extend(site_stats)
            else:
                # Query entire org for current device stats
                stats_resp = mistapi.api.v1.orgs.stats.listOrgDevicesStats(
                    self.apisession, 
                    self.org_id,
                    type="all",
                    fields="*",
                    limit=1000
                )
                org_stats = mistapi.get_all(response=stats_resp, mist_session=self.apisession)
                all_device_stats.extend(org_stats)
            
            # Scan through ALL devices to find active upgrades
            active_upgrades = []
            
            for device_stat in all_device_stats:
                fwupdate = device_stat.get('fwupdate')
                if not fwupdate:
                    continue
                
                fw_status = fwupdate.get('status', 'unknown')
                fw_progress = fwupdate.get('progress', 0)
                fw_timestamp = fwupdate.get('timestamp', 0)
                
                # Check if this is truly an active upgrade (not stale)
                is_active = fw_status in ('inprogress', 'upgrading', 'downloading')
                
                if is_active and fw_progress == 100 and fw_timestamp:
                    # Check for stale upgrade
                    try:
                        upgrade_age_hours = (time.time() - fw_timestamp) / 3600
                        if upgrade_age_hours > 1:
                            is_active = False  # Stale, skip it
                    except (ValueError, OSError, TypeError):
                        pass
                
                if is_active:
                    device_name = device_stat.get('name', 'Unnamed')
                    device_type = device_stat.get('type', 'unknown')
                    device_model = device_stat.get('model', 'Unknown')
                    
                    active_upgrades.append({
                        'name': device_name,
                        'type': device_type,
                        'model': device_model,
                        'progress': fw_progress if fw_progress is not None else 0,
                        'status': fw_status
                    })
            
            # Display active upgrades table
            if active_upgrades:
                print("\n  Devices Currently Upgrading:")
                print("  " + "=" * 86)
                print(f"  {'Device Name':<25} {'Type':<10} {'Model':<15} {'Status':<12} {'Progress':<20}")
                print("  " + "-" * 86)
                
                for upgrade in active_upgrades:
                    progress_bar = create_progress_bar(upgrade['progress'], bar_length=15)
                    print(f"  {upgrade['name']:<25} {upgrade['type']:<10} {upgrade['model']:<15} "
                          f"{upgrade['status']:<12} {progress_bar}")
                
                print("  " + "=" * 86)
            
            return len(active_upgrades)
            
        except Exception as e:
            logging.error(f"Error in monitoring check: {e}", exc_info=True)
            return None
    
    def upgrade_ap_firmware_by_gateway_template(self):
        """
        Advanced AP firmware upgrade organized by Gateway Template assignment.
        
        This method provides template-based firmware upgrades with:
        1. Interactive Gateway Template selection with site count display
        2. Automatic site discovery for selected template
        3. AP enumeration across all sites in template
        4. Model-based firmware version selection
        5. Unified upgrade execution across template sites
        6. Automatic site auto-upgrade configuration
        7. Comprehensive audit logging and progress monitoring
        
        Features:
        - Template selection by index or name
        - Site count and AP count display per template
        - Reuse of existing upgrade strategies and safety measures
        - Maintains all existing safety confirmations and audit trails
        """
        logging.info("Starting template-based AP firmware upgrade...")
        logging.debug("FirmwareManager.upgrade_ap_firmware_by_gateway_template() initiated")
        
        print(" Advanced AP Firmware Upgrade by Gateway Template")
        print("=" * 70)
        
        # Step 1: Ensure required CSVs are fresh
        print("\n  Preparing template and site data...")
        check_and_generate_csv("OrgGatewayTemplates.csv", export_gateway_templates_to_csv)
        check_and_generate_csv("SiteList.csv", export_all_sites_to_csv)
        
        # Step 2: Load gateway templates and build template-to-sites mapping
        template_name_to_id, template_sites_mapping = self._load_template_sites_mapping()
        
        if not template_name_to_id:
            print(" No gateway templates found.")
            logging.warning("No gateway templates available for upgrade")
            return
        
        # Step 3: Display template selection with site counts
        selected_template_id, selected_template_name = self._prompt_template_selection(
            template_name_to_id, template_sites_mapping
        )
        
        if not selected_template_id:
            print(" No template selected. Exiting.")
            logging.info("Template-based upgrade cancelled - no template selected")
            return
        
        # Step 4: Get sites for selected template
        sites_to_upgrade = template_sites_mapping.get(selected_template_id, [])
        
        if not sites_to_upgrade:
            print(f" No sites found using template '{selected_template_name}'.")
            logging.warning(f"No sites found for template {selected_template_name} (ID: {selected_template_id})")
            return
        
        print(f"\n  Template Selection Summary:")
        print(f"   Selected Template: {selected_template_name}")
        print(f"   Template ID: {selected_template_id}")
        print(f"   Sites in Template: {len(sites_to_upgrade)}")
        
        # Log site details
        logging.info(f"Template-based upgrade: '{selected_template_name}' with {len(sites_to_upgrade)} sites")
        for site_info in sites_to_upgrade:
            logging.debug(f"  Site: {site_info['name']} (ID: {site_info['id']})")
        
        # Step 5: Execute firmware upgrade using existing bulk upgrade logic
        # Convert sites_to_upgrade to the format expected by bulk_upgrade_ap_firmware_by_site
        return self._execute_template_based_upgrade(sites_to_upgrade, selected_template_name)
    
    def _ensure_template_csv_freshness(self):
        """
        Ensure that required template and site CSV files are fresh and available.
        
        This method generates or refreshes the CSV files needed for template-based
        operations if they don't exist or are stale.
        """
        logging.debug("Ensuring template CSV files are fresh")
        print("  Preparing template and site data...")
        
        # Generate required CSV files using existing export functions
        check_and_generate_csv("OrgGatewayTemplates.csv", export_gateway_templates_to_csv)
        check_and_generate_csv("SiteList.csv", export_all_sites_to_csv)
        
        logging.debug("Template CSV files ensured fresh")
    
    def _load_template_sites_mapping(self):
        """
        Load gateway templates and create mapping of templates to their assigned sites.
        
        Returns:
            tuple: (template_name_to_id dict, template_sites_mapping dict)
        """
        template_name_to_id = {}
        template_sites_mapping = {}  # template_id -> list of site info dicts
        
        try:
            # Load gateway templates
            gateway_templates_path = get_csv_file_path("OrgGatewayTemplates.csv")
            with open(gateway_templates_path, encoding="utf-8") as f:
                reader = csv.DictReader(f)
                for row in reader:
                    name = row.get("name", "").strip()
                    tid = row.get("id", "").strip()
                    if name and tid:
                        template_name_to_id[name] = tid
                        template_sites_mapping[tid] = []  # Initialize empty list
            
            logging.info(f"Loaded {len(template_name_to_id)} gateway templates")
            
            # Load sites and map them to templates
            site_list_path = get_csv_file_path("SiteList.csv")
            with open(site_list_path, encoding="utf-8") as f:
                reader = csv.DictReader(f)
                for row in reader:
                    gateway_template_id = row.get("gatewaytemplate_id", "").strip()
                    site_id = row.get("id", "").strip()
                    site_name = row.get("name", "").strip()
                    
                    if gateway_template_id in template_sites_mapping and site_id and site_name:
                        template_sites_mapping[gateway_template_id].append({
                            'id': site_id,
                            'name': site_name
                        })
            
            # Log template-to-site mapping statistics
            for template_id, sites in template_sites_mapping.items():
                template_name = next((name for name, tid in template_name_to_id.items() if tid == template_id), "Unknown")
                logging.debug(f"Template '{template_name}': {len(sites)} sites")
            
            return template_name_to_id, template_sites_mapping
            
        except Exception as e:
            logging.error(f"Failed to load template-sites mapping: {e}")
            print(f"! Failed to load template and site data: {e}")
            return {}, {}
    
    def _prompt_template_selection(self, template_name_to_id, template_sites_mapping):
        """
        Present interactive template selection with site counts.
        
        Args:
            template_name_to_id: Dict mapping template names to IDs
            template_sites_mapping: Dict mapping template IDs to site lists
            
        Returns:
            tuple: (selected_template_id, selected_template_name) or (None, None)
        """
        print(f"\n  Available Gateway Templates:")
        print(f"  {'Index':<8} {'Template Name':<40} {'Sites':<8}")
        print(f"  {'-' * 8} {'-' * 40} {'-' * 8}")
        
        # Create indexed list of templates sorted by name
        sorted_templates = sorted(template_name_to_id.items())
        template_index_map = {}
        
        for idx, (template_name, template_id) in enumerate(sorted_templates, 1):
            site_count = len(template_sites_mapping.get(template_id, []))
            print(f"  [{idx:<7}] {template_name:<40} {site_count:<8}")
            template_index_map[str(idx)] = (template_id, template_name)
        
        print(f"\n  Selection Options:")
        print(f"   !? Enter index number (1-{len(sorted_templates)})")
        print(f"   !? Type exact template name") 
        print(f"   !? Press Enter to cancel")
        
        while True:
            try:
                user_input = input(f"\n  Select template: ").strip()
                
                if not user_input:
                    # Empty input - cancel
                    return None, None
                
                # Check if input is an index number
                if user_input in template_index_map:
                    template_id, template_name = template_index_map[user_input]
                    logging.debug(f"Template selected by index {user_input}: {template_name}")
                    return template_id, template_name
                
                # Check if input matches a template name exactly
                if user_input in template_name_to_id:
                    template_id = template_name_to_id[user_input]
                    logging.debug(f"Template selected by name: {user_input}")
                    return template_id, user_input
                
                # No match found
                print(f"   Invalid selection. Please enter a valid index (1-{len(sorted_templates)}) or exact template name.")
                
            except KeyboardInterrupt:
                print(f"\n   Template selection cancelled.")
                return None, None
    
    def _execute_template_based_upgrade(self, sites_to_upgrade, template_name):
        """
        Execute firmware upgrade for all sites in a gateway template.
        
        This method reuses the existing bulk upgrade logic but with template context.
        
        Args:
            sites_to_upgrade: List of site info dicts with 'id' and 'name'
            template_name: Name of the selected template for logging
            
        Returns:
            Results of the upgrade operation
        """
        logging.info(f"Executing template-based firmware upgrade for template '{template_name}' with {len(sites_to_upgrade)} sites")
        
        print(f"\n  Template-Based Upgrade Execution")
        print(f"  Template: {template_name}")
        print(f"  Sites to process: {len(sites_to_upgrade)}")
        print(f"  {'Site Name':<40} {'Site ID':<40}")
        print(f"  {'-' * 40} {'-' * 40}")
        
        for site_info in sites_to_upgrade:
            print(f"  {site_info['name']:<40} {site_info['id']:<40}")
        
        # Use the existing bulk upgrade functionality
        # We'll call the refactored bulk_upgrade method with our site list
        return self.bulk_upgrade_ap_firmware_by_site(sites_to_upgrade_override=sites_to_upgrade)
    
    def execute_firmware_upgrade_with_mode_selection(self):
        """
        Main entry point for firmware upgrades with mode selection.
        
        Presents user with choice between:
        1. Site-based upgrade (existing behavior)
        2. Template-based upgrade (new functionality)
        
        Returns:
            Results of the selected upgrade operation
        """
        logging.info("Starting firmware upgrade with mode selection...")
        logging.debug("FirmwareManager.execute_firmware_upgrade_with_mode_selection() initiated")
        
        print(" Advanced AP Firmware Upgrade")
        print("=" * 60)
        
        # Step 1: Mode selection
        print("\n  Select upgrade mode:")
        print("   [1] By Site - Upgrade specific sites (CSV file, bulk list, or single site selection)")
        print("   [2] By Gateway Template - Upgrade all sites assigned to a selected Gateway Template")
        
        while True:
            try:
                mode_choice = input("\n  Select mode (1-2): ").strip()
                if mode_choice == "1":
                    logging.info("User selected site-based upgrade mode")
                    print("\n  Site-based upgrade mode selected")
                    return self.bulk_upgrade_ap_firmware_by_site()
                elif mode_choice == "2":
                    logging.info("User selected template-based upgrade mode")
                    print("\n  Template-based upgrade mode selected")
                    return self.upgrade_ap_firmware_by_gateway_template()
                else:
                    print("   Invalid selection. Please choose 1 or 2.")
                    logging.debug(f"Invalid mode selection: {mode_choice}")
            except KeyboardInterrupt:
                print("\n\n  Firmware upgrade cancelled by user.")
                logging.info("Firmware upgrade cancelled during mode selection")
                return
    
    def bulk_upgrade_ap_firmware_by_site(self, sites_to_upgrade_override=None):
        """
        Advanced bulk upgrade AP firmware for APs at selected site(s).
        
        This method provides comprehensive firmware upgrade capabilities with:
        1. Bulk site mode: Reads APUpgradeSiteList.CSV for multi-site upgrades
        2. Single site mode: Interactive site selection (fallback if CSV not found)
        3. Template mode: Uses provided sites_to_upgrade_override for template-based upgrades
        4. Automatic site name-to-ID resolution via organization lookup
        5. Firmware version selection per model across all sites
        6. Advanced upgrade strategies (big_bang, canary, rrm, serial) - default: RRM
        7. P2P firmware sharing options (default: enabled)
        8. Scheduling and failure threshold controls
        9. Device filtering and selection rules
        10. Progress monitoring and rollback options
        11. Comprehensive safety measures and audit logging
        12. Per-site upgrade execution with unified reporting
        
        Args:
            sites_to_upgrade_override: Optional list of site dicts for template-based upgrades
                                     Format: [{'id': site_id, 'name': site_name}, ...]
        
        File Format for APUpgradeSiteList.CSV (headerless, one site name per line):
        Main Office
        Branch Office A
        Remote Site B
        
        Note: Site names must exactly match those in the Mist organization.
        """
        # Set up global session context for compatibility with existing helper functions
        global apisession
        original_apisession = apisession
        apisession = self.apisession
        
        try:
            return self._execute_bulk_upgrade(sites_to_upgrade_override)
        finally:
            apisession = original_apisession
    
    def _execute_bulk_upgrade(self, sites_to_upgrade_override):
        """Execute the bulk firmware upgrade with existing implementation."""
        return bulk_upgrade_ap_firmware_by_site_impl(self.org_id, sites_to_upgrade_override)
    
    def _execute_status_check(self, scope_choice, site_filter):
        """Execute the firmware status check with the existing implementation."""
        # Set up the implementation to use this class's session and org_id
        global apisession
        original_apisession = apisession
        apisession = self.apisession
        
        try:
            return check_firmware_upgrade_status_impl(scope_choice, site_filter)
        finally:
            apisession = original_apisession

    # ===============================================================================
    # SWITCH FIRMWARE UPGRADE METHODS
    # ===============================================================================
    
    def execute_switch_firmware_upgrade_with_mode_selection(self):
        """
        Main entry point for switch firmware upgrades with mode selection.
        
        Presents user with choice between:
        1. Site-based upgrade (individual site selection)
        2. Template-based upgrade (Gateway Template assignment - same grouping as APs)
        
        Returns:
            Results of the selected upgrade operation
        """
        logging.info("Starting switch firmware upgrade with mode selection...")
        logging.debug("FirmwareManager.execute_switch_firmware_upgrade_with_mode_selection() initiated")
        
        print(" Advanced Switch Firmware Upgrade")
        print("=" * 60)
        print("")
        print("  DESTRUCTIVE OPERATION WARNING")
        print("  ===========================")
        print("  Switch firmware upgrades will:")
        print("  X  Reboot switches during upgrade process")
        print("  X  Potentially disrupt network connectivity")
        print("  X  Affect production traffic flow")
        print("  X  Require recovery snapshots for Junos devices")
        print("")
        
        # Step 1: Mode selection
        print("  Select upgrade mode:")
        print("   [1] By Site - Upgrade specific sites (individual site selection)")
        print("   [2] By Gateway Template - Upgrade all sites assigned to a selected Gateway Template")
        
        while True:
            try:
                mode_choice = input("\n  Select mode (1-2): ").strip()
                if mode_choice == "1":
                    logging.info("User selected site-based switch upgrade mode")
                    print("\n  Site-based switch upgrade mode selected")
                    return self.bulk_upgrade_switch_firmware_by_site()
                elif mode_choice == "2":
                    logging.info("User selected template-based switch upgrade mode")
                    print("\n  Template-based switch upgrade mode selected")
                    return self.upgrade_switch_firmware_by_gateway_template()
                else:
                    print("  Invalid selection. Please choose 1 or 2.")
                    logging.debug(f"Invalid mode selection: {mode_choice}")
            except KeyboardInterrupt:
                print("\n  Operation cancelled by user.")
                logging.info("Switch firmware upgrade cancelled by user")
                return

    def bulk_upgrade_switch_firmware_by_site(self, sites_to_upgrade_override=None):
        """
        Advanced bulk switch firmware upgrade for switches at selected site(s).
        
        This method provides comprehensive switch firmware upgrade capabilities with:
        1. Bulk site mode: Interactive site selection for multi-site upgrades
        2. Single site mode: Individual site selection (fallback if override not provided)
        3. Template mode: Uses provided sites_to_upgrade_override for template-based upgrades
        4. Switch-specific upgrade parameters (reboot=True, snapshot=True for Junos)
        5. Conservative upgrade strategies optimized for network switches
        6. Enhanced safety measures for network disruption prevention
        7. Model-specific firmware version selection across sites
        8. Per-site upgrade execution with unified reporting
        
        Args:
            sites_to_upgrade_override: Optional list of site dictionaries for template-based upgrades
            
        Returns:
            Upgrade execution results and tracking information
        """
        logging.info("Starting bulk switch firmware upgrade by site...")
        logging.debug("FirmwareManager.bulk_upgrade_switch_firmware_by_site() initiated")
        
        # Set up the implementation to use this class's session and org_id
        global apisession
        original_apisession = apisession
        apisession = self.apisession
        
        try:
            return bulk_upgrade_switch_firmware_by_site_impl(self.org_id, sites_to_upgrade_override)
        finally:
            apisession = original_apisession

    def upgrade_switch_firmware_by_gateway_template(self):
        """
        Advanced switch firmware upgrade organized by Gateway Template assignment.
        
        This method provides template-based switch firmware upgrades with:
        1. Interactive Gateway Template selection with site count display
        2. Automatic site discovery for selected template (same logic as AP system)
        3. Switch enumeration across all sites in template  
        4. Model-based firmware version selection optimized for switches
        5. Unified upgrade execution across template sites
        6. Switch-specific safety measures and network disruption warnings
        7. Comprehensive audit logging and progress monitoring
        
        Features:
        - Template selection by index or name (reuses AP template infrastructure)
        - Site count and switch count display per template
        - Switch-specific upgrade parameters (reboot, snapshot, conservative strategy)
        - Maintains all existing safety confirmations and audit trails
        - Enhanced network disruption warnings for production environments
        """
        logging.info("Starting template-based switch firmware upgrade...")
        logging.debug("FirmwareManager.upgrade_switch_firmware_by_gateway_template() initiated")
        
        print(" Advanced Switch Firmware Upgrade by Gateway Template")
        print("=" * 70)
        
        # Step 1: Ensure required CSVs are fresh (reuse AP template infrastructure)
        self._ensure_template_csv_freshness()
        
        # Step 2: Load template-to-sites mapping (same as AP system)
        template_name_to_id, template_sites_mapping = self._load_template_sites_mapping()
        
        if not template_sites_mapping:
            print("\n! No Gateway Templates with assigned sites found.")
            print("  Make sure sites are assigned to Gateway Templates and try again.")
            logging.warning("No Gateway Templates with site assignments found")
            return
        
        # Step 3: Template selection (reuse AP template selection logic)
        selected_template_id, selected_template_name = self._prompt_template_selection(
            template_name_to_id, template_sites_mapping)
        
        if not selected_template_id:
            print(" No template selected. Exiting.")
            return
        
        # Step 4: Get sites for selected template
        sites_to_upgrade = template_sites_mapping.get(selected_template_id, [])
        
        print(f"\n  Template '{selected_template_name}' includes {len(sites_to_upgrade)} sites")
        logging.info(f"Template {selected_template_name} has {len(sites_to_upgrade)} assigned sites")
        
        return self._execute_template_based_switch_upgrade(sites_to_upgrade, selected_template_name)
    
    def _execute_template_based_switch_upgrade(self, sites_to_upgrade, selected_template_name):
        """Execute the template-based switch upgrade with the existing switch implementation."""
        print(f"  Proceeding with switch firmware upgrade for template: {selected_template_name}")
        print(f"  Target sites: {len(sites_to_upgrade)}")
        
        # Use the switch-specific bulk upgrade implementation
        return self.bulk_upgrade_switch_firmware_by_site(sites_to_upgrade)

    # ===============================================================================
    # SSR FIRMWARE UPGRADE METHODS
    # ===============================================================================
    
    def execute_ssr_firmware_upgrade_with_mode_selection(self):
        """
        Main entry point for SSR firmware upgrades with mode selection.
        
        Presents user with choice between:
        1. Site-based upgrade (individual site selection)
        2. Template-based upgrade (Gateway Template assignment - same grouping as APs/switches)
        
        SECURITY: This is a DESTRUCTIVE operation that will reboot SSR devices and
        disrupt WAN/SD-WAN connectivity. Critical routing infrastructure warnings provided.
        
        Returns:
            Results of the selected upgrade operation
        """
        logging.info("Starting SSR firmware upgrade with mode selection...")
        logging.debug("FirmwareManager.execute_ssr_firmware_upgrade_with_mode_selection() initiated")
        
        print(" Advanced SSR Firmware Upgrade")
        print("=" * 60)
        print("")
        print("  CRITICAL ROUTING INFRASTRUCTURE WARNING")
        print("  ======================================")
        print("  SSR firmware upgrades will:")
        print("  X  Reboot Session Smart Routers")
        print("  X  Disrupt WAN and SD-WAN connectivity")
        print("  X  Affect branch office connectivity")
        print("  X  Impact tunnel establishment and failover")
        print("  X  Require careful HA pair coordination")
        print("  X  Potentially cause extended outages")
        print("")
        print("  RECOMMENDED PRECAUTIONS:")
        print("  X  Schedule maintenance windows")
        print("  X  Verify backup connectivity paths")
        print("  X  Coordinate with network operations")
        print("  X  Monitor upgrade progress closely")
        print("")
        
        # Step 1: Mode selection
        print("  Select upgrade mode:")
        print("   [1] By Site - Upgrade specific sites (individual site selection)")
        print("   [2] By Gateway Template - Upgrade all sites assigned to a selected Gateway Template")
        
        while True:
            try:
                mode_choice = input("\n  Select mode (1-2): ").strip()
                if mode_choice == "1":
                    logging.info("User selected site-based SSR upgrade mode")
                    print("\n  Site-based SSR upgrade mode selected")
                    return self.bulk_upgrade_ssr_firmware_by_site()
                elif mode_choice == "2":
                    logging.info("User selected template-based SSR upgrade mode")
                    print("\n  Template-based SSR upgrade mode selected")
                    return self.upgrade_ssr_firmware_by_gateway_template()
                else:
                    print("  Invalid selection. Please choose 1 or 2.")
                    logging.debug(f"Invalid mode selection: {mode_choice}")
            except KeyboardInterrupt:
                print("\n  Operation cancelled by user.")
                logging.info("SSR firmware upgrade cancelled by user")
                return

    def bulk_upgrade_ssr_firmware_by_site(self, sites_to_upgrade_override=None):
        """
        DESTRUCTIVE: Execute firmware upgrades on Session Smart Routers across selected sites.
        
        This function performs bulk firmware upgrades on SSR routing infrastructure with comprehensive
        safety checks and detailed progress tracking. Supports multiple upgrade strategies
        including big bang, canary testing, and rolling upgrade modes optimized for routing infrastructure.
        
        SECURITY: This operation will reboot Session Smart Routers and WILL cause WAN connectivity disruption.
        All SSRs in target sites will be affected. This impacts SD-WAN tunnels, branch office connectivity,
        and critical routing infrastructure. Use with extreme caution in production.
        
        Args:
            sites_to_upgrade_override: Optional list of site dictionaries for template-based upgrades
            
        Returns:
            dict: Comprehensive upgrade operation results with success/failure tracking
            
        Raises:
            Exception: On critical API failures or validation errors
            
        CRITICAL INFRASTRUCTURE WARNING:
        - SSR reboots will disrupt WAN and SD-WAN connectivity
        - Branch offices may lose connectivity during upgrades
        - SD-WAN tunnels will be re-established after reboot
        - HA pairs require coordinated failover procedures
        - Plan extended maintenance windows for production environments
        - Verify backup connectivity paths before execution
        - Monitor upgrade progress closely for rapid intervention
        """
        # Set up logging for this method
        logger = logging.getLogger(__name__)
        logger.debug(f"Starting bulk SSR firmware upgrade - org_id: {self.org_id}")
        
        # Get organization information
        print("\n-> Validating organization access...")
        try:
            org_info = mistapi.api.v1.orgs.orgs.getOrg(self.apisession, self.org_id)
            if org_info.status_code != 200:
                print(f"X  Error accessing organization: {org_info.status_code}")
                logger.error(f"Failed to access organization {self.org_id}: {org_info.status_code}")
                return {"error": "Organization access failed"}
            
            org_name = org_info.data.get('name', 'Unknown')
            print(f"!? Organization: {org_name}")
            logger.debug(f"Organization validated: {org_name}")
            
        except Exception as e:
            print(f"X  Error validating organization: {str(e)}")
            logger.error(f"Organization validation failed: {str(e)}")
            return {"error": f"Organization validation error: {str(e)}"}

        # Site selection logic
        if sites_to_upgrade_override:
            selected_sites = sites_to_upgrade_override
            print(f"-> Using provided site list: {len(selected_sites)} sites")
        else:
            # Get available sites
            print("\n-> Discovering available sites...")
            try:
                sites_response = mistapi.api.v1.orgs.sites.listOrgSites(self.apisession, self.org_id)
                if sites_response.status_code != 200:
                    print(f"X  Error retrieving sites: {sites_response.status_code}")
                    return {"error": "Failed to retrieve sites"}
                
                all_sites = sites_response.data
                print(f"!? Found {len(all_sites)} total sites")
                
                # Present site selection to user
                print("\nAvailable sites:")
                for index, site in enumerate(all_sites, 1):
                    print(f"{index:3}. {site.get('name', 'Unnamed')} (ID: {site.get('id', 'Unknown')})")
                
                print("\nSite selection options:")
                print("A. All sites")
                print("S. Select specific sites")
                print("C. Cancel operation")
                
                site_choice = input("\nEnter your choice (A/S/C): ").strip().upper()
                
                if site_choice == 'C':
                    print("-> Operation cancelled by user")
                    return {"cancelled": True}
                elif site_choice == 'A':
                    selected_sites = all_sites
                    print(f"-> Selected all {len(selected_sites)} sites")
                elif site_choice == 'S':
                    selected_sites = []
                    print("\nEnter site numbers (comma-separated) or ranges (e.g., 1-5):")
                    site_input = input("Sites: ").strip()
                    
                    # Parse site selection
                    try:
                        for part in site_input.split(','):
                            part = part.strip()
                            if '-' in part:
                                start, end = map(int, part.split('-'))
                                for device_index in range(start-1, end):
                                    if 0 <= device_index < len(all_sites):
                                        selected_sites.append(all_sites[device_index])
                            else:
                                index = int(part) - 1
                                if 0 <= index < len(all_sites):
                                    selected_sites.append(all_sites[index])
                        
                        print(f"-> Selected {len(selected_sites)} sites")
                        
                    except Exception as e:
                        print(f"X  Invalid site selection: {str(e)}")
                        return {"error": "Invalid site selection"}
                else:
                    print("X  Invalid selection")
                    return {"error": "Invalid selection"}
                    
            except Exception as e:
                print(f"X  Error during site discovery: {str(e)}")
                logger.error(f"Site discovery failed: {str(e)}")
                return {"error": f"Site discovery error: {str(e)}"}

        if not selected_sites:
            print("X  No sites selected")
            return {"error": "No sites selected"}

        # SSR firmware upgrade parameter selection
        print(f"\n{'='*60}")
        print("SSR FIRMWARE UPGRADE PARAMETER CONFIGURATION")
        print(f"{'='*60}")
        
        # Strategy selection (conservative defaults for SSR routing infrastructure)
        print("\nUpgrade Strategy Options (optimized for routing infrastructure):")
        print("1. serial      - Upgrade SSRs one by one (safest for routing infrastructure)")  
        print("2. big_bang    - Upgrade all SSRs simultaneously (higher risk)")
        
        while True:
            strategy_choice = input("\nSelect upgrade strategy (1-2, recommend 1): ").strip()
            if strategy_choice == '1':
                upgrade_strategy = 'serial'
                break
            elif strategy_choice == '2':
                upgrade_strategy = 'big_bang'
                print("!? WARNING: big_bang strategy will upgrade all SSRs simultaneously")
                print("   This may cause widespread WAN connectivity disruption")
                break
            else:
                print("X  Please enter 1 or 2")
        
        print(f"-> Selected strategy: {upgrade_strategy}")
        
        # Reboot timing selection (SSR-specific parameter)
        print("\nReboot Timing Options:")
        print("1. Automatic - Reboot immediately after firmware download (recommended)")
        print("2. Manual    - Download firmware only, manual reboot required later")
        
        while True:
            reboot_choice = input("\nReboot timing? (1-2): ").strip()
            if reboot_choice == '1':
                auto_reboot = True
                break
            elif reboot_choice == '2':
                auto_reboot = False
                print("!? WARNING: SSRs require manual reboot to activate new firmware")
                print("   New firmware will not be operational until manual reboot")
                break
            else:
                print("X  Please enter 1 or 2")
        
        print(f"-> Auto reboot: {'Yes' if auto_reboot else 'No'}")
        
        # Channel selection for firmware versions  
        print("\nFirmware Channel Options:")
        print("1. stable - Production-ready releases (recommended)")
        print("2. beta   - Pre-release versions for testing")
        print("3. alpha  - Development versions (not recommended for production)")
        
        while True:
            channel_choice = input("\nSelect firmware channel (1-3): ").strip()
            if channel_choice == '1':
                firmware_channel = 'stable'
                break
            elif channel_choice == '2':
                firmware_channel = 'beta'
                break
            elif channel_choice == '3':
                firmware_channel = 'alpha'
                print("!? WARNING: alpha channel contains development versions")
                print("   Not recommended for production environments")
                break
            else:
                print("X  Please enter 1, 2, or 3")
        
        print(f"-> Firmware channel: {firmware_channel}")

        # SSR-specific firmware version selection
        print(f"\n{'='*60}")
        print("SSR FIRMWARE VERSION SELECTION")
        print(f"{'='*60}")
        
        # Get available firmware versions for SSRs (Session Smart Routers)
        print("\n-> Discovering available SSR firmware versions...")
        try:
            # Use the SSR-specific API to get available firmware versions
            versions_response = mistapi.api.v1.orgs.ssr.listOrgAvailableSsrVersions(
                self.apisession, self.org_id, channel=firmware_channel
            )
            
            if versions_response.status_code != 200:
                print(f"X  Error retrieving SSR firmware versions: {versions_response.status_code}")
                logger.error(f"Failed to retrieve SSR versions: {versions_response.status_code}")
                return {"error": "Failed to retrieve SSR firmware versions"}
            
            available_versions = []
            if hasattr(versions_response, 'data') and versions_response.data:
                for version_obj in versions_response.data:
                    if isinstance(version_obj, dict):
                        version = version_obj.get('version')
                        package = version_obj.get('package', 'SSR')
                        is_default = version_obj.get('default', False)
                        if version:
                            available_versions.append({
                                'version': version,
                                'package': package,
                                'default': is_default
                            })
                    elif isinstance(version_obj, str):
                        # Handle case where API returns just version strings
                        available_versions.append({
                            'version': version_obj,
                            'package': 'SSR',
                            'default': False
                        })
            
            if not available_versions:
                print(f"X  No SSR firmware versions available for {firmware_channel} channel")
                print("   Please check with Juniper support for available SSR firmware versions")
                print("   Or try a different firmware channel (stable/beta/alpha)")
                return {"error": f"No SSR firmware versions available for {firmware_channel} channel"}
            
            print(f"!? Found {len(available_versions)} available SSR firmware versions")
            print(f"  Channel: {firmware_channel}")
            
            # Get SSR inventory to show current firmware versions  
            print("\n-> Checking current SSR devices...")
            ssrs_response = mistapi.api.v1.orgs.inventory.getOrgInventory(
                self.apisession, self.org_id, type="gateway"
            )
            
            current_firmware_versions = set()
            ssr_models_found = set()
            ssr_count = 0
            
            if ssrs_response.status_code == 200:
                # Filter for Session Smart Router models specifically
                all_gateways = ssrs_response.data
                for gateway in all_gateways:
                    gateway_model = gateway.get('model', '')
                    gateway_type = gateway.get('type', '')
                    
                    # Check if this is an SSR by model or type
                    if gateway_type == 'ssr' or 'SSR' in gateway_model or '128T' in gateway_model:
                        ssr_count += 1
                        if gateway.get('version'):
                            current_firmware_versions.add(gateway.get('version'))
                        if gateway.get('model'):
                            ssr_models_found.add(gateway.get('model'))
            
            if ssr_count > 0:
                print(f"!? Found {ssr_count} SSR device(s) in organization")
                if ssr_models_found:
                    print(f"  Models: {', '.join(sorted(ssr_models_found))}")
                if current_firmware_versions:
                    print(f"  Current versions: {', '.join(sorted(current_firmware_versions))}")
            
            # Present firmware version options
            print(f"\n{'='*50}")
            print("AVAILABLE SSR FIRMWARE VERSIONS")
            print(f"{'='*50}")
            
            for i, version_info in enumerate(available_versions, 1):
                version = version_info['version']
                package = version_info['package']
                is_default = version_info['default']
                
                default_marker = " (default)" if is_default else ""
                print(f"{i:2d}. {version} [{package}]{default_marker}")
            
            # Allow user to select firmware version
            while True:
                try:
                    choice = input(f"\nSelect firmware version (1-{len(available_versions)}): ").strip()
                    if not choice:
                        print("X  Please enter a selection")
                        continue
                        
                    version_index = int(choice) - 1
                    if 0 <= version_index < len(available_versions):
                        selected_version = available_versions[version_index]
                        target_version = selected_version['version']
                        break
                    else:
                        print(f"X  Please enter a number between 1 and {len(available_versions)}")
                except ValueError:
                    print("X  Please enter a valid number")
            
            print(f"-> Selected firmware version: {target_version}")
            
        except Exception as e:
            print(f"X  Error during SSR firmware discovery: {str(e)}")
            logger.error(f"SSR firmware discovery failed: {str(e)}")
            return {"error": f"SSR firmware discovery error: {str(e)}"}

        # Configuration summary and confirmation
        print(f"\n{'='*60}")
        print("SSR UPGRADE CONFIGURATION SUMMARY")
        print(f"{'='*60}")
        print(f"Organization: {org_name}")
        print(f"Sites to upgrade: {len(selected_sites)}")
        print(f"Target firmware: {target_version}")
        print(f"Firmware channel: {firmware_channel}")
        print(f"Upgrade strategy: {upgrade_strategy}")
        print(f"Auto reboot: {'Yes' if auto_reboot else 'No'}")
        
        print(f"\n!? CRITICAL ROUTING INFRASTRUCTURE WARNING !?")
        print("SSR firmware upgrades will cause WAN connectivity disruption!")
        print("- SSRs will reboot and SD-WAN tunnels will be offline during upgrade")
        print("- Branch offices may lose connectivity")
        print("- Plan extended maintenance windows")
        print("- Verify backup connectivity paths")
        print("- Coordinate with network operations team")
        print("- Monitor upgrade progress closely")
        
        print(f"\nTo proceed with SSR firmware upgrade, type: UPGRADE")
        confirmation = safe_input("Confirmation: ", "", True, "SSR firmware upgrade confirmation")
        
        if confirmation is None or confirmation != "UPGRADE":
            print("-> Operation cancelled - incorrect confirmation")
            logger.info("SSR firmware upgrade cancelled by user")
            return {"cancelled": True}

        # Execute upgrade operation
        print(f"\n{'='*60}")
        print("EXECUTING SSR FIRMWARE UPGRADE")
        print(f"{'='*60}")
        
        # Initialize results tracking
        upgrade_results = {
            'operation_id': f"ssr_upgrade_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            'target_version': target_version,
            'strategy': upgrade_strategy,
            'channel': firmware_channel,
            'reboot': auto_reboot,
            'sites_processed': 0,
            'ssrs_upgraded': 0,
            'errors': [],
            'start_time': datetime.now().isoformat(),
            'site_results': []
        }
        
        logger.info(f"Starting SSR firmware upgrade operation: {upgrade_results['operation_id']}")
        
        # Define SSR model patterns for device filtering
        ssr_models = ['SSR', '128T']  # Patterns to identify SSR devices
        
        # Get org-level SSR inventory for validation
        print("-> Validating SSR devices from organization inventory...")
        org_ssr_inventory = {}
        try:
            ssrs_response = mistapi.api.v1.orgs.inventory.getOrgInventory(
                self.apisession, self.org_id, type="gateway"
            )
            if ssrs_response.status_code == 200:
                for gateway in ssrs_response.data:
                    gateway_id = gateway.get('id')
                    gateway_model = gateway.get('model', '')
                    gateway_type = gateway.get('type', '')
                    
                    # Check if this is an SSR by model or type
                    if gateway_type == 'ssr' or 'SSR' in gateway_model or '128T' in gateway_model:
                        org_ssr_inventory[gateway_id] = {
                            'model': gateway_model,
                            'type': gateway_type,
                            'version': gateway.get('version', ''),
                            'site_id': gateway.get('site_id', '')
                        }
                print(f"!? Found {len(org_ssr_inventory)} SSR device(s) in organization inventory")
            else:
                logger.error(f"Failed to get org inventory: {ssrs_response.status_code}")
                print("X  Failed to validate SSR inventory")
        except Exception as e:
            logger.error(f"Error getting org SSR inventory: {e}")
            print(f"X  Error validating SSR inventory: {e}")
        
        try:
            # Process each site for SSR upgrades
            for site_index, site in enumerate(selected_sites, 1):
                site_id = site.get('id')
                site_name = site.get('name', 'Unknown')
                
                print(f"\n[{site_index}/{len(selected_sites)}] Processing site: {site_name}")
                logger.info(f"Processing site {site_index}/{len(selected_sites)}: {site_name} (ID: {site_id})")
                
                site_result = {
                    'site_id': site_id,
                    'site_name': site_name,
                    'ssrs_found': 0,
                    'upgrade_initiated': False,
                    'error': None
                }
                
                try:
                    # Get SSRs at this site
                    print(f"  -> Discovering SSRs at {site_name}...")
                    site_devices_response = mistapi.api.v1.sites.devices.listSiteDevices(
                        self.apisession, site_id, type='gateway'
                    )
                    
                    if site_devices_response.status_code != 200:
                        error_msg = f"Failed to retrieve devices for site {site_name}: {site_devices_response.status_code}"
                        print(f"  X  {error_msg}")
                        site_result['error'] = error_msg
                        upgrade_results['errors'].append(error_msg)
                        continue
                    
                    site_devices = site_devices_response.data
                    
                    # Filter for SSRs at this site
                    site_ssrs = []
                    for device in site_devices:
                        device_model = device.get('model', '')
                        device_type = device.get('type', '')
                        device_id = device.get('id', '')
                        
                        # Debug: Log device details
                        logger.debug(f"Device {device_id}: model='{device_model}', type='{device_type}'")
                        
                        # Check if this is an SSR
                        if (device_type == 'gateway' and 
                            (any(ssr_pattern in device_model for ssr_pattern in ssr_models) or 'SSR' in device_model)):
                            site_ssrs.append(device)
                            logger.info(f"Identified SSR device: {device_id} (model: {device_model}, type: {device_type})")
                            print(f"    -> Identified SSR: {device_model} ({device_id})")
                        else:
                            logger.debug(f"Skipping non-SSR device: {device_id} (model: {device_model}, type: {device_type})")
                    
                    site_result['ssrs_found'] = len(site_ssrs)
                    
                    if not site_ssrs:
                        print(f"  -> No SSRs found at {site_name}, skipping")
                        logger.info(f"No SSRs found at site {site_name}")
                        upgrade_results['sites_processed'] += 1
                        upgrade_results['site_results'].append(site_result)
                        continue
                    
                    print(f"  !? Found {len(site_ssrs)} SSR(s) at {site_name}")
                    
                    # Initiate firmware upgrade for SSRs at this site
                    ssr_device_ids = [ssr['id'] for ssr in site_ssrs]
                    
                    # Validate device IDs against org SSR inventory and check firmware versions
                    validated_device_ids = []
                    skipped_device_ids = []
                    for device_id in ssr_device_ids:
                        if device_id in org_ssr_inventory:
                            ssr_info = org_ssr_inventory[device_id]
                            current_version = ssr_info.get('version', '')
                            
                            # Check if device is already at target version
                            if current_version == target_version:
                                logger.info(f"Device {device_id} already at target version {target_version} - skipping")
                                print(f"    -> Device {device_id} already at version {target_version} - skipping")
                                skipped_device_ids.append(device_id)
                            else:
                                # Check for potential firmware downgrade
                                if self._is_firmware_downgrade(current_version, target_version):
                                    logger.warning(f"Device {device_id} downgrade detected: {current_version} -> {target_version} - skipping")
                                    print(f"    ! Downgrade detected: {ssr_info['model']} ({current_version} -> {target_version}) - skipping")
                                    skipped_device_ids.append(device_id)
                                else:
                                    validated_device_ids.append(device_id)
                                    logger.info(f"Validated SSR device: {device_id} (model: {ssr_info['model']}, current: {current_version} -> target: {target_version})")
                                    print(f"    -> Upgrade needed: {ssr_info['model']} ({current_version} -> {target_version})")
                        else:
                            logger.warning(f"Device {device_id} not found in org SSR inventory - skipping")
                            print(f"    !? Device {device_id} not in SSR inventory - skipping")
                            skipped_device_ids.append(device_id)
                    
                    if not validated_device_ids:
                        if skipped_device_ids:
                            reason = "already at target version or not in SSR inventory"
                            logger.info(f"All devices at {site_name} skipped: {reason}")
                            print(f"  -> All devices at {site_name} skipped ({reason})")
                        else:
                            logger.warning(f"No validated SSR devices found at {site_name}")
                            print(f"  -> No validated SSR devices at {site_name}, skipping")
                        upgrade_results['sites_processed'] += 1
                        upgrade_results['site_results'].append(site_result)
                        continue
                    
                    print(f"  -> Initiating firmware upgrade for {len(validated_device_ids)} SSR(s) needing upgrade...")
                    if skipped_device_ids:
                        print(f"  -> Skipped {len(skipped_device_ids)} device(s) (already at target version or other issues)")
                    logger.info(f"Initiating SSR firmware upgrade at {site_name} for validated devices: {validated_device_ids}")
                    
                    # Use Mist API to upgrade SSR firmware
                    # SECURITY: SSR upgrades use org-level API with specific parameters
                    upgrade_body = {
                        'device_ids': validated_device_ids,
                        'channel': firmware_channel,
                        'version': target_version,
                        'strategy': upgrade_strategy
                    }
                    
                    # Add reboot timing if auto-reboot enabled
                    if auto_reboot:
                        # For auto-reboot, don't set reboot_at (use default timing)
                        # The default is start_time, which enables reboot after download
                        pass  # Let API use default reboot timing
                    else:
                        # Disable reboot if auto_reboot is False
                        upgrade_body['reboot_at'] = -1
                    
                    # Debug: Log the upgrade request body
                    logger.info(f"SSR upgrade request body: {upgrade_body}")
                    print(f"  -> Request body: channel='{firmware_channel}', version='{target_version}', strategy='{upgrade_strategy}'")
                    print(f"  -> Device IDs: {validated_device_ids}")
                    
                    # Execute the SSR-specific upgrade API call
                    upgrade_response = mistapi.api.v1.orgs.ssr.upgradeOrgSsrs(
                        self.apisession, 
                        self.org_id,
                        body=upgrade_body
                    )
                    
                    if upgrade_response.status_code in [200, 202]:
                        print(f"  !? Firmware upgrade initiated for {len(validated_device_ids)} SSR(s)")
                        site_result['upgrade_initiated'] = True
                        upgrade_results['ssrs_upgraded'] += len(validated_device_ids)
                        logger.info(f"Successfully initiated SSR firmware upgrade at {site_name}")
                    else:
                        # Log response details for debugging
                        try:
                            # Try multiple ways to get response content
                            if hasattr(upgrade_response, 'data') and upgrade_response.data:
                                response_text = str(upgrade_response.data)
                            elif hasattr(upgrade_response, 'text') and upgrade_response.text:
                                response_text = upgrade_response.text
                            elif hasattr(upgrade_response, 'content') and upgrade_response.content:
                                response_text = upgrade_response.content.decode('utf-8')
                            else:
                                response_text = f"Status: {upgrade_response.status_code}, Headers: {dict(upgrade_response.headers) if hasattr(upgrade_response, 'headers') else 'None'}"
                            
                            # Check for specific error types
                            if 'already at the requested fw version' in response_text.lower():
                                # This is informational, not a real error
                                logger.info(f"SSR upgrade skipped at {site_name}: devices already at target version")
                                print(f"  - SSRs at {site_name} already at target version {target_version}")
                                site_result['upgrade_initiated'] = False
                                site_result['skip_reason'] = 'already_at_version'
                                # Don't count this as an error
                            elif 'downgrade fw version not allowed' in response_text.lower():
                                # This is a validation error, not a system error
                                logger.warning(f"SSR downgrade rejected at {site_name}: API prevents firmware downgrades")
                                print(f"  ! Firmware downgrade not allowed at {site_name} - API validation failed")
                                site_result['upgrade_initiated'] = False
                                site_result['skip_reason'] = 'downgrade_not_allowed'
                                # Don't count this as a critical error
                            else:
                                logger.error(f"SSR upgrade API error response: {response_text}")
                                print(f"  -> API Response: {response_text}")
                                
                                error_msg = f"Upgrade initiation failed for {site_name}: {upgrade_response.status_code}"
                                print(f"  X  {error_msg}")
                                site_result['error'] = error_msg
                                upgrade_results['errors'].append(error_msg)
                                logger.error(f"SSR firmware upgrade failed at {site_name}: {upgrade_response.status_code}")
                                
                        except Exception as e:
                            logger.error(f"Could not read response details: {e}")
                            print(f"  -> Could not read response: {e}")
                            
                            error_msg = f"Upgrade initiation failed for {site_name}: {upgrade_response.status_code}"
                            print(f"  X  {error_msg}")
                            site_result['error'] = error_msg
                            upgrade_results['errors'].append(error_msg)
                            logger.error(f"SSR firmware upgrade failed at {site_name}: {upgrade_response.status_code}")
                    
                except Exception as site_error:
                    error_msg = f"Error processing site {site_name}: {str(site_error)}"
                    print(f"  X  {error_msg}")
                    site_result['error'] = error_msg
                    upgrade_results['errors'].append(error_msg)
                    logger.error(f"Site processing error for {site_name}: {str(site_error)}")
                
                upgrade_results['sites_processed'] += 1
                upgrade_results['site_results'].append(site_result)
            
            # Operation completion
            upgrade_results['end_time'] = datetime.now().isoformat()
            
            print(f"\n{'='*60}")
            print("SSR FIRMWARE UPGRADE OPERATION COMPLETED")
            print(f"{'='*60}")
            print(f"Operation ID: {upgrade_results['operation_id']}")
            print(f"Sites processed: {upgrade_results['sites_processed']}")
            print(f"SSRs upgraded: {upgrade_results['ssrs_upgraded']}")
            print(f"Errors encountered: {len(upgrade_results['errors'])}")
            
            if upgrade_results['errors']:
                print(f"\nErrors:")
                for error in upgrade_results['errors']:
                    print(f"  - {error}")
            
            print(f"\nSSR upgrade operations have been initiated.")
            print(f"Monitor progress through Mist dashboard or API.")
            print(f"Check individual SSR status for completion and connectivity.")
            print(f"Verify SD-WAN tunnel re-establishment after reboots.")
            
            logger.info(f"SSR firmware upgrade operation completed: {upgrade_results['operation_id']}")
            return upgrade_results
            
        except Exception as e:
            error_msg = f"Critical error in SSR firmware upgrade: {str(e)}"
            print(f"\nX  {error_msg}")
            logger.error(error_msg)
            
            upgrade_results['end_time'] = datetime.now().isoformat()
            upgrade_results['error'] = str(e)
            
            return upgrade_results

    def upgrade_ssr_firmware_by_gateway_template(self):
        """
        Advanced SSR firmware upgrade organized by Gateway Template assignment.
        
        This method provides template-based SSR firmware upgrades with:
        1. Interactive Gateway Template selection with site count display
        2. Automatic site discovery for selected template (same logic as AP/switch systems)
        3. SSR enumeration across all sites in template  
        4. Model-based firmware version selection optimized for Session Smart Routers
        5. Unified upgrade execution across template sites
        6. SSR-specific safety measures and WAN connectivity disruption warnings
        7. Comprehensive audit logging and progress monitoring
        8. HA pair coordination and failover considerations
        
        Features:
        - Template selection by index or name (reuses AP/switch template infrastructure)
        - Site count and SSR count display per template
        - SSR-specific upgrade parameters (reboot, snapshot, conservative strategy)
        - Enhanced WAN connectivity warnings for production environments
        - Maintains all existing safety confirmations and audit trails
        
        SECURITY: Template-based upgrades affect multiple sites simultaneously.
        Ensure adequate maintenance windows and backup connectivity before proceeding.
        """
        logging.info("Starting template-based SSR firmware upgrade...")
        logging.debug("FirmwareManager.upgrade_ssr_firmware_by_gateway_template() initiated")
        
        print(" Advanced SSR Firmware Upgrade by Gateway Template")
        print("=" * 70)
        
        # Step 1: Ensure required CSVs are fresh (reuse AP/switch template infrastructure)
        self._ensure_template_csv_freshness()
        
        # Step 2: Load template-to-sites mapping (same as AP/switch systems)
        template_name_to_id, template_sites_mapping = self._load_template_sites_mapping()
        
        if not template_sites_mapping:
            print("\n! No Gateway Templates with assigned sites found.")
            print("  Make sure sites are assigned to Gateway Templates and try again.")
            logging.warning("No Gateway Templates with site assignments found")
            return
        
        # Step 3: Template selection (reuse AP/switch template selection logic)
        selected_template_id, selected_template_name = self._prompt_template_selection(
            template_name_to_id, template_sites_mapping)
        
        if not selected_template_id:
            print(" No template selected. Exiting.")
            return
        
        # Step 4: Get sites for selected template
        sites_to_upgrade = template_sites_mapping.get(selected_template_id, [])
        
        print(f"\n  Template '{selected_template_name}' includes {len(sites_to_upgrade)} sites")
        logging.info(f"Template {selected_template_name} has {len(sites_to_upgrade)} assigned sites")
        
        return self._execute_template_based_ssr_upgrade(sites_to_upgrade, selected_template_name)
    
    def _execute_template_based_ssr_upgrade(self, sites_to_upgrade, selected_template_name):
        """Execute the template-based SSR upgrade with the existing SSR implementation."""
        print(f"  Proceeding with SSR firmware upgrade for template: {selected_template_name}")
        print(f"  Target sites: {len(sites_to_upgrade)}")
        
        # Use the SSR-specific bulk upgrade implementation
        return self.bulk_upgrade_ssr_firmware_by_site(sites_to_upgrade)


def create_progress_bar(progress_percentage, bar_length=20):
    """
    Create an ASCII progress bar visualization for upgrade progress.
    
    Args:
        progress_percentage (int): Progress value from 0 to 100
        bar_length (int): Total length of the progress bar in characters
    
    Returns:
        str: Formatted progress bar string like "[=========>          ] 45%"
    """
    if progress_percentage is None or progress_percentage < 0:
        progress_percentage = 0
    elif progress_percentage > 100:
        progress_percentage = 100
    
    filled_length = int(bar_length * progress_percentage / 100)
    
    if filled_length == bar_length:
        # Complete: all filled
        bar = '=' * bar_length
    elif filled_length == 0:
        # Just started: all empty
        bar = ' ' * bar_length
    else:
        # In progress: filled portion + arrow + empty portion
        bar = '=' * (filled_length - 1) + '>' + ' ' * (bar_length - filled_length)
    
    return f"[{bar}] {progress_percentage:3d}%"


def check_firmware_upgrade_status_direct():
    """
    Direct firmware status check using FirmwareManager.
    Avoids the deprecated wrapper chain that causes double prompting.
    """
    logging.info("Starting firmware status check using FirmwareManager directly")
    org_id = get_cached_or_prompted_org_id()
    
    firmware_manager = FirmwareManager(apisession, org_id)
    return firmware_manager.check_firmware_upgrade_status()

def check_firmware_upgrade_status():
    """
    DEPRECATED: Use FirmwareManager.check_firmware_upgrade_status() instead.
    
    Maintained for backward compatibility.
    
    Check current firmware upgrade status across the organization.
    
    This function provides comprehensive upgrade status monitoring with:
    1. Device-level firmware status from device statistics (fwupdate field)
    2. Site-level upgrade operations and history
    3. Organization-wide upgrade tracking
    4. Current version vs. available version comparison
    5. Upgrade progress monitoring for active operations
    6. Failed upgrade identification and retry status
    7. Bulk status export to CSV for analysis
    8. Interactive site/device filtering options
    
    Reports include:
    - Current firmware versions and upgrade status per device
    - Active upgrade operations with progress tracking
    - Failed upgrades with error details and retry information
    - Upgrade history and completion statistics
    - Version mismatch identification across sites
    """
    org_id = get_cached_or_prompted_org_id()
    firmware_manager = FirmwareManager(apisession, org_id)
    return firmware_manager.check_firmware_upgrade_status()


def check_firmware_upgrade_status_impl(scope_choice=None, site_filter=None):
    """
    Implementation function for firmware upgrade status checking.
    This contains the actual logic from the original function.
    """
    logging.info("Starting firmware upgrade status check...")
    logging.debug("check_firmware_upgrade_status_impl() initiated")
    org_id = get_cached_or_prompted_org_id()
    logging.debug(f"Using org_id: {org_id}")
    
    # Scope selection is now handled by FirmwareManager.check_firmware_upgrade_status()
    # This function receives the validated scope_choice and site_filter parameters
    
    if scope_choice == '2' and site_filter is None:
        # Get specific site selection
        logging.debug("User selected specific site mode")
        site_filter = prompt_site_selection()
        if not site_filter:
            print(" No site selected. Exiting.")
            logging.warning("No site selected in specific site mode")
            return
        logging.debug(f"Selected site filter: {site_filter}")
    
    # Step 2: Fetch device statistics to get current firmware status
    print(f"\n  Fetching device statistics...")
    logging.debug(f"Fetching device statistics with scope: {scope_choice}, site_filter: {site_filter}")
    all_device_stats = []
    upgrade_results = []
    
    try:
        if site_filter:
            # Single site mode
            print(f"   Fetching stats for selected site...")
            logging.debug(f"Fetching stats for single site: {site_filter}")
            stats_resp = mistapi.api.v1.sites.stats.listSiteDevicesStats(
                apisession, 
                site_filter,
                type="all",
                limit=1000
            )
            site_stats = mistapi.get_all(response=stats_resp, mist_session=apisession)
            all_device_stats.extend(site_stats)
            
            print(f"   Retrieved stats for {len(site_stats)} devices at selected site")
            logging.info(f"Retrieved stats for {len(site_stats)} devices at site {site_filter}")
        else:
            # Organization-wide mode
            print(f"   Fetching organization-wide device statistics...")
            logging.debug(f"Fetching organization-wide stats for org: {org_id}")
            # Request all standard fields plus fwupdate field for firmware upgrade status
            # Using fields=* to get all available fields including fwupdate
            stats_resp = mistapi.api.v1.orgs.stats.listOrgDevicesStats(
                apisession, 
                org_id,
                type="all",
                fields="*",
                limit=1000
            )
            org_stats = mistapi.get_all(response=stats_resp, mist_session=apisession)
            all_device_stats.extend(org_stats)
            
            print(f"   Retrieved stats for {len(org_stats)} devices organization-wide")
            logging.info(f"Retrieved stats for {len(org_stats)} devices organization-wide")
            
    except Exception as e:
        print(f"! Failed to fetch device statistics: {e}")
        logging.error(f"Failed to fetch device statistics: {e}")
        return
    
    if not all_device_stats:
        print(" No device statistics found.")
        return
    
    # Step 3: Process device firmware status
    print(f"\n  Analyzing firmware status for {len(all_device_stats)} devices...")
    
    firmware_status_summary = {
        'total_devices': 0,
        'devices_with_fwupdate': 0,
        'upgrade_in_progress': 0,
        'upgrade_failed': 0,
        'upgrade_completed': 0,
        'upgrade_unknown': 0,
        'devices_by_status': {},
        'devices_by_version': {},
        'devices_by_model': {},
        'progress_total': 0,
        'progress_count': 0,
        'devices_upgrading': []
    }
    
    # Get site information for enrichment
    print(f"   Fetching site information for device enrichment...")
    try:
        all_sites = fetch_all_sites_with_limit(org_id)
        site_lookup = {site.get('id'): site.get('name', 'Unknown') for site in all_sites}
    except Exception as e:
        logging.warning(f"Failed to fetch site information: {e}")
        site_lookup = {}
    
    for device_stats in all_device_stats:
        device_id = device_stats.get('id', 'Unknown')
        device_name = device_stats.get('name', 'Unnamed')
        device_mac = device_stats.get('mac', 'Unknown')
        device_model = device_stats.get('model', 'Unknown')
        device_type = device_stats.get('type', 'Unknown')
        device_version = device_stats.get('version', 'Unknown')
        site_id = device_stats.get('site_id', 'Unknown')
        site_name = site_lookup.get(site_id, 'Unknown Site')
        last_seen = device_stats.get('last_seen', 0)
        
        # Process fwupdate status
        fwupdate = device_stats.get('fwupdate', {})
        if fwupdate:
            firmware_status_summary['devices_with_fwupdate'] += 1
            
            fw_status = fwupdate.get('status', 'unknown')
            fw_progress = fwupdate.get('progress', 0)
            fw_timestamp = fwupdate.get('timestamp', 0)
            fw_status_id = fwupdate.get('status_id', 0)
            fw_will_retry = fwupdate.get('will_retry', False)
            
            # Categorize by status
            # Note: API documentation says inprogress/failed/upgraded but actual responses 
            # may return success/upgrading/downloading/failed due to device type differences
            if fw_status in ('inprogress', 'upgrading', 'downloading'):
                # Check if this is a stale completed upgrade (100% complete but status not updated)
                is_stale_upgrade = False
                if fw_progress == 100 and fw_timestamp and isinstance(fw_timestamp, (int, float)) and fw_timestamp > 0:
                    try:
                        upgrade_age_hours = (time.time() - fw_timestamp) / 3600
                        if upgrade_age_hours > 1:  # More than 1 hour old at 100%
                            is_stale_upgrade = True
                            logging.debug(f"Device {device_name} shows 100% complete {upgrade_age_hours:.1f}h ago but still marked '{fw_status}' - treating as completed")
                    except (ValueError, OSError, TypeError) as e:
                        logging.debug(f"Could not calculate upgrade age for {device_name}: {e}")
                
                if is_stale_upgrade:
                    # Treat as completed rather than in-progress
                    firmware_status_summary['upgrade_completed'] += 1
                else:
                    firmware_status_summary['upgrade_in_progress'] += 1
                    # Track progress statistics for in-progress upgrades
                    if fw_progress is not None and isinstance(fw_progress, (int, float)):
                        firmware_status_summary['progress_total'] += fw_progress
                        firmware_status_summary['progress_count'] += 1
                        
                    # Track device details for progress display (only truly active upgrades)
                    firmware_status_summary['devices_upgrading'].append({
                        'device_name': device_name,
                        'device_mac': device_mac,
                        'device_type': device_type,
                        'device_model': device_model,
                        'site_name': site_name,
                        'current_version': device_version,
                        'progress': fw_progress if fw_progress is not None else 0,
                        'fw_time_str': None,  # Will be set below
                        'fw_timestamp': fw_timestamp  # Store for later filtering
                    })
            elif fw_status == 'failed':
                firmware_status_summary['upgrade_failed'] += 1
            elif fw_status in ('upgraded', 'success'):
                firmware_status_summary['upgrade_completed'] += 1
            else:
                firmware_status_summary['upgrade_unknown'] += 1
            
            # Track status distribution
            if fw_status not in firmware_status_summary['devices_by_status']:
                firmware_status_summary['devices_by_status'][fw_status] = 0
            firmware_status_summary['devices_by_status'][fw_status] += 1
            
            # Format timestamps for display
            fw_time_str = "Unknown"
            if fw_timestamp and isinstance(fw_timestamp, (int, float)) and fw_timestamp > 0:
                try:
                    fw_time_str = datetime.fromtimestamp(fw_timestamp).strftime('%Y-%m-%d %H:%M:%S')
                except (ValueError, OSError, TypeError) as e:
                    logging.debug(f"Invalid firmware timestamp {fw_timestamp}: {e}")
                    fw_time_str = f"Invalid timestamp: {fw_timestamp}"
            
            # Update timestamp in devices_upgrading if device was added
            if fw_status in ('inprogress', 'upgrading', 'downloading') and firmware_status_summary['devices_upgrading']:
                firmware_status_summary['devices_upgrading'][-1]['fw_time_str'] = fw_time_str
            
            last_seen_str = "Unknown"
            if last_seen and isinstance(last_seen, (int, float)) and last_seen > 0:
                try:
                    last_seen_str = datetime.fromtimestamp(last_seen).strftime('%Y-%m-%d %H:%M:%S')
                except (ValueError, OSError, TypeError) as e:
                    logging.debug(f"Invalid last_seen timestamp {last_seen}: {e}")
                    last_seen_str = f"Invalid timestamp: {last_seen}"
        else:
            fw_status = "no_upgrade_info"
            fw_progress = 0
            fw_timestamp = 0
            fw_status_id = 0
            fw_will_retry = False
            fw_time_str = "N/A"
            last_seen_str = "Unknown"
            if last_seen and isinstance(last_seen, (int, float)) and last_seen > 0:
                try:
                    last_seen_str = datetime.fromtimestamp(last_seen).strftime('%Y-%m-%d %H:%M:%S')
                except (ValueError, OSError, TypeError) as e:
                    logging.debug(f"Invalid last_seen timestamp {last_seen}: {e}")
                    last_seen_str = f"Invalid timestamp: {last_seen}"
        
        # Track version distribution
        if device_version not in firmware_status_summary['devices_by_version']:
            firmware_status_summary['devices_by_version'][device_version] = 0
        firmware_status_summary['devices_by_version'][device_version] += 1
        
        # Track model distribution
        if device_model not in firmware_status_summary['devices_by_model']:
            firmware_status_summary['devices_by_model'][device_model] = 0
        firmware_status_summary['devices_by_model'][device_model] += 1
        
        # Track device type distribution
        if 'devices_by_type' not in firmware_status_summary:
            firmware_status_summary['devices_by_type'] = {}
        if device_type not in firmware_status_summary['devices_by_type']:
            firmware_status_summary['devices_by_type'][device_type] = 0
        firmware_status_summary['devices_by_type'][device_type] += 1
        
        firmware_status_summary['total_devices'] += 1
        
        # Apply scope filtering
        include_device = True
        if scope_choice == '3':  # Active upgrades only - exclude stale completed upgrades
            is_active_upgrade = fw_status in ('inprogress', 'upgrading', 'downloading')
            if is_active_upgrade and fw_progress == 100:
                # Check if it's a stale upgrade
                if fw_timestamp and isinstance(fw_timestamp, (int, float)) and fw_timestamp > 0:
                    try:
                        upgrade_age_hours = (time.time() - fw_timestamp) / 3600
                        if upgrade_age_hours > 1:
                            is_active_upgrade = False  # Exclude from active upgrades filter
                    except (ValueError, OSError, TypeError):
                        pass
            include_device = is_active_upgrade
        elif scope_choice == '4':  # Failed upgrades only
            include_device = fw_status == 'failed'
        
        if include_device:
            # Create a visual progress display for CSV
            progress_display = "N/A"
            
            # Check if this is a stale completed upgrade (for display purposes)
            is_stale_display = False
            if fw_status in ('inprogress', 'upgrading', 'downloading') and fw_progress == 100:
                if fw_timestamp and isinstance(fw_timestamp, (int, float)) and fw_timestamp > 0:
                    try:
                        upgrade_age_hours = (time.time() - fw_timestamp) / 3600
                        if upgrade_age_hours > 1:
                            is_stale_display = True
                    except (ValueError, OSError, TypeError):
                        pass
            
            if is_stale_display:
                # Show as complete in CSV even though API status says inprogress
                progress_display = "[===============] 100% (Complete - Stale)"
            elif fw_status in ('inprogress', 'upgrading', 'downloading') and fw_progress is not None:
                progress_display = create_progress_bar(fw_progress, bar_length=15)
            elif fw_status in ('upgraded', 'success'):
                progress_display = "[===============] 100% (Complete)"
            elif fw_status == 'failed':
                progress_display = "[!!!!! FAILED !!!!!]"
            
            upgrade_results.append({
                'Site ID': site_id,
                'Site Name': site_name,
                'Device ID': device_id,
                'Device Name': device_name,
                'Device MAC': device_mac,
                'Device Model': device_model,
                'Device Type': device_type,
                'Current Version': device_version,
                'Last Seen': last_seen_str,
                'FW Upgrade Status': fw_status,
                'FW Progress %': fw_progress,
                'FW Progress Display': progress_display,
                'FW Status ID': fw_status_id,
                'FW Will Retry': fw_will_retry,
                'FW Timestamp': fw_time_str,
                'Timestamp': datetime.now(timezone.utc).isoformat()
            })
    
    # Step 4: Display summary statistics
    print(f"\n  Firmware Status Summary:")
    print(f"   X  Total devices analyzed: {firmware_status_summary['total_devices']}")
    print(f"   X  Devices with upgrade info: {firmware_status_summary['devices_with_fwupdate']}")
    print(f"   X  Upgrades in progress: {firmware_status_summary['upgrade_in_progress']}")
    
    # Calculate and display average progress for in-progress upgrades
    if firmware_status_summary['progress_count'] > 0:
        avg_progress = firmware_status_summary['progress_total'] / firmware_status_summary['progress_count']
        progress_bar = create_progress_bar(int(avg_progress))
        print(f"   X  Average upgrade progress: {progress_bar}")
    
    print(f"   X  Upgrades completed: {firmware_status_summary['upgrade_completed']}")
    print(f"   X  Upgrades failed: {firmware_status_summary['upgrade_failed']}")
    print(f"   X  Unknown status: {firmware_status_summary['upgrade_unknown']}")
    
    if firmware_status_summary['devices_by_status']:
        print(f"\n  Status Distribution:")
        for status, count in sorted(firmware_status_summary['devices_by_status'].items()):
            print(f"   X  {status}: {count} devices")
    
    print(f"\n  Device Type Distribution:")
    if 'devices_by_type' in firmware_status_summary and firmware_status_summary['devices_by_type']:
        sorted_types = sorted(firmware_status_summary['devices_by_type'].items(), 
                            key=lambda x: x[1], reverse=True)
        for device_type, count in sorted_types:
            type_display = {
                'ap': 'Access Points',
                'switch': 'Switches', 
                'gateway': 'Gateways/SSRs',
                'ssr': 'Session Smart Routers'
            }.get(device_type, device_type.upper())
            print(f"   X  {type_display}: {count} devices")
    else:
        print(f"   X  No device type information available")
    
    print(f"\n  Version Distribution:")
    sorted_versions = sorted(firmware_status_summary['devices_by_version'].items(), 
                           key=lambda x: x[1], reverse=True)
    for version, count in sorted_versions[:10]:  # Show top 10 versions
        print(f"   X  {version}: {count} devices")
    if len(sorted_versions) > 10:
        print(f"   ... and {len(sorted_versions) - 10} more versions")
    
    print(f"\n  Model Distribution:")
    sorted_models = sorted(firmware_status_summary['devices_by_model'].items(), 
                          key=lambda x: x[1], reverse=True)
    for model, count in sorted_models[:10]:  # Show top 10 models
        print(f"   X  {model}: {count} devices")
    if len(sorted_models) > 10:
        print(f"   ... and {len(sorted_models) - 10} more models")
    
    # Step 4b: Display detailed progress for devices currently upgrading
    if firmware_status_summary['devices_upgrading']:
        print(f"\n  Devices Currently Upgrading (Real-Time Progress):")
        print(f"  {'='*90}")
        
        # Sort by progress (highest first) to show devices closest to completion
        sorted_upgrading = sorted(firmware_status_summary['devices_upgrading'], 
                                 key=lambda x: x['progress'], reverse=True)
        
        # Display header
        print(f"  {'Device Name':<25} {'Type':<8} {'Site':<20} {'Progress':<30}")
        print(f"  {'-'*25} {'-'*8} {'-'*20} {'-'*30}")
        
        for device in sorted_upgrading[:20]:  # Show top 20 upgrading devices
            device_name_short = device['device_name'][:24] if device['device_name'] else 'Unnamed'
            device_type_short = device['device_type'][:7] if device['device_type'] else 'Unknown'
            site_name_short = device['site_name'][:19] if device['site_name'] else 'Unknown'
            progress_bar = create_progress_bar(device['progress'], bar_length=15)
            
            print(f"  {device_name_short:<25} {device_type_short:<8} {site_name_short:<20} {progress_bar}")
        
        if len(sorted_upgrading) > 20:
            print(f"  ... and {len(sorted_upgrading) - 20} more devices upgrading")
        
        print(f"  {'='*90}")
        
        # Show progress distribution
        progress_ranges = {
            '0-25%': 0,
            '26-50%': 0,
            '51-75%': 0,
            '76-99%': 0,
            '100%': 0
        }
        
        for device in sorted_upgrading:
            progress = device['progress']
            if progress == 0 or progress <= 25:
                progress_ranges['0-25%'] += 1
            elif progress <= 50:
                progress_ranges['26-50%'] += 1
            elif progress <= 75:
                progress_ranges['51-75%'] += 1
            elif progress < 100:
                progress_ranges['76-99%'] += 1
            else:
                progress_ranges['100%'] += 1
        
        print(f"\n  Progress Distribution:")
        for range_label, count in progress_ranges.items():
            if count > 0:
                print(f"   X  {range_label}: {count} device(s)")
    
    # Step 5: Check for active upgrade operations
    print(f"\n  Checking for active upgrade operations...")
    active_upgrades = []
    
    # Step 5a: Check for active SSR upgrade operations (org-level)
    try:
        print(f"   Checking for active SSR upgrade operations...")
        ssr_upgrades_resp = mistapi.api.v1.orgs.ssr.listOrgSsrUpgrades(apisession, org_id)
        
        if ssr_upgrades_resp.status_code == 200 and hasattr(ssr_upgrades_resp, 'data'):
            ssr_upgrades = ssr_upgrades_resp.data
            if ssr_upgrades:
                print(f"   !? Found {len(ssr_upgrades)} SSR upgrade operation(s)")
                
                for ssr_upgrade in ssr_upgrades:
                    upgrade_id = ssr_upgrade.get('id', 'Unknown')
                    status = ssr_upgrade.get('status', 'Unknown')
                    strategy = ssr_upgrade.get('strategy', 'Unknown')
                    channel = ssr_upgrade.get('channel', 'Unknown')
                    device_type = ssr_upgrade.get('device_type', 'SSR')
                    counts = ssr_upgrade.get('counts', {})
                    versions = ssr_upgrade.get('versions', {})
                    
                    # Extract device count and version information
                    total_devices = sum(counts.values()) if counts else 0
                    upgrading_count = counts.get('upgrading', 0)
                    success_count = counts.get('success', 0)
                    failed_count = counts.get('failed', 0)
                    queued_count = counts.get('queued', 0)
                    
                    # Try to extract version from versions mapping (take first available)
                    target_versions = list(versions.values()) if versions else []
                    version_info = f"-> {target_versions[0]}" if target_versions else "Multiple versions"
                    if len(target_versions) > 1:
                        version_info = f"Multiple versions ({len(target_versions)} different)"
                    
                    # Create status summary
                    status_parts = []
                    if upgrading_count > 0:
                        status_parts.append(f"{upgrading_count} upgrading")
                    if success_count > 0:
                        status_parts.append(f"{success_count} completed")
                    if failed_count > 0:
                        status_parts.append(f"{failed_count} failed")
                    if queued_count > 0:
                        status_parts.append(f"{queued_count} queued")
                    
                    status_summary = " | ".join(status_parts) if status_parts else f"Status: {status}"
                    
                    print(f"      SSR Upgrade {upgrade_id[:8]}... [{strategy} strategy]: {status_summary}")
                    print(f"         Channel: {channel} | Devices: {total_devices} | {version_info}")
                    
                    active_upgrades.append({
                        'upgrade_id': upgrade_id,
                        'site_id': 'N/A (Org-level)',
                        'site_name': 'SSR Upgrade (Org-level)',
                        'status': status,
                        'strategy': strategy,
                        'channel': channel,
                        'device_type': device_type,
                        'source': 'ssr_api',
                        'total_devices': total_devices,
                        'upgrading': upgrading_count,
                        'success': success_count,
                        'failed': failed_count,
                        'queued': queued_count,
                        'versions': versions,
                        'details': ssr_upgrade
                    })
            else:
                print(f"   -> No active SSR upgrade operations found")
        else:
            print(f"   -> Failed to retrieve SSR upgrade operations: {ssr_upgrades_resp.status_code}")
            
    except Exception as e:
        print(f"   -> Error checking SSR upgrade operations: {e}")
        logging.warning(f"Failed to check SSR upgrade operations: {e}")
    
    # Step 5b: Check stored upgrade IDs from site-level upgrades (AP/Switch)
    print(f"   Checking for site-level upgrade operations...")
    upgrade_tracking_file = "ActiveUpgrades.json"
    stored_upgrades = []
    
    if os.path.exists(upgrade_tracking_file):
        try:
            with open(upgrade_tracking_file, 'r', encoding='utf-8') as f:
                stored_upgrades = json.load(f)
            
            if stored_upgrades:
                # Filter to current org_id
                org_upgrades = [u for u in stored_upgrades if u.get('org_id') == org_id]
                if org_upgrades:
                    print(f"   !? Found {len(org_upgrades)} stored upgrade operation(s) from ActiveUpgrades.json")
                    
                    # Check status of each stored upgrade
                    for upgrade_record in org_upgrades:
                        upgrade_id = upgrade_record.get('upgrade_id')
                        site_id = upgrade_record.get('site_id')
                        site_name = upgrade_record.get('site_name', 'Unknown')
                        
                        if upgrade_id and site_id:
                            try:
                                # Get specific upgrade details
                                upgrade_resp = mistapi.api.v1.sites.devices.getSiteDeviceUpgrade(
                                    apisession, site_id, upgrade_id
                                )
                                
                                if upgrade_resp and hasattr(upgrade_resp, 'data') and upgrade_resp.data:
                                    upgrade_details = upgrade_resp.data
                                    status = upgrade_details.get('status', 'Unknown')
                                    strategy = upgrade_details.get('strategy', 'Unknown')
                                    target_version = upgrade_details.get('target_version', 'Unknown')
                                    
                                    print(f"      Upgrade {upgrade_id[:8]}... at site '{site_name}': Status = {status}")
                                    
                                    active_upgrades.append({
                                        'upgrade_id': upgrade_id,
                                        'site_id': site_id,
                                        'site_name': site_name,
                                        'status': status,
                                        'strategy': strategy,
                                        'target_version': target_version,
                                        'source': 'stored_tracking',
                                        'details': upgrade_details
                                    })
                                else:
                                    print(f"      Upgrade {upgrade_id[:8]}... at site '{site_name}': No longer active or not found")
                                    
                            except Exception as e:
                                print(f"      Failed to check upgrade {upgrade_id[:8]}... at site '{site_name}': {e}")
                                logging.warning(f"Failed to check stored upgrade {upgrade_id}: {e}")
                else:
                    print(f"   -> No stored upgrades match current organization")
        except Exception as e:
            print(f"   -> Failed to read stored upgrade tracking data: {e}")
            logging.warning(f"Failed to read stored upgrade tracking: {e}")
    else:
        print(f"   -> No site-level upgrade tracking file found (checking organization records)")
    
    # Step 5c: Check organization audit logs for recent upgrade events
    try:
        print(f"   Checking organization audit logs for recent upgrade events...")
        
        # Search for upgrade-related audit events in the last 24 hours
        end_time = int(time.time())
        start_time = end_time - (24 * 60 * 60)  # 24 hours ago
        
        audit_resp = mistapi.api.v1.orgs.logs.listOrgAuditLogs(
            apisession, 
            org_id,
            start=start_time,
            end=end_time,
            limit=1000
        )
        
        audit_logs = mistapi.get_all(response=audit_resp, mist_session=apisession)
        
        if audit_logs:
            upgrade_events = []
            for log_entry in audit_logs:
                message = log_entry.get('message', '').lower()
                if any(keyword in message for keyword in ['upgrade', 'firmware', 'version']):
                    upgrade_events.append(log_entry)
            
            if upgrade_events:
                print(f"   !? Found {len(upgrade_events)} upgrade-related audit event(s) in last 24 hours")
                
                # Show recent upgrade events (most recent first)
                for event in sorted(upgrade_events, key=lambda x: x.get('timestamp', 0), reverse=True)[:5]:
                    timestamp = event.get('timestamp', 0)
                    try:
                        event_time = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')
                    except:
                        event_time = 'Unknown'
                    
                    admin_name = event.get('admin_name', 'System')
                    message = event.get('message', 'No message')
                    site_name = event.get('site_name', 'Organization')
                    
                    # Clean up the message format for better readability
                    if 'device[' in message.lower() and '] upgrade' in message.lower():
                        # Extract device MAC and version from message
                        import re
                        device_match = re.search(r'device\[([^\]]+)\]', message, re.IGNORECASE)
                        version_match = re.search(r'version\s+([^\s]+)', message, re.IGNORECASE)
                        
                        device_id = device_match.group(1) if device_match else 'Unknown Device'
                        version = version_match.group(1) if version_match else 'Unknown Version'
                        
                        print(f"      -> {event_time} | {admin_name} | Device {device_id} upgrade to {version}")
                    else:
                        print(f"      -> {event_time} | {admin_name} | {site_name}: {message}")
            else:
                print(f"   -> No upgrade-related events found in audit logs")
        else:
            print(f"   -> No audit logs available for the last 24 hours")
            
    except Exception as e:
        print(f"   -> Error checking audit logs: {e}")
        logging.warning(f"Failed to search org audit logs for upgrades: {e}")
    
    # Step 5d: Check organization-level device events for upgrade activity
    try:
        print(f"   Checking organization device events for upgrade activity...")
        
        # Search for device upgrade events
        device_events_resp = mistapi.api.v1.orgs.devices.searchOrgDeviceEvents(
            apisession,
            org_id,
            type="SYSTEM_UPGRADE_COMPLETED,SYSTEM_UPGRADE_FAILED,SYSTEM_UPGRADE_STARTED",
            start=start_time,
            end=end_time,
            limit=50
        )
        
        device_events = mistapi.get_all(response=device_events_resp, mist_session=apisession)
        
        if device_events:
            print(f"   !? Found {len(device_events)} device upgrade event(s) in last 24 hours")
            
            # Group events by type for cleaner display
            events_by_type = {}
            for event in device_events:
                event_type = event.get('type', 'Unknown')
                if event_type not in events_by_type:
                    events_by_type[event_type] = []
                events_by_type[event_type].append(event)
            
            # Display summary by event type
            for event_type, type_events in events_by_type.items():
                # Clean up event type names for display
                type_display = event_type.replace('SYSTEM_UPGRADE_', '').title()
                print(f"      {type_display}: {len(type_events)} event(s)")
                
                # Show most recent events of this type
                for event in sorted(type_events, key=lambda x: x.get('timestamp', 0), reverse=True)[:3]:
                    timestamp = event.get('timestamp', 0)
                    try:
                        event_time = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')
                    except:
                        event_time = 'Unknown'
                    
                    device_name = event.get('device_name', 'Unknown Device')
                    site_name = event.get('site_name', 'Unknown Site')
                    
                    print(f"         -> {event_time} | {device_name} at {site_name}")
        else:
            print(f"   -> No device upgrade events found in last 24 hours")
            
    except Exception as e:
        print(f"   -> Error checking device events: {e}")
        logging.warning(f"Failed to search device upgrade events: {e}")
    
    # Step 6: Check individual site upgrade operations
    if not site_filter:
        print(f"\n   Checking individual site upgrade operations (first 5 sites)...")
    sites_to_check = [site_filter] if site_filter else list(site_lookup.keys())
    
    sites_with_upgrades = 0
    for site_id in sites_to_check[:5]:  # Limit to first 5 sites for performance
        try:
            site_name = site_lookup.get(site_id, 'Unknown')
            
            upgrades_resp = mistapi.api.v1.sites.devices.listSiteDeviceUpgrades(apisession, site_id)
            site_upgrades = mistapi.get_all(response=upgrades_resp, mist_session=apisession)
            
            if site_upgrades:
                sites_with_upgrades += 1
                print(f"   Site '{site_name}': !? {len(site_upgrades)} upgrade operation(s)")
                
                for upgrade in site_upgrades:
                    upgrade_id = upgrade.get('id', 'Unknown')
                    upgrade_status = upgrade.get('status', 'Unknown')
                    upgrade_strategy = upgrade.get('strategy', 'Unknown')
                    target_version = upgrade.get('target_version', 'Unknown')
                    start_time = upgrade.get('start_time', 0)
                    enable_p2p = upgrade.get('enable_p2p', False)
                    counts = upgrade.get('counts', {})
                    
                    # Format start time
                    start_time_str = "Unknown"
                    if start_time:
                        try:
                            start_time_str = datetime.fromtimestamp(start_time).strftime('%Y-%m-%d %H:%M:%S')
                        except:
                            start_time_str = str(start_time)
                    
                    # Create progress summary
                    total = counts.get('total', 0)
                    downloaded = counts.get('downloaded', 0)
                    rebooted = counts.get('rebooted', 0)
                    failed = counts.get('failed', 0)
                    
                    progress_parts = []
                    if total > 0:
                        if downloaded > 0:
                            progress_parts.append(f"{downloaded}/{total} downloaded")
                        if rebooted > 0:
                            progress_parts.append(f"{rebooted}/{total} rebooted")
                        if failed > 0:
                            progress_parts.append(f"{failed} failed")
                    
                    progress_info = " | ".join(progress_parts) if progress_parts else f"Status: {upgrade_status}"
                    
                    print(f"      Upgrade {upgrade_id[:8]}... [{upgrade_strategy}]: {progress_info}")
                    print(f"         Target: {target_version} | Started: {start_time_str}")
                    
                    active_upgrades.append({
                        'site_id': site_id,
                        'site_name': site_name,
                        'upgrade_id': upgrade_id,
                        'status': upgrade_status,
                        'strategy': upgrade_strategy,
                        'target_version': target_version,
                        'start_time': start_time_str,
                        'enable_p2p': enable_p2p,
                        'total_devices': counts.get('total', 0),
                        'downloaded': counts.get('downloaded', 0),
                        'download_requested': counts.get('download_requested', 0),
                        'rebooted': counts.get('rebooted', 0),
                        'reboot_in_progress': counts.get('reboot_in_progress', 0),
                        'failed': counts.get('failed', 0),
                        'skipped': counts.get('skipped', 0),
                        'source': 'site_lookup',
                        'timestamp': datetime.now(timezone.utc).isoformat()
                    })
            # Don't print anything for sites with no upgrades to reduce noise
                
        except Exception as e:
            print(f"   Site '{site_name}': -> Error checking upgrades: {e}")
            logging.warning(f"Failed to check upgrades for site {site_id}: {e}")
    
    # Summary message for site-level checks
    if not site_filter:
        sites_without_upgrades = min(5, len(sites_to_check)) - sites_with_upgrades
        if sites_without_upgrades > 0:
            print(f"   -> {sites_without_upgrades} site(s) have no active upgrade operations")
    
    # Step 6: Export results to CSV
    timestamp_suffix = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    if upgrade_results:
        device_status_file = f"FirmwareUpgradeStatus_{timestamp_suffix}.csv"
        fieldnames = ['Site ID', 'Site Name', 'Device ID', 'Device Name', 'Device MAC', 
                     'Device Model', 'Device Type', 'Current Version', 'Last Seen',
                     'FW Upgrade Status', 'FW Progress %', 'FW Progress Display', 
                     'FW Status ID', 'FW Will Retry', 'FW Timestamp', 'Timestamp']
        
        try:
            DataExporter.save_data_to_output(upgrade_results, device_status_file)
            print(f"\n[SUCCESS] Device firmware status exported to: data/{device_status_file}")
            print(f"   [DATA] {len(upgrade_results)} device records exported")
            logging.info(f"Exported {len(upgrade_results)} device firmware status records to data/{device_status_file}")
            
        except Exception as e:
            print(f"! Failed to export device status: {e}")
            logging.error(f"Failed to export device status: {e}")
    
    if active_upgrades:
        upgrade_ops_file = os.path.join("data", f"ActiveUpgradeOperations_{timestamp_suffix}.csv")
        upgrade_fieldnames = ['site_id', 'site_name', 'upgrade_id', 'status', 'strategy',
                             'target_version', 'start_time', 'enable_p2p', 'total_devices',
                             'downloaded', 'download_requested', 'rebooted', 'reboot_in_progress',
                             'failed', 'skipped', 'source', 'timestamp']
        
        try:
            # Normalize the active_upgrades data for CSV export
            mapped_upgrades = []
            for upgrade in active_upgrades:
                # Handle both direct field access and details field extraction
                details = upgrade.get('details', {})
                counts = details.get('counts', {}) if details else {}
                
                # Extract or use existing start_time
                start_time = upgrade.get('start_time') or details.get('start_time', 0)
                start_time_str = start_time  # Use as-is if already formatted
                if isinstance(start_time, (int, float)) and start_time > 0:
                    try:
                        start_time_str = datetime.fromtimestamp(start_time).strftime('%Y-%m-%d %H:%M:%S')
                    except:
                        start_time_str = str(start_time)
                elif not start_time_str:
                    start_time_str = "Unknown"
                
                # Extract or use existing enable_p2p
                enable_p2p = upgrade.get('enable_p2p')
                if enable_p2p is None and details:
                    enable_p2p = details.get('enable_p2p', 'Unknown')
                
                mapped_upgrade = {
                    'site_id': upgrade.get('site_id', 'Unknown'),
                    'site_name': upgrade.get('site_name', 'Unknown'),
                    'upgrade_id': upgrade.get('upgrade_id', 'Unknown'),
                    'status': upgrade.get('status', 'Unknown'),
                    'strategy': upgrade.get('strategy', 'Unknown'),
                    'target_version': upgrade.get('target_version', 'Unknown'),
                    'start_time': start_time_str,
                    'enable_p2p': enable_p2p,
                    'total_devices': upgrade.get('total_devices') or counts.get('total', 0),
                    'downloaded': upgrade.get('downloaded') or counts.get('downloaded', 0),
                    'download_requested': upgrade.get('download_requested') or counts.get('download_requested', 0),
                    'rebooted': upgrade.get('rebooted') or counts.get('rebooted', 0),
                    'reboot_in_progress': upgrade.get('reboot_in_progress') or counts.get('reboot_in_progress', 0),
                    'failed': upgrade.get('failed') or counts.get('failed', 0),
                    'skipped': upgrade.get('skipped') or counts.get('skipped', 0),
                    'source': upgrade.get('source', 'unknown'),
                    'timestamp': upgrade.get('timestamp') or datetime.now(timezone.utc).isoformat()
                }
                mapped_upgrades.append(mapped_upgrade)
            
            with open(upgrade_ops_file, mode='w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=upgrade_fieldnames)
                writer.writeheader()
                writer.writerows(mapped_upgrades)
            
            print(f"! Active upgrade operations exported to: {upgrade_ops_file}")
            print(f"   {len(active_upgrades)} upgrade operations exported")
            logging.info(f"Exported {len(active_upgrades)} active upgrade operations to {upgrade_ops_file}")
            
        except Exception as e:
            print(f"! Failed to export upgrade operations: {e}")
            logging.error(f"Failed to export upgrade operations: {e}")
    
    # Step 7: Summary and recommendations
    print(f"\n  Summary and Recommendations:")
    
    if firmware_status_summary['upgrade_failed'] > 0:
        print(f"   {firmware_status_summary['upgrade_failed']} devices have failed upgrades")
        print(f"   Check failed devices for retry eligibility or manual intervention")
    
    if firmware_status_summary['upgrade_in_progress'] > 0:
        print(f"   {firmware_status_summary['upgrade_in_progress']} devices currently upgrading")
        print(f"   Monitor progress and avoid disrupting these devices")
    
    if len(firmware_status_summary['devices_by_version']) > 3:
        print(f"   Multiple firmware versions detected ({len(firmware_status_summary['devices_by_version'])} different versions)")
        print(f"   Consider standardizing on a consistent firmware version")
    
    if active_upgrades:
        print(f"   {len(active_upgrades)} active upgrade operations found")
        print(f"   Monitor upgrade progress in exported CSV files")
    else:
        print(f"   No active upgrade operations detected")
    
    print(f"\n  Status check complete. Check exported CSV files for detailed analysis.")
    logging.info("Firmware upgrade status check completed successfully")


def get_auto_upgrade_time_settings():
    """
    Helper function to get auto-upgrade time scheduling settings from user input.
    Returns a dictionary with time_of_day and optionally day_of_week settings.
    """
    time_settings = {}
    
    # Time of day configuration
    while True:
        try:
            print(f"   Enter upgrade time (24-hour format, e.g., 02:00, 14:30):")
            time_input = input("   Time of day (default=02:00): ").strip() or "02:00"
            
            # Validate time format
            try:
                # Try to parse the time to validate format
                datetime.strptime(time_input, "%H:%M")
                time_settings["time_of_day"] = time_input
                print(f"   Upgrade time set to: {time_input}")
                break
            except ValueError:
                print(f"   Invalid time format. Please use HH:MM (24-hour format)")
                
        except KeyboardInterrupt:
            print("\n   Time configuration cancelled")
            time_settings["time_of_day"] = "02:00"  # Default fallback
            break
    
    # Day of week configuration
    print(f"\n   Select upgrade schedule:")
    print(f"      [1] Every day (recommended for most environments)")
    print(f"      [2] Specific day of week")
    
    try:
        schedule_choice = input("   Select option (1-2, default=1): ").strip() or "1"
        
        if schedule_choice == "2":
            # Specific day selection
            days = {
                "1": ("sun", "Sunday"),
                "2": ("mon", "Monday"), 
                "3": ("tue", "Tuesday"),
                "4": ("wed", "Wednesday"),
                "5": ("thu", "Thursday"),
                "6": ("fri", "Friday"),
                "7": ("sat", "Saturday")
            }
            
            print(f"   Select day of week:")
            for key, (day_value, day_name) in days.items():
                print(f"      [{key}] {day_name}")
            
            day_choice = input("   Select day (1-7): ").strip()
            if day_choice in days:
                day_value, day_name = days[day_choice]
                time_settings["day_of_week"] = day_value
                print(f"   Upgrade day set to: {day_name}")
            else:
                print(f"   Invalid selection, defaulting to every day")
                # Don't set day_of_week (None means every day)
        else:
            # Every day (don't set day_of_week)
            print(f"   Upgrade schedule: Every day at {time_settings.get('time_of_day', '02:00')}")
            
    except KeyboardInterrupt:
        print("\n   Schedule configuration cancelled, defaulting to every day")
    
    return time_settings


def bulk_upgrade_ap_firmware_by_site():
    """
    Advanced AP firmware upgrade with mode selection.
    
    This function provides comprehensive firmware upgrade capabilities with:
    1. Mode Selection: Choose between site-based or template-based upgrades
    2. Site Mode: Bulk site list, single site selection, or CSV file input
    3. Template Mode: Gateway Template selection with automatic site discovery
    4. Advanced upgrade strategies (big_bang, canary, rrm, serial) - default: RRM
    5. P2P firmware sharing options (default: enabled)
    6. Scheduling and failure threshold controls
    7. Comprehensive safety measures and audit logging
    """
    org_id = get_cached_or_prompted_org_id()
    firmware_manager = FirmwareManager(apisession, org_id)
    return firmware_manager.execute_firmware_upgrade_with_mode_selection()


def bulk_upgrade_switch_firmware_by_site():
    """
    Advanced switch firmware upgrade with mode selection.
    
    This function provides comprehensive switch firmware upgrade capabilities with:
    1. Mode Selection: Choose between site-based or template-based upgrades
    2. Site Mode: Individual site selection or bulk site list input
    3. Template Mode: Gateway Template selection with automatic site discovery
    4. Switch-specific upgrade strategies (serial, big_bang) - default: serial
    5. Network disruption safety measures and recovery snapshots
    6. Scheduling and failure threshold controls optimized for switches
    7. Comprehensive safety measures and audit logging for production networks
    """
    org_id = get_cached_or_prompted_org_id()
    firmware_manager = FirmwareManager(apisession, org_id)
    return firmware_manager.execute_switch_firmware_upgrade_with_mode_selection()





def bulk_upgrade_ap_firmware_by_site_impl(org_id, sites_to_upgrade_override=None):
    """
    Implementation function for bulk AP firmware upgrade.
    
    Args:
        org_id: Organization ID
        sites_to_upgrade_override: Optional list of site dicts for template-based upgrades
    """
    logging.info("Starting advanced bulk AP firmware upgrade by site...")
    logging.debug("bulk_upgrade_ap_firmware_by_site_impl() initiated")
    logging.debug(f"Using org_id: {org_id}")
    
    # Step 1: Determine site selection method (override, file, or interactive)
    sites_to_upgrade = []
    
    if sites_to_upgrade_override is not None:
        # Template-based upgrade mode - use provided sites
        sites_to_upgrade = sites_to_upgrade_override
        print(f"! Template-based upgrade mode - using {len(sites_to_upgrade)} provided sites")
        logging.info(f"Using template-provided sites: {len(sites_to_upgrade)} sites")
        for site in sites_to_upgrade:
            logging.debug(f"  Template site: {site['name']} (ID: {site['id']})")
    else:
        # Check for bulk site upgrade file or get single site selection
        bulk_upgrade_file = "APUpgradeSiteList.CSV"
        bulk_upgrade_file_path = get_csv_file_path(bulk_upgrade_file)
        
        if os.path.exists(bulk_upgrade_file_path):
            print(f"! Found {bulk_upgrade_file} - Loading sites for bulk upgrade...")
            logging.info(f"Found {bulk_upgrade_file} file, proceeding with bulk site upgrade")
            logging.debug(f"Bulk upgrade file path: {os.path.abspath(bulk_upgrade_file_path)}")
        
            # First, get all sites in the organization for reverse lookup
            print(f"   Fetching organization sites for name-to-ID lookup...")
            logging.debug("Fetching organization sites for name-to-ID mapping")
            try:
                all_org_sites = fetch_all_sites_with_limit(org_id)
                logging.debug(f"Retrieved {len(all_org_sites)} organization sites")
                
                # Build lookup dictionary: site_name -> site_id
                site_name_to_id = {}
                for site in all_org_sites:
                    site_name = site.get("name", "").strip()
                    site_id = site.get("id", "").strip()
                    if site_name and site_id:
                        site_name_to_id[site_name] = site_id
                
                logging.info(f"Built lookup table for {len(site_name_to_id)} organization sites")
                logging.debug(f"Site name mappings: {list(site_name_to_id.keys())[:10]}...")  # Log first 10 site names
                
            except Exception as e:
                print(f"! Failed to fetch organization sites: {e}")
                logging.error(f"Failed to fetch organization sites for lookup: {e}")
                return
            
            # Read site names from file (headerless format)
            try:
                logging.debug(f"Reading site names from {bulk_upgrade_file_path}")
                with open(bulk_upgrade_file_path, 'r', encoding='utf-8') as f:
                    site_names = []
                    for line_num, line in enumerate(f, 1):
                        site_name = line.strip()
                        if site_name:  # Skip empty lines
                            site_names.append(site_name)
                            logging.debug(f"Line {line_num}: Added site '{site_name}'")
                
                if not site_names:
                    print(f"! No site names found in {bulk_upgrade_file}")
                    logging.error(f"No site names found in {bulk_upgrade_file}")
                    return
                
                print(f"   Read {len(site_names)} site names from file")
                logging.info(f"Read {len(site_names)} site names from file: {site_names}")
                
                # Resolve site names to site IDs
                sites_to_upgrade = []
                missing_sites = []
                
                for site_name in site_names:
                    if site_name in site_name_to_id:
                        site_id = site_name_to_id[site_name]
                        sites_to_upgrade.append({
                            'name': site_name,
                            'id': site_id
                        })
                        logging.debug(f"Resolved site '{site_name}' to ID: {site_id}")
                    else:
                        missing_sites.append(site_name)
                        logging.warning(f"Site '{site_name}' not found in organization")
                
                # Report results
                if missing_sites:
                    print(f"   Warning: {len(missing_sites)} site(s) not found in organization:")
                    for missing_site in missing_sites:
                        print(f"      !? '{missing_site}'")
                    print(f"   Available sites in organization:")
                    available_names = sorted(site_name_to_id.keys())
                    for name in available_names[:10]:  # Show first 10 as examples
                        print(f"      !? '{name}'")
                    if len(available_names) > 10:
                        print(f"      ... and {len(available_names) - 10} more")
                        
                if not sites_to_upgrade:
                    print(f"! No valid sites found - none of the names in {bulk_upgrade_file} match organization sites")
                    logging.error(f"No valid sites found in {bulk_upgrade_file}")
                    return
                
                print(f"! Successfully resolved {len(sites_to_upgrade)} site(s) for bulk upgrade:")
                for site in sites_to_upgrade:
                    print(f"   !? {site['name']} (ID: {site['id']})")
                
                logging.info(f"Resolved {len(sites_to_upgrade)} sites for bulk upgrade from {bulk_upgrade_file}")
                
            except Exception as e:
                print(f"! Failed to read {bulk_upgrade_file}: {e}")
                logging.error(f"Failed to read {bulk_upgrade_file}: {e}")
                return
        else:
            print(f"! {bulk_upgrade_file} not found - Single site mode")
            print(f"   To enable bulk upgrade mode, create '{bulk_upgrade_file}' in the data/ folder")
            print(f"   File format: one site name per line (no header)")
            logging.info(f"{bulk_upgrade_file} not found, proceeding with single site selection")
        
        # Single site selection (existing behavior)
        site_id = prompt_site_selection()
        if not site_id:
            logging.error("No site selected. Exiting.")
            print(" No site selected. Exiting.")
            return
        
        # Get site name for display
        try:
            sites = fetch_all_sites_with_limit(org_id)
            site_name = next((site["name"] for site in sites if site.get("id") == site_id), site_id)
            logging.info(f"Selected site: {site_name} (ID: {site_id})")
        except Exception as e:
            logging.error(f"Failed to get site name: {e}")
            site_name = site_id
        
        # Convert single site to list format for unified processing
        sites_to_upgrade = [{
            'name': site_name,
            'id': site_id
        }]
    
    # Step 2: Get all APs across all selected sites
    all_aps = []
    all_sites_aps = {}  # Track APs per site
    
    print(f"\n  Fetching APs across {len(sites_to_upgrade)} site(s)...")
    logging.debug(f"Starting AP discovery across {len(sites_to_upgrade)} sites")
    
    for site_info in sites_to_upgrade:
        site_id = site_info['id']
        site_name = site_info['name']
        
        try:
            print(f"   Fetching APs at site '{site_name}'...")
            logging.debug(f"Fetching APs for site: {site_name} (ID: {site_id})")
            response = mistapi.api.v1.sites.devices.listSiteDevices(apisession, site_id, type="ap")
            site_aps = mistapi.get_all(response=response, mist_session=apisession)
            
            if site_aps:
                # Add site info to each AP for tracking
                for ap in site_aps:
                    ap['_site_id'] = site_id
                    ap['_site_name'] = site_name
                
                all_aps.extend(site_aps)
                all_sites_aps[site_id] = {
                    'name': site_name,
                    'aps': site_aps,
                    'count': len(site_aps)
                }
                
                print(f"      Found {len(site_aps)} APs at '{site_name}'")
                logging.info(f"Found {len(site_aps)} APs at site {site_name} (ID: {site_id})")
                logging.debug(f"AP models at {site_name}: {list(set(ap.get('model', 'Unknown') for ap in site_aps))}")
            else:
                print(f"      No APs found at site '{site_name}'")
                logging.warning(f"No APs found at site {site_name} (ID: {site_id})")
                all_sites_aps[site_id] = {
                    'name': site_name,
                    'aps': [],
                    'count': 0
                }
                
        except Exception as e:
            print(f"      Failed to fetch APs for site '{site_name}': {e}")
            logging.error(f"Failed to fetch APs for site {site_id} ({site_name}): {e}")
            all_sites_aps[site_id] = {
                'name': site_name,
                'aps': [],
                'count': 0,
                'error': str(e)
            }
    
    if not all_aps:
        print(" No APs found across any selected sites.")
        logging.warning("No APs found across any selected sites")
        return
    
    total_aps = len(all_aps)
    sites_with_aps = len([s for s in all_sites_aps.values() if s['count'] > 0])
    
    print(f"\n  AP Discovery Summary:")
    print(f"   !? Total APs found: {total_aps}")
    print(f"   !? Sites with APs: {sites_with_aps}/{len(sites_to_upgrade)}")
    
    for site_id, site_data in all_sites_aps.items():
        site_name = site_data['name']
        ap_count = site_data['count']
        if 'error' in site_data:
            print(f"   !? {site_name}: {ap_count} APs (Error: {site_data['error']})")
        else:
            print(f"   !? {site_name}: {ap_count} APs")
    
    logging.info(f"Total AP discovery: {total_aps} APs across {sites_with_aps} sites")
    
    # Use all_aps for the rest of the processing (existing variable name)
    aps = all_aps
    
    # Debug: Log the structure of the first AP to see available fields
    if aps and len(aps) > 0:
        sample_ap = aps[0]
        available_fields = list(sample_ap.keys()) if isinstance(sample_ap, dict) else []
        logging.debug(f"Sample AP device structure - Available fields: {available_fields}")
        
        # Check if version field exists in device config (it typically doesn't)
        if 'version' in sample_ap:
            logging.debug(f"Found version in device config: {sample_ap.get('version')}")
        else:
            logging.debug("No version field in device config - will need to use device stats")
    
    # Step 3: Group APs by model and get current firmware versions from device stats
    aps_by_model = {}
    ap_versions = {}  # Cache for AP versions {device_id: version}
    
    print(f"\n  Getting current firmware versions from device statistics...")
    
    # For multi-site upgrades, we need to fetch stats per site
    all_ap_stats = []
    stats_lookup = {}
    
    for site_id, site_data in all_sites_aps.items():
        if site_data['count'] == 0:
            continue  # Skip sites with no APs
            
        site_name = site_data['name']
        site_aps = site_data['aps']
        
        print(f"   Fetching device statistics for {len(site_aps)} APs at '{site_name}'...")
        
        try:
            stats_resp = mistapi.api.v1.sites.stats.listSiteDevicesStats(
                apisession, 
                site_id, 
                type="ap",
                limit=1000
            )
            site_ap_stats = mistapi.get_all(response=stats_resp, mist_session=apisession)
            
            if site_ap_stats:
                all_ap_stats.extend(site_ap_stats)
                
                # Build lookup for this site's devices
                for stats in site_ap_stats:
                    device_id = stats.get("id") or stats.get("device_id") or stats.get("mac")
                    if device_id:
                        stats_lookup[device_id] = stats
                
                logging.info(f"Retrieved stats for {len(site_ap_stats)} devices at site {site_name}")
            else:
                logging.warning(f"No device stats returned for site {site_name}")
                
        except Exception as e:
            logging.error(f"Failed to fetch bulk device stats for site {site_name}: {e}")
            print(f"   Failed to fetch stats for site '{site_name}': {e}")
            # Continue with other sites
    
    if all_ap_stats:
        logging.info(f"Retrieved stats for {len(all_ap_stats)} total devices via bulk API calls across {len(sites_to_upgrade)} sites")
        logging.debug(f"Stats lookup built with {len(stats_lookup)} entries")
        
        # Log sample stats structure for debugging
        if len(all_ap_stats) > 0:
            sample_stats = all_ap_stats[0]
            available_fields = list(sample_stats.keys()) if isinstance(sample_stats, dict) else []
            logging.debug(f"Sample device stats structure - Available fields: {available_fields}")
            
            # Look for version-related fields in bulk stats
            version_fields = [field for field in available_fields if 'version' in field.lower()]
            if version_fields:
                logging.debug(f"Version-related fields in bulk stats: {version_fields}")
            else:
                logging.debug("No version fields found in bulk stats data")
    else:
        logging.warning("No device stats retrieved from any site")
        print(f"   No device statistics retrieved - falling back to individual calls")
    
    # Process each AP device
    for ap in aps:
        model = ap.get("model", "Unknown")
        device_id = ap.get("id")
        device_mac = ap.get("mac")
        ap_site_id = ap.get("_site_id")  # Site info added during discovery
        ap_site_name = ap.get("_site_name")
        
        if model not in aps_by_model:
            aps_by_model[model] = []
        aps_by_model[model].append(ap)
        
        # Get current firmware version from device stats
        current_version = "Unknown"
        
        if device_id:
            # First try to use the bulk stats lookup
            stats_data = None
            for lookup_key in [device_id, device_mac]:  # Try both ID and MAC as keys
                if lookup_key and lookup_key in stats_lookup:
                    stats_data = stats_lookup[lookup_key]
                    logging.debug(f"Found stats for device {ap.get('name', 'Unnamed')} using key {lookup_key}")
                    break
            
            # If bulk lookup failed, fall back to individual API call
            if not stats_data and ap_site_id:
                try:
                    logging.debug(f"Bulk lookup failed for device {device_id}, making individual API call to site {ap_site_name}")
                    stats_resp = mistapi.api.v1.sites.stats.getSiteDeviceStats(apisession, ap_site_id, device_id)
                    stats_data = getattr(stats_resp, "data", {})
                except Exception as e:
                    logging.warning(f"Could not get individual stats for device {device_id} at site {ap_site_name}: {e}")
                    stats_data = {}
            
            # Extract version from stats data
            if isinstance(stats_data, dict):
                # Enhanced debugging for first device only to avoid log spam
                if ap == aps[0]:  # Only debug first AP device
                    available_fields = list(stats_data.keys())
                    logging.debug(f"First device {ap.get('name', 'Unnamed')} (ID: {device_id}) stats fields: {available_fields}")
                    
                    # Look for version-related fields
                    version_fields = [field for field in available_fields if 'version' in field.lower()]
                    if version_fields:
                        logging.debug(f"Version-related fields found: {version_fields}")
                        for field in version_fields:
                            logging.debug(f"Field '{field}': {stats_data.get(field)}")
                
                # Extract version using standard field
                current_version = stats_data.get("version", "Unknown")
                logging.debug(f"Device {ap.get('name', 'Unnamed')} (ID: {device_id}) at site {ap_site_name} version: {current_version}")
            else:
                logging.debug(f"Stats data for device {device_id} is not a dict: {type(stats_data)}")
        
        ap_versions[device_id] = current_version
    
    # Summary of API optimization
    bulk_stats_count = len([v for v in ap_versions.values() if v != "Unknown"])
    individual_calls_needed = len([v for v in ap_versions.values() if v == "Unknown"])
    
    print(f"   Retrieved {bulk_stats_count} device versions via bulk API call")
    if individual_calls_needed > 0:
        print(f"   {individual_calls_needed} devices required individual calls")
    
    logging.info(f"API optimization: {bulk_stats_count} versions from bulk call, {individual_calls_needed} individual calls needed")
    
    print(f"\n  AP Models found across {len(sites_to_upgrade)} site(s):")
    for model, devices in aps_by_model.items():
        # Get current versions for this model from our cached stats
        current_versions = set()
        for device in devices:
            device_id = device.get("id")
            current_version = ap_versions.get(device_id, "Unknown")
            current_versions.add(current_version)
        
        # Format current versions display
        if current_versions and "Unknown" not in current_versions:
            current_versions_sorted = sorted(current_versions, reverse=True)
            versions_text = ", ".join(current_versions_sorted)
            print(f"   !? {model}: {len(devices)} devices (Current versions: {versions_text})")
        else:
            print(f"   !? {model}: {len(devices)} devices (Current versions: Unknown)")
            
        # Show individual device details with site information for better visibility
        if len(sites_to_upgrade) > 1:
            # Multi-site mode - show site grouping
            devices_by_site = {}
            for device in devices:
                site_name = device.get("_site_name", "Unknown Site")
                if site_name not in devices_by_site:
                    devices_by_site[site_name] = []
                devices_by_site[site_name].append(device)
            
            for site_name, site_devices in devices_by_site.items():
                print(f"      {site_name} ({len(site_devices)} devices):")
                for device in site_devices:
                    device_name = device.get("name", "Unnamed")
                    device_mac = device.get("mac", "Unknown")
                    device_id = device.get("id")
                    device_version = ap_versions.get(device_id, "Unknown")
                    print(f"         - {device_name} (MAC: {device_mac}): v{device_version}")
        else:
            # Single site mode - show devices directly
            for device in devices:
                device_name = device.get("name", "Unnamed")
                device_mac = device.get("mac", "Unknown")
                device_id = device.get("id")
                device_version = ap_versions.get(device_id, "Unknown")
                print(f"      - {device_name} (MAC: {device_mac}): v{device_version}")
    
    # Step 4: Get available firmware versions for each model
    print(f"\n  Fetching available firmware versions...")
    logging.debug("Fetching available firmware versions from API")
    try:
        versions_response = mistapi.api.v1.orgs.devices.listOrgAvailableDeviceVersions(apisession, org_id)
        available_versions = versions_response.data
        logging.info(f"Retrieved firmware versions for organization")
        
        # Log raw response for debugging
        if available_versions:
            total_versions = len(available_versions) if isinstance(available_versions, list) else 0
            logging.debug(f"Raw firmware API response contains {total_versions} version entries")
            if total_versions > 0 and isinstance(available_versions, list):
                sample_version = available_versions[0]
                logging.debug(f"Sample version entry structure: {sample_version}")
                
                # Debug: Check if we have "models" vs "model" field
                has_models = any(v.get("models") for v in available_versions[:5] if isinstance(v, dict))
                has_model = any(v.get("model") for v in available_versions[:5] if isinstance(v, dict))
                logging.debug(f"Firmware API field analysis: has_models={has_models}, has_model={has_model}")
                
                # Debug: Check available metadata fields
                if isinstance(sample_version, dict):
                    available_fields = list(sample_version.keys())
                    logging.debug(f"Available firmware metadata fields: {available_fields}")
        else:
            logging.warning("No firmware versions returned from API")
            
    except Exception as e:
        logging.error(f"Failed to fetch available firmware versions: {e}")
        print(f"! Failed to fetch available firmware versions: {e}")
        return
    
    # Step 5: Let user select firmware version for each model
    upgrade_plan = {}
    print(f"\n  Firmware Version Selection:")
    print("=" * 60)
    
    # Show current version summary across all APs
    print(f"! Current Firmware Status Summary:")
    all_current_versions = {}
    for model, devices in aps_by_model.items():
        for device in devices:
            device_id = device.get("id")
            version = ap_versions.get(device_id, "Unknown")
            
            if version not in all_current_versions:
                all_current_versions[version] = []
            all_current_versions[version].append(f"{device.get('name', 'Unnamed')} ({model})")
    
    for version, device_list in sorted(all_current_versions.items(), reverse=True):
        print(f"   Version {version}: {len(device_list)} devices")
        for device_info in device_list:
            print(f"      !? {device_info}")
    print()
    
    # Show summary of what was found
    if available_versions:
        total_raw_versions = len(available_versions) if isinstance(available_versions, list) else 0
        print(f"! Found {total_raw_versions} firmware entries from API")
        logging.info(f"Processing {total_raw_versions} raw firmware version entries for {len(aps_by_model)} AP models")
        
        # Debug: Show what models are available in firmware data
        all_firmware_models = set()
        model_version_ranges = {}  # Track version ranges per model
        
        if isinstance(available_versions, list):
            for version_info in available_versions:
                if isinstance(version_info, dict):
                    # Try both "models" (plural) and "model" (singular) fields
                    models = version_info.get("models", [])
                    model = version_info.get("model")
                    version_num = version_info.get("version", "Unknown")
                    
                    target_models = models if models else ([model] if model else [])
                    
                    for target_model in target_models:
                        all_firmware_models.add(target_model)
                        
                        # Track version ranges for compatibility analysis
                        if target_model not in model_version_ranges:
                            model_version_ranges[target_model] = []
                        model_version_ranges[target_model].append(version_num)
        
        site_models = set(aps_by_model.keys())
        
        logging.debug(f"Models found at site: {sorted(site_models)}")
        logging.debug(f"Models with firmware available: {sorted(all_firmware_models)}")
        
        # Check for model matches
        matching_models = site_models.intersection(all_firmware_models)
        missing_models = site_models - all_firmware_models
        
        if matching_models:
            logging.info(f"Models with firmware available: {sorted(matching_models)}")
        if missing_models:
            logging.warning(f"Models without specific firmware versions: {sorted(missing_models)}")
            print(f"!  Models without specific firmware versions: {', '.join(sorted(missing_models))}")
        
        # Analyze version compatibility across models using API data
        if len(matching_models) > 1:
            print(f"\n  Version Compatibility Analysis (API-based):")
            print(f"   Analyzing firmware compatibility across {len(matching_models)} AP models...")
            
            # Build comprehensive model-to-versions mapping from API data
            api_model_versions = {}
            all_versions_in_api = set()
            
            for model in matching_models:
                if model in model_version_ranges:
                    model_versions = set(model_version_ranges[model])
                    api_model_versions[model] = model_versions
                    all_versions_in_api.update(model_versions)
            
            # Find versions that are compatible across multiple models
            version_compatibility = {}  # version -> set of compatible models
            
            for version in all_versions_in_api:
                compatible_models = set()
                for model, model_versions in api_model_versions.items():
                    if version in model_versions:
                        compatible_models.add(model)
                
                if len(compatible_models) > 1:  # Version works with multiple models
                    version_compatibility[version] = compatible_models
            
            # Sort versions by compatibility (most compatible first)
            if version_compatibility:
                sorted_by_compatibility = sorted(
                    version_compatibility.items(),
                    key=lambda x: (len(x[1]), tuple(map(int, x[0].split("."))) if x[0].replace(".", "").isdigit() else (0,)),
                    reverse=True
                )
                
                print(f"   Cross-compatible versions (work with multiple models):")
                for version, compatible_models in sorted_by_compatibility[:10]:  # Show top 10
                    model_list = ", ".join(sorted(compatible_models))
                    coverage = f"{len(compatible_models)}/{len(matching_models)}"
                    if len(compatible_models) == len(matching_models):
                        print(f"      {version}: ALL models ({model_list}) - UNIVERSAL")
                    elif len(compatible_models) >= len(matching_models) * 0.7:  # 70%+ coverage
                        print(f"      {version}: {coverage} models ({model_list}) - HIGH COMPATIBILITY")
                    else:
                        print(f"      {version}: {coverage} models ({model_list})")
                
                # Highlight universal versions
                universal_versions = [v for v, models in version_compatibility.items() if len(models) == len(matching_models)]
                if universal_versions:
                    sorted_universal = sorted(universal_versions, key=lambda x: tuple(map(int, x.split("."))) if x.replace(".", "").isdigit() else (0,), reverse=True)
                    print(f"\n   UNIVERSAL versions (compatible with ALL {len(matching_models)} models):")
                    print(f"      {', '.join(sorted_universal[:5])}{' ...' if len(sorted_universal) > 5 else ''}")
                    print(f"   Recommendation: Use universal version for simplified management")
                    logging.info(f"Found {len(universal_versions)} universal versions across all models")
                else:
                    print(f"\n    NO universal versions found - mixed-version upgrade required")
                    print(f"   Recommendation: Select optimal version per model based on compatibility matrix above")
                    logging.warning("No universal firmware versions found across all AP models")
            else:
                print(f"    NO cross-compatible versions found - each model has unique firmware options")
                logging.warning("No cross-compatible versions found between models")
            
            # Show model-specific version counts for context
            print(f"\n   Model-specific firmware availability:")
            for model in sorted(matching_models):
                if model in api_model_versions:
                    versions = api_model_versions[model]
                    sorted_versions = sorted(versions, key=lambda x: tuple(map(int, x.split("."))) if x.replace(".", "").isdigit() else (0,), reverse=True)
                    latest_version = sorted_versions[0] if sorted_versions else "Unknown"
                    oldest_version = sorted_versions[-1] if len(sorted_versions) > 1 else latest_version
                    
                    if len(sorted_versions) > 1:
                        range_text = f"{oldest_version} to {latest_version}"
                    else:
                        range_text = latest_version
                    
                    print(f"      !? {model}: {len(versions)} versions ({range_text})")
        
        elif len(matching_models) == 1:
            model = list(matching_models)[0]
            print(f"\n  Single model environment: {model}")
            if model in model_version_ranges:
                versions = model_version_ranges[model]
                print(f"   {len(versions)} firmware versions available for {model}")
            else:
                print(f"    No specific firmware versions found for {model}")
    
    
    for model, devices in aps_by_model.items():
        # Filter available versions for this specific model only
        raw_model_versions = []
        if available_versions and isinstance(available_versions, list):
            for version_info in available_versions:
                if isinstance(version_info, dict):
                    # Check both "models" (plural) and "model" (singular) fields
                    models = version_info.get("models", [])
                    single_model = version_info.get("model")
                    
                    # Only include versions that explicitly list this model
                    if model in models or single_model == model:
                        raw_model_versions.append(version_info)
        
        # Deduplicate versions by version number while preserving metadata
        version_dict = {}
        for version_info in raw_model_versions:
            version_num = version_info.get("version", "Unknown")
            if version_num not in version_dict:
                version_dict[version_num] = version_info
            else:
                # Merge metadata from duplicate entries, preferring non-empty values
                existing = version_dict[version_num]
                for key in ["release_date", "recommended", "package_url"]:
                    if not existing.get(key) and version_info.get(key):
                        existing[key] = version_info.get(key)
        
        # Convert back to list and sort by version (newest first, assuming semantic versioning)
        model_versions = list(version_dict.values())
        try:
            # Sort by version number (descending) - handle different version formats
            model_versions.sort(key=lambda x: tuple(map(int, x.get("version", "0.0.0").split("."))), reverse=True)
        except ValueError:
            # Fallback to string sorting if version parsing fails
            model_versions.sort(key=lambda x: x.get("version", ""), reverse=True)
        
        if not model_versions:
            print(f"!  No firmware versions found for model '{model}' - skipping {len(devices)} devices")
            print(f"   (This model may not have specific firmware versions listed, or no updates available)")
            logging.warning(f"No firmware versions found for model {model} - checked {len(raw_model_versions)} entries before deduplication")
            continue
        
        logging.debug(f"Model {model}: Found {len(raw_model_versions)} raw entries, {len(model_versions)} unique versions after deduplication")
        
        print(f"\n  Model: {model} ({len(devices)} devices)")
        
        # Show current versions of devices using cached stats
        current_versions = set()
        for device in devices:
            device_id = device.get("id")
            current_version = ap_versions.get(device_id, "Unknown")
            current_versions.add(current_version)
        
        if current_versions and "Unknown" not in current_versions:
            current_versions_sorted = sorted(current_versions, reverse=True)
            print(f"   Current versions in use: {', '.join(current_versions_sorted)}")
        else:
            print(f"   Current versions in use: Unknown")
        
        # Check if this model has version compatibility constraints
        if len(aps_by_model) > 1:
            model_version_count = len(model_versions)
            all_other_models = [m for m in aps_by_model.keys() if m != model]
            
            # Estimate version compatibility with other models
            compatibility_note = ""
            if model_version_count < 10:  # Fewer versions might indicate older/constrained model
                compatibility_note = f" (Limited version range - may have compatibility constraints)"
            elif model_version_count > 30:  # Many versions might indicate newer/flexible model
                compatibility_note = f" (Wide version range available)"
            
            if compatibility_note:
                print(f"   Model compatibility: {model_version_count} versions available{compatibility_note}")
                if len(all_other_models) > 0:
                    print(f"   Note: Different models may support different version ranges")
        
        # Display available versions with index - now model-specific and deduplicated
        print(f"   Available firmware versions for {model} ({len(model_versions)} found):")
        
        # Build cross-compatibility information if multiple models present
        other_models = [m for m in aps_by_model.keys() if m != model]
        cross_compatibility = {}
        
        if other_models and 'model_version_ranges' in locals():
            # Check which versions from this model are also available for other models
            for version_info in model_versions:
                version_num = version_info.get("version", "Unknown")
                compatible_models = []
                
                # Check each other model to see if they also support this version
                for other_model in other_models:
                    if other_model in model_version_ranges:
                        if version_num in model_version_ranges[other_model]:
                            compatible_models.append(other_model)
                
                if compatible_models:
                    cross_compatibility[version_num] = compatible_models
        
        for idx, version in enumerate(model_versions):
            version_num = version.get("version", "Unknown")
            is_recommended = version.get("recommended", False)
            
            # Build version display with indicators
            indicators = []
            
            if is_recommended:
                indicators.append("RECOMMENDED")
            
            # Check if this version is currently in use
            if version_num in current_versions:
                indicators.append("CURRENT")
            
            # Check cross-compatibility with other models
            if version_num in cross_compatibility:
                compatible_models = cross_compatibility[version_num]
                if len(compatible_models) == len(other_models):
                    indicators.append("UNIVERSAL")  # Works with all other models
                elif len(compatible_models) >= len(other_models) * 0.7:  # 70%+ compatibility
                    indicators.append("HIGH COMPAT")
                else:
                    indicators.append("SOME COMPAT")
            elif other_models:  # Only show if there are other models to be compatible with
                indicators.append("MODEL SPECIFIC")
            
            # Format indicators
            indicator_text = f" [{', '.join(indicators)}]" if indicators else ""
            
            # Show cross-compatibility details for better user understanding
            compat_detail = ""
            if version_num in cross_compatibility:
                compatible_models = cross_compatibility[version_num]
                if len(compatible_models) > 0:
                    compat_detail = f" (also works with: {', '.join(compatible_models)})"
            
            print(f"      [{idx}] {version_num}{indicator_text}{compat_detail}")
            
            # Log version details for debugging
            logging.debug(f"Model {model} version {idx}: {version_num}, recommended: {is_recommended}, cross_compatible: {cross_compatibility.get(version_num, [])}")
        
        # Add guidance about cross-compatibility if multiple models present
        if other_models and cross_compatibility:
            universal_versions = [v for v, models in cross_compatibility.items() if len(models) == len(other_models)]
            if universal_versions:
                print(f"   UNIVERSAL versions work with all models: {', '.join(universal_versions[:3])}")
            else:
                high_compat_versions = [v for v, models in cross_compatibility.items() if len(models) >= len(other_models) * 0.7]
                if high_compat_versions:
                    print(f"   HIGH COMPATIBILITY versions work with most models: {', '.join(high_compat_versions[:3])}")
        
        print()  # Add blank line for readability
        
        # Get user selection
        while True:
            try:
                user_input = input(f"Select firmware version for {model} (0-{len(model_versions)-1}, or 's' to skip): ").strip().lower()
                
                if user_input == 's':
                    print(f"!  Skipping firmware upgrade for {model}")
                    logging.info(f"User chose to skip firmware upgrade for model {model}")
                    break
                
                version_idx = int(user_input)
                if 0 <= version_idx < len(model_versions):
                    selected_version = model_versions[version_idx]
                    version_num = selected_version.get("version", "Unknown")
                    is_recommended = selected_version.get("recommended", False)
                    is_current = version_num in current_versions
                    
                    # Provide feedback about the selection
                    selection_notes = []
                    if is_recommended:
                        selection_notes.append("RECOMMENDED")
                    if is_current:
                        selection_notes.append("CURRENT")
                    
                    notes_text = f" ({', '.join(selection_notes)})" if selection_notes else ""
                    
                    upgrade_plan[model] = {
                        "version": version_num,
                        "version_info": selected_version,
                        "devices": devices
                    }
                    print(f"! Selected version {version_num} for {model}{notes_text}")
                    logging.info(f"User selected firmware version {version_num} for model {model} (recommended: {is_recommended}, current: {is_current})")
                    break
                else:
                    print(f"! Invalid selection. Please enter a number between 0 and {len(model_versions)-1}, or 's' to skip.")
                    
            except ValueError:
                print(" Invalid input. Please enter a number or 's' to skip.")
            except KeyboardInterrupt:
                print("\n Operation cancelled by user.")
                logging.info("Bulk AP firmware upgrade cancelled by user interrupt")
                return
    
    if not upgrade_plan:
        print(" No firmware upgrades selected. Exiting.")
        logging.info("No firmware upgrades selected by user")
        return
    
    # Step 5.5: Upgrade Plan Summary and Compatibility Validation
    print(f"\n  Upgrade Plan Summary:")
    print("=" * 60)
    
    total_devices_to_upgrade = 0
    selected_versions = set()
    models_in_plan = list(upgrade_plan.keys())
    
    for model, plan_info in upgrade_plan.items():
        version = plan_info["version"]
        device_count = len(plan_info["devices"])
        total_devices_to_upgrade += device_count
        selected_versions.add(version)
        
        print(f"   {model}: {device_count} devices firmware {version}")
    
    print(f"\n  Summary:")
    print(f"   !? Total models: {len(upgrade_plan)}")
    print(f"   !? Total devices: {total_devices_to_upgrade}")
    print(f"   !? Firmware versions: {len(selected_versions)}")
    
    # Highlight coordination considerations for mixed-version upgrades
    if len(selected_versions) > 1:
        sorted_versions = sorted(selected_versions, key=lambda x: tuple(map(int, x.split("."))) if x.replace(".", "").isdigit() else (0,), reverse=True)
        print(f"\n   Multi-Version Upgrade Detected:")
        print(f"   Versions selected: {', '.join(sorted_versions)}")
        
        # Analyze if any selected versions are cross-compatible
        if 'model_version_ranges' in locals():
            # Check if any of the selected versions could have been universal
            could_be_universal = []
            for version in selected_versions:
                compatible_count = 0
                for model in models_in_plan:
                    if model in model_version_ranges and version in model_version_ranges[model]:
                        compatible_count += 1
                
                if compatible_count == len(models_in_plan):
                    could_be_universal.append(version)
            
            if could_be_universal:
                print(f"   Analysis: Version(s) {', '.join(could_be_universal)} could work with ALL models")
                print(f"   Consider: You chose model-specific versions despite universal options available")
                print(f"      This may be optimal for performance/features per model")
            else:
                print(f"   Analysis: No single version compatible with all selected models")
                print(f"   Multi-version upgrade is necessary due to model firmware constraints")
        
        print(f"\n   Coordination considerations:")
        print(f"      !? Each model will upgrade to its optimal version")
        print(f"      !? Network features may vary between firmware versions")
        print(f"      !? Monitor compatibility for shared network functions")
        print(f"      !? Consider upgrade timing to minimize impact")
        
        logging.info(f"Multi-version upgrade plan: {len(selected_versions)} different versions across {len(models_in_plan)} models")
        
        # Ask user for confirmation on mixed-version upgrade
        print(f"\n  Proceed with multi-version upgrade plan?")
        confirm_mixed = input("   Continue? (y/n, default=y): ").strip().lower() or "y"
        if confirm_mixed not in ['y', 'yes']:
            print(" Mixed-version upgrade cancelled by user.")
            logging.info("Mixed-version upgrade cancelled by user")
            return
        else:
            print(" Multi-version upgrade plan confirmed.")
    else:
        single_version = list(selected_versions)[0]
        print(f"\n Single-Version Upgrade:")
        print(f"   All {len(models_in_plan)} model(s) will upgrade to firmware {single_version}")
        
        # Analyze if this version is truly universal or if users just happened to select the same version
        if len(models_in_plan) > 1 and 'model_version_ranges' in locals():
            universal_compatibility = True
            for model in models_in_plan:
                if model not in model_version_ranges or single_version not in model_version_ranges[model]:
                    universal_compatibility = False
                    break
            
            if universal_compatibility:
                print(f"   Excellent choice: {single_version} is UNIVERSAL (compatible with all models)")
                print(f"   Unified firmware version simplifies management and ensures feature consistency")
            else:
                print(f"    Note: Selected version may not be verified as compatible with all models")
                print(f"   Proceed with caution and monitor compatibility during upgrade")
        else:
            print(f"   Consistent firmware version across all AP models")
        
        logging.info(f"Single-version upgrade plan: all models upgrading to {single_version}")
    
    print(f"\n  Ready to proceed with advanced configuration...")
    logging.info(f"Upgrade plan validated: {total_devices_to_upgrade} devices across {len(models_in_plan)} models")
    
    # Step 6: Advanced Configuration Options
    print(f"\n  Advanced Upgrade Configuration:")
    print("=" * 60)
    
    # Select upgrade strategy
    strategies = {
        "1": ("big_bang", "Upgrade all devices at once (fastest, higher risk)"),
        "2": ("canary", "Phased rollout with configurable phases (safer, slower)"),
        "3": ("rrm", "Radio Resource Management aware upgrade (AP-only, intelligent)"),
        "4": ("serial", "One device at a time (safest, slowest)")
    }
    
    print(" Select upgrade strategy:")
    for key, (strategy, description) in strategies.items():
        print(f"   [{key}] {strategy.upper()}: {description}")
    
    while True:
        try:
            strategy_choice = input("Select strategy (1-4, default=3 for rrm): ").strip() or "3"
            if strategy_choice in strategies:
                selected_strategy, strategy_desc = strategies[strategy_choice]
                print(f"! Selected strategy: {selected_strategy.upper()}")
                logging.info(f"User selected upgrade strategy: {selected_strategy}")
                break
            else:
                print(" Invalid selection. Please choose 1-4.")
        except KeyboardInterrupt:
            print("\n Operation cancelled by user.")
            logging.info("AP firmware upgrade cancelled during strategy selection")
            return
    
    # Advanced strategy-specific options
    upgrade_config = {
        "strategy": selected_strategy,
        "force": False,
        "enable_p2p": True,  # Default to enabled
        "max_failure_percentage": 5,
        "start_time": None,
        "canary_phases": [1, 10, 50, 100],
        "p2p_cluster_size": 10,
        "reboot": True  # APs auto-reboot, but explicit for clarity
    }
    
    # Strategy-specific configuration
    if selected_strategy == "canary":
        print(f"\n  Canary Strategy Configuration:")
        
        # Custom phases
        use_custom_phases = input("Use custom canary phases? (y/N): ").strip().lower()
        if use_custom_phases in ['y', 'yes']:
            while True:
                try:
                    phases_input = input("Enter comma-separated percentages (e.g., 2,10,25,100): ").strip()
                    if phases_input:
                        phases = [int(phase_str.strip()) for phase_str in phases_input.split(',')]
                        if all(1 <= p <= 100 for p in phases) and phases[-1] == 100:
                            upgrade_config["canary_phases"] = phases
                            print(f"! Custom phases: {phases}")
                            break
                        else:
                            print(" Phases must be 1-100 and end with 100")
                    else:
                        break
                except ValueError:
                    print(" Invalid format. Use comma-separated numbers.")
        
        # Failure threshold
        try:
            failure_input = input(f"Max failure percentage per phase (default={upgrade_config['max_failure_percentage']}%): ").strip()
            if failure_input:
                failure_pct = int(failure_input)
                if 0 <= failure_pct <= 100:
                    upgrade_config["max_failure_percentage"] = failure_pct
                    print(f"! Max failures: {failure_pct}%")
        except ValueError:
            print(" Invalid input, using default failure threshold")
    
    elif selected_strategy == "rrm":
        print(f"\n  RRM Strategy Configuration:")
        
        # RRM-specific options
        rrm_options = {
            "rrm_node_order": "fringe_to_center",
            "rrm_first_batch_percentage": 2,
            "rrm_max_batch_percentage": 10,
            "rrm_slow_ramp": True,
            "rrm_mesh_upgrade": "sequential"
        }
        
        # Node order selection
        node_orders = {
            "1": ("fringe_to_center", "Start from edge APs (recommended)"),
            "2": ("center_to_fringe", "Start from central APs")
        }
        
        print("Select RRM node order:")
        for key, (order, desc) in node_orders.items():
            print(f"   [{key}] {desc}")
        
        order_choice = input("Select order (1-2, default=1): ").strip() or "1"
        if order_choice in node_orders:
            rrm_options["rrm_node_order"] = node_orders[order_choice][0]
            print(f"! Node order: {node_orders[order_choice][1]}")
        
        # Batch size configuration
        try:
            first_batch = input(f"First batch percentage (default={rrm_options['rrm_first_batch_percentage']}%): ").strip()
            if first_batch:
                rrm_options["rrm_first_batch_percentage"] = int(first_batch)
            
            max_batch = input(f"Max batch percentage (default={rrm_options['rrm_max_batch_percentage']}%): ").strip()
            if max_batch:
                rrm_options["rrm_max_batch_percentage"] = int(max_batch)
        except ValueError:
            print(" Invalid input, using default batch sizes")
        
        upgrade_config.update(rrm_options)
    
    # P2P Configuration (all strategies)
    print(f"\n  Peer-to-Peer (P2P) Configuration:")
    enable_p2p = input("Enable AP-to-AP firmware sharing? (Y/n): ").strip().lower()
    if enable_p2p not in ['n', 'no']:
        upgrade_config["enable_p2p"] = True
        print(" P2P enabled - APs will share firmware locally")
        
        try:
            cluster_size = input(f"P2P cluster size (default={upgrade_config['p2p_cluster_size']}): ").strip()
            if cluster_size:
                upgrade_config["p2p_cluster_size"] = int(cluster_size)
                print(f"! P2P cluster size: {upgrade_config['p2p_cluster_size']}")
        except ValueError:
            print(" Invalid input, using default cluster size")
    else:
        print(" P2P disabled - all firmware downloads from cloud")
    
    # Scheduling Options
    print(f"\n  Scheduling Options:")
    schedule_later = input("Schedule upgrade for later? (y/N): ").strip().lower()
    if schedule_later in ['y', 'yes']:
        while True:
            try:
                time_input = input("Enter start time (YYYY-MM-DD HH:MM or +minutes): ").strip()
                if time_input.startswith('+'):
                    # Relative time
                    minutes = int(time_input[1:])
                    start_time = int(time.time()) + (minutes * 60)
                    upgrade_config["start_time"] = start_time
                    scheduled_time = datetime.fromtimestamp(start_time).strftime('%Y-%m-%d %H:%M')
                    print(f"! Scheduled for: {scheduled_time}")
                    break
                else:
                    # Absolute time
                    dt = datetime.strptime(time_input, '%Y-%m-%d %H:%M')
                    start_time = int(dt.timestamp())
                    upgrade_config["start_time"] = start_time
                    print(f"! Scheduled for: {time_input}")
                    break
            except ValueError:
                print(" Invalid format. Use 'YYYY-MM-DD HH:MM' or '+minutes'")
                retry = input("Try again? (y/N): ").strip().lower()
                if retry not in ['y', 'yes']:
                    break
    
    # Force upgrade option
    force_upgrade = input("Force upgrade even if same version? (y/N): ").strip().lower()
    if force_upgrade in ['y', 'yes']:
        upgrade_config["force"] = True
        print(" Force upgrade enabled")
    
    # Display final configuration
    print(f"\n  Final Upgrade Configuration:")
    print("=" * 60)
    print(f"Strategy: {upgrade_config['strategy'].upper()}")
    if upgrade_config.get('start_time'):
        scheduled_time = datetime.fromtimestamp(upgrade_config['start_time']).strftime('%Y-%m-%d %H:%M')
        print(f"Scheduled: {scheduled_time}")
    else:
        print("Scheduled: Immediate")
    print(f"P2P Enabled: {upgrade_config['enable_p2p']}")
    if upgrade_config['enable_p2p']:
        print(f"P2P Cluster Size: {upgrade_config['p2p_cluster_size']}")
    print(f"Force Upgrade: {upgrade_config['force']}")
    print(f"Max Failure Rate: {upgrade_config['max_failure_percentage']}%")
    
    if upgrade_config['strategy'] == 'canary':
        print(f"Canary Phases: {upgrade_config['canary_phases']}%")
    elif upgrade_config['strategy'] == 'rrm':
        print(f"RRM Node Order: {upgrade_config.get('rrm_node_order', 'fringe_to_center')}")
        print(f"RRM First Batch: {upgrade_config.get('rrm_first_batch_percentage', 2)}%")
        print(f"RRM Max Batch: {upgrade_config.get('rrm_max_batch_percentage', 10)}%")
    
    # Step 7: Display upgrade plan and affected devices
    total_devices = sum(len(plan["devices"]) for plan in upgrade_plan.values())
    print(f"\n  Final Firmware Upgrade Plan:")
    print("=" * 60)
    
    if len(sites_to_upgrade) > 1:
        print(f"Bulk Upgrade Mode: {len(sites_to_upgrade)} sites")
        for site_info in sites_to_upgrade:
            site_count = sum(1 for plan in upgrade_plan.values() 
                           for device in plan["devices"] 
                           if device.get("_site_id") == site_info['id'])
            print(f"   !? {site_info['name']}: {site_count} devices")
    else:
        print(f"Site: {sites_to_upgrade[0]['name']}")
    
    print(f"Total devices to upgrade: {total_devices}")
    print(f"Upgrade strategy: {upgrade_config['strategy'].upper()}")
    
    for model, plan in upgrade_plan.items():
        version = plan["version"]
        devices = plan["devices"]
        print(f"\n  {model} Firmware {version} ({len(devices)} devices):")
        
        if len(sites_to_upgrade) > 1:
            # Group devices by site for multi-site display
            devices_by_site = {}
            for device in devices:
                site_name = device.get("_site_name", "Unknown Site")
                if site_name not in devices_by_site:
                    devices_by_site[site_name] = []
                devices_by_site[site_name].append(device)
            
            for site_name, site_devices in devices_by_site.items():
                print(f"   {site_name} ({len(site_devices)} devices):")
                for device in site_devices:
                    device_name = device.get("name", "Unnamed")
                    mac = device.get("mac", "Unknown")
                    device_id = device.get("id")
                    current_version = ap_versions.get(device_id, "Unknown")
                    print(f"      !? {device_name} (MAC: {mac}) - Current: {current_version}")
        else:
            # Single site display
            for device in devices:
                device_name = device.get("name", "Unnamed")
                mac = device.get("mac", "Unknown")
                device_id = device.get("id")
                current_version = ap_versions.get(device_id, "Unknown")
                print(f"   !? {device_name} (MAC: {mac}) - Current: {current_version}")
    
    # Step 8: Display warnings and get user confirmation
    warning_lines = [
        " CRITICAL WARNING - ADVANCED FIRMWARE UPGRADE OPERATION:",
        "!? This action will UPGRADE FIRMWARE on Access Point devices",
        "!? APs will REBOOT during the upgrade process",
        "!? Wi-Fi connectivity will be TEMPORARILY LOST during upgrades", 
        "!? Users will experience Wi-Fi service interruptions",
        "!? Firmware upgrades can take 5-15 minutes per device",
        "!? Failed upgrades may require manual recovery",
        "!? This is a DISRUPTIVE network operation",
        "!? Always ensure you have physical access to devices if recovery is needed",
        f"!? Upgrade strategy: {upgrade_config['strategy'].upper()}",
        f"!? Max failure tolerance: {upgrade_config['max_failure_percentage']}%",
        "!? P2P enabled: " + ("Yes" if upgrade_config['enable_p2p'] else "No"),
        "!? The script owner bears NO LIABILITY for any consequences",
        "!? Proceed only if you understand and accept these risks"
    ]
    
    print("\n" + "??" * 50)
    for line in warning_lines:
        print(line)
    print("??" * 50)
    
    print(f"\n  Summary:")
    if len(sites_to_upgrade) > 1:
        print(f"   !? Bulk upgrade across {len(sites_to_upgrade)} sites")
        sites_with_devices = len(set(device.get("_site_name") for plan in upgrade_plan.values() for device in plan["devices"]))
        print(f"   !? Sites with devices to upgrade: {sites_with_devices}")
    else:
        print(f"   !? Site: {sites_to_upgrade[0]['name']}")
    print(f"   !? Total APs to upgrade: {total_devices}")
    print(f"   !? Models affected: {len(upgrade_plan)}")
    print(f"   !? Strategy: {upgrade_config['strategy'].upper()}")
    
    # Show target firmware versions for each model
    print(f"   !? Target firmware versions:")
    for model, plan in upgrade_plan.items():
        version = plan["version"]
        device_count = len(plan["devices"])
        print(f"      - {model}: v{version} ({device_count} devices)")
    
    if upgrade_config.get('start_time'):
        scheduled_time = datetime.fromtimestamp(upgrade_config['start_time']).strftime('%Y-%m-%d %H:%M')
        print(f"   !? Scheduled: {scheduled_time}")
    else:
        print(f"   !? Scheduled: Immediate")
    
    # Get user confirmation with liability waiver
    print(f"\n  Do you want to proceed with upgrading {total_devices} AP devices?")
    print("   Type 'UPGRADE' (all caps) to confirm, or anything else to cancel:")
    print("   By typing 'UPGRADE', you acknowledge and accept all risks and liability.")
    
    try:
        user_input = input(">>> ").strip()
        if user_input != "UPGRADE":
            print(" Advanced firmware upgrade operation cancelled by user.")
            logging.info("Advanced AP firmware upgrade operation cancelled by user input")
            return
        else:
            print(" User confirmed advanced firmware upgrade operation. Proceeding...")
            logging.info(f"! LIABILITY WAIVER ACCEPTED: User confirmed advanced AP firmware upgrade for {total_devices} devices at site {site_name}")
            logging.info(f"User input: '{user_input}' - User accepts full responsibility for firmware upgrade risks")
            logging.info(f"Upgrade strategy: {upgrade_config['strategy']}, P2P: {upgrade_config['enable_p2p']}, Max failures: {upgrade_config['max_failure_percentage']}%")
            # Log detailed upgrade plan for audit trail
            plan_summary = []
            for model, plan in upgrade_plan.items():
                plan_summary.append(f"{model}?{plan['version']} ({len(plan['devices'])} devices)")
            logging.info(f"Upgrade plan: {'; '.join(plan_summary)}")
            
    except KeyboardInterrupt:
        print("\n Firmware upgrade operation cancelled by user (Ctrl+C).")
        logging.info("AP firmware upgrade operation cancelled by user interrupt")
        return
    except Exception as e:
        print(f"! Error getting user input: {e}")
        logging.error(f"Error getting user input for upgrade confirmation: {e}")
        return
    
    # Step 9: Execute advanced firmware upgrades
    print("\n  Starting advanced AP firmware upgrade operations...")
    print("=" * 60)
    logging.info("Starting firmware upgrade execution phase")
    logging.debug(f"Upgrade strategy: {upgrade_config['strategy']}, P2P: {upgrade_config['enable_p2p']}, Max failures: {upgrade_config['max_failure_percentage']}%")
    
    results = []
    successful_upgrades = 0
    failed_upgrades = 0
    upgrade_ids = []  # Track multiple upgrade IDs for multi-site
    
    # Organize devices by site for execution
    logging.debug("Organizing devices by site for execution")
    devices_by_site = {}
    for model, plan in upgrade_plan.items():
        version = plan["version"]
        devices = plan["devices"]
        logging.debug(f"Processing {len(devices)} devices for model {model} firmware {version}")
        
        for device in devices:
            site_id = device.get("_site_id")
            site_name = device.get("_site_name")
            
            if site_id not in devices_by_site:
                logging.debug(f"Creating new site entry for {site_name} (ID: {site_id})")
                devices_by_site[site_id] = {
                    'name': site_name,
                    'devices': [],
                    'models': {}
                }
            
            devices_by_site[site_id]['devices'].append(device)
            
            # Track model/version per site
            if model not in devices_by_site[site_id]['models']:
                devices_by_site[site_id]['models'][model] = {
                    'version': version,
                    'devices': []
                }
            devices_by_site[site_id]['models'][model]['devices'].append(device)
    
    logging.debug(f"Device organization complete: {len(devices_by_site)} sites")
    for site_id, site_data in devices_by_site.items():
        logging.debug(f"  Site {site_data['name']}: {len(site_data['devices'])} devices, {len(site_data['models'])} models")
    
    total_sites_to_upgrade = len(devices_by_site)
    total_devices = sum(len(site_data['devices']) for site_data in devices_by_site.values())
    logging.debug(f"Upgrade execution will process {total_devices} devices across {total_sites_to_upgrade} sites")
    
    print(f"\n  Executing upgrades across {total_sites_to_upgrade} site(s) with {total_devices} devices...")
    
    for site_index, (site_id, site_data) in enumerate(devices_by_site.items(), 1):
        site_name = site_data['name']
        site_devices = site_data['devices']
        site_models = site_data['models']
        
        logging.debug(f"Starting upgrade execution for site {site_index}/{total_sites_to_upgrade}: {site_name}")
        print(f"\n   Site {site_index}/{total_sites_to_upgrade}: {site_name} ({len(site_devices)} devices)")
        print(f"   Strategy: {upgrade_config['strategy'].upper()}")
        print(f"   P2P Enabled: {upgrade_config['enable_p2p']}")
        
        try:
            # Prepare upgrade request for this site
            device_ids = [device.get("id") for device in site_devices if device.get("id")]
            
            # Check if all devices in this site use the same firmware version
            site_versions = set(model_info['version'] for model_info in site_models.values())
            
            if len(site_versions) == 1:
                # Single version for this site
                target_version = list(site_versions)[0]
                
                upgrade_body = {
                    "strategy": upgrade_config["strategy"],
                    "force": upgrade_config["force"],
                    "enable_p2p": upgrade_config["enable_p2p"],
                    "max_failure_percentage": upgrade_config["max_failure_percentage"],
                    "reboot": upgrade_config["reboot"],
                    "version": target_version,
                    "device_ids": device_ids
                }
                
                # Add P2P configuration
                if upgrade_config["enable_p2p"]:
                    upgrade_body["p2p_cluster_size"] = upgrade_config["p2p_cluster_size"]
                
                # Add strategy-specific options
                if upgrade_config["strategy"] == "canary":
                    upgrade_body["canary_phases"] = upgrade_config["canary_phases"]
                elif upgrade_config["strategy"] == "rrm":
                    for key in ["rrm_node_order", "rrm_first_batch_percentage", "rrm_max_batch_percentage", "rrm_slow_ramp", "rrm_mesh_upgrade"]:
                        if key in upgrade_config:
                            upgrade_body[key] = upgrade_config[key]
                
                # Add scheduling
                if upgrade_config.get("start_time"):
                    upgrade_body["start_time"] = upgrade_config["start_time"]
                
                logging.debug(f"Prepared upgrade body for site {site_name}: {upgrade_body}")
                print(f"      Upgrading all devices to version {target_version}...")
                logging.info(f"Initiating upgrade for site {site_name} with {len(device_ids)} devices to version {target_version}")
                
                logging.debug(f"Calling upgradeSiteDevices API for site {site_id}")
                resp = mistapi.api.v1.sites.devices.upgradeSiteDevices(
                    apisession,
                    site_id,
                    body=upgrade_body
                )
                logging.debug(f"upgradeSiteDevices API call completed for site {site_name}")
                
                # Handle response
                upgrade_id = None
                if hasattr(resp, "data") and resp.data:
                    logging.debug(f"Upgrade response data: {resp.data}")
                    if isinstance(resp.data, dict) and "upgrade_id" in resp.data:
                        upgrade_id = resp.data["upgrade_id"]
                        upgrade_ids.append(upgrade_id)
                        logging.debug(f"Upgrade ID captured: {upgrade_id}")
                        print(f"      Upgrade initiated - ID: {upgrade_id}")
                    else:
                        logging.debug(f"Upgrade initiated without specific upgrade ID")
                        print(f"      Upgrade command sent successfully")
                else:
                    logging.warning(f"Upgrade response missing data for site {site_name}")
                
                successful_upgrades += len(site_devices)
                logging.info(f"! Site {site_name} upgrade initiated for {len(site_devices)} devices")
                
            else:
                # Multiple versions for this site - separate calls per model
                print(f"      Multiple firmware versions for site - executing per model...")
                
                for model, model_info in site_models.items():
                    model_version = model_info['version']
                    model_devices = model_info['devices']
                    model_device_ids = [device.get("id") for device in model_devices if device.get("id")]
                    
                    print(f"         !? {model}: {len(model_devices)} devices v{model_version}")
                    
                    model_upgrade_body = {
                        "strategy": upgrade_config["strategy"],
                        "force": upgrade_config["force"],
                        "enable_p2p": upgrade_config["enable_p2p"],
                        "max_failure_percentage": upgrade_config["max_failure_percentage"],
                        "reboot": upgrade_config["reboot"],
                        "version": model_version,
                        "device_ids": model_device_ids
                    }
                    
                    # Add P2P and strategy configs
                    if upgrade_config["enable_p2p"]:
                        model_upgrade_body["p2p_cluster_size"] = upgrade_config["p2p_cluster_size"]
                    if upgrade_config["strategy"] == "canary":
                        model_upgrade_body["canary_phases"] = upgrade_config["canary_phases"]
                    elif upgrade_config["strategy"] == "rrm":
                        for key in ["rrm_node_order", "rrm_first_batch_percentage", "rrm_max_batch_percentage", "rrm_slow_ramp", "rrm_mesh_upgrade"]:
                            if key in upgrade_config:
                                model_upgrade_body[key] = upgrade_config[key]
                    if upgrade_config.get("start_time"):
                        model_upgrade_body["start_time"] = upgrade_config["start_time"]
                    
                    logging.debug(f"Prepared per-model upgrade body for {model}: {model_upgrade_body}")
                    logging.debug(f"Calling upgradeSiteDevices API for {model} devices in site {site_name}")
                    model_resp = mistapi.api.v1.sites.devices.upgradeSiteDevices(
                        apisession,
                        site_id,
                        body=model_upgrade_body
                    )
                    logging.debug(f"Per-model upgradeSiteDevices API call completed for {model}")
                    
                    # Handle model upgrade response
                    if hasattr(model_resp, "data") and model_resp.data and isinstance(model_resp.data, dict):
                        logging.debug(f"Per-model upgrade response data for {model}: {model_resp.data}")
                        if "upgrade_id" in model_resp.data:
                            model_upgrade_id = model_resp.data["upgrade_id"]
                            upgrade_ids.append(model_upgrade_id)
                            logging.debug(f"Per-model upgrade ID captured for {model}: {model_upgrade_id}")
                            print(f"            {model} upgrade initiated - ID: {model_upgrade_id}")
                        else:
                            logging.debug(f"Per-model upgrade initiated for {model} without specific upgrade ID")
                            print(f"            {model} upgrade command sent")
                    else:
                        logging.warning(f"Per-model upgrade response missing data for {model} in site {site_name}")
                    
                    successful_upgrades += len(model_devices)
            
            # Log each device for audit trail
            for device in site_devices:
                device_name = device.get("name", "Unnamed")
                device_mac = device.get("mac", "Unknown")
                device_id = device.get("id", "Unknown")
                device_model = device.get("model", "Unknown")
                current_version = ap_versions.get(device_id, "Unknown")
                
                # Find target version for this device
                target_version = "Unknown"
                for model, plan in upgrade_plan.items():
                    if device in plan["devices"]:
                        target_version = plan["version"]
                        break
                
                results.append({
                    "Site ID": site_id,
                    "Site Name": site_name,
                    "Device ID": device_id,
                    "Device Name": device_name,
                    "Device MAC": device_mac,
                    "Model": device_model,
                    "Current Version": current_version,
                    "Target Version": target_version,
                    "Strategy": upgrade_config["strategy"],
                    "P2P Enabled": upgrade_config["enable_p2p"],
                    "Max Failure %": upgrade_config["max_failure_percentage"],
                    "Force Upgrade": upgrade_config["force"],
                    "Upgrade ID": upgrade_id or "Multiple",
                    "Status": "Advanced Upgrade Initiated",
                    "Timestamp": datetime.now(timezone.utc).isoformat()
                })
                
        except Exception as e:
            error_status = f"ERROR: {e}"
            print(f"      Failed to initiate upgrade for site {site_name}: {e}")
            logging.error(f"! Failed to initiate upgrade for site {site_name}: {e}")
            failed_upgrades += len(site_devices)
            
            # Log failed devices
            for device in site_devices:
                device_name = device.get("name", "Unnamed")
                device_mac = device.get("mac", "Unknown")
                device_id = device.get("id", "Unknown")
                device_model = device.get("model", "Unknown")
                current_version = ap_versions.get(device_id, "Unknown")
                
                # Find target version for this device
                target_version = "Unknown"
                for model, plan in upgrade_plan.items():
                    if device in plan["devices"]:
                        target_version = plan["version"]
                        break
                
                results.append({
                    "Site ID": site_id,
                    "Site Name": site_name,
                    "Device ID": device_id,
                    "Device Name": device_name,
                    "Device MAC": device_mac,
                    "Model": device_model,
                    "Current Version": current_version,
                    "Target Version": target_version,
                    "Strategy": upgrade_config["strategy"],
                    "P2P Enabled": upgrade_config["enable_p2p"],
                    "Max Failure %": upgrade_config["max_failure_percentage"],
                    "Force Upgrade": upgrade_config["force"],
                    "Upgrade ID": "Failed",
                    "Status": error_status,
                    "Timestamp": datetime.now(timezone.utc).isoformat()
                })
    
    # Step 10: Configure site auto-upgrade settings
    logging.debug(f"Starting auto-upgrade configuration for site {site_name} (ID: {site_id})")
    print(f"\n  Configuring site auto-upgrade settings...")
    
    # Collect unique versions from upgrade plan
    target_versions = set(plan["version"] for plan in upgrade_plan.values())
    logging.debug(f"Collected target versions from upgrade plan: {target_versions}")
    
    if len(target_versions) == 1:
        # Single version - configure auto-upgrade for the site
        target_version = list(target_versions)[0]
        logging.debug(f"Single target version detected: {target_version}")
        
        auto_upgrade_prompt = input(f"Configure site auto-upgrade settings? (Y/n): ").strip().lower()
        logging.debug(f"User auto-upgrade configuration choice: '{auto_upgrade_prompt}'")
        if auto_upgrade_prompt not in ['n', 'no']:
            try:
                logging.debug(f"Proceeding with auto-upgrade configuration for site {site_name}")
                print(f"   Configuring site auto-upgrade settings...")
                
                # Get current site settings to check existing auto-upgrade configuration
                logging.debug(f"Retrieving current auto-upgrade settings for site {site_name}")
                current_auto_upgrade = None
                current_settings = {}
                try:
                    current_settings_resp = mistapi.api.v1.sites.setting.getSiteSetting(apisession, site_id)
                    logging.debug(f"API call getSiteSetting completed for site {site_id}")
                    current_settings = current_settings_resp.data if hasattr(current_settings_resp, 'data') else {}
                    current_auto_upgrade = current_settings.get("auto_upgrade", {}) if isinstance(current_settings, dict) else {}
                    
                    logging.debug(f"Current auto-upgrade settings retrieved: {current_auto_upgrade}")
                    
                    # Display current auto-upgrade settings if they exist
                    if current_auto_upgrade and current_auto_upgrade.get("enabled"):
                        logging.debug(f"Auto-upgrade currently enabled for site {site_name}")
                        print(f"   Current auto-upgrade settings:")
                        print(f"      !? Enabled: Yes")
                        print(f"      !? Version: {current_auto_upgrade.get('version', 'Not set')}")
                        print(f"      !? Time of day: {current_auto_upgrade.get('time_of_day', 'Not set')}")
                        day_of_week = current_auto_upgrade.get('day_of_week')
                        if day_of_week:
                            print(f"      !? Day of week: {day_of_week}")
                        else:
                            print(f"      !? Day of week: Every day")
                    else:
                        logging.debug(f"Auto-upgrade currently disabled or not configured for site {site_name}")
                        print(f"   Current auto-upgrade: Disabled or not configured")
                        
                except Exception as e:
                    logging.warning(f"Could not retrieve current site settings: {e}")
                    print(f"   Could not retrieve current settings: {e}")
                
                # Auto-upgrade configuration options
                logging.debug(f"Presenting auto-upgrade configuration options for target version {target_version}")
                print(f"\n   Auto-upgrade configuration options:")
                print(f"      [1] Enable auto-upgrade to version {target_version}")
                print(f"      [2] Disable auto-upgrade") 
                print(f"      [3] Skip auto-upgrade configuration")
                
                config_choice = input(f"   Select option (1-3, default=1): ").strip() or "1"
                logging.debug(f"User auto-upgrade configuration choice: '{config_choice}'")
                
                if config_choice == "2":
                    # Disable auto-upgrade
                    logging.debug(f"Configuring auto-upgrade to be disabled for site {site_name}")
                    new_auto_upgrade = {
                        "enabled": False
                    }
                    print(f"   Auto-upgrade will be disabled")
                elif config_choice == "3":
                    # Skip configuration
                    logging.debug(f"Skipping auto-upgrade configuration for site {site_name}")
                    print("   Skipping site auto-upgrade configuration")
                    logging.info("User chose to skip site auto-upgrade configuration")
                    # Continue without configuring auto-upgrade
                    pass
                else:
                    # Enable auto-upgrade (option 1 or fallback)
                    logging.debug(f"Enabling comprehensive auto-upgrade for site {site_name} with target version {target_version}")
                    print(f"   Configuring comprehensive auto-upgrade settings...")
                    print(f"   This ensures all AP models get appropriate firmware automatically")
                    
                    # Build custom_versions dictionary starting with models from the upgrade plan
                    custom_versions = {}
                    models_in_upgrade_plan = set(upgrade_plan.keys())
                    logging.debug(f"Models in upgrade plan: {models_in_upgrade_plan}")
                    
                    # Get all models from the upgrade plan and set their target versions
                    for model, plan in upgrade_plan.items():
                        model_version = plan["version"]
                        custom_versions[model] = model_version
                        logging.debug(f"Setting custom version for {model}: {model_version}")
                        print(f"      {model}: {model_version} (from upgrade plan)")
                    
                    logging.debug(f"Starting AP model family analysis for comprehensive auto-upgrade coverage")
                    print(f"\n   Analyzing all available AP models for comprehensive auto-upgrade coverage...")
                    
                    # Get all available models from the firmware API
                    all_available_models = set()
                    model_version_ranges = {}
                    
                    if 'available_versions' in locals() and available_versions:
                        logging.debug(f"Processing available_versions data with {len(available_versions)} entries")
                        for version_info in available_versions:
                            if isinstance(version_info, dict):
                                # Try both "models" (plural) and "model" (singular) fields
                                models = version_info.get("models", [])
                                model = version_info.get("model")
                                version_num = version_info.get("version", "Unknown")
                                
                                target_models = models if models else ([model] if model else [])
                                
                                for target_model in target_models:
                                    if target_model:
                                        all_available_models.add(target_model)
                                        
                                        # Track version ranges for comprehensive coverage
                                        if target_model not in model_version_ranges:
                                            model_version_ranges[target_model] = []
                                        model_version_ranges[target_model].append(version_num)
                    
                    logging.debug(f"All available models discovered: {all_available_models}")
                    logging.debug(f"Model version ranges: {len(model_version_ranges)} models tracked")
                    
                    # Find models that are available but not in the current upgrade plan
                    models_not_in_plan = all_available_models - models_in_upgrade_plan
                    logging.debug(f"Models not in upgrade plan: {models_not_in_plan}")
                    
                    if models_not_in_plan:
                        logging.debug(f"Processing {len(models_not_in_plan)} additional AP models for auto-upgrade configuration")
                        print(f"\n   Found {len(models_not_in_plan)} additional AP models available for auto-upgrade:")
                        
                        # Group models by their available firmware versions (AP families)
                        def get_version_signature(model_versions):
                            """Create a signature of available versions for grouping"""
                            return tuple(sorted(set(model_versions)))
                        
                        model_families = {}  # signature -> list of models
                        for model in models_not_in_plan:
                            if model in model_version_ranges:
                                signature = get_version_signature(model_version_ranges[model])
                                if signature not in model_families:
                                    model_families[signature] = []
                                model_families[signature].append(model)
                        
                        logging.debug(f"Created {len(model_families)} AP model families based on firmware version signatures")
                        
                        # Display grouped models
                        family_count = 0
                        for signature, models in model_families.items():
                            family_count += 1
                            logging.debug(f"Family {family_count}: {models} with {len(signature)} firmware versions")
                            if len(models) > 1:
                                print(f"      !? AP Family {family_count}: {', '.join(sorted(models))} ({len(signature)} firmware versions)")
                            else:
                                print(f"      !? {models[0]} ({len(signature)} firmware versions)")
                        
                        print(f"\n   Configure auto-upgrade for additional models:")
                        print(f"   Models with identical firmware versions are grouped together as families.")
                        print(f"   This ensures new APs of ANY model will auto-upgrade to appropriate firmware.")
                        
                        configure_additional = input(f"   Configure auto-upgrade for additional models? (Y/n): ").strip().lower()
                        logging.debug(f"User choice for additional model configuration: '{configure_additional}'")
                        
                        if configure_additional not in ['n', 'no']:
                            logging.debug(f"Proceeding with firmware version selection for {len(model_families)} model families")
                            print(f"\n   Selecting firmware versions for additional model families...")
                            print(f"   Strategy: Highest version per major revision (e.g., highest 0.12.x, highest 0.14.x)")
                            
                            # Process each family group
                            for family_idx, (signature, models) in enumerate(model_families.items(), 1):
                                if not models:  # Skip empty groups
                                    logging.debug(f"Skipping empty family group {family_idx}")
                                    continue
                                    
                                logging.debug(f"Processing family {family_idx} with models: {models}")
                                
                                # Get firmware versions for this family (all models have the same versions)
                                representative_model = models[0]
                                if representative_model not in model_version_ranges:
                                    logging.warning(f"No firmware versions found for representative model {representative_model} in family {models}")
                                    print(f"      No firmware versions found for model family {models}")
                                    continue
                                
                                family_versions = model_version_ranges[representative_model]
                                logging.debug(f"Family {family_idx} has {len(family_versions)} firmware versions: {family_versions}")
                                
                                # Group versions by major revision
                                major_revisions = {}
                                for version in family_versions:
                                    try:
                                        # Extract major.minor (e.g., "0.12" from "0.12.27452")
                                        parts = version.split(".")
                                        if len(parts) >= 2:
                                            major_minor = f"{parts[0]}.{parts[1]}"
                                            if major_minor not in major_revisions:
                                                major_revisions[major_minor] = []
                                            major_revisions[major_minor].append(version)
                                    except:
                                        # Fallback for non-standard version formats
                                        major_minor = "other"
                                        if major_minor not in major_revisions:
                                            major_revisions[major_minor] = []
                                        major_revisions[major_minor].append(version)
                                
                                logging.debug(f"Family {family_idx} major revisions: {list(major_revisions.keys())}")
                                
                                # Find highest version for each major revision
                                highest_per_major = {}
                                for major_minor, versions in major_revisions.items():
                                    try:
                                        # Sort versions within this major revision
                                        sorted_versions = sorted(versions, key=lambda x: tuple(map(int, x.split("."))), reverse=True)
                                        highest_per_major[major_minor] = sorted_versions[0]
                                    except:
                                        # Fallback to string sorting
                                        sorted_versions = sorted(versions, reverse=True)
                                        highest_per_major[major_minor] = sorted_versions[0]
                                
                                logging.debug(f"Family {family_idx} highest versions per major: {highest_per_major}")
                                
                                # Display family information
                                if len(models) > 1:
                                    print(f"\n      AP Family {family_idx}: {', '.join(sorted(models))}")
                                    print(f"         These models share identical firmware version compatibility")
                                else:
                                    print(f"\n      Model: {models[0]}")
                                    
                                print(f"         Available major revisions with highest versions:")
                                
                                # Display options for this family
                                major_options = {}
                                for idx, (major_minor, highest_version) in enumerate(sorted(highest_per_major.items()), 1):
                                    print(f"            [{idx}] {major_minor}.x {highest_version}")
                                    major_options[str(idx)] = highest_version
                                
                                print(f"            [s] Skip this family")
                                
                                # Get user selection for the entire family
                                while True:
                                    try:
                                        family_name = f"Family {family_idx}" if len(models) > 1 else models[0]
                                        user_choice = input(f"         Select firmware for {family_name} (1-{len(major_options)}, s): ").strip().lower()
                                        
                                        if user_choice == 's':
                                            print(f"         Skipping {family_name}")
                                            break
                                        elif user_choice in major_options:
                                            selected_version = major_options[user_choice]
                                            # Apply the selected version to all models in this family
                                            for model in models:
                                                custom_versions[model] = selected_version
                                            print(f"         {family_name} firmware {selected_version}")
                                            print(f"            Applied to: {', '.join(sorted(models))}")
                                            break
                                        else:
                                            print(f"         Invalid selection. Please choose 1-{len(major_options)} or 's'.")
                                    except KeyboardInterrupt:
                                        print("\n         Configuration cancelled.")
                                        break
                    
                    # Validate that we have comprehensive model coverage
                    total_models_configured = len(custom_versions)
                    models_from_plan = len(models_in_upgrade_plan)
                    models_additionally_configured = total_models_configured - models_from_plan
                    
                    print(f"\n   Auto-upgrade coverage summary:")
                    print(f"      !? Models from upgrade plan: {models_from_plan}")
                    print(f"      !? Additional models configured: {models_additionally_configured}")
                    print(f"      !? Total models configured: {total_models_configured}")
                    
                    if total_models_configured > 0:
                        print(f"\n   Complete auto-upgrade model configuration:")
                        for model, version in sorted(custom_versions.items()):
                            status = "from upgrade plan" if model in models_in_upgrade_plan else "additional coverage"
                            print(f"      !? {model} firmware {version} ({status})")

                    new_auto_upgrade = {
                        "enabled": True,
                        "version": "custom",  # Use "custom" to indicate custom_versions are in use
                        "custom_versions": custom_versions
                    }
                    
                    # Time scheduling configuration
                    print(f"\n   Auto-upgrade time scheduling:")
                    
                    # Check if user wants to maintain current time settings or change them
                    if current_auto_upgrade and current_auto_upgrade.get("time_of_day"):
                        current_time = current_auto_upgrade.get("time_of_day")
                        current_day = current_auto_upgrade.get("day_of_week")
                        
                        if current_day:
                            current_schedule = f"{current_time} on {current_day}"
                        else:
                            current_schedule = f"{current_time} every day"
                        
                        maintain_time = input(f"   Keep current schedule ({current_schedule})? (Y/n): ").strip().lower()
                        
                        if maintain_time not in ['n', 'no']:
                            # Maintain current time settings
                            new_auto_upgrade["time_of_day"] = current_time
                            if current_day:
                                new_auto_upgrade["day_of_week"] = current_day
                            print(f"   Maintaining current schedule: {current_schedule}")
                        else:
                            # Configure new time settings
                            print(f"   Configure new auto-upgrade schedule:")
                            new_auto_upgrade.update(get_auto_upgrade_time_settings())
                    else:
                        # No current time settings, get new ones
                        print(f"   Configure auto-upgrade schedule:")
                        new_auto_upgrade.update(get_auto_upgrade_time_settings())
                
                # Only proceed with site settings update if user didn't choose to skip
                if config_choice != "3":
                    # Prepare final site settings update
                    logging.debug(f"Preparing auto-upgrade settings update for site {site_name}")
                    site_settings_body = {
                        "auto_upgrade": new_auto_upgrade
                    }
                    logging.debug(f"Auto-upgrade configuration to apply: {new_auto_upgrade}")
                    
                    # Merge with existing settings to preserve other configurations
                    if isinstance(current_settings, dict):
                        logging.debug(f"Merging with existing site settings to preserve other configurations")
                        current_settings.update(site_settings_body)
                        site_settings_body = current_settings
                    
                    # Update site settings
                    logging.debug(f"Calling updateSiteSettings API for site {site_id}")
                    settings_resp = mistapi.api.v1.sites.setting.updateSiteSettings(
                        apisession,
                        site_id,
                        body=site_settings_body
                    )
                    logging.debug(f"updateSiteSettings API call completed for site {site_name}")
                    
                    if new_auto_upgrade.get("enabled", False):
                        logging.info(f"Site auto-upgrade configured successfully for {site_name}")
                        print(f"   Site auto-upgrade configured successfully")
                        custom_versions = new_auto_upgrade.get("custom_versions", {})
                        if custom_versions:
                            logging.debug(f"Auto-upgrade configured with {len(custom_versions)} custom model versions")
                            print(f"   New/replacement APs will auto-upgrade per model:")
                            for model, version in custom_versions.items():
                                print(f"      !? {model}: {version}")
                            
                            # Log with model details
                            version_summary = ", ".join([f"{m}:{v}" for m, v in custom_versions.items()])
                            logging.info(f"Site auto-upgrade configured: site={site_id}, custom_versions={version_summary}")
                        else:
                            logging.debug(f"Auto-upgrade configured with standard version {target_version}")
                            print(f"   New/replacement APs will auto-upgrade to configured version")
                            logging.info(f"Site auto-upgrade configured: site={site_id}")
                    else:
                        logging.info(f"Site auto-upgrade disabled for {site_name}")
                        print(f"   Site auto-upgrade disabled successfully")
                        print(f"   New/replacement APs will NOT auto-upgrade")
                        logging.info(f"Site auto-upgrade disabled: site={site_id}")
                    
                    # Add to results for audit trail
                    if new_auto_upgrade.get("enabled", False):
                        # Auto-upgrade is enabled - set appropriate values
                        custom_versions = new_auto_upgrade.get("custom_versions", {})
                        if custom_versions:
                            version_summary = ", ".join([f"{m}:{v}" for m, v in custom_versions.items()])
                            target_version_display = f"Custom: {version_summary}"
                            status_display = "Site Auto-Upgrade Configured (Custom Versions)"
                        else:
                            target_version_display = target_version
                            status_display = "Site Auto-Upgrade Configured"
                    else:
                        # Auto-upgrade is disabled
                        target_version_display = "DISABLED"
                        status_display = "Site Auto-Upgrade Disabled"
                    
                    auto_upgrade_result = {
                        "Site ID": site_id,
                        "Site Name": site_name,
                        "Device ID": "SITE_CONFIG",
                        "Device Name": "Auto-Upgrade Setting",
                        "Device MAC": "N/A",
                        "Model": "Site Configuration",
                        "Current Version": "N/A",
                        "Target Version": target_version_display,
                        "Strategy": upgrade_config["strategy"],
                        "P2P Enabled": upgrade_config["enable_p2p"],
                        "Max Failure %": upgrade_config["max_failure_percentage"],
                        "Force Upgrade": upgrade_config["force"],
                        "Upgrade ID": "SITE_AUTO_UPGRADE",
                        "Status": status_display,
                        "Timestamp": datetime.now(timezone.utc).isoformat()
                    }
                    
                    results.append(auto_upgrade_result)
                
            except Exception as e:
                error_msg = f"Failed to configure site auto-upgrade: {e}"
                print(f"   {error_msg}")
                logging.error(f"! {error_msg}")
                
                # Add failure to results
                auto_upgrade_result = {
                    "Site ID": site_id,
                    "Site Name": site_name,
                    "Device ID": "SITE_CONFIG",
                    "Device Name": "Auto-Upgrade Setting",
                    "Device MAC": "N/A",
                    "Model": "Site Configuration",
                    "Current Version": "N/A",
                    "Target Version": "ERROR",
                    "Strategy": upgrade_config["strategy"],
                    "P2P Enabled": upgrade_config["enable_p2p"],
                    "Max Failure %": upgrade_config["max_failure_percentage"],
                    "Force Upgrade": upgrade_config["force"],
                    "Upgrade ID": "SITE_AUTO_UPGRADE",
                    "Status": f"ERROR: {e}",
                    "Timestamp": datetime.now(timezone.utc).isoformat()
                }
                results.append(auto_upgrade_result)
        else:
            print("   Skipping site auto-upgrade configuration")
            logging.info("User chose to skip site auto-upgrade configuration")
    
    else:
        # Multiple versions - need careful auto-upgrade configuration
        print(f"   Multiple firmware versions in upgrade plan:")
        for version in sorted(target_versions):
            models_with_version = [model for model, plan in upgrade_plan.items() if plan["version"] == version]
            print(f"      !? Version {version}: {', '.join(models_with_version)}")
        
        print(f"\n   Auto-Upgrade Configuration for Mixed-Model Environment:")
        print(f"   Site auto-upgrade must handle different AP models with different firmware capabilities.")
        
        # Analyze what models exist in the upgrade plan
        all_models_in_plan = set(upgrade_plan.keys())
        print(f"\n   AP Models in this upgrade plan: {', '.join(sorted(all_models_in_plan))}")
        
        # Provide enhanced options for mixed-model auto-upgrade
        print(f"\n   Auto-upgrade options for mixed-model environment:")
        print(f"      [1] Configure custom versions per model (RECOMMENDED)")
        print(f"         !? Each AP model gets its optimal firmware version")
        print(f"         !? New APs will auto-upgrade to model-appropriate firmware")
        print(f"         !? Handles model compatibility constraints automatically")
        print(f"      [2] Disable auto-upgrade")
        print(f"         !? Manual firmware management required for new APs")
        print(f"         !? Prevents version conflicts but requires more maintenance")
        print(f"      [3] Skip auto-upgrade configuration")
        print(f"         !? Leave current auto-upgrade settings unchanged")
        
        auto_upgrade_choice = input("   Select auto-upgrade option (1-3, default=1): ").strip() or "1"
        
        try:
            if auto_upgrade_choice == "3":
                print("   Skipping site auto-upgrade configuration")
                logging.info("User chose to skip site auto-upgrade configuration for multi-version upgrade")
                
            elif auto_upgrade_choice == "2":
                # Disable auto-upgrade
                print(f"   Disabling site auto-upgrade...")
                
                # Configure auto-upgrade disabled
                site_settings_body = {
                    "auto_upgrade": {
                        "enabled": False
                    }
                }
                
                # Get and merge current settings
                try:
                    current_settings_resp = mistapi.api.v1.sites.setting.getSiteSetting(apisession, site_id)
                    current_settings = current_settings_resp.data if hasattr(current_settings_resp, 'data') else {}
                    if isinstance(current_settings, dict):
                        current_settings.update(site_settings_body)
                        site_settings_body = current_settings
                except Exception as e:
                    logging.warning(f"Could not retrieve current site settings: {e}")
                
                settings_resp = mistapi.api.v1.sites.setting.updateSiteSettings(
                    apisession,
                    site_id,
                    body=site_settings_body
                )
                
                print(f"   Site auto-upgrade disabled successfully")
                print(f"   New/replacement APs will NOT auto-upgrade")
                print(f"   Manual firmware management will be required for new devices")
                logging.info(f"Site auto-upgrade disabled: site={site_id} (user selected disable from multi-version upgrade)")
                
                # Add to results
                auto_upgrade_result = {
                    "Site ID": site_id,
                    "Site Name": site_name,
                    "Device ID": "SITE_CONFIG",
                    "Device Name": "Auto-Upgrade Setting",
                    "Device MAC": "N/A",
                    "Model": "Site Configuration",
                    "Current Version": "N/A",
                    "Target Version": "DISABLED",
                    "Strategy": upgrade_config["strategy"],
                    "P2P Enabled": upgrade_config["enable_p2p"],
                    "Max Failure %": upgrade_config["max_failure_percentage"],
                    "Force Upgrade": upgrade_config["force"],
                    "Upgrade ID": "SITE_AUTO_UPGRADE",
                    "Status": "Site Auto-Upgrade Disabled (Mixed-Model Protection)",
                    "Timestamp": datetime.now(timezone.utc).isoformat()
                }
                results.append(auto_upgrade_result)
                
            else:
                # Option 1 (default): Configure custom versions per model
                print(f"   Configuring model-specific auto-upgrade versions...")
                print(f"   This ensures each AP model gets compatible firmware automatically")
                
                # Build custom_versions dictionary starting with models from the upgrade plan
                custom_versions = {}
                models_in_upgrade_plan = set(upgrade_plan.keys())
                
                for model, plan in upgrade_plan.items():
                    model_version = plan["version"]
                    custom_versions[model] = model_version
                    print(f"      {model} firmware {model_version} (from upgrade plan)")
                
                print(f"\n   Analyzing all available AP models for comprehensive auto-upgrade coverage...")
                
                # Get all available models from the firmware API
                all_available_models = set()
                model_version_ranges = {}
                
                if 'available_versions' in locals() and available_versions:
                    for version_info in available_versions:
                        if isinstance(version_info, dict):
                            # Try both "models" (plural) and "model" (singular) fields
                            models = version_info.get("models", [])
                            model = version_info.get("model")
                            version_num = version_info.get("version", "Unknown")
                            
                            target_models = models if models else ([model] if model else [])
                            
                            for target_model in target_models:
                                if target_model:
                                    all_available_models.add(target_model)
                                    
                                    # Track version ranges for comprehensive coverage
                                    if target_model not in model_version_ranges:
                                        model_version_ranges[target_model] = []
                                    model_version_ranges[target_model].append(version_num)
                
                # Find models that are available but not in the current upgrade plan
                models_not_in_plan = all_available_models - models_in_upgrade_plan
                
                if models_not_in_plan:
                    print(f"\n   Found {len(models_not_in_plan)} additional AP models available for auto-upgrade:")
                    
                    # Group models by their available firmware versions (AP families)
                    def get_version_signature(model_versions):
                        """Create a signature of available versions for grouping"""
                        return tuple(sorted(set(model_versions)))
                    
                    model_families = {}  # signature -> list of models
                    for model in models_not_in_plan:
                        if model in model_version_ranges:
                            signature = get_version_signature(model_version_ranges[model])
                            if signature not in model_families:
                                model_families[signature] = []
                            model_families[signature].append(model)
                    
                    # Display grouped models
                    family_count = 0
                    for signature, models in model_families.items():
                        family_count += 1
                        if len(models) > 1:
                            print(f"      !? AP Family {family_count}: {', '.join(sorted(models))} ({len(signature)} firmware versions)")
                        else:
                            print(f"      !? {models[0]} ({len(signature)} firmware versions)")
                    
                    print(f"\n   Configure auto-upgrade for additional models:")
                    print(f"   Models with identical firmware versions are grouped together as families.")
                    print(f"   This ensures new APs of ANY model will auto-upgrade to appropriate firmware.")
                    
                    configure_additional = input(f"   Configure auto-upgrade for additional models? (Y/n): ").strip().lower()
                    
                    if configure_additional not in ['n', 'no']:
                        print(f"\n   Selecting firmware versions for additional model families...")
                        print(f"   Strategy: Highest version per major revision (e.g., highest 0.12.x, highest 0.14.x)")
                        
                        # Process each family group
                        for family_idx, (signature, models) in enumerate(model_families.items(), 1):
                            if not models:  # Skip empty groups
                                continue
                                
                            # Get firmware versions for this family (all models have the same versions)
                            representative_model = models[0]
                            if representative_model not in model_version_ranges:
                                print(f"      No firmware versions found for model family {models}")
                                continue
                            
                            family_versions = model_version_ranges[representative_model]
                            
                            # Group versions by major revision
                            major_revisions = {}
                            for version in family_versions:
                                try:
                                    # Extract major.minor (e.g., "0.12" from "0.12.27452")
                                    parts = version.split(".")
                                    if len(parts) >= 2:
                                        major_minor = f"{parts[0]}.{parts[1]}"
                                        if major_minor not in major_revisions:
                                            major_revisions[major_minor] = []
                                        major_revisions[major_minor].append(version)
                                except:
                                    # Fallback for non-standard version formats
                                    major_minor = "other"
                                    if major_minor not in major_revisions:
                                        major_revisions[major_minor] = []
                                    major_revisions[major_minor].append(version)
                            
                            # Find highest version for each major revision
                            highest_per_major = {}
                            for major_minor, versions in major_revisions.items():
                                try:
                                    # Sort versions within this major revision
                                    sorted_versions = sorted(versions, key=lambda x: tuple(map(int, x.split("."))), reverse=True)
                                    highest_per_major[major_minor] = sorted_versions[0]
                                except:
                                    # Fallback to string sorting
                                    sorted_versions = sorted(versions, reverse=True)
                                    highest_per_major[major_minor] = sorted_versions[0]
                            
                            # Display family information
                            if len(models) > 1:
                                print(f"\n      AP Family {family_idx}: {', '.join(sorted(models))}")
                                print(f"         These models share identical firmware version compatibility")
                            else:
                                print(f"\n      Model: {models[0]}")
                                
                            print(f"         Available major revisions with highest versions:")
                            
                            # Display options for this family
                            major_options = {}
                            for idx, (major_minor, highest_version) in enumerate(sorted(highest_per_major.items()), 1):
                                print(f"            [{idx}] {major_minor}.x {highest_version}")
                                major_options[str(idx)] = highest_version
                            
                            print(f"            [s] Skip this family")
                            
                            # Get user selection for the entire family
                            while True:
                                try:
                                    family_name = f"Family {family_idx}" if len(models) > 1 else models[0]
                                    user_choice = input(f"         Select firmware for {family_name} (1-{len(major_options)}, s): ").strip().lower()
                                    
                                    if user_choice == 's':
                                        print(f"         Skipping {family_name}")
                                        break
                                    elif user_choice in major_options:
                                        selected_version = major_options[user_choice]
                                        # Apply the selected version to all models in this family
                                        for model in models:
                                            custom_versions[model] = selected_version
                                        print(f"         {family_name} firmware {selected_version}")
                                        print(f"            Applied to: {', '.join(sorted(models))}")
                                        break
                                    else:
                                        print(f"         Invalid selection. Please choose 1-{len(major_options)} or 's'.")
                                except KeyboardInterrupt:
                                    print("\n         Configuration cancelled.")
                                    break
                
                # Validate that we have comprehensive model coverage
                total_models_configured = len(custom_versions)
                models_from_plan = len(models_in_upgrade_plan)
                models_additionally_configured = total_models_configured - models_from_plan
                
                print(f"\n   Auto-upgrade coverage summary:")
                print(f"      !? Models from upgrade plan: {models_from_plan}")
                print(f"      !? Additional models configured: {models_additionally_configured}")
                print(f"      !? Total models configured: {total_models_configured}")
                
                if total_models_configured > 0:
                    print(f"\n   Complete auto-upgrade model configuration:")
                    for model, version in sorted(custom_versions.items()):
                        status = "from upgrade plan" if model in models_in_upgrade_plan else "additional coverage"
                        print(f"      !? {model} firmware {version} ({status})")
                
                # Configure auto-upgrade with comprehensive model-specific versions
                new_auto_upgrade = {
                    "enabled": True,
                    "version": "custom",  # Use "custom" to indicate custom_versions are in use
                    "custom_versions": custom_versions
                }
                
                # Time scheduling configuration for comprehensive auto-upgrade
                print(f"\n   Auto-upgrade time scheduling:")
                print(f"   Configure when new APs should automatically upgrade their firmware.")
                
                # Get time settings (reuse existing function)
                time_settings = get_auto_upgrade_time_settings()
                new_auto_upgrade.update(time_settings)
                
                # Prepare final site settings update
                site_settings_body = {
                    "auto_upgrade": new_auto_upgrade
                }
                
                # Get and merge current settings to preserve other configurations
                try:
                    current_settings_resp = mistapi.api.v1.sites.setting.getSiteSetting(apisession, site_id)
                    current_settings = current_settings_resp.data if hasattr(current_settings_resp, 'data') else {}
                    if isinstance(current_settings, dict):
                        current_settings.update(site_settings_body)
                        site_settings_body = current_settings
                except Exception as e:
                    logging.warning(f"Could not retrieve current site settings for merge: {e}")
                
                # Update site settings
                settings_resp = mistapi.api.v1.sites.setting.updateSiteSettings(
                    apisession,
                    site_id,
                    body=site_settings_body
                )
                
                print(f"   Site auto-upgrade configured with model-specific versions")
                print(f"   New APs will auto-upgrade to model-appropriate firmware:")
                
                # Show the configured versions
                for model, version in custom_versions.items():
                    print(f"      !? New {model} APs firmware {version}")
                
                # Show time schedule
                time_of_day = new_auto_upgrade.get("time_of_day", "02:00")
                day_of_week = new_auto_upgrade.get("day_of_week")
                if day_of_week:
                    schedule_text = f"every {day_of_week} at {time_of_day}"
                else:
                    schedule_text = f"daily at {time_of_day}"
                    
                print(f"   Schedule: {schedule_text}")
                print(f"   + Model compatibility: Protected - each model gets appropriate firmware")
                
                logging.info(f"Site auto-upgrade configured with custom versions: {custom_versions}")
                logging.info(f"Auto-upgrade schedule: {schedule_text}")
                
                # Build version summary for results
                version_summary = ", ".join([f"{m}:{v}" for m, v in custom_versions.items()])
                
                # Add to results
                auto_upgrade_result = {
                    "Site ID": site_id,
                    "Site Name": site_name,
                    "Device ID": "SITE_CONFIG",
                    "Device Name": "Auto-Upgrade Setting",
                    "Device MAC": "N/A",
                    "Model": "Site Configuration",
                    "Current Version": "N/A",
                    "Target Version": f"Custom: {version_summary}",
                    "Strategy": upgrade_config["strategy"],
                    "P2P Enabled": upgrade_config["enable_p2p"],
                    "Max Failure %": upgrade_config["max_failure_percentage"],
                    "Force Upgrade": upgrade_config["force"],
                    "Upgrade ID": "SITE_AUTO_UPGRADE",
                    "Status": "Site Auto-Upgrade Configured (Model-Specific Versions)",
                    "Timestamp": datetime.now(timezone.utc).isoformat()
                }
                results.append(auto_upgrade_result)
                
        except Exception as e:
            print(f"   Error during auto-upgrade configuration: {e}")
            logging.error(f"Error during auto-upgrade configuration: {e}")

    # Step 11: Offer to check upgrade status
    if successful_upgrades > 0:
        print(f"\n Firmware upgrade{'s' if successful_upgrades > 1 else ''} initiated successfully!")
        print(f"   {successful_upgrades} upgrade{'s' if successful_upgrades > 1 else ''} started across {len(devices_by_site)} site{'s' if len(devices_by_site) > 1 else ''}")
        
        if upgrade_ids:
            logging.info(f"Upgrade monitoring - {len(upgrade_ids)} upgrade(s) initiated across {len(devices_by_site)} site(s)")
            
            # Store upgrade IDs to file for later retrieval by option 60
            try:
                upgrade_tracking_file = "ActiveUpgrades.json"
                upgrade_tracking_data = []
                
                # Load existing data if file exists
                if os.path.exists(upgrade_tracking_file):
                    try:
                        with open(upgrade_tracking_file, 'r', encoding='utf-8') as f:
                            upgrade_tracking_data = json.load(f)
                    except:
                        upgrade_tracking_data = []
                
                # Add new upgrade records with detailed model tracking
                timestamp = datetime.now(timezone.utc).isoformat()
                for upgrade_id in upgrade_ids:
                    # Find which site this upgrade belongs to by matching with results
                    matching_sites = []
                    upgrade_models = {}
                    total_devices_for_upgrade = 0
                    
                    # Search through results to find devices with this upgrade_id
                    for result in results:
                        result_upgrade_id = result.get("Upgrade ID")
                        if result_upgrade_id == upgrade_id or (result_upgrade_id == "Multiple" and len(upgrade_ids) == 1):
                            site_id = result["Site ID"]
                            site_name = result["Site Name"]
                            model = result["Model"]
                            target_version = result["Target Version"]
                            
                            # Track site info
                            site_key = f"{site_id}|{site_name}"
                            if site_key not in matching_sites:
                                matching_sites.append(site_key)
                            
                            # Track model/version combinations
                            if model not in upgrade_models:
                                upgrade_models[model] = {
                                    'version': target_version,
                                    'device_count': 0
                                }
                            upgrade_models[model]['device_count'] += 1
                            total_devices_for_upgrade += 1
                    
                    # Use first matching site, or fallback to first site from devices_by_site
                    if matching_sites:
                        site_parts = matching_sites[0].split("|", 1)
                        upgrade_site_id = site_parts[0]
                        upgrade_site_name = site_parts[1] if len(site_parts) > 1 else "Unknown"
                    else:
                        # Fallback to first site
                        first_site_id = next(iter(devices_by_site.keys()))
                        upgrade_site_id = first_site_id
                        upgrade_site_name = devices_by_site[first_site_id]['name']
                    
                    # Determine upgrade type for better tracking
                    upgrade_type = "mixed_model" if len(upgrade_models) > 1 else "single_model"
                    version_summary = None
                    
                    if upgrade_type == "single_model":
                        # Single model - use the version directly
                        model_name = next(iter(upgrade_models.keys()))
                        version_summary = f"{model_name} {upgrade_models[model_name]['version']}"
                    else:
                        # Multiple models - create summary
                        model_summaries = []
                        for model, info in upgrade_models.items():
                            model_summaries.append(f"{model}:{info['version']}")
                        version_summary = ", ".join(model_summaries)
                    
                    upgrade_record = {
                        'upgrade_id': upgrade_id,
                        'site_id': upgrade_site_id,
                        'site_name': upgrade_site_name,
                        'org_id': org_id,
                        'strategy': upgrade_config['strategy'],
                        'initiated_timestamp': timestamp,
                        'total_devices': total_devices_for_upgrade or sum(len(plan["devices"]) for plan in upgrade_plan.values()),
                        'upgrade_type': upgrade_type,
                        'version_summary': version_summary,
                        'models_and_versions': {model: info['version'] for model, info in upgrade_models.items()} or {model: plan["version"] for model, plan in upgrade_plan.items()},
                        'device_counts_by_model': {model: info['device_count'] for model, info in upgrade_models.items()},
                        'p2p_enabled': upgrade_config.get('enable_p2p', False),
                        'max_failure_percentage': upgrade_config.get('max_failure_percentage', 5),
                        'status': 'initiated'
                    }
                    upgrade_tracking_data.append(upgrade_record)
                
                # Clean up old records (older than 7 days)
                cutoff_time = datetime.now(timezone.utc) - timedelta(days=7)
                upgrade_tracking_data = [
                    record for record in upgrade_tracking_data
                    if datetime.fromisoformat(record.get('initiated_timestamp', '1970-01-01T00:00:00+00:00')) > cutoff_time
                ]
                
                # Save updated data
                with open(upgrade_tracking_file, 'w', encoding='utf-8') as f:
                    json.dump(upgrade_tracking_data, f, indent=2, ensure_ascii=False)
                
                print(f"   Upgrade tracking data saved to {upgrade_tracking_file}")
                logging.info(f"Saved {len(upgrade_ids)} upgrade IDs to tracking file {upgrade_tracking_file}")
                
            except Exception as e:
                print(f"   Warning: Failed to save upgrade tracking data: {e}")
                logging.warning(f"Failed to save upgrade tracking data: {e}")
        
        # Offer to check upgrade status now
        print(f"\n Reminder: You can monitor upgrade progress using menu option 60")
        print(f"   Option 60: Check current firmware upgrade status across organization")
        
        try:
            check_now = input(f"\n Would you like to check the upgrade status now? (y/n): ").strip().lower()
            if check_now in ['y', 'yes']:
                print(f"\n Checking upgrade status...")
                check_firmware_upgrade_status()
            else:
                print(f"   You can check upgrade status anytime using menu option 60")
        except (EOFError, KeyboardInterrupt):
            print(f"\n   You can check upgrade status anytime using menu option 60")
    
    # Step 12: Write results to CSV
    try:
        results_filename = os.path.join("data", f"AdvancedAPFirmwareUpgrade_{site_name.replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv")
        with open(results_filename, "w", newline='', encoding="utf-8") as f:
            if results:
                fieldnames = ["Site ID", "Site Name", "Device ID", "Device Name", "Device MAC", 
                            "Model", "Current Version", "Target Version", "Strategy", "P2P Enabled",
                            "Max Failure %", "Force Upgrade", "Upgrade ID", "Status", "Timestamp"]
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(results)
        
        print(f"\n  Advanced Firmware Upgrade Operation Completed!")
        print(f"   Successful upgrades initiated: {successful_upgrades} devices")
        print(f"   Failed upgrade attempts: {failed_upgrades} devices")
        print(f"   Detailed results logged to: {results_filename}")
        print(f"   Strategy used: {upgrade_config['strategy'].upper()}")
        
        # Show mixed-model upgrade summary if applicable
        unique_versions_used = set()
        models_upgraded = set()
        for result in results:
            if result.get("Status") != "ERROR":
                unique_versions_used.add(result.get("Target Version", "Unknown"))
                models_upgraded.add(result.get("Model", "Unknown"))
        
        if len(unique_versions_used) > 1:
            print(f"   Mixed-Model Upgrade: {len(models_upgraded)} models, {len(unique_versions_used)} firmware versions")
            for model in sorted(models_upgraded):
                # Find the version for this model
                model_version = "Unknown"
                for result in results:
                    if result.get("Model") == model and result.get("Status") != "ERROR":
                        model_version = result.get("Target Version", "Unknown")
                        break
                model_device_count = sum(1 for r in results if r.get("Model") == model and r.get("Status") != "ERROR")
                print(f"      !? {model}: {model_device_count} devices firmware {model_version}")
            print(f"   This is normal behavior when different AP models support different firmware ranges")
        else:
            single_version = list(unique_versions_used)[0] if unique_versions_used else "Unknown"
            print(f"   Unified Upgrade: All {len(models_upgraded)} model(s) upgrading to firmware {single_version}")
        
        if upgrade_id:
            print(f"   Primary Upgrade ID: {upgrade_id}")
        
        print(f"\n  Important Notes:")
        print(f"   !? Upgrades will continue in the background")
        print(f"   !? Monitor device status in Mist portal or API")
        print(f"   !? Strategy '{upgrade_config['strategy']}' controls rollout pace")
        if upgrade_config['enable_p2p']:
            print(f"   !? P2P enabled - APs will share firmware locally")
        print(f"   !? APs will reboot during upgrade process")
        print(f"   !? Full upgrade process may take 5-15 minutes per device")
        if upgrade_config['strategy'] in ['canary', 'rrm']:
            print(f"   !? Phased rollout will continue automatically based on strategy")
        if upgrade_config.get('start_time'):
            scheduled_time = datetime.fromtimestamp(upgrade_config['start_time']).strftime('%Y-%m-%d %H:%M')
            print(f"   !? Upgrade scheduled for: {scheduled_time}")
        
        # Check if auto-upgrade was configured or disabled
        auto_upgrade_configured = any(r.get("Device ID") == "SITE_CONFIG" and "Configured" in r.get("Status", "") for r in results)
        auto_upgrade_disabled = any(r.get("Device ID") == "SITE_CONFIG" and "Disabled" in r.get("Status", "") for r in results)
        
        if auto_upgrade_configured:
            print(f"   !? Site auto-upgrade configured - new APs will auto-upgrade")
        elif auto_upgrade_disabled:
            print(f"   !? Site auto-upgrade disabled - new APs will NOT auto-upgrade")
        
        logging.info(f"! Advanced AP firmware upgrade results written to {results_filename} ({len(results)} entries)")
        logging.info(f"Advanced firmware upgrade summary: {successful_upgrades} successful, {failed_upgrades} failed, strategy: {upgrade_config['strategy']}")
        
    except Exception as e:
        logging.error(f"! Failed to write results to CSV: {e}")
        print(f"! Failed to write results to CSV: {e}")


# SWITCH FIRMWARE UPGRADE IMPLEMENTATION FUNCTION
def bulk_upgrade_switch_firmware_by_site_impl(org_id, sites_to_upgrade_override=None):
    """
    DESTRUCTIVE: Execute firmware upgrades on switches across selected sites.
    
    This function performs bulk firmware upgrades on network switches with comprehensive
    safety checks and detailed progress tracking. Supports multiple upgrade strategies
    including big bang, canary testing, and rolling upgrade modes.
    
    SECURITY: This operation will reboot network switches and may cause network disruption.
    All switches in target sites will be affected. Use with extreme caution in production.
    
    Args:
        org_id: Organization ID
        sites_to_upgrade_override: Optional list of site dictionaries for template-based upgrades
        
    Returns:
        dict: Upgrade operation results and status information
        canary_percentage (int): Percentage of devices for canary testing
        rrm_rollout_percentage (int): Percentage per wave for rolling upgrades
        delay_between_canary_and_rrm (int): Minutes between canary and rollout phases
        delay_between_rrm_waves (int): Minutes between rolling upgrade waves
        csv_export_path (str): Path for exporting upgrade operation details
        
    Returns:
        dict: Comprehensive upgrade operation results with success/failure tracking
        
    Raises:
        Exception: On critical API failures or validation errors
        
    NETWORK IMPACT WARNING:
    - Switch reboots will disrupt network connectivity
    - Plan maintenance windows for production environments
    - Verify backup connectivity paths before execution
    - Monitor upgrade progress closely for rapid intervention
    """
    # Set up logging for this function
    logger = logging.getLogger(__name__)
    logger.debug(f"Starting bulk switch firmware upgrade - org_id: {org_id}")
    
    # Get organization information
    print("\n-> Validating organization access...")
    try:
        org_info = mistapi.api.v1.orgs.orgs.getOrg(apisession, org_id)
        if org_info.status_code != 200:
            print(f"X  Error accessing organization: {org_info.status_code}")
            logger.error(f"Failed to access organization {org_id}: {org_info.status_code}")
            return {"error": "Organization access failed"}
        
        org_name = org_info.data.get('name', 'Unknown')
        print(f"!? Organization: {org_name}")
        logger.debug(f"Organization validated: {org_name}")
        
    except Exception as e:
        print(f"X  Error validating organization: {str(e)}")
        logger.error(f"Organization validation failed: {str(e)}")
        return {"error": f"Organization validation error: {str(e)}"}

    # Site selection logic
    if sites_to_upgrade_override:
        selected_sites = sites_to_upgrade_override
        print(f"-> Using provided site list: {len(selected_sites)} sites")
    else:
        # Get available sites
        print("\n-> Discovering available sites...")
        try:
            sites_response = mistapi.api.v1.orgs.sites.listOrgSites(apisession, org_id)
            if sites_response.status_code != 200:
                print(f"X  Error retrieving sites: {sites_response.status_code}")
                return {"error": "Failed to retrieve sites"}
            
            all_sites = sites_response.data
            print(f"!? Found {len(all_sites)} total sites")
            
            # Present site selection to user
            print("\nAvailable sites:")
            for index, site in enumerate(all_sites, 1):
                print(f"{index:3}. {site.get('name', 'Unnamed')} (ID: {site.get('id', 'Unknown')})")
            
            print("\nSite selection options:")
            print("A. All sites")
            print("S. Select specific sites")
            print("C. Cancel operation")
            
            site_choice = input("\nEnter your choice (A/S/C): ").strip().upper()
            
            if site_choice == 'C':
                print("-> Operation cancelled by user")
                return {"cancelled": True}
            elif site_choice == 'A':
                selected_sites = all_sites
                print(f"-> Selected all {len(selected_sites)} sites")
            elif site_choice == 'S':
                selected_sites = []
                print("\nEnter site numbers (comma-separated) or ranges (e.g., 1-5):")
                site_input = input("Sites: ").strip()
                
                # Parse site selection
                try:
                    for part in site_input.split(','):
                        part = part.strip()
                        if '-' in part:
                            start, end = map(int, part.split('-'))
                            for device_index in range(start-1, end):
                                if 0 <= device_index < len(all_sites):
                                    selected_sites.append(all_sites[device_index])
                        else:
                            index = int(part) - 1
                            if 0 <= index < len(all_sites):
                                selected_sites.append(all_sites[index])
                    
                    print(f"-> Selected {len(selected_sites)} sites")
                    
                except Exception as e:
                    print(f"X  Invalid site selection: {str(e)}")
                    return {"error": "Invalid site selection"}
            else:
                print("X  Invalid selection")
                return {"error": "Invalid selection"}
                
        except Exception as e:
            print(f"X  Error during site discovery: {str(e)}")
            logger.error(f"Site discovery failed: {str(e)}")
            return {"error": f"Site discovery error: {str(e)}"}

    if not selected_sites:
        print("X  No sites selected")
        return {"error": "No sites selected"}

    # Switch firmware upgrade parameter selection
    print(f"\n{'='*60}")
    print("SWITCH FIRMWARE UPGRADE PARAMETER CONFIGURATION")
    print(f"{'='*60}")
    
    # Strategy selection
    print("\nUpgrade Strategy Options:")
    print("1. big_bang    - Upgrade all switches simultaneously (fastest)")
    print("2. serial      - Upgrade switches one by one (safest)")
    print("3. canary      - Test subset first, then upgrade remaining")
    
    while True:
        strategy_choice = input("\nSelect upgrade strategy (1-3): ").strip()
        if strategy_choice == '1':
            upgrade_strategy = 'big_bang'
            break
        elif strategy_choice == '2':
            upgrade_strategy = 'serial'
            break
        elif strategy_choice == '3':
            upgrade_strategy = 'canary'
            break
        else:
            print("X  Please enter 1, 2, or 3")
    
    print(f"-> Selected strategy: {upgrade_strategy}")
    
    # Force upgrade selection
    print("\nForce Upgrade Options:")
    print("1. Yes - Force upgrade even if same version (recommended for testing)")
    print("2. No  - Skip devices already on target version (recommended for production)")
    
    while True:
        force_choice = input("\nForce upgrade? (1-2): ").strip()
        if force_choice == '1':
            force_upgrade = True
            break
        elif force_choice == '2':
            force_upgrade = False
            break
        else:
            print("X  Please enter 1 or 2")
    
    print(f"-> Force upgrade: {'Yes' if force_upgrade else 'No'}")
    
    # Reboot selection
    print("\nReboot Options:")
    print("1. Yes - Reboot after upgrade (required for switches - recommended)")
    print("2. No  - No reboot (not recommended for switches)")
    
    while True:
        reboot_choice = input("\nReboot after upgrade? (1-2): ").strip()
        if reboot_choice == '1':
            auto_reboot = True
            break
        elif reboot_choice == '2':
            auto_reboot = False
            print("!? WARNING: Switches typically require reboot to complete firmware upgrade")
            break
        else:
            print("X  Please enter 1 or 2")
    
    print(f"-> Auto reboot: {'Yes' if auto_reboot else 'No'}")
    
    # Recovery snapshot selection (Junos specific)
    print("\nRecovery Snapshot Options (Junos devices only):")
    print("1. Yes - Take recovery snapshot after device reboots (recommended for Junos)")
    print("2. No  - Skip recovery snapshot (faster but no post-upgrade backup)")
    
    while True:
        snapshot_choice = input("\nTake recovery snapshot after reboot? (1-2): ").strip()
        if snapshot_choice == '1':
            take_snapshot = True
            break
        elif snapshot_choice == '2':
            take_snapshot = False
            break
        else:
            print("X  Please enter 1 or 2")
    
    print(f"-> Recovery snapshot after reboot: {'Yes' if take_snapshot else 'No'}")

    # Firmware version selection
    print(f"\n{'='*60}")
    print("FIRMWARE VERSION SELECTION")
    print(f"{'='*60}")
    
    # Get available firmware versions for switches
    print("\n-> Discovering available switch firmware versions...")
    try:
        # Get switch inventory to determine current firmware and models
        switches_response = mistapi.api.v1.orgs.inventory.getOrgInventory(
            apisession, org_id, type="switch"
        )
        
        if switches_response.status_code != 200:
            print(f"X  Error retrieving switch inventory: {switches_response.status_code}")
            return {"error": "Failed to retrieve switch inventory"}
        
        switches = switches_response.data
        if not switches:
            print("X  No switches found in organization")
            return {"error": "No switches found"}
        
        print(f"!? Found {len(switches)} switches")
        
        # Extract unique current firmware versions and models
        current_firmware_versions = set()
        switch_models = set()
        for switch in switches:
            if switch.get('version'):
                current_firmware_versions.add(switch.get('version'))
            if switch.get('model'):
                switch_models.add(switch.get('model'))
        
        print(f"-> Switch models found: {', '.join(sorted(switch_models))}")
        print(f"-> Current firmware versions: {', '.join(sorted(current_firmware_versions))}")
        
        if not switch_models:
            print("!? WARNING: No switch models detected - firmware filtering may not work properly")
            logger.warning("No switch models found in inventory - firmware compatibility checking disabled")
        
        # Check for cached firmware data first, then query API if needed
        print("\n-> Checking for cached firmware versions...")
        available_versions = []
        compatible_versions = {}  # Initialize here for scope
        firmware_data = []
        
        # Define cache file path and freshness threshold
        # Name reflects API endpoint: /orgs/{org_id}/devices/versions?type=switch
        cache_file = os.path.join("data", "cached_org_devices_versions_switch.csv")
        cache_freshness_hours = 24  # Cache is fresh for 24 hours
        use_cached_data = False
        
        # Check if cache file exists and is fresh
        if os.path.exists(cache_file):
            try:
                file_age_hours = (datetime.now().timestamp() - os.path.getmtime(cache_file)) / 3600
                if file_age_hours < cache_freshness_hours:
                    # Check if file has content before using it
                    file_size = os.path.getsize(cache_file)
                    if file_size == 0:
                        print(f"-> Cache file exists but is empty, will query API")
                        logger.info("Cache file is empty, will refresh from API")
                    else:
                        print(f"!? Found fresh cached firmware data ({file_age_hours:.1f} hours old)")
                        logger.info(f"Using cached firmware data from {cache_file} (age: {file_age_hours:.1f} hours)")
                        
                        # Read cached data and validate content
                        with open(cache_file, 'r', newline='', encoding='utf-8') as csvfile:
                            reader = csv.DictReader(csvfile)
                            for row in reader:
                                # Convert CSV row back to API format
                                firmware_entry = {
                                    'version': row['version'],
                                    'model': row['model'],
                                    'record_id': int(row.get('record_id', 0)) if row.get('record_id') else None,
                                    'record_size': int(row.get('record_size', 0)) if row.get('record_size') else None,
                                    'record_md5': row.get('record_md5', ''),
                                    '_short': row.get('_short', ''),
                                }
                                firmware_data.append(firmware_entry)
                        
                        # Validate that we actually loaded data
                        if firmware_data:
                            use_cached_data = True
                            logger.info(f"Loaded {len(firmware_data)} firmware entries from cache")
                        else:
                            print(f"-> Cache file has no valid data rows, will query API")
                            logger.info("Cache file exists but contains no valid data, will refresh from API")
                else:
                    print(f"-> Cache file exists but is stale ({file_age_hours:.1f} hours old, threshold: {cache_freshness_hours}h)")
                    logger.info(f"Cache file stale, will refresh from API")
            except Exception as cache_error:
                logger.warning(f"Error reading cache file: {cache_error}")
                print("-> Cache file unreadable, will query API")
        else:
            print("-> No cache file found, will query API")
            logger.info("No cached firmware data found")
        
        # Query API if cache not used
        if not use_cached_data:
            print("-> Querying available firmware versions from Mist API...")
            try:
                # Use the proper listOrgAvailableDeviceVersions API with type=switch parameter
                logger.debug("Calling listOrgAvailableDeviceVersions API for switch firmware")
                versions_response = mistapi.api.v1.orgs.devices.listOrgAvailableDeviceVersions(
                    apisession, 
                    org_id, 
                    type="switch"
                )
                
                if versions_response and hasattr(versions_response, 'data') and versions_response.data:
                    firmware_data = versions_response.data
                    logger.debug(f"API returned {len(firmware_data)} firmware entries for switches")
                
                    # Save fresh API data to cache file
                    try:
                        os.makedirs("data", exist_ok=True)  # Ensure data directory exists
                        with open(cache_file, 'w', newline='', encoding='utf-8') as csvfile:
                            fieldnames = ['version', 'model', 'record_id', 'record_size', 'record_md5', '_short']
                            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                            writer.writeheader()
                            
                            for entry in firmware_data:
                                if isinstance(entry, dict):
                                    # Write relevant fields to cache
                                    cache_row = {
                                        'version': entry.get('version', ''),
                                        'model': entry.get('model', ''),
                                        'record_id': entry.get('record_id', ''),
                                        'record_size': entry.get('record_size', ''),
                                        'record_md5': entry.get('record_md5', ''),
                                        '_short': entry.get('_short', ''),
                                    }
                                    writer.writerow(cache_row)
                        
                        print(f"!? Cached {len(firmware_data)} firmware entries to {cache_file}")
                        logger.info(f"Saved {len(firmware_data)} firmware entries to cache file")
                        
                    except Exception as save_error:
                        logger.warning(f"Failed to save firmware cache: {save_error}")
                        print(f"!? Warning: Could not cache firmware data: {save_error}")
                
                else:
                    logger.warning("API returned empty or invalid firmware data")
                    raise Exception("No firmware data returned from API")
                    
            except Exception as api_error:
                logger.error(f"Failed to query switch firmware versions from API: {api_error}")
                print(f"X  Error querying firmware versions: {api_error}")
                print("   Cannot proceed without current firmware version list.")
                return {"error": f"API firmware query failed: {api_error}"}
        
        # Process firmware data (works for both cached and fresh API data)
        if firmware_data:
            print(f"-> Processing {len(firmware_data)} firmware entries...")
            
            # Filter firmware versions by device model compatibility
            for firmware_entry in firmware_data:
                if isinstance(firmware_entry, dict):
                    # Get version number and model - API returns individual model per entry
                    version = firmware_entry.get('version')
                    firmware_model = firmware_entry.get('model')  # Single model, not array
                    
                    if version and firmware_model:
                        # Check if this firmware model is compatible with any of our switch models
                        if firmware_model in switch_models:
                            if version not in compatible_versions:
                                compatible_versions[version] = set()
                            compatible_versions[version].add(firmware_model)
                            available_versions.append(version)
                            
                            logger.debug(f"Firmware version {version} compatible with organization model: {firmware_model}")
                        else:
                            logger.debug(f"Firmware version {version} NOT compatible - available for: {firmware_model}, organization has: {sorted(switch_models)}")
                        

            
            # Remove duplicates and sort versions (newest first)
            unique_versions = list(set(available_versions))
            
            def version_sort_key(version_string):
                """
                Create sort key for proper version number ordering.
                Handles Juniper version formats like: 24.4R2.23, 24.4R1-S2.12, 23.4R3.11
                """
                try:
                    # Remove common prefixes and suffixes, normalize separators
                    normalized = version_string.replace('-S', '.').replace('R', '.')
                    # Split into parts and convert numbers to integers for proper numeric sorting
                    parts = []
                    for part in normalized.split('.'):
                        # Try to convert to int, fall back to string comparison
                        try:
                            parts.append(int(part))
                        except ValueError:
                            # Keep as string for non-numeric parts, but ensure consistent ordering
                            parts.append(part.lower())
                    return parts
                except Exception:
                    # Fallback to string sorting if parsing fails
                    return [version_string.lower()]
            
            available_versions = sorted(unique_versions, key=version_sort_key, reverse=True)
            
            # Count firmware entries by model for debugging
            model_counts = {}
            vjunos_versions = []
            ex4100_versions = []
            for entry in firmware_data:
                if isinstance(entry, dict):
                    model = entry.get('model', 'Unknown')
                    version = entry.get('version')
                    model_counts[model] = model_counts.get(model, 0) + 1
                    if model == 'VJUNOS' and version:
                        vjunos_versions.append(version)
                    elif model == 'EX4100-F-12P' and version:
                        ex4100_versions.append(version)
            
            logger.debug(f"Firmware model distribution in data:")
            for model in sorted(switch_models):
                count = model_counts.get(model, 0)
                logger.debug(f"  {model}: {count} firmware entries")
            
            if vjunos_versions:
                logger.debug(f"Sample VJUNOS versions found: {sorted(set(vjunos_versions))[:5]}")
            else:
                logger.debug("No VJUNOS firmware entries found in data")
            
            # Log compatibility summary
            logger.info(f"Successfully filtered {len(available_versions)} compatible switch firmware versions from {len(firmware_data)} total entries")
            if compatible_versions:
                logger.debug("Firmware compatibility summary:")
                for version in available_versions[:5]:  # Log top 5 versions
                    models_list = sorted(compatible_versions.get(version, []))
                    logger.debug(f"  {version}: {models_list}")
            else:
                logger.warning("No compatible firmware versions found for organization switch models")
        else:
            logger.error("No firmware data available for processing")
            print("X  No firmware data available")
            return {"error": "No firmware data available"}
            
        # Validate we have compatible firmware versions
        if not available_versions:
            if switch_models:
                error_msg = f"No compatible firmware versions found for switch models: {', '.join(sorted(switch_models))}"
                print(f"X  {error_msg}")
                print("   This may indicate:")
                print("   - Switch models are not supported by current firmware releases")
                print("   - API data may be incomplete or outdated")
                print("   - Switch models may need manual firmware specification")
            else:
                error_msg = "No switch firmware versions available from API"
                print(f"X  {error_msg}")
            
            logger.error(error_msg)
            
            # Offer manual firmware version entry as fallback
            print(f"\nFallback Option:")
            print("You can still proceed by manually specifying a firmware version.")
            print("!? WARNING: Manual entry bypasses model compatibility checks!")
            print("Ensure the firmware version you enter is compatible with your switch models.")
            
            fallback_choice = input("\nProceed with manual firmware entry? (y/N): ").strip().lower()
            if fallback_choice not in ['y', 'yes']:
                print("-> Operation cancelled")
                return {"error": "No compatible firmware versions and manual entry declined"}
                
            # Manual firmware entry
            print("\nManual firmware version entry:")
            print(f"Switch models in organization: {', '.join(sorted(switch_models))}")
            print("Examples: 23.4R2.21, 22.4R3.25, 21.4R3.15, 20.4R3.8")
            
            while True:
                manual_version = input("Enter firmware version: ").strip()
                if manual_version:
                    target_version = manual_version
                    print(f"!? Using manually specified firmware version: {target_version}")
                    print("   Model compatibility has NOT been verified!")
                    logger.warning(f"Using manually specified firmware {target_version} - compatibility not verified for models: {sorted(switch_models)}")
                    break
                else:
                    print("X  Firmware version is required")
            
            # Skip the normal selection process
            available_versions = [target_version]
        
        if available_versions:
            print(f"!? Found {len(available_versions)} compatible firmware versions")
            
            # Present firmware versions as indexed list with model compatibility
            print("\nAvailable firmware versions (filtered by device model compatibility):")
            print("Index | Version      | Compatible Models                | Notes")
            print("------|--------------|----------------------------------|------")
            
            for idx, version in enumerate(available_versions, 1):
                notes = ""
                if version in current_firmware_versions:
                    notes = "(Currently installed)"
                elif idx == 1:
                    notes = "(Latest/Recommended)"
                
                # Show which models this version is compatible with
                version_models = sorted(compatible_versions.get(version, []))
                models_str = ", ".join(version_models) if version_models else "Unknown"
                if len(models_str) > 32:  # Truncate if too long
                    models_str = models_str[:29] + "..."
                
                print(f"{idx:5} | {version:12} | {models_str:32} | {notes}")
            
            # Get user selection
            while True:
                try:
                    print(f"\nSelect firmware version by index (1-{len(available_versions)}):")
                    selection = input("Enter index number: ").strip()
                    
                    if not selection:
                        print("X  Selection required")
                        continue
                        
                    selection_idx = int(selection) - 1  # Convert to 0-based index
                    
                    if 0 <= selection_idx < len(available_versions):
                        target_version = available_versions[selection_idx]
                        print(f"-> Selected firmware version: {target_version}")
                        break
                    else:
                        print(f"X  Invalid selection. Please enter a number between 1 and {len(available_versions)}")
                        
                except ValueError:
                    print("X  Invalid input. Please enter a number")
                except KeyboardInterrupt:
                    print("\n-> Operation cancelled by user")
                    return {"cancelled": True}
        else:
            # Fallback to manual entry if no versions found
            print("-> No firmware versions available from API, using manual entry")
            print("\nPlease enter target firmware version manually:")
            print("Examples: 23.4R2.21, 22.4R3.25, 21.4R3.15, 20.4R3.8")
            
            target_version = input("Target firmware version: ").strip()
            if not target_version:
                print("X  Firmware version is required")
                return {"error": "No firmware version specified"}
        
        print(f"-> Target firmware version: {target_version}")
        
    except Exception as e:
        print(f"X  Error during firmware discovery: {str(e)}")
        logger.error(f"Firmware discovery failed: {str(e)}")
        return {"error": f"Firmware discovery error: {str(e)}"}

    # Configuration summary and confirmation
    print(f"\n{'='*60}")
    print("UPGRADE CONFIGURATION SUMMARY")
    print(f"{'='*60}")
    print(f"Organization: {org_name}")
    print(f"Sites to upgrade: {len(selected_sites)}")
    print(f"Target firmware: {target_version}")
    print(f"Upgrade strategy: {upgrade_strategy}")
    print(f"Force upgrade: {'Yes' if force_upgrade else 'No'}")
    print(f"Auto reboot: {'Yes' if auto_reboot else 'No'}")
    print(f"Recovery snapshot after reboot: {'Yes' if take_snapshot else 'No'}")
    
    print(f"\n!? CRITICAL WARNING !?")
    print("Switch firmware upgrades will cause network disruption!")
    print("- Switches will reboot and be offline during upgrade")
    print("- Plan appropriate maintenance windows") 
    print("- Ensure backup connectivity if needed")
    print("- Monitor upgrade progress closely")
    
    print(f"\nTo proceed with switch firmware upgrade, type: UPGRADE SWITCHES")
    confirmation = safe_input("Confirmation: ", "", True, "switch firmware upgrade confirmation")
    
    if confirmation is None or confirmation != "UPGRADE SWITCHES":
        print("-> Operation cancelled - incorrect confirmation")
        logger.info("Switch firmware upgrade cancelled by user")
        return {"cancelled": True}

    # Execute upgrade operation
    print(f"\n{'='*60}")
    print("EXECUTING SWITCH FIRMWARE UPGRADE")
    print(f"{'='*60}")
    
    # Initialize results tracking
    upgrade_results = {
        'operation_id': f"switch_upgrade_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        'target_version': target_version,
        'strategy': upgrade_strategy,
        'force': force_upgrade,
        'reboot': auto_reboot,
        'snapshot': take_snapshot,
        'sites_processed': 0,
        'sites_successful': 0,
        'sites_failed': 0,
        'site_results': [],
        'start_time': datetime.now().isoformat(),
        'end_time': None
    }
    
    logger.info(f"Starting switch firmware upgrade: {upgrade_results['operation_id']}")

    try:
        # Process each site for switch firmware upgrade
        for site_index, site_info in enumerate(selected_sites, 1):
            site_id = site_info.get('id')
            site_name = site_info.get('name', 'Unknown Site')
            
            print(f"\n-> Processing site {site_index}/{len(selected_sites)}: {site_name}")
            logger.debug(f"Processing site: {site_name} ({site_id})")
            
            try:
                # Get switches for this site
                site_devices_response = mistapi.api.v1.sites.devices.listSiteDevices(
                    apisession, site_id, type="switch"
                )
                
                if site_devices_response.status_code != 200:
                    print(f"  X  Error retrieving devices: {site_devices_response.status_code}")
                    upgrade_results['sites_failed'] += 1
                    upgrade_results['site_results'].append({
                        'site_id': site_id,
                        'site_name': site_name,
                        'status': 'failed',
                        'error': f"Device retrieval failed: {site_devices_response.status_code}"
                    })
                    continue
                
                site_switches = [d for d in site_devices_response.data if d.get('type') == 'switch']
                
                if not site_switches:
                    print(f"  -> No switches found in site")
                    upgrade_results['sites_processed'] += 1
                    upgrade_results['site_results'].append({
                        'site_id': site_id,
                        'site_name': site_name,
                        'status': 'skipped',
                        'switches_count': 0,
                        'reason': 'No switches found'
                    })
                    continue
                
                print(f"  -> Found {len(site_switches)} switches")
                
                # Extract switch device IDs for targeted upgrade
                switch_device_ids = [switch.get('id') for switch in site_switches if switch.get('id')]
                
                if not switch_device_ids:
                    logger.error(f"No valid switch device IDs found for site {site_name}")
                    upgrade_results['sites_failed'] += 1
                    upgrade_results['site_results'].append({
                        'site_id': site_id,
                        'site_name': site_name,
                        'status': 'failed',
                        'reason': 'No valid switch device IDs'
                    })
                    continue
                
                logger.debug(f"Switch device IDs for upgrade: {switch_device_ids}")
                
                # Prepare upgrade request with device IDs to target only switches
                upgrade_request = {
                    'version': target_version,
                    'strategy': upgrade_strategy,
                    'force': force_upgrade,
                    'reboot': auto_reboot,
                    'snapshot': take_snapshot,
                    'device_ids': switch_device_ids  # Target only the switch devices
                }
                
                print(f"  -> Initiating firmware upgrade...")
                logger.debug(f"Upgrade request for site {site_name}: {upgrade_request}")
                
                # Execute upgrade via Mist API
                upgrade_response = mistapi.api.v1.sites.devices.upgradeSiteDevices(
                    apisession, site_id, body=upgrade_request
                )
                
                if upgrade_response.status_code in [200, 202]:
                    print(f"  !? Upgrade initiated successfully")
                    upgrade_results['sites_successful'] += 1
                    upgrade_results['site_results'].append({
                        'site_id': site_id,
                        'site_name': site_name,
                        'status': 'initiated',
                        'switches_count': len(site_switches),
                        'target_version': target_version,
                        'strategy': upgrade_strategy,
                        'response_code': upgrade_response.status_code
                    })
                    logger.info(f"Switch firmware upgrade initiated for site: {site_name}")
                    
                else:
                    print(f"  X  Upgrade failed: HTTP {upgrade_response.status_code}")
                    upgrade_results['sites_failed'] += 1
                    upgrade_results['site_results'].append({
                        'site_id': site_id,
                        'site_name': site_name,
                        'status': 'failed',
                        'switches_count': len(site_switches),
                        'error': f"API error: {upgrade_response.status_code}",
                        'response': upgrade_response.data if hasattr(upgrade_response, 'data') else None
                    })
                    logger.error(f"Switch firmware upgrade failed for site {site_name}: {upgrade_response.status_code}")
            
            except Exception as e:
                print(f"  X  Error processing site: {str(e)}")
                upgrade_results['sites_failed'] += 1
                upgrade_results['site_results'].append({
                    'site_id': site_id,
                    'site_name': site_name,
                    'status': 'error',
                    'error': str(e)
                })
                logger.error(f"Exception processing site {site_name}: {str(e)}")
            
            upgrade_results['sites_processed'] += 1
        
        # Finalize results
        upgrade_results['end_time'] = datetime.now().isoformat()
        
        print(f"\n{'='*60}")
        print("SWITCH FIRMWARE UPGRADE SUMMARY")
        print(f"{'='*60}")
        print(f"Operation ID: {upgrade_results['operation_id']}")
        print(f"Sites processed: {upgrade_results['sites_processed']}")
        print(f"Sites successful: {upgrade_results['sites_successful']}")  
        print(f"Sites failed: {upgrade_results['sites_failed']}")
        print(f"Target firmware: {target_version}")
        print(f"Strategy: {upgrade_strategy}")
        
        if upgrade_results['sites_failed'] > 0:
            print(f"\n!? {upgrade_results['sites_failed']} sites encountered errors:")
            for result in upgrade_results['site_results']:
                if result['status'] in ['failed', 'error']:
                    print(f"  - {result['site_name']}: {result.get('error', 'Unknown error')}")
        
        print(f"\nUpgrade operations have been initiated.")
        print(f"Monitor progress through Mist dashboard or API.")
        print(f"Check individual switch status for completion.")
        
        logger.info(f"Switch firmware upgrade operation completed: {upgrade_results['operation_id']}")
        return upgrade_results
        
    except Exception as e:
        error_msg = f"Critical error in switch firmware upgrade: {str(e)}"
        print(f"\nX  {error_msg}")
        logger.error(error_msg)
        
        upgrade_results['end_time'] = datetime.now().isoformat()
        upgrade_results['error'] = str(e)
        
        return upgrade_results



# ============================================================================
# ANOMALY EXPORT SECTION - Site Anomaly Events for AI/ML Analysis  
# ============================================================================


def get_potential_anomaly_metrics():
    """
    Dynamically build a list of potential site-scoped anomaly metrics based on 
    ConstInsightMetrics.csv (if available) or fallback to known working metrics.
    """
    try:
        anomaly_metrics_path = get_csv_file_path("ConstInsightMetrics.csv")
        
        if not os.path.exists(anomaly_metrics_path):
            logging.warning("ConstInsightMetrics.csv not found. Please export organization constants first (menu option 11).")
            # Return fallback metrics
            return [
                {"metric_name": "client-roam-band5", "description": "5GHz roaming anomalies", "priority": True},
                {"metric_name": "client-roam-band24", "description": "2.4GHz roaming anomalies", "priority": True},
                {"metric_name": "ap-availability", "description": "AP availability anomalies", "priority": True}
            ]
        
        potential_metrics = []
        
        # Read and parse insight metrics CSV
        with open(anomaly_metrics_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                metric_key = row.get('key', '').strip().lower()
                metric_name = row.get('name', '').strip()
                metric_scope = row.get('scope', '').strip().lower()
                
                # Filter for site-scoped metrics that might have anomaly data
                if metric_scope == 'site' and metric_key:
                    # Prioritize known anomaly-related metrics
                    is_priority = any(keyword in metric_key for keyword in [
                        'roam', 'availability', 'capacity', 'coverage', 'client',
                        'throughput', 'latency', 'band', 'ap-', 'switch-'
                    ])
                    
                    potential_metrics.append({
                        "metric_name": metric_key,
                        "description": metric_name if metric_name else f"Anomaly events for {metric_key}",
                        "priority": is_priority
                    })
        
        # Sort by priority (known working metrics first)
        potential_metrics.sort(key=lambda x: (not x.get("priority", False), x["metric_name"]))
        
        logging.info(f"Found {len(potential_metrics)} potential anomaly metrics from ConstInsightMetrics.csv")
        return potential_metrics
        
    except Exception as e:
        logging.error(f"Error reading ConstInsightMetrics.csv: {str(e)}")
        # Return fallback metrics if CSV parsing fails
        return [
            {"metric_name": "client-roam-band5", "description": "5GHz roaming anomalies", "priority": True},
            {"metric_name": "client-roam-band24", "description": "2.4GHz roaming anomalies", "priority": True},
            {"metric_name": "ap-availability", "description": "AP availability anomalies", "priority": True}
        ]


# The following section was corrupted during refactoring - removing orphaned content
        print("X  No sites selected")
        return {"error": "No sites selected"}

    # SSR firmware upgrade parameter selection
    print(f"\n{'='*60}")
    print("SSR FIRMWARE UPGRADE PARAMETER CONFIGURATION")
    print(f"{'='*60}")
    
    # Strategy selection (conservative defaults for SSR routing infrastructure)
    print("\nUpgrade Strategy Options (optimized for routing infrastructure):")
    print("1. serial      - Upgrade SSRs one by one (safest for routing infrastructure)")  
    print("2. big_bang    - Upgrade all SSRs simultaneously (higher risk)")
    
    while True:
        strategy_choice = input("\nSelect upgrade strategy (1-2, recommend 1): ").strip()
        if strategy_choice == '1':
            upgrade_strategy = 'serial'
            break
        elif strategy_choice == '2':
            upgrade_strategy = 'big_bang'
            print("!? WARNING: big_bang strategy will upgrade all SSRs simultaneously")
            print("   This may cause widespread WAN connectivity disruption")
            break
        else:
            print("X  Please enter 1 or 2")
    
    print(f"-> Selected strategy: {upgrade_strategy}")
    
    # Reboot timing selection (SSR-specific parameter)
    print("\nReboot Timing Options:")
    print("1. Automatic - Reboot immediately after firmware download (recommended)")
    print("2. Manual    - Download firmware only, manual reboot required later")
    
    while True:
        reboot_choice = input("\nReboot timing? (1-2): ").strip()
        if reboot_choice == '1':
            auto_reboot = True
            break
        elif reboot_choice == '2':
            auto_reboot = False
            print("!? WARNING: SSRs require manual reboot to activate new firmware")
            print("   New firmware will not be operational until manual reboot")
            break
        else:
            print("X  Please enter 1 or 2")
    
    print(f"-> Auto reboot: {'Yes' if auto_reboot else 'No'}")
    
    # Channel selection for firmware versions  
    print("\nFirmware Channel Options:")
    print("1. stable - Production-ready releases (recommended)")
    print("2. beta   - Pre-release versions for testing")
    print("3. alpha  - Development versions (not recommended for production)")
    
    while True:
        channel_choice = input("\nSelect firmware channel (1-3): ").strip()
        if channel_choice == '1':
            firmware_channel = 'stable'
            break
        elif channel_choice == '2':
            firmware_channel = 'beta'
            break
        elif channel_choice == '3':
            firmware_channel = 'alpha'
            print("!? WARNING: alpha channel contains development versions")
            print("   Not recommended for production environments")
            break
        else:
            print("X  Please enter 1, 2, or 3")
    
    print(f"-> Firmware channel: {firmware_channel}")

    # SSR-specific firmware version selection
    print(f"\n{'='*60}")
    print("SSR FIRMWARE VERSION SELECTION")
    print(f"{'='*60}")
    
    # Get available firmware versions for SSRs (Session Smart Routers)
    print("\n-> Discovering available SSR firmware versions...")
    try:
        # Use the SSR-specific API to get available firmware versions
        versions_response = mistapi.api.v1.orgs.ssr.listOrgAvailableSsrVersions(
            apisession, org_id, channel=firmware_channel
        )
        
        if versions_response.status_code != 200:
            print(f"X  Error retrieving SSR firmware versions: {versions_response.status_code}")
            logger.error(f"Failed to retrieve SSR versions: {versions_response.status_code}")
            return {"error": "Failed to retrieve SSR firmware versions"}
        
        available_versions = []
        if hasattr(versions_response, 'data') and versions_response.data:
            for version_obj in versions_response.data:
                if isinstance(version_obj, dict):
                    version = version_obj.get('version')
                    package = version_obj.get('package', 'SSR')
                    is_default = version_obj.get('default', False)
                    if version:
                        available_versions.append({
                            'version': version,
                            'package': package,
                            'default': is_default
                        })
                elif isinstance(version_obj, str):
                    # Handle case where API returns just version strings
                    available_versions.append({
                        'version': version_obj,
                        'package': 'SSR',
                        'default': False
                    })
        
        if not available_versions:
            print(f"X  No SSR firmware versions available for {firmware_channel} channel")
            print("   Please check with Juniper support for available SSR firmware versions")
            print("   Or try a different firmware channel (stable/beta/alpha)")
            return {"error": f"No SSR firmware versions available for {firmware_channel} channel"}
        
        print(f"!? Found {len(available_versions)} available SSR firmware versions")
        print(f"  Channel: {firmware_channel}")
        
        # Get SSR inventory to show current firmware versions  
        print("\n-> Checking current SSR devices...")
        ssrs_response = mistapi.api.v1.orgs.inventory.getOrgInventory(
            apisession, org_id, type="gateway"
        )
        
        current_firmware_versions = set()
        ssr_models_found = set()
        ssr_count = 0
        
        if ssrs_response.status_code == 200:
            # Filter for Session Smart Router models specifically
            all_gateways = ssrs_response.data
            for gateway in all_gateways:
                gateway_model = gateway.get('model', '')
                gateway_type = gateway.get('type', '')
                
                # Check if this is an SSR by model or type
                if gateway_type == 'ssr' or 'SSR' in gateway_model or '128T' in gateway_model:
                    ssr_count += 1
                    if gateway.get('version'):
                        current_firmware_versions.add(gateway.get('version'))
                    if gateway.get('model'):
                        ssr_models_found.add(gateway.get('model'))
        
        if ssr_count > 0:
            print(f"!? Found {ssr_count} SSR device(s) in organization")
            if ssr_models_found:
                print(f"  Models: {', '.join(sorted(ssr_models_found))}")
            if current_firmware_versions:
                print(f"  Current versions: {', '.join(sorted(current_firmware_versions))}")
        
        # Present firmware version options
        print(f"\n{'='*50}")
        print("AVAILABLE SSR FIRMWARE VERSIONS")
        print(f"{'='*50}")
        
        for i, version_info in enumerate(available_versions, 1):
            version = version_info['version']
            package = version_info['package']
            is_default = version_info['default']
            
            default_marker = " (default)" if is_default else ""
            print(f"{i:2d}. {version} [{package}]{default_marker}")
        
        # Allow user to select firmware version
        while True:
            try:
                choice = input(f"\nSelect firmware version (1-{len(available_versions)}): ").strip()
                if not choice:
                    print("X  Please enter a selection")
                    continue
                    
                version_index = int(choice) - 1
                if 0 <= version_index < len(available_versions):
                    selected_version = available_versions[version_index]
                    target_version = selected_version['version']
                    break
                else:
                    print(f"X  Please enter a number between 1 and {len(available_versions)}")
            except ValueError:
                print("X  Please enter a valid number")
        
        print(f"-> Selected firmware version: {target_version}")
        
    except Exception as e:
        print(f"X  Error during SSR firmware discovery: {str(e)}")
        logger.error(f"SSR firmware discovery failed: {str(e)}")
        return {"error": f"SSR firmware discovery error: {str(e)}"}

    # Configuration summary and confirmation
    print(f"\n{'='*60}")
    print("SSR UPGRADE CONFIGURATION SUMMARY")
    print(f"{'='*60}")
    print(f"Organization: {org_name}")
    print(f"Sites to upgrade: {len(selected_sites)}")
    print(f"Target firmware: {target_version}")
    print(f"Firmware channel: {firmware_channel}")
    print(f"Upgrade strategy: {upgrade_strategy}")
    print(f"Auto reboot: {'Yes' if auto_reboot else 'No'}")
    
    print(f"\n!? CRITICAL ROUTING INFRASTRUCTURE WARNING !?")
    print("SSR firmware upgrades will cause WAN connectivity disruption!")
    print("- SSRs will reboot and SD-WAN tunnels will be offline during upgrade")
    print("- Branch offices may lose connectivity")
    print("- Plan extended maintenance windows")
    print("- Verify backup connectivity paths")
    print("- Coordinate with network operations team")
    print("- Monitor upgrade progress closely")
    
    print(f"\nTo proceed with SSR firmware upgrade, type: UPGRADE")
    confirmation = safe_input("Confirmation: ", "", True, "SSR firmware upgrade confirmation")
    
    if confirmation is None or confirmation != "UPGRADE":
        print("-> Operation cancelled - incorrect confirmation")
        logger.info("SSR firmware upgrade cancelled by user")
        return {"cancelled": True}

    # Execute upgrade operation
    print(f"\n{'='*60}")
    print("EXECUTING SSR FIRMWARE UPGRADE")
    print(f"{'='*60}")
    
    # Initialize results tracking
    upgrade_results = {
        'operation_id': f"ssr_upgrade_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        'target_version': target_version,
        'strategy': upgrade_strategy,
        'channel': firmware_channel,
        'reboot': auto_reboot,
        'sites_processed': 0,
        'ssrs_upgraded': 0,
        'errors': [],
        'start_time': datetime.now().isoformat(),
        'site_results': []
    }
    
    logger.info(f"Starting SSR firmware upgrade operation: {upgrade_results['operation_id']}")
    
    # Define SSR model patterns for device filtering
    ssr_models = ['SSR', '128T']  # Patterns to identify SSR devices
    
    # Get org-level SSR inventory for validation
    print("-> Validating SSR devices from organization inventory...")
    org_ssr_inventory = {}
    try:
        ssrs_response = mistapi.api.v1.orgs.inventory.getOrgInventory(
            apisession, org_id, type="gateway"
        )
        if ssrs_response.status_code == 200:
            for gateway in ssrs_response.data:
                gateway_id = gateway.get('id')
                gateway_model = gateway.get('model', '')
                gateway_type = gateway.get('type', '')
                
                # Check if this is an SSR by model or type
                if gateway_type == 'ssr' or 'SSR' in gateway_model or '128T' in gateway_model:
                    org_ssr_inventory[gateway_id] = {
                        'model': gateway_model,
                        'type': gateway_type,
                        'version': gateway.get('version', ''),
                        'site_id': gateway.get('site_id', '')
                    }
            print(f"!? Found {len(org_ssr_inventory)} SSR device(s) in organization inventory")
        else:
            logger.error(f"Failed to get org inventory: {ssrs_response.status_code}")
            print("X  Failed to validate SSR inventory")
    except Exception as e:
        logger.error(f"Error getting org SSR inventory: {e}")
        print(f"X  Error validating SSR inventory: {e}")
    
    try:
        # Process each site for SSR upgrades
        for site_index, site in enumerate(selected_sites, 1):
            site_id = site.get('id')
            site_name = site.get('name', 'Unknown')
            
            print(f"\n[{site_index}/{len(selected_sites)}] Processing site: {site_name}")
            logger.info(f"Processing site {site_index}/{len(selected_sites)}: {site_name} (ID: {site_id})")
            
            site_result = {
                'site_id': site_id,
                'site_name': site_name,
                'ssrs_found': 0,
                'upgrade_initiated': False,
                'error': None
            }
            
            try:
                # Get SSRs at this site
                print(f"  -> Discovering SSRs at {site_name}...")
                site_devices_response = mistapi.api.v1.sites.devices.listSiteDevices(
                    apisession, site_id, type='gateway'
                )
                
                if site_devices_response.status_code != 200:
                    error_msg = f"Failed to retrieve devices for site {site_name}: {site_devices_response.status_code}"
                    print(f"  X  {error_msg}")
                    site_result['error'] = error_msg
                    upgrade_results['errors'].append(error_msg)
                    continue
                
                site_devices = site_devices_response.data
                
                # Filter for SSRs at this site
                site_ssrs = []
                for device in site_devices:
                    device_model = device.get('model', '')
                    device_type = device.get('type', '')
                    device_id = device.get('id', '')
                    
                    # Debug: Log device details
                    logger.debug(f"Device {device_id}: model='{device_model}', type='{device_type}'")
                    
                    # Check if this is an SSR
                    if (device_type == 'gateway' and 
                        (any(ssr_pattern in device_model for ssr_pattern in ssr_models) or 'SSR' in device_model)):
                        site_ssrs.append(device)
                        logger.info(f"Identified SSR device: {device_id} (model: {device_model}, type: {device_type})")
                        print(f"    -> Identified SSR: {device_model} ({device_id})")
                    else:
                        logger.debug(f"Skipping non-SSR device: {device_id} (model: {device_model}, type: {device_type})")
                
                site_result['ssrs_found'] = len(site_ssrs)
                
                if not site_ssrs:
                    print(f"  -> No SSRs found at {site_name}, skipping")
                    logger.info(f"No SSRs found at site {site_name}")
                    upgrade_results['sites_processed'] += 1
                    upgrade_results['site_results'].append(site_result)
                    continue
                
                print(f"  !? Found {len(site_ssrs)} SSR(s) at {site_name}")
                
                # Initiate firmware upgrade for SSRs at this site
                ssr_device_ids = [ssr['id'] for ssr in site_ssrs]
                
                # Validate device IDs against org SSR inventory and check firmware versions
                validated_device_ids = []
                skipped_device_ids = []
                for device_id in ssr_device_ids:
                    if device_id in org_ssr_inventory:
                        ssr_info = org_ssr_inventory[device_id]
                        current_version = ssr_info.get('version', '')
                        
                        # Check if device is already at target version
                        if current_version == target_version:
                            logger.info(f"Device {device_id} already at target version {target_version} - skipping")
                            print(f"    -> Device {device_id} already at version {target_version} - skipping")
                            skipped_device_ids.append(device_id)
                        else:
                            # Check for potential firmware downgrade
                            if self._is_firmware_downgrade(current_version, target_version):
                                logger.warning(f"Device {device_id} downgrade detected: {current_version} -> {target_version} - skipping")
                                print(f"    ! Downgrade detected: {ssr_info['model']} ({current_version} -> {target_version}) - skipping")
                                skipped_device_ids.append(device_id)
                            else:
                                validated_device_ids.append(device_id)
                                logger.info(f"Validated SSR device: {device_id} (model: {ssr_info['model']}, current: {current_version} -> target: {target_version})")
                                print(f"    -> Upgrade needed: {ssr_info['model']} ({current_version} -> {target_version})")
                    else:
                        logger.warning(f"Device {device_id} not found in org SSR inventory - skipping")
                        print(f"    !? Device {device_id} not in SSR inventory - skipping")
                        skipped_device_ids.append(device_id)
                
                if not validated_device_ids:
                    if skipped_device_ids:
                        reason = "already at target version or not in SSR inventory"
                        logger.info(f"All devices at {site_name} skipped: {reason}")
                        print(f"  -> All devices at {site_name} skipped ({reason})")
                    else:
                        logger.warning(f"No validated SSR devices found at {site_name}")
                        print(f"  -> No validated SSR devices at {site_name}, skipping")
                    upgrade_results['sites_processed'] += 1
                    upgrade_results['site_results'].append(site_result)
                    continue
                
                print(f"  -> Initiating firmware upgrade for {len(validated_device_ids)} SSR(s) needing upgrade...")
                if skipped_device_ids:
                    print(f"  -> Skipped {len(skipped_device_ids)} device(s) (already at target version or other issues)")
                logger.info(f"Initiating SSR firmware upgrade at {site_name} for validated devices: {validated_device_ids}")
                
                # Use Mist API to upgrade SSR firmware
                # SECURITY: SSR upgrades use org-level API with specific parameters
                upgrade_body = {
                    'device_ids': validated_device_ids,
                    'channel': firmware_channel,
                    'version': target_version,
                    'strategy': upgrade_strategy
                }
                
                # Add reboot timing if auto-reboot enabled
                if auto_reboot:
                    # For auto-reboot, don't set reboot_at (use default timing)
                    # The default is start_time, which enables reboot after download
                    pass  # Let API use default reboot timing
                else:
                    # Disable reboot if auto_reboot is False
                    upgrade_body['reboot_at'] = -1
                
                # Debug: Log the upgrade request body
                logger.info(f"SSR upgrade request body: {upgrade_body}")
                print(f"  -> Request body: channel='{firmware_channel}', version='{target_version}', strategy='{upgrade_strategy}'")
                print(f"  -> Device IDs: {validated_device_ids}")
                
                # Execute the SSR-specific upgrade API call
                upgrade_response = mistapi.api.v1.orgs.ssr.upgradeOrgSsrs(
                    apisession, 
                    org_id,
                    body=upgrade_body
                )
                
                if upgrade_response.status_code in [200, 202]:
                    print(f"  !? Firmware upgrade initiated for {len(validated_device_ids)} SSR(s)")
                    site_result['upgrade_initiated'] = True
                    upgrade_results['ssrs_upgraded'] += len(validated_device_ids)
                    logger.info(f"Successfully initiated SSR firmware upgrade at {site_name}")
                else:
                    # Log response details for debugging
                    try:
                        # Try multiple ways to get response content
                        if hasattr(upgrade_response, 'data') and upgrade_response.data:
                            response_text = str(upgrade_response.data)
                        elif hasattr(upgrade_response, 'text') and upgrade_response.text:
                            response_text = upgrade_response.text
                        elif hasattr(upgrade_response, 'content') and upgrade_response.content:
                            response_text = upgrade_response.content.decode('utf-8')
                        else:
                            response_text = f"Status: {upgrade_response.status_code}, Headers: {dict(upgrade_response.headers) if hasattr(upgrade_response, 'headers') else 'None'}"
                        
                        # Check for specific error types
                        if 'already at the requested fw version' in response_text.lower():
                            # This is informational, not a real error
                            logger.info(f"SSR upgrade skipped at {site_name}: devices already at target version")
                            print(f"  - SSRs at {site_name} already at target version {target_version}")
                            site_result['upgrade_initiated'] = False
                            site_result['skip_reason'] = 'already_at_version'
                            # Don't count this as an error
                        elif 'downgrade fw version not allowed' in response_text.lower():
                            # This is a validation error, not a system error
                            logger.warning(f"SSR downgrade rejected at {site_name}: API prevents firmware downgrades")
                            print(f"  ! Firmware downgrade not allowed at {site_name} - API validation failed")
                            site_result['upgrade_initiated'] = False
                            site_result['skip_reason'] = 'downgrade_not_allowed'
                            # Don't count this as a critical error
                        else:
                            logger.error(f"SSR upgrade API error response: {response_text}")
                            print(f"  -> API Response: {response_text}")
                            
                            error_msg = f"Upgrade initiation failed for {site_name}: {upgrade_response.status_code}"
                            print(f"  X  {error_msg}")
                            site_result['error'] = error_msg
                            upgrade_results['errors'].append(error_msg)
                            logger.error(f"SSR firmware upgrade failed at {site_name}: {upgrade_response.status_code}")
                            
                    except Exception as e:
                        logger.error(f"Could not read response details: {e}")
                        print(f"  -> Could not read response: {e}")
                        
                        error_msg = f"Upgrade initiation failed for {site_name}: {upgrade_response.status_code}"
                        print(f"  X  {error_msg}")
                        site_result['error'] = error_msg
                        upgrade_results['errors'].append(error_msg)
                        logger.error(f"SSR firmware upgrade failed at {site_name}: {upgrade_response.status_code}")
                
            except Exception as site_error:
                error_msg = f"Error processing site {site_name}: {str(site_error)}"
                print(f"  X  {error_msg}")
                site_result['error'] = error_msg
                upgrade_results['errors'].append(error_msg)
                logger.error(f"Site processing error for {site_name}: {str(site_error)}")
            
            upgrade_results['sites_processed'] += 1
            upgrade_results['site_results'].append(site_result)
        
        # Operation completion
        upgrade_results['end_time'] = datetime.now().isoformat()
        
        print(f"\n{'='*60}")
        print("SSR FIRMWARE UPGRADE OPERATION COMPLETED")
        print(f"{'='*60}")
        print(f"Operation ID: {upgrade_results['operation_id']}")
        print(f"Sites processed: {upgrade_results['sites_processed']}")
        print(f"SSRs upgraded: {upgrade_results['ssrs_upgraded']}")
        print(f"Errors encountered: {len(upgrade_results['errors'])}")
        
        if upgrade_results['errors']:
            print(f"\nErrors:")
            for error in upgrade_results['errors']:
                print(f"  - {error}")
        
        print(f"\nSSR upgrade operations have been initiated.")
        print(f"Monitor progress through Mist dashboard or API.")
        print(f"Check individual SSR status for completion and connectivity.")
        print(f"Verify SD-WAN tunnel re-establishment after reboots.")
        
        logger.info(f"SSR firmware upgrade operation completed: {upgrade_results['operation_id']}")
        return upgrade_results
        
    except Exception as e:
        error_msg = f"Critical error in SSR firmware upgrade: {str(e)}"
        print(f"\nX  {error_msg}")
        logger.error(error_msg)
        
        upgrade_results['end_time'] = datetime.now().isoformat()
        upgrade_results['error'] = str(e)
        
        return upgrade_results


# ============================================================================
# ANOMALY EXPORT SECTION - Site Anomaly Events for AI/ML Analysis
# ============================================================================


# Removed duplicate function definition - proper implementation exists above


# Removed duplicate placeholder function - proper implementation below


# Removed duplicate function definition - using the proper one below

# Removed duplicate function definition - proper implementation exists at line 23437


def export_site_anomaly_metrics_to_csv():
    """Export comprehensive anomaly events for a selected site to SiteAnomalyEvents_[SiteName].csv.
    
    Dynamically discovers potential anomaly metrics from ConstInsightMetrics.csv and uses 
    GET /api/v1/sites/:site_id/anomaly/:metric endpoint to retrieve anomaly events
    for all site-scoped metrics related to anomaly detection (capacity, coverage, roaming, 
    client connectivity, AP availability, etc.).
    """
    print("Export Site Anomaly Events:")
    logging.info("Starting export of site anomaly events...")
    
    # Get site selection
    site_id = prompt_site_selection()
    if not site_id:
        print("! No site selected. Exiting.")
        return
    
    # Get site name for filename
    try:
        response = mistapi.api.v1.sites.listSites(apisession, site_id)
        sites = mistapi.get_all(response=response, mist_session=apisession)
        site_name = next((site["name"] for site in sites if site["id"] == site_id), site_id)
    except:
        site_name = site_id
    
    # Clean site name for filename
    sanitized_site_name = EnhancedSSHRunner.sanitize_filename(site_name)
    filename = f"SiteAnomalyEvents_{sanitized_site_name}.csv"
    
    # Dynamically discover potential anomaly metrics from ConstInsightMetrics.csv
    print("! Discovering potential anomaly metrics from Mist API definitions...")
    potential_metrics = get_potential_anomaly_metrics()
    
    # Extract just the metric names for API calls
    site_anomaly_metrics = [metric["metric_name"] for metric in potential_metrics]
    
    # Log discovered metrics
    print(f"! Found {len(site_anomaly_metrics)} potential anomaly metrics:")
    for metric_info in potential_metrics:
        print(f"  - {metric_info['metric_name']}: {metric_info['description'][:60]}...")
    
    if not site_anomaly_metrics:
        print("! No potential anomaly metrics found. Please check ConstInsightMetrics.csv availability.")
        return
    
    all_anomaly_data = []
    metrics_retrieved = 0
    
    print(f"! Retrieving {len(site_anomaly_metrics)} different site anomaly events...")
    
    # Temporarily suppress mistapi error logging to keep console clean
    mistapi_loggers = ['apirequest', 'apiresponse', 'mistapi', 'mistapi.apirequest', 'mistapi.apiresponse']
    original_levels = {}
    for logger_name in mistapi_loggers:
        logger = logging.getLogger(logger_name)
        original_levels[logger_name] = logger.level
        logger.setLevel(logging.CRITICAL)  # Suppress ERROR logs temporarily
    
    try:
        for metric in site_anomaly_metrics:
            try:
                # Call the site anomaly API endpoint
                response = mistapi.api.v1.sites.anomaly.listSiteAnomalyEvents(
                    apisession, 
                    site_id, 
                    metric
                )
                anomaly_data = getattr(response, 'data', response) or {}
                
                if anomaly_data:
                    # Add metric type identifier to each data point
                    anomaly_data['metric_type'] = metric
                    anomaly_data['site_id'] = site_id
                    anomaly_data['site_name'] = site_name
                    anomaly_data['data_type'] = 'site_anomaly_events'
                    all_anomaly_data.append(anomaly_data)
                    metrics_retrieved += 1
                    print(f"!? Retrieved {metric} anomaly events")
                    logging.debug(f"Successfully retrieved {metric} anomaly events for site {site_id}")
                else:
                    print(f"! No {metric} anomaly events available")
                    logging.info(f"No {metric} anomaly events available for site {site_id}")
            except Exception as metric_error:
                print(f"! Error retrieving {metric} anomaly events: {metric_error}")
                logging.warning(f"Error retrieving {metric} anomaly events for site {site_id}: {metric_error}")
        
        # Process and save all collected anomaly data
        if all_anomaly_data:
            processed = flatten_nested_fields_in_list(all_anomaly_data)
            processed = escape_multiline_strings_for_csv(processed)
            DataExporter.save_data_to_output(processed, filename)
            print(f"! {metrics_retrieved} site anomaly event types exported to {filename}")
            logging.info(f"Exported {metrics_retrieved} site anomaly event types for {site_name} to {filename}")
        else:
            print(f"! 0 anomaly events exported to {filename} (no data available)")
            logging.warning(f"No anomaly events available for site {site_name}")
            DataExporter.save_data_to_output([], filename)
            
    except Exception as e:
        print(f"! Error exporting site anomaly events: {e}")
        logging.error(f"Failed to export site anomaly events for {site_name}: {e}")
    finally:
        # Restore original logging levels
        for logger_name, original_level in original_levels.items():
            logging.getLogger(logger_name).setLevel(original_level)


def export_site_device_anomaly_to_csv():
    """Export device-specific anomaly events for a selected device to SiteDeviceAnomalyEvents_[SiteName]_[DeviceName].csv.
    
    Uses GET /api/v1/sites/:site_id/anomaly/:metric/device/:device_id endpoint to retrieve device anomaly events.
    """
    print("Export Site Device Anomaly Events:")
    logging.info("Starting export of site device anomaly events...")
    
    # Get site selection
    site_id = prompt_site_selection()
    if not site_id:
        print("! No site selected. Exiting.")
        return
    
    # Get site name for filename
    try:
        response = mistapi.api.v1.sites.listSites(apisession, site_id)
        sites = mistapi.get_all(response=response, mist_session=apisession)
        site_name = next((site["name"] for site in sites if site["id"] == site_id), site_id)
    except:
        site_name = site_id
    
    # Get device selection
    device_selection = prompt_device_selection(site_id)
    if not device_selection:
        print("! No device selected. Exiting.")
        return
    
    device_mac = device_selection[0]
    device_name = device_selection[1]
    
    # Clean names for filename
    sanitized_site_name = EnhancedSSHRunner.sanitize_filename(site_name)
    sanitized_device_name = EnhancedSSHRunner.sanitize_filename(device_name)
    filename = f"SiteDeviceAnomalyEvents_{sanitized_site_name}_{sanitized_device_name}.csv"
    
    # Define device-specific anomaly metrics
    device_anomaly_metrics = [
        "ap_availability",
        "throughput",
        "capacity"
    ]
    
    all_device_anomaly_data = []
    metrics_retrieved = 0
    
    print(f"! Retrieving {len(device_anomaly_metrics)} different device anomaly events for {device_name}...")
    
    # Temporarily suppress mistapi error logging to keep console clean
    mistapi_loggers = ['apirequest', 'apiresponse', 'mistapi', 'mistapi.apirequest', 'mistapi.apiresponse']
    original_levels = {}
    for logger_name in mistapi_loggers:
        logger = logging.getLogger(logger_name)
        original_levels[logger_name] = logger.level
        logger.setLevel(logging.CRITICAL)  # Suppress ERROR logs temporarily
    
    try:
        for metric in device_anomaly_metrics:
            try:
                # Call the site device anomaly API endpoint
                response = mistapi.api.v1.sites.anomaly.getSiteAnomalyEventsForDevice(
                    apisession, 
                    site_id, 
                    metric, 
                    device_mac
                )
                device_anomaly_data = getattr(response, 'data', response) or {}
                
                if device_anomaly_data:
                    # Add metadata
                    device_anomaly_data['metric_type'] = metric
                    device_anomaly_data['site_id'] = site_id
                    device_anomaly_data['site_name'] = site_name
                    device_anomaly_data['device_mac'] = device_mac
                    device_anomaly_data['device_name'] = device_name
                    device_anomaly_data['data_type'] = 'device_anomaly_events'
                    all_device_anomaly_data.append(device_anomaly_data)
                    metrics_retrieved += 1
                    print(f"!? Retrieved {metric} device anomaly data")
                    logging.debug(f"Successfully retrieved {metric} device anomaly data for {device_mac}")
                else:
                    print(f"! No {metric} device anomaly data available")
                    logging.info(f"No {metric} device anomaly data available for {device_mac}")
            except Exception as metric_error:
                print(f"! Error retrieving {metric} device anomaly data: {metric_error}")
                logging.warning(f"Error retrieving {metric} device anomaly data for {device_mac}: {metric_error}")
        
        # Process and save all collected device anomaly data
        if all_device_anomaly_data:
            processed = flatten_nested_fields_in_list(all_device_anomaly_data)
            processed = escape_multiline_strings_for_csv(processed)
            DataExporter.save_data_to_output(processed, filename)
            print(f"! {metrics_retrieved} device anomaly event types exported to {filename}")
            logging.info(f"Exported {metrics_retrieved} device anomaly event types for {device_name} to {filename}")
        else:
            print(f"! 0 device anomaly events exported to {filename} (no data available)")
            logging.warning(f"No device anomaly events available for {device_name}")
            DataExporter.save_data_to_output([], filename)
            
    except Exception as e:
        print(f"! Error exporting device anomaly events: {e}")
        logging.error(f"Failed to export device anomaly events for {device_name}: {e}")
    finally:
        # Restore original logging levels
        for logger_name, original_level in original_levels.items():
            logging.getLogger(logger_name).setLevel(original_level)


def export_site_client_anomaly_to_csv():
    """Export client-specific anomaly events for a selected client to SiteClientAnomalyEvents_[SiteName]_[ClientMAC].csv.
    
    Uses GET /api/v1/sites/:site_id/anomaly/:metric/client/:client_mac endpoint to retrieve client anomaly events
    including connection success rates, band-specific roaming performance, and throughput issues.
    """
    print("Export Site Client Anomaly Events:")
    logging.info("Starting export of site client anomaly events...")
    
    # Get site selection
    site_id = prompt_site_selection()
    if not site_id:
        print("! No site selected. Exiting.")
        return
    
    # Get site name for filename
    try:
        response = mistapi.api.v1.sites.listSites(apisession, site_id)
        sites = mistapi.get_all(response=response, mist_session=apisession)
        site_name = next((site["name"] for site in sites if site["id"] == site_id), site_id)
    except:
        site_name = site_id
    
    # Use the guided client selection function with the site_id
    client_mac, client_type, selected_site_id = prompt_client_selection(site_id)
    if not client_mac:
        print("! No client selected. Exiting.")
        return
    
    # Get hostname from the client MAC (we'll need to look it up)
    client_hostname = "Unknown"
    try:
        # Search for the client to get hostname
        response = mistapi.api.v1.sites.stats.listSiteWirelessClientsStats(apisession, site_id, limit=100, duration="1d")
        clients = getattr(response, 'data', response) or []
        
        for client in clients:
            if client.get('mac') == client_mac:
                client_hostname = client.get('hostname', client.get('name', 'Unknown'))
                break
                
    except Exception as e:
        logging.warning(f"Could not retrieve client hostname for {client_mac}: {e}")
        client_hostname = client_mac  # Fallback to MAC address 
    
    # Clean names for filename
    sanitized_site_name = EnhancedSSHRunner.sanitize_filename(site_name)
    filename = f"SiteClientAnomalyEvents_{sanitized_site_name}_{client_mac.replace(':', '')}.csv"
    
    # Define client-specific anomaly metrics (verified working metrics)
    client_anomaly_metrics = [
        "successful_connect",    # Note: uses underscore, not hyphen for client endpoint
        "roaming",              # Client roaming issues  
        "throughput"            # Client throughput anomalies
    ]
    
    all_client_anomaly_data = []
    metrics_retrieved = 0
    
    print(f"! Retrieving {len(client_anomaly_metrics)} different client anomaly events for {client_mac} ({client_hostname})...")
    
    # Temporarily suppress mistapi error logging to keep console clean
    mistapi_loggers = ['apirequest', 'apiresponse', 'mistapi', 'mistapi.apirequest', 'mistapi.apiresponse']
    original_levels = {}
    for logger_name in mistapi_loggers:
        logger = logging.getLogger(logger_name)
        original_levels[logger_name] = logger.level
        logger.setLevel(logging.CRITICAL)  # Suppress ERROR logs temporarily
    
    try:
        for metric in client_anomaly_metrics:
            try:
                # Call the site client anomaly API endpoint
                response = mistapi.api.v1.sites.anomaly.getSiteAnomalyEventsForClient(
                    apisession, 
                    site_id, 
                    client_mac, 
                    metric
                )
                client_anomaly_data = getattr(response, 'data', response) or {}
                
                if client_anomaly_data:
                    # Add metadata
                    client_anomaly_data['metric_type'] = metric
                    client_anomaly_data['site_id'] = site_id
                    client_anomaly_data['site_name'] = site_name
                    client_anomaly_data['client_mac'] = client_mac
                    client_anomaly_data['client_hostname'] = client_hostname
                    client_anomaly_data['data_type'] = 'client_anomaly_events'
                    all_client_anomaly_data.append(client_anomaly_data)
                    metrics_retrieved += 1
                    print(f"!? Retrieved {metric} client anomaly data")
                    logging.debug(f"Successfully retrieved {metric} client anomaly data for {client_mac}")
                else:
                    print(f"! No {metric} client anomaly data available")
                    logging.info(f"No {metric} client anomaly data available for {client_mac}")
            except Exception as metric_error:
                print(f"! Error retrieving {metric} client anomaly data: {metric_error}")
                logging.warning(f"Error retrieving {metric} client anomaly data for {client_mac}: {metric_error}")
        
        # Process and save all collected client anomaly data
        if all_client_anomaly_data:
            processed = flatten_nested_fields_in_list(all_client_anomaly_data)
            processed = escape_multiline_strings_for_csv(processed)
            DataExporter.save_data_to_output(processed, filename)
            print(f"! {metrics_retrieved} client anomaly event types exported to {filename}")
            logging.info(f"Exported {metrics_retrieved} client anomaly event types for {client_mac} to {filename}")
        else:
            print(f"! 0 client anomaly events exported to {filename} (no data available)")
            logging.warning(f"No client anomaly events available for {client_mac}")
            DataExporter.save_data_to_output([], filename)
            
    except Exception as e:
        print(f"! Error exporting client anomaly events: {e}")
        logging.error(f"Failed to export client anomaly events for {client_mac}: {e}")
    finally:
        # Restore original logging levels
        for logger_name, original_level in original_levels.items():
            logging.getLogger(logger_name).setLevel(original_level)


# ============================================================================
# WLAN RADIUS AUTHENTICATION TIMER MANAGEMENT
# ============================================================================

def manage_wlan_radius_auth_timers(debug=False):
    """
    Interactive WLAN RADIUS authentication timer management.
    
    Workflow:
    1. Select site
    2. List WLANs using RADIUS/RadSec authentication
    3. Show inheritance information (site-level vs template-level)
    4. Allow modification of auth_servers_timeout, auth_servers_retries, 
       auth_server_selection, and fast_dot1x_timers
    5. Push changes to appropriate endpoint (site WLAN or template)
    
    SECURITY: Modifies WLAN authentication configuration - requires explicit confirmation.
    
    Args:
        debug (bool): Enable verbose debug output
    """
    logging.info("Starting WLAN RADIUS authentication timer management")
    
    # Enable debug logging if requested
    if debug:
        original_level = logging.getLogger().level
        logging.getLogger().setLevel(logging.DEBUG)
        logging.debug("Debug mode enabled - verbose output active for WLAN template troubleshooting")
    
    # Step 1: Select site
    site_id = prompt_and_log_site_selection()
    if not site_id:
        logging.warning("No site selected for WLAN management")
        print("\n[!] No site selected. Exiting.")
        return
    
    org_id = get_cached_or_prompted_org_id()
    if not org_id:
        logging.error("Could not determine organization ID")
        print("\n[!] Unable to determine organization ID. Exiting.")
        return
    
    # Step 2: Fetch site information
    logging.info(f"Fetching site information for site ID: {site_id}")
    try:
        site_response = mistapi.api.v1.sites.sites.getSiteInfo(apisession, site_id)
        if site_response.status_code != 200:
            logging.error(f"Failed to fetch site info: HTTP {site_response.status_code}")
            print(f"\n[!] Failed to fetch site information. Exiting.")
            return
        
        site_info = site_response.data
        site_name = site_info.get('name', 'Unknown Site')
        site_template_id = site_info.get('sitetemplate_id')
        
        logging.info(f"Site: {site_name}")
        if site_template_id:
            logging.info(f"Site Template ID: {site_template_id}")
        else:
            logging.info("No site template assigned")
            
    except Exception as error:
        logging.error(f"Error fetching site info: {error}")
        print(f"\n[!] Error fetching site information: {error}")
        return
    
    # Step 3: Fetch site-level WLANs
    logging.info("Fetching WLANs configured at site level...")
    site_wlans = []
    try:
        site_wlans_response = mistapi.api.v1.sites.wlans.listSiteWlans(apisession, site_id)
        if site_wlans_response.status_code == 200:
            site_wlans = site_wlans_response.data
            logging.info(f"Found {len(site_wlans)} site-level WLANs")
        else:
            logging.warning(f"Failed to fetch site WLANs: HTTP {site_wlans_response.status_code}")
    except Exception as error:
        logging.error(f"Error fetching site WLANs: {error}")
    
    # Step 4: Fetch template-level WLANs if site template is assigned
    site_template_wlans = []
    template_name = None
    if site_template_id:
        logging.info("Fetching WLANs from site template...")
        try:
            # Get template info first for name
            template_response = mistapi.api.v1.orgs.sitetemplates.getOrgSiteTemplate(
                apisession, org_id, site_template_id
            )
            if template_response.status_code == 200:
                template_data = template_response.data
                template_name = template_data.get('name', 'Unknown Template')
                
                # Extract WLANs from template
                if 'wlans' in template_data and template_data['wlans']:
                    site_template_wlans = list(template_data['wlans'].values())
                    logging.info(f"Found {len(site_template_wlans)} site template-level WLANs")
            else:
                logging.warning(f"Failed to fetch site template: HTTP {template_response.status_code}")
        except Exception as error:
            logging.error(f"Error fetching site template: {error}")
    
    # Step 5: Fetch org-level WLANs and check if they use templates assigned to this site
    logging.info("Fetching org-level WLANs to check for template-based configurations...")
    org_wlans = []
    assigned_template_ids = set()
    
    try:
        # First, get all WLAN templates and determine which are assigned to this site
        logging.debug("Fetching WLAN templates to determine which are assigned to this site")
        wlan_templates_response = mistapi.api.v1.orgs.templates.listOrgTemplates(apisession, org_id)
        if wlan_templates_response.status_code == 200:
            wlan_templates = wlan_templates_response.data
            logging.info(f"Found {len(wlan_templates)} org-level WLAN templates")
            logging.debug(f"Retrieved {len(wlan_templates)} WLAN templates from API")
            
            # Determine which templates are assigned to this site
            for wlan_template in wlan_templates:
                template_id = wlan_template.get('id')
                template_name_wlan = wlan_template.get('name', 'Unknown Template')
                
                logging.debug(f"Checking WLAN template: {template_name_wlan} (ID: {template_id})")
                
                # Check if this WLAN template is applied to our site
                applies = wlan_template.get('applies', {})
                assigned_site_ids = applies.get('site_ids', []) if isinstance(applies, dict) else []
                assigned_sitegroup_ids = applies.get('sitegroup_ids', []) if isinstance(applies, dict) else []
                assigned_wxtag_ids = applies.get('wxtag_ids', []) if isinstance(applies, dict) else []
                org_id_apply = applies.get('org_id') if isinstance(applies, dict) else None
                
                logging.debug(f"  applies field: {applies}")
                logging.debug(f"  assigned_site_ids: {assigned_site_ids}")
                logging.debug(f"  assigned_sitegroup_ids: {assigned_sitegroup_ids}")
                logging.debug(f"  assigned_wxtag_ids: {assigned_wxtag_ids}")
                logging.debug(f"  org_id_apply: {org_id_apply}")
                
                # Check various application methods
                is_assigned = False
                assignment_method = None
                
                if org_id_apply:
                    is_assigned = True
                    assignment_method = 'Org-wide (all sites)'
                elif site_id in assigned_site_ids:
                    is_assigned = True
                    assignment_method = 'Explicit site assignment'
                elif assigned_sitegroup_ids:
                    site_groups = site_info.get('sitegroup_ids', [])
                    if any(sg in assigned_sitegroup_ids for sg in site_groups):
                        is_assigned = True
                        assignment_method = 'Site group assignment'
                elif assigned_wxtag_ids:
                    site_tags = site_info.get('wxtag_ids', [])
                    if any(tag in assigned_wxtag_ids for tag in site_tags):
                        is_assigned = True
                        assignment_method = 'WxTag matching'
                
                if is_assigned:
                    assigned_template_ids.add(template_id)
                    logging.debug(f"  Template {template_name_wlan} IS assigned via: {assignment_method}")
                else:
                    logging.debug(f"  Template {template_name_wlan} is NOT assigned to this site")
            
            logging.info(f"Found {len(assigned_template_ids)} WLAN templates assigned to this site")
            logging.debug(f"Assigned template IDs: {assigned_template_ids}")
        else:
            logging.warning(f"Failed to fetch WLAN templates: HTTP {wlan_templates_response.status_code}")
        
        # Now fetch org WLANs and check if they reference assigned templates
        logging.debug("Fetching org WLANs to find those using assigned templates")
        org_wlans_response = mistapi.api.v1.orgs.wlans.listOrgWlans(apisession, org_id)
        if org_wlans_response.status_code == 200:
            all_org_wlans = org_wlans_response.data
            logging.info(f"Found {len(all_org_wlans)} total org WLANs")
            logging.debug(f"Retrieved {len(all_org_wlans)} org WLANs from API")
            
            # Filter to WLANs that use templates assigned to this site
            for wlan in all_org_wlans:
                wlan_template_id = wlan.get('template_id')
                wlan_ssid = wlan.get('ssid', 'Unknown')
                
                logging.debug(f"Checking org WLAN: {wlan_ssid} (template_id: {wlan_template_id})")
                
                if wlan_template_id and wlan_template_id in assigned_template_ids:
                    # This WLAN uses a template assigned to this site
                    wlan['_inheritance_level'] = 'org_wlan_with_template'
                    wlan['_wlan_template_id'] = wlan_template_id
                    
                    # Find template name for reference
                    template_info = next((t for t in wlan_templates if t.get('id') == wlan_template_id), None)
                    if template_info:
                        wlan['_wlan_template_name'] = template_info.get('name', 'Unknown Template')
                    else:
                        wlan['_wlan_template_name'] = 'Unknown Template'
                    
                    org_wlans.append(wlan)
                    logging.info(f"Org WLAN '{wlan_ssid}' uses template '{wlan.get('_wlan_template_name')}' assigned to this site")
                    logging.debug(f"  Added org WLAN: {wlan_ssid}")
                elif wlan_template_id:
                    logging.debug(f"  WLAN uses template {wlan_template_id} which is NOT assigned to this site - skipping")
                else:
                    logging.debug(f"  WLAN has no template_id - skipping")
            
            if org_wlans:
                logging.info(f"Found {len(org_wlans)} org WLANs using templates assigned to this site")
            else:
                logging.info("No org WLANs found using templates assigned to this site")
        else:
            logging.warning(f"Failed to fetch org WLANs: HTTP {org_wlans_response.status_code}")
            
    except Exception as error:
        logging.error(f"Error fetching org WLANs or templates: {error}")
    
    # Step 6: Filter WLANs to only those using RADIUS or RadSec
    def uses_radius_auth(wlan):
        """Check if WLAN uses RADIUS or RadSec authentication."""
        ssid = wlan.get('ssid', 'Unknown')
        
        # Check for RADIUS servers
        has_auth_servers = bool(wlan.get('auth_servers'))
        
        # Check for RadSec configuration
        has_radsec = False
        radsec_config = wlan.get('radsec', {})
        if isinstance(radsec_config, dict):
            has_radsec = radsec_config.get('enabled', False)
        
        # Check auth type
        auth_config = wlan.get('auth', {})
        if isinstance(auth_config, dict):
            auth_type = auth_config.get('type', '')
            uses_eap = auth_type in ['eap', 'eap192']
        else:
            uses_eap = False
            auth_type = 'none'
        
        # Debug logging
        logging.debug(f"WLAN '{ssid}': auth_servers={has_auth_servers}, radsec={has_radsec}, auth_type={auth_type}, uses_eap={uses_eap}")
        
        return has_auth_servers or has_radsec or uses_eap
    
    # Filter site WLANs
    filtered_site_wlans = []
    for wlan in site_wlans:
        if uses_radius_auth(wlan):
            wlan['_inheritance_level'] = 'site'
            wlan['_inheritance_source'] = f"Site: {site_name}"
            filtered_site_wlans.append(wlan)
    
    # Filter site template WLANs
    filtered_site_template_wlans = []
    for wlan in site_template_wlans:
        if uses_radius_auth(wlan):
            wlan['_inheritance_level'] = 'site_template'
            wlan['_inheritance_source'] = f"Site Template: {template_name}"
            wlan['_template_id'] = site_template_id
            filtered_site_template_wlans.append(wlan)
    
    # Filter org WLANs (those using templates assigned to this site)
    filtered_org_wlans = []
    for wlan in org_wlans:
        if uses_radius_auth(wlan):
            # Metadata already set during org WLAN fetching
            template_name_wlan = wlan.get('_wlan_template_name', 'Unknown Template')
            wlan['_inheritance_source'] = f"Org WLAN using template: {template_name_wlan}"
            filtered_org_wlans.append(wlan)
    
    # Combine all filtered WLANs
    all_radius_wlans = filtered_site_wlans + filtered_site_template_wlans + filtered_org_wlans
    
    if not all_radius_wlans:
        print(f"\n[!] No WLANs using RADIUS or RadSec authentication found at this site.")
        print(f"[!] Only WLANs with RADIUS auth servers or RadSec configuration are shown.")
        logging.info("No RADIUS/RadSec WLANs found")
        return
    
    # Step 6: Display WLANs with current configuration
    print(f"\n{'='*100}")
    print(f"RADIUS/RadSec Authenticated WLANs at Site: {site_name}")
    print(f"{'='*100}\n")
    
    for index, wlan in enumerate(all_radius_wlans, start=1):
        ssid = wlan.get('ssid', 'Unknown SSID')
        wlan_id = wlan.get('id', 'Unknown ID')
        enabled = wlan.get('enabled', False)
        inheritance = wlan.get('_inheritance_level', 'unknown')
        source = wlan.get('_inheritance_source', 'Unknown')
        
        # Get current timer values
        timeout = wlan.get('auth_servers_timeout', 5)
        retries = wlan.get('auth_servers_retries', 2)
        selection = wlan.get('auth_server_selection', 'ordered')
        fast_timers = wlan.get('fast_dot1x_timers', False)
        
        # Get auth server info
        auth_servers = wlan.get('auth_servers', [])
        server_count = len(auth_servers) if auth_servers else 0
        
        # Get RadSec info
        radsec_enabled = False
        radsec_config = wlan.get('radsec', {})
        if isinstance(radsec_config, dict):
            radsec_enabled = radsec_config.get('enabled', False)
        
        print(f"[{index}] SSID: {ssid}")
        print(f"    ID: {wlan_id}")
        print(f"    Status: {'Enabled' if enabled else 'Disabled'}")
        print(f"    Inheritance: {inheritance.upper()} - {source}")
        print(f"    ")
        print(f"    Authentication Configuration:")
        print(f"      - RADIUS Servers: {server_count}")
        print(f"      - RadSec: {'Enabled' if radsec_enabled else 'Disabled'}")
        print(f"    ")
        print(f"    Current Timer Settings:")
        print(f"      - auth_servers_timeout: {timeout} seconds")
        print(f"      - auth_servers_retries: {retries}")
        print(f"      - auth_server_selection: {selection}")
        print(f"      - fast_dot1x_timers: {fast_timers}")
        print(f"")
    
    print(f"{'='*100}\n")
    
    # Step 7: Prompt for WLAN selection
    try:
        selection_input = safe_input(
            f"Select WLAN to modify (1-{len(all_radius_wlans)}) or 'q' to quit: ",
            context="wlan_selection"
        ).strip().lower()
        
        if selection_input == 'q':
            print("\n[*] Exiting WLAN management.")
            return
        
        selected_index = int(selection_input) - 1
        if selected_index < 0 or selected_index >= len(all_radius_wlans):
            print(f"\n[!] Invalid selection. Must be between 1 and {len(all_radius_wlans)}.")
            return
        
        selected_wlan = all_radius_wlans[selected_index]
        
    except ValueError:
        print(f"\n[!] Invalid input. Please enter a number between 1 and {len(all_radius_wlans)}.")
        return
    
    # Step 8: Display current configuration and prompt for new values
    print(f"\n{'='*100}")
    print(f"Modifying WLAN: {selected_wlan.get('ssid')}")
    print(f"Inheritance: {selected_wlan.get('_inheritance_level').upper()}")
    print(f"{'='*100}\n")
    
    print(f"Current Configuration:")
    print(f"  auth_servers_timeout: {selected_wlan.get('auth_servers_timeout', 5)} seconds")
    print(f"  auth_servers_retries: {selected_wlan.get('auth_servers_retries', 2)}")
    print(f"  auth_server_selection: {selected_wlan.get('auth_server_selection', 'ordered')}")
    print(f"  fast_dot1x_timers: {selected_wlan.get('fast_dot1x_timers', False)}")
    print(f"")
    
    # Prompt for new values (press Enter to keep current)
    print(f"Enter new values (press Enter to keep current):\n")
    
    try:
        # Timeout
        timeout_input = safe_input(
            f"auth_servers_timeout (1-30) [{selected_wlan.get('auth_servers_timeout', 5)}]: ",
            default_value=str(selected_wlan.get('auth_servers_timeout', 5)),
            context="timeout_input"
        ).strip()
        new_timeout = int(timeout_input) if timeout_input else selected_wlan.get('auth_servers_timeout', 5)
        if new_timeout < 1 or new_timeout > 30:
            print(f"\n[!] Timeout must be between 1 and 30 seconds. Using current value.")
            new_timeout = selected_wlan.get('auth_servers_timeout', 5)
        
        # Retries
        retries_input = safe_input(
            f"auth_servers_retries (0-10) [{selected_wlan.get('auth_servers_retries', 2)}]: ",
            default_value=str(selected_wlan.get('auth_servers_retries', 2)),
            context="retries_input"
        ).strip()
        new_retries = int(retries_input) if retries_input else selected_wlan.get('auth_servers_retries', 2)
        if new_retries < 0 or new_retries > 10:
            print(f"\n[!] Retries must be between 0 and 10. Using current value.")
            new_retries = selected_wlan.get('auth_servers_retries', 2)
        
        # Selection mode
        selection_input = safe_input(
            f"auth_server_selection (ordered/unordered) [{selected_wlan.get('auth_server_selection', 'ordered')}]: ",
            default_value=selected_wlan.get('auth_server_selection', 'ordered'),
            context="selection_input"
        ).strip().lower()
        new_selection = selection_input if selection_input in ['ordered', 'unordered'] else selected_wlan.get('auth_server_selection', 'ordered')
        
        # Fast timers
        fast_input = safe_input(
            f"fast_dot1x_timers (true/false) [{str(selected_wlan.get('fast_dot1x_timers', False)).lower()}]: ",
            default_value=str(selected_wlan.get('fast_dot1x_timers', False)).lower(),
            context="fast_timers_input"
        ).strip().lower()
        new_fast = fast_input == 'true' if fast_input in ['true', 'false'] else selected_wlan.get('fast_dot1x_timers', False)
        
    except ValueError as error:
        print(f"\n[!] Invalid input: {error}. Exiting.")
        return
    
    # Step 9: Calculate and display behavior impact
    print(f"\n{'='*100}")
    print(f"Calculated Authentication Behavior:")
    print(f"{'='*100}\n")
    
    # Get number of RADIUS servers configured
    auth_servers = selected_wlan.get('auth_servers', [])
    server_count = len(auth_servers) if auth_servers else 1  # Assume at least 1 for calculation
    
    # Calculate timing behavior
    single_server_max_time = new_timeout * new_retries
    all_servers_max_time = single_server_max_time * server_count
    
    print(f"RADIUS Server Configuration:")
    print(f"  - Configured servers: {server_count}")
    print(f"  - Server selection mode: {new_selection}")
    print(f"")
    
    print(f"Timeout Behavior:")
    print(f"  - Timeout per attempt: {new_timeout} seconds")
    print(f"  - Retry attempts per server: {new_retries}")
    print(f"  - Maximum time per server: {single_server_max_time} seconds ({new_timeout}s x {new_retries} retries)")
    print(f"")
    
    if server_count > 1:
        if new_selection == 'ordered':
            print(f"Failover Behavior (ordered mode):")
            print(f"  - Primary server: Server #1 (always tries first)")
            print(f"  - Failover sequence: Server #1 -> Server #2 -> ... -> Server #{server_count}")
            print(f"  - Returns to Server #1 for next authentication")
            print(f"  - Maximum time if all servers fail: {all_servers_max_time} seconds")
        else:
            print(f"Load Balancing Behavior (unordered mode):")
            print(f"  - Server selection: Round-robin or random")
            print(f"  - No server preference")
            print(f"  - Maximum time if all servers fail: {all_servers_max_time} seconds")
    else:
        print(f"Single Server Behavior:")
        print(f"  - Maximum authentication failure time: {single_server_max_time} seconds")
    
    print(f"")
    
    # Always calculate fast dot1x timer values for reference
    quiet_period = new_timeout / 2
    transmit_period = new_timeout / 2
    supplicant_timeout = 10  # Fixed default
    max_requests = 3  # Fixed default
    
    if new_fast:
        print(f"Fast 802.1X Timers (ENABLED):")
        print(f"  - quiet-period: {quiet_period:.1f} seconds (auth_servers_timeout / 2)")
        print(f"  - transmit-period: {transmit_period:.1f} seconds (auth_servers_timeout / 2)")
        print(f"  - retries: {new_retries} (from auth_servers_retries)")
        print(f"  - supplicant-timeout: {supplicant_timeout} seconds (fixed default)")
        print(f"  - max-requests: {max_requests} (fixed default)")
        print(f"")
        print(f"  Impact: Faster authentication and retry cycles")
        print(f"  Best for: Modern clients, stable networks, quick roaming")
    else:
        print(f"Standard 802.1X Timers (DISABLED):")
        print(f"  - Current mode: Uses standard 802.1X defaults")
        print(f"  - quiet-period: ~60 seconds (standard default)")
        print(f"  - transmit-period: ~30 seconds (standard default)")
        print(f"")
        print(f"  If fast_dot1x_timers were enabled, would calculate:")
        print(f"    - quiet-period: {quiet_period:.1f} seconds (auth_servers_timeout / 2)")
        print(f"    - transmit-period: {transmit_period:.1f} seconds (auth_servers_timeout / 2)")
        print(f"    - retries: {new_retries} (from auth_servers_retries)")
        print(f"    - supplicant-timeout: {supplicant_timeout} seconds (fixed default)")
        print(f"    - max-requests: {max_requests} (fixed default)")
        print(f"")
        print(f"  Impact: Slower but more conservative authentication")
        print(f"  Best for: Legacy clients, unstable networks, maximum compatibility")
    
    print(f"")
    print(f"Expected Client Experience:")
    print(f"  - Success case: 1-3 seconds (single request/response)")
    print(f"  - First server timeout: ~{single_server_max_time} seconds")
    if server_count > 1:
        print(f"  - All servers fail: ~{all_servers_max_time} seconds")
    print(f"")
    
    # Step 10: Confirm changes
    print(f"{'='*100}")
    print(f"Proposed Configuration Changes:")
    print(f"{'='*100}")
    print(f"  auth_servers_timeout: {selected_wlan.get('auth_servers_timeout', 5)} -> {new_timeout}")
    print(f"  auth_servers_retries: {selected_wlan.get('auth_servers_retries', 2)} -> {new_retries}")
    print(f"  auth_server_selection: {selected_wlan.get('auth_server_selection', 'ordered')} -> {new_selection}")
    print(f"  fast_dot1x_timers: {selected_wlan.get('fast_dot1x_timers', False)} -> {new_fast}")
    print(f"")
    
    if selected_wlan.get('_inheritance_level') == 'site_template':
        print(f"[!] WARNING: This WLAN is inherited from site template: {selected_wlan.get('_inheritance_source')}")
        print(f"[!] Changes will affect ALL sites using this template!")
    elif selected_wlan.get('_inheritance_level') == 'org_wlan_with_template':
        print(f"[!] WARNING: This WLAN is from an org-level WLAN template: {selected_wlan.get('_inheritance_source')}")
        assignment = selected_wlan.get('_org_template_assignment', 'assigned sites')
        template_name_wlan = selected_wlan.get('_wlan_template_name', 'Unknown')
        print(f"[!] Changes will affect ALL sites where WLAN template '{template_name_wlan}' is applied: {assignment}")
    
    print(f"")
    confirmation = safe_input(
        "Type 'APPLY' to apply these changes: ",
        context="confirmation"
    ).strip()
    
    if confirmation != 'APPLY':
        print("\n[*] Changes cancelled. No modifications made.")
        logging.info("User cancelled WLAN authentication timer changes")
        return
    
    # Step 11: Build update payload
    update_payload = {
        'auth_servers_timeout': new_timeout,
        'auth_servers_retries': new_retries,
        'auth_server_selection': new_selection,
        'fast_dot1x_timers': new_fast
    }
    
    # Step 12: Apply changes to appropriate endpoint
    try:
        if selected_wlan.get('_inheritance_level') == 'site':
            # Update site-level WLAN
            print(f"\n[*] Updating site-level WLAN...")
            logging.info(f"Updating site WLAN {selected_wlan.get('id')} with payload: {update_payload}")
            
            response = mistapi.api.v1.sites.wlans.updateSiteWlan(
                apisession,
                site_id,
                selected_wlan.get('id'),
                update_payload
            )
            
            if response.status_code == 200:
                print(f"[+] Successfully updated WLAN: {selected_wlan.get('ssid')}")
                logging.info(f"Successfully updated site WLAN {selected_wlan.get('id')}")
            else:
                print(f"[!] Failed to update WLAN: HTTP {response.status_code}")
                logging.error(f"Failed to update site WLAN: HTTP {response.status_code}, Response: {response.data}")
                
        elif selected_wlan.get('_inheritance_level') == 'site_template':
            # Update site template-level WLAN
            print(f"\n[*] Updating site template-level WLAN...")
            template_id = selected_wlan.get('_template_id')
            wlan_id = selected_wlan.get('id')
            
            logging.info(f"Updating site template WLAN {wlan_id} in template {template_id} with payload: {update_payload}")
            
            # First, get the full template to modify
            template_response = mistapi.api.v1.orgs.sitetemplates.getOrgSiteTemplate(
                apisession, org_id, template_id
            )
            
            if template_response.status_code != 200:
                print(f"[!] Failed to fetch site template: HTTP {template_response.status_code}")
                logging.error(f"Failed to fetch site template for update: HTTP {template_response.status_code}")
                return
            
            template_data = template_response.data
            
            # Update the specific WLAN in the template's wlans dictionary
            if 'wlans' not in template_data or not isinstance(template_data['wlans'], dict):
                print(f"[!] Site template does not contain wlans data structure")
                logging.error("Site template missing wlans dictionary")
                return
            
            # Find and update the WLAN
            wlan_found = False
            for wlan_key, wlan_data in template_data['wlans'].items():
                if wlan_data.get('id') == wlan_id:
                    # Update the WLAN configuration
                    wlan_data.update(update_payload)
                    wlan_found = True
                    break
            
            if not wlan_found:
                print(f"[!] WLAN not found in site template")
                logging.error(f"WLAN {wlan_id} not found in site template {template_id}")
                return
            
            # Push the updated template back
            update_response = mistapi.api.v1.orgs.sitetemplates.updateOrgSiteTemplate(
                apisession,
                org_id,
                template_id,
                template_data
            )
            
            if update_response.status_code == 200:
                print(f"[+] Successfully updated site template WLAN: {selected_wlan.get('ssid')}")
                print(f"[+] All sites using this template will inherit these changes")
                logging.info(f"Successfully updated site template WLAN {wlan_id} in template {template_id}")
            else:
                print(f"[!] Failed to update site template: HTTP {update_response.status_code}")
                logging.error(f"Failed to update site template: HTTP {update_response.status_code}, Response: {update_response.data}")
        
        elif selected_wlan.get('_inheritance_level') == 'org_wlan_with_template':
            # Update org-level WLAN (which references a template)
            print(f"\n[*] Updating org-level WLAN...")
            wlan_id = selected_wlan.get('id')
            
            if not wlan_id:
                print(f"[!] Missing WLAN ID - cannot update")
                logging.error(f"Missing WLAN id for org WLAN update")
                return
            
            logging.info(f"Updating org WLAN {wlan_id} with payload: {update_payload}")
            
            # Update the org WLAN directly
            response = mistapi.api.v1.orgs.wlans.updateOrgWlan(
                apisession,
                org_id,
                wlan_id,
                update_payload
            )
            
            if response.status_code == 200:
                print(f"[+] Successfully updated org WLAN: {selected_wlan.get('ssid')}")
                template_name = selected_wlan.get('_wlan_template_name', 'Unknown')
                print(f"[+] WLAN uses template '{template_name}' for its base configuration")
                logging.info(f"Successfully updated org WLAN {wlan_id}")
            else:
                print(f"[!] Failed to update org WLAN: HTTP {response.status_code}")
                logging.error(f"Failed to update org WLAN: HTTP {response.status_code}, Response: {response.data}")
        
        else:
            print(f"[!] Unknown inheritance level: {selected_wlan.get('_inheritance_level')}")
            logging.error(f"Unknown inheritance level for WLAN")
            return
            
    except Exception as error:
        print(f"\n[!] Error applying changes: {error}")
        logging.error(f"Error applying WLAN authentication timer changes: {error}", exc_info=True)
        return
    
    print(f"\n[+] WLAN authentication timer management completed successfully")
    logging.info("WLAN authentication timer management completed")


def ssh_runner_main():
    """SSH Runner entry point - delegates to class-based application logic"""
    try:
        # Create argument parser
        parser = EnhancedSSHRunner.create_argument_parser()
        
        # Parse arguments
        args = parser.parse_args()
        
        # Run the application
        ssh_main_success = EnhancedSSHRunner.run_application(args)
        
        # If application returns False and it's likely due to missing parameters, show help
        if not ssh_main_success:
            # Check if we have the basic requirements that would indicate help is needed
            if not any([args.hostname, args.interactive]):
                parser.print_help()
        
        # Exit with appropriate code
        sys.exit(0 if ssh_main_success else 1)
        
    except argparse.ArgumentTypeError as e:
        print(f"[ERROR] Invalid argument: {e}")
        sys.exit(1)
    except KeyboardInterrupt:
        print("\n[INTERRUPT] Operation cancelled by user")
        sys.exit(130)
    except Exception as e:
        print(f"[ERROR] Fatal error: {e}")
        sys.exit(1)


def ssh_runner_interactive():
    """SSH Runner wrapper for menu system integration - runs with auto-detection and interactive prompts"""
    try:
        print("\n>> Enhanced SSH Command Runner")
        print("=" * 60)
        
        # Get global CLI args to check for --no-env flag
        cli_args = globals().get('args') if 'args' in globals() else None
        no_env_flag = cli_args.no_env if cli_args and hasattr(cli_args, 'no_env') else False
        
        # Try to load .env configuration first
        env_config = {}
        if not no_env_flag:
            env_config = EnhancedSSHRunner.load_ssh_config_from_env()
        
        # Determine what we have and what we need
        hosts = env_config.get('hosts', [])
        username = env_config.get('username')
        password = env_config.get('password')
        commands = env_config.get('commands', [])
        
        # Interactive prompts for missing required data
        missing_data = []
        
        # Check for hosts
        if not hosts:
            missing_data.append("SSH hosts")
            try:
                host_input = input("Enter SSH host(s) (comma-separated for multiple): ").strip()
                if host_input:
                    hosts = [h.strip() for h in host_input.split(',') if h.strip()]
                else:
                    print("X  SSH host is required")
                    return False
            except (EOFError, KeyboardInterrupt):
                print("\n[CANCELLED] Operation cancelled by user")
                return False
        
        # Check for username
        if not username:
            missing_data.append("SSH username")
            try:
                username = input("Enter SSH username: ").strip()
                if not username:
                    print("X  SSH username is required")
                    return False
            except (EOFError, KeyboardInterrupt):
                print("\n[CANCELLED] Operation cancelled by user")
                return False
        
        # Check for password
        if not password:
            missing_data.append("SSH password")
            try:
                import getpass
                password = getpass.getpass("Enter SSH password: ")
                if not password:
                    print("X  SSH password is required")
                    return False
            except (EOFError, KeyboardInterrupt):
                print("\n[CANCELLED] Operation cancelled by user")
                return False
        
        # Check for commands
        if not commands:
            print("\nNo commands configured. You can:")
            print("1. Enter a single command now")
            print("2. Use default commands from data/SSH_COMMANDS.CSV (if available)")
            try:
                choice = input("Enter command or press Enter for CSV fallback: ").strip()
                if choice:
                    commands = [choice]
            except (EOFError, KeyboardInterrupt):
                print("\n[CANCELLED] Operation cancelled by user")
                return False
        
        # Show summary of what will be executed
        if missing_data:
            print(f"\n!? Interactively provided: {', '.join(missing_data)}")
        
        print(f"!? Target hosts: {', '.join(hosts)}")
        print(f"!? Username: {username}")
        print(f"!? Commands: {len(commands)} command(s)")
        if commands:
            for idx, cmd in enumerate(commands, 1):
                print(f"  {idx}. {cmd}")
        
        # Create a mock args object with the collected data
        class MockArgs:
            def __init__(self, hosts, username, password, commands, no_env_setting):
                self.interactive = False
                self.hostname = hosts[0] if len(hosts) == 1 else None  # Single host mode
                self.username = username
                self.password = None  # Never pass password via args for security - use env_config
                self.command = commands[0] if len(commands) == 1 else None  # Single command mode
                self.port = 22
                self.timeout = 30
                self.shell = True
                self.no_shell = False
                # SECURITY: Force no_env=False when we have interactive data to enable env_config loading
                self.no_env = False  # Always enable env loading when we have interactive data
                self.log_level = 'INFO'
                self.debug = False
                self.max_threads = None
                self.secure = False
                # For multi-host scenarios, we'll need to handle this differently
                self._hosts = hosts
                self._commands = commands
        
        # Create args with collected data
        args = MockArgs(hosts, username, password, commands, no_env_flag)
        
        # SECURITY: For interactive mode, we need to override the env_config loading
        # to include the interactively-collected password. We'll temporarily patch
        # the load_ssh_config_from_env method to return our interactive data.
        original_load_method = EnhancedSSHRunner.load_ssh_config_from_env
        
        def mock_load_ssh_config():
            """Return our interactively collected configuration"""
            return {
                'hosts': hosts,
                'username': username,
                'password': password,  # This is the securely collected password
                'commands': commands
            }
        
        try:
            # Temporarily replace the env loader with our interactive data
            EnhancedSSHRunner.load_ssh_config_from_env = mock_load_ssh_config
        
            # Handle multi-host execution if needed
            if len(hosts) > 1 or len(commands) > 1:
                print(f"\n!? Executing {len(commands)} command(s) on {len(hosts)} host(s)")
                
                # Use the EnhancedSSHRunner's multi-host execution directly
                config = {
                    'hosts': hosts,
                    'username': username,
                    'password': password,
                    'commands': commands,
                    'port': 22,
                    'timeout': 30,
                    'shell_mode': True,
                    'max_threads': min(len(hosts), 4)  # Reasonable thread limit
                }
                
                summary = EnhancedSSHRunner.execute_ssh_commands_multi_host(config)
                
                # Display results
                successful = sum(1 for result in summary.values() if result.get('success', False))
                total = len(summary)
                
                print(f"\n!? Execution Summary: {successful}/{total} hosts successful")
                for host, result in summary.items():
                    status = "!?" if result.get('success', False) else "X "
                    print(f"  {status} {host}: {result.get('status', 'Unknown')}")
                
                return successful > 0
            else:
                # Single host/command - use standard execution
                ssh_runner_success = EnhancedSSHRunner.run_application(args)
                
                if ssh_runner_success:
                    print("\n[OK] SSH runner completed successfully")
                else:
                    print("\n[ERROR] SSH runner completed with errors")
                
                return ssh_runner_success
            
        finally:
            # Always restore the original method for security
            EnhancedSSHRunner.load_ssh_config_from_env = original_load_method
        
    except KeyboardInterrupt:
        print("\n[INTERRUPT] Operation cancelled by user")
        return False
    except Exception as e:
        print(f"[ERROR] Fatal error: {e}")
        logging.error(f"SSH Runner error: {e}", exc_info=True)
        return False


# ============================================================================
# TERMINAL USER INTERFACE (TUI) CLASS - Hierarchical Mist API Explorer
# ============================================================================

class MistHelperTUI:
    """Terminal User Interface for exploring the Mist API library hierarchy.
    
    This class provides an interactive, keyboard-driven API browser that lets you:
    - Navigate Thomas Munzer's mistapi package structure
    - Discover available modules (orgs, sites, const, etc.)
    - Explore functions within each module
    - View function signatures and documentation
    - Execute API calls with parameter prompts
    - Display results in formatted tables
    
    Navigation starts at mistapi.api.v1 and allows drill-down into:
    - Modules (orgs, sites, msps, etc.)
    - Sub-modules (orgs.devices, sites.clients, etc.)
    - Functions (listOrgSites, getSiteDevices, etc.)
    
    Design Philosophy:
    - Hierarchical: Mirror the actual mistapi package structure
    - Discovery-driven: Learn the API by exploring
    - Interactive: Execute calls directly from the browser
    - Safe: Read-only operations clearly marked
    - Educational: See signatures and docstrings
    """
    
    def __init__(self, debug_mode=False):
        """Initialize the TUI API explorer.
        
        Args:
            debug_mode (bool): Enable detailed logging of navigation and input
        """
        self.debug_mode = debug_mode
        self.dotenv_values = self._load_dotenv_only()
        
        try:
            from rich.console import Console
            from rich.live import Live
            from rich.panel import Panel
            from rich.table import Table
            from rich.layout import Layout
            from rich import box
            from rich.syntax import Syntax
            from rich.markdown import Markdown
            self.Console = Console
            self.Live = Live
            self.Panel = Panel
            self.Table = Table
            self.Layout = Layout
            self.box = box
            self.Syntax = Syntax
            self.Markdown = Markdown
        except ImportError:
            logging.error("TUI_MODE: Rich library not available - cannot start TUI mode")
            print("[ERROR] Rich library required for TUI mode. Install with: pip install rich")
            sys.exit(1)
        
        # Create console without size override (let it detect terminal size naturally)
        self.console = self.Console()
        self.running = True
        
        # Navigation state - hierarchical path through the API
        self.current_path = []  # e.g., ['orgs', 'sites'] for mistapi.api.v1.orgs.sites
        self.current_items = []  # Current level's items (modules or functions)
        self.current_selection = 0  # Selected index in current_items
        self.breadcrumb = "mistapi.api.v1"  # Display path
        
        # Results from last API call
        self.last_result = None
        self.last_parsed_data = None  # Parsed data from APIResponse
        self.last_error = None
        
        # Execution control - state machine for function execution
        self.execution_state = None  # None, 'prompting', 'executing', 'viewing_results'
        self.current_function = None  # Function being executed
        self.function_params = {}  # Parameters collected so far
        self.param_list = []  # List of parameters to collect
        self.current_param_index = 0  # Current parameter being prompted
        self.input_buffer = ""  # Current input being typed
        self.output_lines = []  # Lines to display in output panel
        self.results_scroll_offset = 0  # Scroll position for results grid (which result)
        self.result_row_scroll = 0  # Scroll position within current result (which row)
        
        if self.debug_mode:
            logging.debug("TUI_DEBUG: Debug mode ENABLED for TUI navigation")
        
        # Platform detection for keyboard input
        self.IS_WINDOWS = platform.system() == 'Windows'
        
        # Import platform-specific keyboard modules
        if self.IS_WINDOWS:
            import msvcrt
            self.msvcrt = msvcrt
        else:
            import select
            import tty
            import termios
            self.select = select
            self.tty = tty
            self.termios = termios
            self.old_terminal_settings = None  # Will be set in run()
        
        # API session reference (needed for function execution)
        self.apisession = None  # Will be set by main script if available
        
        logging.info("TUI_MODE: MistHelperTUI API Explorer initialized")
    
    def _load_dotenv_only(self):
        """Load values ONLY from .env file, not system environment.
        
        Returns:
            dict: Key-value pairs from .env file only
        """
        dotenv_dict = {}
        env_file = '.env'
        
        if not os.path.exists(env_file):
            return dotenv_dict
        
        try:
            with open(env_file, 'r', encoding='utf-8', errors='ignore') as f:
                for line in f:
                    line = line.strip()
                    
                    # Skip empty lines and comments
                    if not line or line.startswith('#'):
                        continue
                    
                    # Parse key=value
                    if '=' in line:
                        key, value = line.split('=', 1)
                        key = key.strip()
                        value = value.strip()
                        
                        # Remove quotes if present
                        if value.startswith('"') and value.endswith('"'):
                            value = value[1:-1]
                        elif value.startswith("'") and value.endswith("'"):
                            value = value[1:-1]
                        
                        dotenv_dict[key] = value
            
            if self.debug_mode:
                # Log loaded keys (not values, for security)
                logging.debug(f"TUI_DEBUG: Loaded {len(dotenv_dict)} values from .env file: {list(dotenv_dict.keys())}")
        
        except Exception as e:
            logging.warning(f"TUI: Could not read .env file: {e}")
        
        return dotenv_dict
    
    def _get_terminal_height(self):
        """Get the current terminal height, accounting for UI chrome.
        
        Returns:
            int: Number of rows available for data display (terminal height minus UI overhead)
        """
        try:
            # Get console size from Rich
            console_height = self.console.size.height
            
            # Account for UI chrome when viewing results grid:
            # - Main panel top border: 1 line
            # - Main panel title (breadcrumb): 1 line  
            # - Results grid panel top border: 1 line
            # - Results grid panel title (Result X of Y...): 1 line
            # - Table header row (Field | Value): 1 line
            # - Results grid panel bottom border: 1 line
            # - Help text inside main panel footer: 2 lines
            # - Main panel bottom border: 1 line
            # Total overhead: 9 lines minimum
            ui_overhead = 10  # Extra buffer for safety
            
            # Calculate available rows for TABLE DATA only, with minimum of 10
            available_rows = max(10, console_height - ui_overhead)
            
            if self.debug_mode:
                logging.debug(f"TUI_DEBUG: Terminal height={console_height}, UI overhead={ui_overhead}, "
                             f"available for TABLE DATA={available_rows}")
            
            return available_rows
            
        except Exception as error:
            if self.debug_mode:
                logging.debug(f"TUI_DEBUG: Could not detect terminal height: {error}, defaulting to 20 rows")
            return 20  # Fallback default
    
    def _discover_current_level(self):
        """Discover modules and functions at the current navigation level.
        
        This method introspects the mistapi package to find:
        - Sub-modules (e.g., at mistapi.api.v1: orgs, sites, const, etc.)
        - Functions (e.g., at mistapi.api.v1.orgs: listOrgs, getOrg, etc.)
        
        Updates self.current_items with discovered elements.
        """
        import importlib
        import inspect
        
        self.current_items = []
        
        try:
            # Build the module path based on current navigation
            if not self.current_path:
                # Root level: mistapi.api.v1
                module_path = "mistapi.api.v1"
            else:
                # Deeper level: mistapi.api.v1.orgs, etc.
                module_path = "mistapi.api.v1." + ".".join(self.current_path)
            
            if self.debug_mode:
                logging.debug(f"TUI_DEBUG: Discovering items at module path: {module_path}")
            
            # Update breadcrumb
            self.breadcrumb = module_path
            
            # Try to import the module
            try:
                module = importlib.import_module(module_path)
                if self.debug_mode:
                    logging.debug(f"TUI_DEBUG: Successfully imported module: {module_path}")
            except ImportError as import_error:
                logging.error(f"TUI: Could not import {module_path}: {import_error}")
                self.current_items = [("error", f"Module not found: {module_path}", None)]
                return
            
            # Discover sub-modules and functions
            for name in dir(module):
                # Skip private/internal items
                if name.startswith('_'):
                    continue
                
                try:
                    item = getattr(module, name)
                    
                    # Check if it's a sub-module
                    if inspect.ismodule(item):
                        # Only show modules from mistapi package
                        if hasattr(item, '__package__') and 'mistapi' in str(item.__package__):
                            self.current_items.append({
                                'type': 'module',
                                'name': name,
                                'object': item,
                                'description': f"Module: {name}"
                            })
                    
                    # Check if it's a callable function
                    elif callable(item) and not inspect.isclass(item):
                        # Get function signature
                        try:
                            sig = inspect.signature(item)
                            params = str(sig)
                        except:
                            params = "(...)"
                        
                        # Get docstring first line
                        doc = inspect.getdoc(item)
                        short_doc = doc.split('\n')[0] if doc else "No description"
                        if len(short_doc) > 60:
                            short_doc = short_doc[:57] + "..."
                        
                        self.current_items.append({
                            'type': 'function',
                            'name': name,
                            'object': item,
                            'signature': params,
                            'description': short_doc,
                            'full_doc': doc
                        })
                        
                except Exception as error:
                    logging.debug(f"TUI: Skipping {name}: {error}")
                    continue
            
            # Sort: modules first, then functions alphabetically
            self.current_items.sort(key=lambda x: (0 if x['type'] == 'module' else 1, x['name']))
            
            if not self.current_items:
                self.current_items = [{'type': 'empty', 'name': '(empty)', 'description': 'No items found at this level'}]
            
            logging.info(f"TUI: Discovered {len(self.current_items)} items at {module_path}")
            
            if self.debug_mode:
                modules_count = sum(1 for item in self.current_items if item.get('type') == 'module')
                functions_count = sum(1 for item in self.current_items if item.get('type') == 'function')
                logging.debug(f"TUI_DEBUG: Discovery complete - {modules_count} modules, {functions_count} functions")
                if self.current_items:
                    item_names = [item.get('name', 'unknown') for item in self.current_items[:10]]
                    logging.debug(f"TUI_DEBUG: First items (max 10): {', '.join(item_names)}")
            
        except Exception as error:
            logging.error(f"TUI: Discovery error: {error}", exc_info=True)
            self.current_items = [{'type': 'error', 'name': 'Error', 'description': str(error)}]
    
    def check_keyboard_input(self):
        """Check for keyboard input in a cross-platform way.
        
        Returns:
            str or None: Key pressed, or None if no input
        """
        try:
            if self.IS_WINDOWS:
                # Windows: Use msvcrt for non-blocking keyboard check
                if self.msvcrt.kbhit():
                    key = self.msvcrt.getch()
                    if self.debug_mode:
                        logging.debug(f"TUI_DEBUG: Raw key byte received: {repr(key)}")
                    # Handle multi-byte sequences for arrow keys and page keys
                    if key == b'\xe0' or key == b'\x00':
                        key = self.msvcrt.getch()
                        if self.debug_mode:
                            logging.debug(f"TUI_DEBUG: Special key detected - second byte value: {repr(key)}")
                        if key == b'H':  # Up arrow
                            return 'up'
                        elif key == b'P':  # Down arrow
                            return 'down'
                        elif key == b'K':  # Left arrow
                            return 'left'
                        elif key == b'M':  # Right arrow
                            return 'right'
                        elif key == b'I':  # Page Up (0x49)
                            if self.debug_mode:
                                logging.debug("TUI_DEBUG: Page Up key detected")
                            return 'page_up'
                        elif key == b'Q':  # Page Down (0x51)
                            if self.debug_mode:
                                logging.debug("TUI_DEBUG: Page Down key detected")
                            return 'page_down'
                        elif key == b'G':  # Home
                            return 'h'
                        elif key == b'O':  # End
                            return 'e'
                        else:
                            # Log unhandled special keys for debugging
                            if self.debug_mode:
                                logging.debug(f"TUI_DEBUG: Unhandled special key: {repr(key)}")
                    decoded = key.decode('utf-8', errors='ignore').lower()
                    if self.debug_mode:
                        logging.debug(f"TUI_DEBUG: Decoded key: {repr(decoded)}")
                    return decoded
            else:
                # Unix/Linux: Use select for non-blocking check
                if self.select.select([sys.stdin], [], [], 0)[0]:
                    key = sys.stdin.read(1)
                    if self.debug_mode:
                        logging.debug(f"TUI_DEBUG: Unix - Raw key received: {repr(key)}")
                    # Handle escape sequences for arrow keys and special keys
                    if key == '\x1b':  # ESC
                        if self.debug_mode:
                            logging.debug("TUI_DEBUG: Unix - ESC character detected, checking for arrow sequence...")
                        
                        # CRITICAL FIX: Container/TTY environments have extreme latency.
                        # Arrow keys send 3-byte sequences: ESC [ {A|B|C|D}
                        # The ESC arrives first, then the remaining bytes with significant latency.
                        # Container SSH forwarding can introduce >200ms inter-byte delays.
                        import time
                        
                        # Progressive read strategy with multiple waits to handle variable latency
                        remaining_chars = ''
                        max_attempts = 4
                        wait_increment = 0.05  # 50ms per attempt = up to 200ms total
                        
                        for attempt in range(max_attempts):
                            # Check if data already available
                            if self.select.select([sys.stdin], [], [], 0)[0]:
                                # Read all currently buffered data
                                while self.select.select([sys.stdin], [], [], 0)[0]:
                                    char = sys.stdin.read(1)
                                    remaining_chars += char
                                    if self.debug_mode:
                                        esc_char = '\x1b'
                                        full_sequence = esc_char + remaining_chars
                                        logging.debug(f"TUI_DEBUG: Unix - Read char: {repr(char)}, sequence so far: {repr(full_sequence)}")
                                
                                # If we got a complete arrow sequence, stop waiting
                                if remaining_chars.startswith('[') and len(remaining_chars) >= 2:
                                    if remaining_chars[1] in 'ABCD':
                                        if self.debug_mode:
                                            logging.debug(f"TUI_DEBUG: Unix - Complete arrow sequence detected early (attempt {attempt+1})")
                                        break
                            
                            # If this isn't the last attempt and we haven't found a complete sequence, wait for more data
                            if attempt < max_attempts - 1:
                                time.sleep(wait_increment)
                                if self.debug_mode:
                                    logging.debug(f"TUI_DEBUG: Unix - Waiting for more bytes (attempt {attempt+1}/{max_attempts})")
                        
                        if self.debug_mode:
                            esc_char = '\x1b'
                            full_sequence = esc_char + remaining_chars
                            logging.debug(f"TUI_DEBUG: Unix - Complete escape sequence: {repr(full_sequence)}")
                        
                        # Parse the complete escape sequence
                        if remaining_chars.startswith('['):
                            arrow_code = remaining_chars[1:2] if len(remaining_chars) > 1 else ''
                            
                            if arrow_code == 'A':
                                if self.debug_mode:
                                    logging.debug("TUI_DEBUG: Unix - UP arrow detected")
                                return 'up'
                            elif arrow_code == 'B':
                                if self.debug_mode:
                                    logging.debug("TUI_DEBUG: Unix - DOWN arrow detected")
                                return 'down'
                            elif arrow_code == 'C':
                                if self.debug_mode:
                                    logging.debug("TUI_DEBUG: Unix - RIGHT arrow detected")
                                return 'right'
                            elif arrow_code == 'D':
                                if self.debug_mode:
                                    logging.debug("TUI_DEBUG: Unix - LEFT arrow detected")
                                return 'left'
                            elif arrow_code == '5' and len(remaining_chars) > 2 and remaining_chars[2] == '~':
                                if self.debug_mode:
                                    logging.debug("TUI_DEBUG: Page Up key detected (Unix)")
                                return 'page_up'
                            elif arrow_code == '6' and len(remaining_chars) > 2 and remaining_chars[2] == '~':
                                if self.debug_mode:
                                    logging.debug("TUI_DEBUG: Page Down key detected (Unix)")
                                return 'page_down'
                            elif arrow_code == 'H':  # Home
                                return 'h'
                            elif arrow_code == 'F':  # End
                                return 'e'
                            else:
                                if self.debug_mode:
                                    logging.debug(f"TUI_DEBUG: Unix - Unrecognized escape sequence: ESC[{arrow_code}")
                                return None
                        elif remaining_chars:
                            if self.debug_mode:
                                logging.debug(f"TUI_DEBUG: Unix - ESC followed by non-bracket sequence: {repr(remaining_chars)}")
                            return None
                        else:
                            if self.debug_mode:
                                logging.debug("TUI_DEBUG: Unix - Standalone ESCAPE key (no following characters)")
                        return 'escape'
                    return key.lower()
        except Exception as error:
            logging.debug(f"TUI_MODE: Keyboard input error - {error}")
        return None
    
    def create_layout(self):
        """Create the TUI layout with hierarchical API navigation.
        
        Shows:
        - Breadcrumb header (current location in mistapi)
        - Left: List of modules/functions at current level
        - Right: Details panel (function signature, docstring, or result)
        
        Returns:
            Panel: Rich Panel containing the complete hierarchical layout
        """
        if self.debug_mode:
            logging.debug(f"TUI_DEBUG: create_layout() called - execution_state={self.execution_state}, path={self.current_path}, selection={self.current_selection}")
        
        from rich.console import Group
        from rich.columns import Columns
        
        # Use fixed standard dimensions to prevent flickering
        # Standard terminal is typically 80x24, but we'll use comfortable modern size
        FIXED_PANEL_HEIGHT = 20  # Fixed height for stable rendering
        available_height = FIXED_PANEL_HEIGHT
        
        # Create breadcrumb display with btop-style header
        breadcrumb_text = f"[bold bright_cyan]{self.breadcrumb}[/bold bright_cyan]"
        if self.current_path:
            path_display = "  ".join(self.current_path)
            breadcrumb_text += f" [dim bright_black] {path_display}[/dim bright_black]"
        
        breadcrumb_panel = self.Panel(
            breadcrumb_text,
            style="bright_white on grey11",
            border_style="bright_cyan",
            box=self.box.ROUNDED
        )
        
        # Miller Columns layout: each hierarchy level gets its own vertical column
        # btop-inspired color scheme: cyan borders, green accents, orange highlights
        
        # Create current level items column with btop-style colors and scrolling viewport
        # Calculate maximum name length for dynamic column width
        max_name_length = max((len(item.get('name', '')) for item in self.current_items), default=10)
        
        # Get terminal width and calculate percentage-based width for left panel
        # Use 40% of terminal width to ensure full names display without truncation
        import shutil
        terminal_width, _ = shutil.get_terminal_size()
        percentage_width = int(terminal_width * 0.40)
        
        # Account for Rich table/panel overhead:
        # - Table padding: (0, 1) = 2 chars left+right
        # - Grid padding: 1 = 2 chars 
        # - Row content: prefix(1) + icon(2) + spaces(3) = 6 chars
        # Total overhead: ~10 chars
        content_width_needed = max_name_length + 10
        
        # Use percentage width, but ensure it fits content (min 35 chars for readability)
        column_width = max(35, min(content_width_needed, percentage_width))
        
        if self.debug_mode:
            logging.debug(f"TUI_DEBUG: Column sizing - terminal_width={terminal_width}, max_name={max_name_length}, "
                        f"percentage_width={percentage_width}, final_column_width={column_width}")
        
        items_table = self.Table(show_header=False, box=self.box.ROUNDED, padding=(0, 1))
        items_table.add_column("Item", style="white", width=column_width, no_wrap=False, overflow="ellipsis")
        
        # Calculate viewport for scrolling (show items around current selection)
        # Reserve 2 lines for panel borders, use remaining for items
        viewport_height = available_height - 2
        total_items = len(self.current_items)
        
        # Calculate scroll offset to keep selection visible
        # Try to keep selection in middle of viewport when possible
        if total_items <= viewport_height:
            # All items fit, no scrolling needed
            viewport_start = 0
            viewport_end = total_items
            if self.debug_mode:
                logging.debug(f"TUI_DEBUG: All items fit - showing {total_items} items (viewport height: {viewport_height})")
        else:
            # Need scrolling - center selection in viewport
            viewport_start = max(0, self.current_selection - viewport_height // 2)
            viewport_end = min(total_items, viewport_start + viewport_height)
            
            # Adjust if we're near the end
            if viewport_end == total_items:
                viewport_start = max(0, total_items - viewport_height)
            
            if self.debug_mode:
                logging.debug(f"TUI_DEBUG: Scrolling viewport - selection={self.current_selection}, total={total_items}, "
                            f"viewport=[{viewport_start}:{viewport_end}], visible_items={viewport_end - viewport_start}")
        
        # Render only visible items in viewport
        for idx in range(viewport_start, viewport_end):
            item = self.current_items[idx]
            item_type = item.get('type', 'unknown')
            item_name = item.get('name', 'unknown')
            
            # btop-inspired colors: cyan for modules, green for functions
            if item_type == 'module':
                icon = ""  # Right arrow for directories (like btop)
                color = "bright_cyan"
            elif item_type == 'function':
                icon = ""  # Bullet for items
                color = "bright_green"
            elif item_type == 'error':
                icon = ""
                color = "bright_red"
            else:
                icon = ""
                color = "dim"
            
            # Highlight selected item with orange/yellow (btop highlight style)
            if idx == self.current_selection:
                prefix = ""  # Solid block for selection
                color = "bright_yellow"
                style = "bold"
                
                if self.debug_mode:
                    logging.debug(f"TUI_DEBUG: Highlighting selection - index={idx}, name='{item_name}', "
                                f"type={item_type}, viewport_position={idx - viewport_start}")
            else:
                prefix = " "
                style = ""
            
            # Don't truncate - let the full name show
            display_name = item_name
            
            if style:
                items_table.add_row(f"[{style} {color}]{prefix} {icon} {display_name}[/{style} {color}]")
            else:
                items_table.add_row(f"[{color}]{prefix} {icon} {display_name}[/{color}]")
        
        # Add scroll indicators if needed
        if self.debug_mode and total_items > viewport_height:
            logging.debug(f"TUI_DEBUG: Scroll indicators - can_scroll_up={viewport_start > 0}, "
                        f"can_scroll_down={viewport_end < total_items}")
        
        # Current level column with btop-style cyan border
        level_name = self.current_path[-1] if self.current_path else "root"
        # Dynamic width based on content, plus padding for borders (4 chars)
        panel_width = column_width + 4
        items_panel = self.Panel(
            items_table,
            title=f"[bold bright_cyan]{level_name}[/bold bright_cyan]",
            border_style="bright_cyan",
            height=available_height,
            width=panel_width
        )
        
        # Create details panel (right panel)
        details_lines = []
        
        if 0 <= self.current_selection < len(self.current_items):
            selected = self.current_items[self.current_selection]
            item_type = selected.get('type')
            
            if item_type == 'function':
                # Show function signature and docstring
                func_name = selected.get('name', 'unknown')
                signature = selected.get('signature', '(...)')
                full_doc = selected.get('full_doc', 'No documentation available')
                
                details_lines.append(f"[bold bright_green]Function:[/bold bright_green] {func_name}")
                details_lines.append("")
                details_lines.append(f"[bold bright_cyan]Signature:[/bold bright_cyan]")
                details_lines.append(f"[bright_yellow]{func_name}{signature}[/bright_yellow]")
                details_lines.append("")
                details_lines.append(f"[bold]Documentation:[/bold]")
                
                # Limit doc display to fit available height
                doc_lines = full_doc.split('\n')
                max_doc_lines = max(5, available_height - 10)  # Reserve space for header/signature
                for line in doc_lines[:max_doc_lines]:
                    details_lines.append(line)
                if len(doc_lines) > max_doc_lines:
                    details_lines.append(f"[dim]...(truncated, {len(doc_lines) - max_doc_lines} more lines)[/dim]")
                
            elif item_type == 'module':
                # Show module info
                module_name = selected.get('name', 'unknown')
                details_lines.append(f"[bold bright_cyan]Module:[/bold bright_cyan] {module_name}")
                details_lines.append("")
                details_lines.append("[dim bright_black]Press Enter to explore this module[/dim bright_black]")
            
            elif item_type == 'error':
                details_lines.append(f"[bold red]Error:[/bold red]")
                details_lines.append(selected.get('description', 'Unknown error'))
        
        # Show last result if available (limit to prevent overflow)
        if self.last_result is not None:
            details_lines.append("")
            details_lines.append("[bold green]Last Result:[/bold green]")
            result_preview = str(self.last_result)
            max_result_chars = 300
            if len(result_preview) > max_result_chars:
                result_preview = result_preview[:max_result_chars] + "..."
            # Limit result lines too
            result_lines = result_preview.split('\n')
            max_result_lines = 10
            for line in result_lines[:max_result_lines]:
                details_lines.append(f"[dim]{line}[/dim]")
            if len(result_lines) > max_result_lines:
                details_lines.append(f"[dim]...({len(result_lines) - max_result_lines} more lines)[/dim]")
        
        if self.last_error:
            details_lines.append("")
            details_lines.append("[bold red]Last Error:[/bold red]")
            details_lines.append(f"[dim]{self.last_error}[/dim]")
        
        if not details_lines:
            details_lines.append("[dim]Select an item to view details[/dim]")
        
        # Limit total details lines to fit in panel
        max_total_lines = available_height - 2  # Account for panel borders
        if len(details_lines) > max_total_lines:
            details_lines = details_lines[:max_total_lines]
            details_lines.append("[dim]...(content truncated to fit screen)[/dim]")
        
        details_panel = self.Panel(
            "\n".join(details_lines),
            title="[bold bright_green]Details[/bold bright_green]",
            border_style="bright_green",
            box=self.box.ROUNDED,
            height=available_height
        )
        
        # Create output panel at bottom for execution results and input prompts
        output_height = 8
        output_content = []
        
        if self.execution_state == 'prompting':
            # Show parameter input prompt with clear header
            output_content.append("[bold bright_cyan] Function Execution - Parameter Input [/bold bright_cyan]")
            output_content.append("")
            if self.current_function:
                output_content.append(f"[bright_green]Function:[/bright_green] {self.current_function.get('name', 'unknown')}")
            
            # Show current parameter being requested with prominent display
            if self.current_param_index < len(self.param_list):
                output_content.append("")
                param_info = self.param_list[self.current_param_index]
                param_name = param_info['name']
                required_tag = "[red][REQUIRED][/red]" if not param_info.get('has_default') else "[dim][optional][/dim]"
                default_info = f" [dim](default: {param_info.get('default')})[/dim]" if param_info.get('has_default') else ""
                
                # Create a clear input prompt box
                output_content.append(f"[bold bright_yellow] Input Needed: {param_name} {required_tag}[/bold bright_yellow]")
                if default_info:
                    output_content.append(f"[bright_yellow][/bright_yellow] {default_info}")
                output_content.append(f"[bright_yellow]>[/bright_yellow] [bold white on grey11]{self.input_buffer}[/bold white on grey11]")
            
            # Show previously collected parameters at bottom
            if self.current_param_index > 0:
                output_content.append("")
                output_content.append(f"[dim]Already provided ({self.current_param_index}/{len(self.param_list)}):[/dim]")
                for idx in range(min(3, self.current_param_index)):  # Show last 3
                    param_info = self.param_list[self.current_param_index - 1 - idx]
                    param_name = param_info['name']
                    param_value = self.function_params.get(param_name, '')
                    # Redact sensitive values
                    if any(x in param_name.lower() for x in ['pass', 'token', 'key', 'secret']):
                        display_value = "***REDACTED***"
                    else:
                        display_value = str(param_value)[:40]  # Truncate long values
                    output_content.append(f"  [dim] {param_name}:[/dim] {display_value}")
        
        elif self.execution_state == 'executing':
            output_content.append("[bold bright_cyan]Executing API Call...[/bold bright_cyan]")
            output_content.append("")
            output_content.append("[dim]Please wait...[/dim]")
        
        elif self.output_lines:
            # Show output from last execution
            for line in self.output_lines[-output_height+2:]:  # Show last N lines that fit
                output_content.append(line)
        else:
            output_content.append("[dim]Output will appear here after executing functions[/dim]")
        
        output_panel = self.Panel(
            "\n".join(output_content) if output_content else "[dim]No output[/dim]",
            title="[bold bright_magenta]Output[/bold bright_magenta]",
            border_style="bright_magenta",
            box=self.box.ROUNDED,
            height=output_height
        )
        
        # Create help text with btop-style colors
        if self.execution_state == 'viewing_results':
            help_text = (
                "[bold bright_yellow]Results View:[/bold bright_yellow] "
                "[bright_cyan][/bright_cyan] Scroll (10)  "
                "[bright_cyan]PgUp/PgDn[/bright_cyan] Scroll (20)  "
                "[bright_green]H[/bright_green] Top  "
                "[bright_green]E[/bright_green] End  "
                "[bright_magenta]Esc[/bright_magenta] Close  "
                "[bright_red]Q[/bright_red] Quit"
            )
        elif self.execution_state == 'prompting':
            help_text = (
                "[bold bright_yellow]Input Mode:[/bold bright_yellow] "
                "[bright_green]Type[/bright_green] value  "
                "[bright_cyan]Enter[/bright_cyan] Submit  "
                "[bright_magenta]Esc[/bright_magenta] Cancel"
            )
        else:
            help_text = (
                "[bold bright_yellow]Navigation:[/bold bright_yellow] "
                "[bright_cyan][/bright_cyan] Move  "
                "[bright_green]Enter[/bright_green] Drill/Execute  "
                "[bright_magenta]Esc[/bright_magenta] Back  "
                "[bright_red]Q[/bright_red] Quit"
            )
        
        # Combine into side-by-side layout using Table.grid for reliable column rendering
        from rich.table import Table as RichTable
        layout_table = RichTable.grid(padding=1, expand=True)
        # Use dynamic panel width (already includes border padding)
        layout_table.add_column(width=panel_width, no_wrap=True)
        layout_table.add_column(ratio=1)  # Details column (fills remaining space)
        layout_table.add_row(items_panel, details_panel)
        
        # Group all elements
        content_group = Group(breadcrumb_panel, "", layout_table, "", output_panel, "", help_text)
        
        main_panel = self.Panel(
            content_group,
            title="[bold bright_cyan]MistHelper TUI[/bold bright_cyan]",
            border_style="bright_blue",
            box=self.box.ROUNDED
        )
        
        # If viewing results, show the results grid instead of main panel
        if self.execution_state == 'viewing_results':
            results_grid = self._create_results_grid()
            if results_grid:
                # Create a Layout to properly position help text at the bottom
                # Use minimum_size=0 to prevent width constraints
                results_layout = self.Layout(minimum_size=0)
                results_layout.split_column(
                    self.Layout(results_grid, name="results", ratio=95, minimum_size=0),
                    self.Layout(
                        self.Panel(
                            "[yellow]Controls: [bright_yellow][/bright_yellow] Results | [bright_yellow][/bright_yellow] Scroll (10) | [bright_yellow]PgUp/PgDn[/bright_yellow] Scroll (20) | [bright_yellow]H[/bright_yellow] Top | [bright_yellow]E[/bright_yellow] End | [bright_yellow]ESC[/bright_yellow] Close | [bright_yellow]Q[/bright_yellow] Quit[/yellow]",
                            border_style="dim",
                            box=self.box.SIMPLE
                        ),
                        name="footer",
                        size=3  # Fixed height for footer (1 line text + 2 lines borders)
                    )
                )
                if self.debug_mode:
                    logging.debug("TUI_DEBUG: create_layout() returning results grid with footer layout")
                return results_layout
        
        if self.debug_mode:
            logging.debug("TUI_DEBUG: create_layout() completed successfully - returning main_panel")
        
        return main_panel
    
    def handle_input(self, key):
        """Handle keyboard input for Miller Columns navigation and input prompts.
        
        Controls (Navigation Mode):
        - Up/Down: Navigate through items in current column
        - Enter: Drill into module (shows new column to right) or execute function
        - Escape: Go back up one level (removes rightmost column)
        - Q: Quit TUI mode
        
        Controls (Input Mode):
        - Type: Add characters to input buffer
        - Backspace: Remove last character
        - Enter: Submit current parameter value
        - Escape: Cancel execution
        
        Args:
            key (str): Key pressed by user
        """
        if self.debug_mode:
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]
            logging.debug(f"TUI_DEBUG: [{timestamp}] Key pressed: {repr(key)} (state={self.execution_state}, path={self.current_path}, selection={self.current_selection})")
        
        # Handle results viewing mode
        if self.execution_state == 'viewing_results':
            if self.debug_mode:
                logging.debug(f"TUI_DEBUG: In viewing_results mode - testing key {repr(key)} against handlers")
            
            if key == 'left':
                # Previous result
                self.results_scroll_offset = max(0, self.results_scroll_offset - 1)
                self.result_row_scroll = 0  # Reset row scroll for new result
                if self.debug_mode:
                    logging.debug(f"TUI_DEBUG: Previous result - offset now {self.results_scroll_offset}")
            elif key == 'right':
                # Next result
                if self.last_parsed_data:
                    results = self.last_parsed_data.get('results', [])
                    max_offset = max(0, len(results) - 1)
                    self.results_scroll_offset = min(max_offset, self.results_scroll_offset + 1)
                    self.result_row_scroll = 0  # Reset row scroll for new result
                    if self.debug_mode:
                        logging.debug(f"TUI_DEBUG: Next result - offset now {self.results_scroll_offset} (max {max_offset})")
            elif key == 'up':
                # Scroll up within current result (show earlier rows) - 10 rows at a time for faster navigation
                old_scroll = self.result_row_scroll
                self.result_row_scroll = max(0, self.result_row_scroll - 10)
                if self.debug_mode:
                    logging.debug(f"TUI_DEBUG: UP key - row scroll {old_scroll} -> {self.result_row_scroll}")
            elif key == 'down':
                # Scroll down within current result (show later rows) - 10 rows at a time for faster navigation
                # Don't cap here - let _create_results_grid() handle bounds checking
                old_scroll = self.result_row_scroll
                self.result_row_scroll += 10
                if self.debug_mode:
                    logging.debug(f"TUI_DEBUG: DOWN key - row scroll {old_scroll} -> {self.result_row_scroll}")
            elif key == 'page_up':
                # Page up - scroll up 20 rows at a time (2x faster than arrow keys)
                self.result_row_scroll = max(0, self.result_row_scroll - 20)
                if self.debug_mode:
                    logging.debug(f"TUI_DEBUG: Page up - row offset now {self.result_row_scroll}")
            elif key == 'page_down':
                # Page down - scroll down 20 rows at a time (2x faster than arrow keys)
                self.result_row_scroll += 20
                if self.debug_mode:
                    logging.debug(f"TUI_DEBUG: Page down - row offset now {self.result_row_scroll}")
            elif key == 'h':
                # Jump to top (Home)
                self.result_row_scroll = 0
                if self.debug_mode:
                    logging.debug(f"TUI_DEBUG: Jump to top - row offset now {self.result_row_scroll}")
            elif key == 'e':
                # Jump to end
                if self.last_parsed_data:
                    results = self.last_parsed_data.get('results', [])
                    if results and self.results_scroll_offset < len(results):
                        # Set to large number, windowing logic will cap it properly
                        self.result_row_scroll = 999999
                        if self.debug_mode:
                            logging.debug(f"TUI_DEBUG: Jump to end - row offset set to max")
            elif key in ['escape', '\x1b']:
                # Close results grid
                self.execution_state = None
                self.results_scroll_offset = 0
                self.result_row_scroll = 0
                if self.debug_mode:
                    logging.debug("TUI_DEBUG: Closed results grid, returned to navigation")
            elif key == 'q':
                # Quit from results view
                self.running = False
                if self.debug_mode:
                    logging.debug("TUI_DEBUG: Q pressed in results view - quitting")
            else:
                # Unhandled key in results view
                if self.debug_mode:
                    logging.debug(f"TUI_DEBUG: Unhandled key in viewing_results mode: {repr(key)}")
            return  # Don't process navigation commands while viewing results
        
        # Handle input mode for parameter collection
        if self.execution_state == 'prompting':
            if key in ['\r', '\n']:  # Enter - submit parameter
                self._submit_parameter()
            elif key in ['escape', '\x1b']:  # Escape - cancel execution
                self._cancel_execution()
            elif key == '\x7f' or key == '\x08' or key == 'backspace':  # Backspace
                if self.input_buffer:
                    self.input_buffer = self.input_buffer[:-1]
            elif len(key) == 1 and key.isprintable():  # Regular character
                self.input_buffer += key
            return  # Don't process navigation commands in input mode
        
        # Navigation mode
        if key == 'up':
            if self.debug_mode:
                logging.debug(f"TUI_DEBUG: Processing UP arrow key")
            # Move selection up in current column
            old_selection = self.current_selection
            self.current_selection = max(0, self.current_selection - 1)
            
            if self.debug_mode:
                item_name = self.current_items[self.current_selection].get('name', 'unknown') if self.current_items else 'none'
                logging.debug(f"TUI_DEBUG: UP arrow - selection moved {old_selection} -> {self.current_selection} (now on: {item_name})")
                logging.debug(f"TUI_DEBUG: UP arrow processing complete")
            
        elif key == 'down':
            if self.debug_mode:
                logging.debug(f"TUI_DEBUG: Processing DOWN arrow key")
            # Move selection down in current column
            old_selection = self.current_selection
            self.current_selection = min(len(self.current_items) - 1, self.current_selection + 1)
            
            if self.debug_mode:
                item_name = self.current_items[self.current_selection].get('name', 'unknown') if self.current_items else 'none'
                logging.debug(f"TUI_DEBUG: DOWN arrow - selection moved {old_selection} -> {self.current_selection} (now on: {item_name})")
                logging.debug(f"TUI_DEBUG: DOWN arrow processing complete")
            
        elif key in ['\r', '\n']:  # Enter key
            if self.debug_mode:
                logging.debug(f"TUI_DEBUG: Processing ENTER key")
            # Select current item
            if 0 <= self.current_selection < len(self.current_items):
                selected = self.current_items[self.current_selection]
                item_type = selected.get('type')
                item_name = selected.get('name')
                
                if self.debug_mode:
                    logging.debug(f"TUI_DEBUG: ENTER pressed - selected item: {item_name} (type={item_type})")
                
                if item_type == 'module':
                    # Drill into module
                    module_name = selected.get('name')
                    self.current_path.append(module_name)
                    self.current_selection = 0
                    self._discover_current_level()
                    logging.info(f"TUI: Navigated into module: {module_name}")
                    
                    if self.debug_mode:
                        logging.debug(f"TUI_DEBUG: Module drill-down complete - new path: {self.current_path}, items count: {len(self.current_items)}")
                    
                elif item_type == 'function':
                    # Start function execution (parameter prompting)
                    if self.debug_mode:
                        logging.debug(f"TUI_DEBUG: Starting function execution: {item_name}")
                    self._start_function_execution(selected)
                    
                if self.debug_mode:
                    logging.debug(f"TUI_DEBUG: ENTER key processing complete")
                    
        elif key in ['escape', '\x1b']:
            if self.debug_mode:
                logging.debug(f"TUI_DEBUG: Processing ESCAPE key - current path: {self.current_path}")
            # Go back up one level
            if self.current_path:
                removed = self.current_path.pop()
                self.current_selection = 0
                self._discover_current_level()
                logging.info(f"TUI: Navigated back from: {removed}")
                
                if self.debug_mode:
                    logging.debug(f"TUI_DEBUG: ESCAPE pressed - backed out of {removed}, new path: {self.current_path}, items count: {len(self.current_items)}")
            else:
                # Already at root, escape quits
                if self.debug_mode:
                    logging.debug("TUI_DEBUG: ESCAPE pressed at root level - quitting TUI")
                self.running = False
            
            if self.debug_mode:
                logging.debug(f"TUI_DEBUG: ESCAPE key processing complete")
                
        elif key == 'q':
            if self.debug_mode:
                logging.debug("TUI_DEBUG: Processing Q key - initiating quit")
            # Quit TUI mode
            self.running = False
            if self.debug_mode:
                logging.debug("TUI_DEBUG: Q key processing complete - running flag set to False")
        
        else:
            # Unknown/unhandled key
            if self.debug_mode:
                logging.debug(f"TUI_DEBUG: Unhandled key: {repr(key)}")
    
    def _start_function_execution(self, selected_item):
        """Start function execution by preparing parameter collection."""
        import inspect
        
        self.current_function = selected_item
        func = selected_item.get('object')
        func_name = selected_item.get('name')
        
        if not func or not callable(func):
            self.output_lines = ["[ERROR] Selected item is not callable"]
            return
        
        if self.debug_mode:
            logging.debug(f"TUI_DEBUG: Preparing parameter collection for: {func_name}")
        
        # Get function signature
        try:
            sig = inspect.signature(func)
            self.param_list = []
            self.function_params = {}
            
            # Build list of parameters to collect
            for param_name, param in sig.parameters.items():
                if param_name == 'self':
                    continue
                
                # Auto-fill mist_session
                if param_name in ('mist_session', 'apisession'):
                    if hasattr(self, 'apisession') and self.apisession is not None:
                        self.function_params[param_name] = self.apisession
                        continue
                    else:
                        self.output_lines = ["[ERROR] API session not available"]
                        return
                
                # Auto-fill from .env file (not system environment)
                if param_name in self.dotenv_values:
                    self.function_params[param_name] = self.dotenv_values[param_name]
                    if self.debug_mode:
                        logging.debug(f"TUI_DEBUG: Auto-filled {param_name} from .env file: {self.dotenv_values[param_name]}")
                    continue
                
                # Add to collection list
                has_default = param.default != inspect.Parameter.empty
                self.param_list.append({
                    'name': param_name,
                    'has_default': has_default,
                    'default': param.default if has_default else None
                })
            
            # Start prompting for parameters
            if self.param_list:
                self.execution_state = 'prompting'
                self.current_param_index = 0
                self.input_buffer = ""
                if self.debug_mode:
                    logging.debug(f"TUI_DEBUG: Starting parameter prompting - {len(self.param_list)} parameters to collect")
            else:
                # No parameters needed, execute immediately
                self._execute_function()
        
        except Exception as error:
            self.output_lines = [f"[ERROR] Failed to prepare execution: {error}"]
            logging.error(f"TUI: Failed to prepare execution of {func_name}: {error}", exc_info=True)
    
    def _submit_parameter(self):
        """Submit the current parameter value and move to next parameter or execute.
        
        Rules:
        - Required parameters: Must have a value, error if empty
        - Optional parameters with value: Add to function params
        - Optional parameters without value (blank): Skip entirely, don't add to params
        """
        if self.current_param_index >= len(self.param_list):
            return
        
        param_info = self.param_list[self.current_param_index]
        param_name = param_info['name']
        value = self.input_buffer.strip()
        
        # Handle parameter based on required/optional and value presence
        if not value:
            # Empty input
            if not param_info['has_default']:
                # Required parameter missing
                self.output_lines = [f"[ERROR] {param_name} is required"]
                return
            else:
                # Special handling for 'limit' parameter - default to 1000
                if param_name == 'limit':
                    self.function_params[param_name] = 1000
                    if self.debug_mode:
                        logging.debug(f"TUI_DEBUG: Parameter set to default 1000 - {param_name}")
                else:
                    # Optional parameter with no value - explicitly set to None to override function defaults
                    self.function_params[param_name] = None
                    if self.debug_mode:
                        logging.debug(f"TUI_DEBUG: Parameter set to None (optional, no value provided) - {param_name}")
        else:
            # Value provided - store parameter (convert to int if it's the limit parameter)
            if param_name == 'limit':
                try:
                    self.function_params[param_name] = int(value)
                    if self.debug_mode:
                        logging.debug(f"TUI_DEBUG: Parameter stored as int - {param_name}: {value}")
                except ValueError:
                    self.output_lines = [f"[ERROR] {param_name} must be a number"]
                    return
            else:
                self.function_params[param_name] = value
                if self.debug_mode:
                    display_value = "***REDACTED***" if any(x in param_name.lower() for x in ['pass', 'token', 'key', 'secret']) else value
                    logging.debug(f"TUI_DEBUG: Parameter stored - {param_name}: {display_value}")
        
        # Move to next parameter
        self.current_param_index += 1
        self.input_buffer = ""
        
        # Check if all parameters collected
        if self.current_param_index >= len(self.param_list):
            self._execute_function()
    
    def _cancel_execution(self):
        """Cancel the current function execution."""
        if self.debug_mode:
            logging.debug("TUI_DEBUG: Function execution cancelled by user")
        
        self.execution_state = None
        self.current_function = None
        self.function_params = {}
        self.param_list = []
        self.current_param_index = 0
        self.input_buffer = ""
        self.output_lines = ["[CANCELLED] Execution cancelled by user"]
    
    def _execute_function(self):
        """Execute the function with collected parameters and handle pagination."""
        if not self.current_function:
            return
        
        func = self.current_function.get('object')
        func_name = self.current_function.get('name')
        
        self.execution_state = 'executing'
        self.output_lines = ["[EXECUTING] Running API call..."]
        
        if self.debug_mode:
            param_summary = {k: "***REDACTED***" if any(x in k.lower() for x in ['pass', 'token', 'key', 'secret']) else v 
                           for k, v in self.function_params.items()}
            logging.debug(f"TUI_DEBUG: Executing {func_name} with parameters: {param_summary}")
        
        try:
            # Execute the API call
            result = func(**self.function_params)
            
            # Extract actual data from APIResponse if applicable
            parsed_data = self._parse_api_response(result)
            
            # Handle pagination using mistapi's next() method (cursor-based pagination)
            if isinstance(parsed_data, dict) and 'results' in parsed_data:
                accumulated_results = list(parsed_data.get('results', []))
                page_count = 1
                
                # Check if we should paginate (next URL exists means more results available)
                while (hasattr(result, 'next') and 
                       result.next is not None):
                    
                    page_count += 1
                    self.output_lines = [f"[EXECUTING] Fetching page {page_count} (total results so far: {len(accumulated_results)})..."]
                    
                    if self.debug_mode:
                        logging.debug(f"TUI_DEBUG: Following next URL for pagination - page {page_count}, next: {result.next}")
                    
                    try:
                        # Get the next page using the mist_session and the next URL
                        # result.next is a string URL, not a method
                        next_url = result.next
                        
                        # Use the mist_session from parameters to make the next request
                        mist_session = self.function_params.get('mist_session') or self.function_params.get('apisession')
                        if mist_session and next_url:
                            # Make a GET request to the next URL
                            result = mist_session.mist_get(next_url)
                        else:
                            if self.debug_mode:
                                logging.debug(f"TUI_DEBUG: Missing mist_session or next_url, stopping pagination")
                            break
                        
                        if result:
                            parsed_data = self._parse_api_response(result)
                            
                            if isinstance(parsed_data, dict) and 'results' in parsed_data:
                                new_results = parsed_data.get('results', [])
                                if new_results:
                                    accumulated_results.extend(new_results)
                                    
                                    if self.debug_mode:
                                        logging.debug(f"TUI_DEBUG: Page {page_count} retrieved - added {len(new_results)} results (total: {len(accumulated_results)})")
                                else:
                                    # No more results
                                    if self.debug_mode:
                                        logging.debug(f"TUI_DEBUG: Page {page_count} returned no results, stopping pagination")
                                    break
                            else:
                                if self.debug_mode:
                                    logging.debug(f"TUI_DEBUG: Page {page_count} response format unexpected, stopping pagination")
                                break
                        else:
                            if self.debug_mode:
                                logging.debug(f"TUI_DEBUG: next() returned None, stopping pagination")
                            break
                    except Exception as e:
                        if self.debug_mode:
                            logging.debug(f"TUI_DEBUG: Error during pagination: {e}, stopping")
                        break
                
                # Update parsed_data with all accumulated results
                if page_count > 1:
                    parsed_data['results'] = accumulated_results
                    if self.debug_mode:
                        logging.debug(f"TUI_DEBUG: Pagination complete - {page_count} pages, {len(accumulated_results)} total results")
                elif hasattr(result, 'next') and result.next is not None:
                    if self.debug_mode:
                        logging.debug(f"TUI_DEBUG: Single page retrieved but next URL exists - may need different pagination approach")
            
            # Save result to file if debug mode is enabled
            if self.debug_mode:
                self._save_debug_result(func_name, result, parsed_data)
            
            # Store and format result
            self.last_result = result
            self.last_parsed_data = parsed_data
            self.output_lines = self._format_result_output(parsed_data, func_name, result)
            
            # Check if results should be displayed in popup grid
            if self._should_show_results_grid(parsed_data):
                self.execution_state = 'viewing_results'
                self.results_scroll_offset = 0
                logging.info(f"TUI: Results grid available - entering viewing_results state with {len(parsed_data.get('results', []))} items")
                if self.debug_mode:
                    logging.debug(f"TUI_DEBUG: Results available for grid view - entering viewing_results state")
            else:
                logging.info(f"TUI: Execution complete - no grid display (data type: {type(parsed_data).__name__})")
            
            logging.info(f"TUI: Successfully executed {func_name}")
            
        except Exception as error:
            self.last_error = str(error)
            self.output_lines = [
                f"[ERROR] Execution failed",
                f"Function: {func_name}",
                f"Error: {error}"
            ]
            logging.error(f"TUI: Execution of {func_name} failed - {error}", exc_info=True)
            
            if self.debug_mode:
                logging.debug(f"TUI_DEBUG: Exception details: {type(error).__name__}: {error}")
        
        finally:
            # Reset execution state (but preserve 'viewing_results' if set)
            if self.execution_state != 'viewing_results':
                self.execution_state = None
            
            # Always reset these
            self.current_function = None
            self.function_params = {}
            self.param_list = []
            self.current_param_index = 0
            self.input_buffer = ""
    
    def _parse_api_response(self, result):
        """Parse APIResponse object to extract actual data."""
        # Check if this is a mistapi APIResponse object
        if hasattr(result, 'data'):
            if self.debug_mode:
                logging.debug(f"TUI_DEBUG: Detected APIResponse object, extracting data attribute")
            return result.data
        else:
            return result
    
    def _should_show_results_grid(self, parsed_data):
        """Determine if parsed data should be shown in a grid popup.
        
        Returns True if:
        - Data is a dict with 'results' key
        - Results is a non-empty list
        - Results contain dict items (tabular data)
        """
        if self.debug_mode:
            logging.debug(f"TUI_DEBUG: Checking if results should show in grid - data type: {type(parsed_data).__name__}")
        
        if not isinstance(parsed_data, dict):
            if self.debug_mode:
                logging.debug("TUI_DEBUG: Not a dict - skipping grid display")
            return False
        
        results = parsed_data.get('results')
        if self.debug_mode:
            logging.debug(f"TUI_DEBUG: Results type: {type(results).__name__}, length: {len(results) if isinstance(results, list) else 'N/A'}")
        
        if not isinstance(results, list) or len(results) == 0:
            if self.debug_mode:
                logging.debug("TUI_DEBUG: Results empty or not a list - skipping grid display")
            return False
        
        # Check if results contain dict items (tabular data)
        if len(results) > 0 and isinstance(results[0], dict):
            if self.debug_mode:
                logging.debug(f"TUI_DEBUG: Results contain {len(results)} dict items - WILL SHOW GRID")
            return True
        
        if self.debug_mode:
            logging.debug("TUI_DEBUG: Results items are not dicts - skipping grid display")
        return False
    
    def _create_results_grid(self):
        """Create a Rich display showing results as individual cards/panels.
        
        Shows one result at a time with prev/next navigation.
        Returns a Panel containing the current result card.
        """
        if not self.last_parsed_data or not isinstance(self.last_parsed_data, dict):
            return None
        
        results = self.last_parsed_data.get('results', [])
        if not results:
            return None
        
        from rich.table import Table as RichTable
        from rich.columns import Columns
        
        # Calculate which result to show based on scroll offset
        current_result_idx = self.results_scroll_offset
        if current_result_idx >= len(results):
            current_result_idx = len(results) - 1
            self.results_scroll_offset = current_result_idx
        
        result = results[current_result_idx]
        
        # Format single result as a readable table
        def format_value(value, depth=0):
            """Format a value with proper indentation and styling."""
            if value is None or value == '':
                return "[dim]<empty>[/dim]"
            elif isinstance(value, bool):
                return f"[bright_cyan]{str(value)}[/bright_cyan]"
            elif isinstance(value, (int, float)):
                return f"[bright_green]{value}[/bright_green]"
            elif isinstance(value, str):
                # Highlight UUIDs and IPs
                if len(value) == 36 and '-' in value:  # UUID format
                    return f"[bright_magenta]{value}[/bright_magenta]"
                elif '.' in value and all(part.isdigit() or part == '' for part in value.split('.')):  # IP-like
                    return f"[bright_cyan]{value}[/bright_cyan]"
                else:
                    return f"[white]{value}[/white]"
            elif isinstance(value, list):
                if not value:
                    return "[dim]<empty list>[/dim]"
                elif all(isinstance(item, (str, int, float, bool, type(None))) for item in value):
                    # Simple list - always show all items inline, no limit
                    return f"[bright_yellow][ {', '.join(str(v) if v is not None else '<empty>' for v in value)} ][/bright_yellow]"
                else:
                    # Complex list (contains dicts/lists) - will be expanded by flatten_for_display
                    return f"[yellow] {len(value)} items (expanded below)[/yellow]"
            elif isinstance(value, dict):
                return f"[magenta] {len(value)} keys (expanded below)[/magenta]"
            else:
                return f"[white]{str(value)}[/white]"
        
        def flatten_for_display(data, depth=0):
            """Flatten dict/list for display with proper grouping and visual hierarchy."""
            rows = []
            
            if isinstance(data, dict):
                for key, value in data.items():
                    if isinstance(value, dict) and value:
                        indent = "  " * depth
                        if depth == 0:
                            key_style = f"[bold bright_cyan on grey15]{indent} {key.upper()}[/bold bright_cyan on grey15]"
                        elif depth == 1:
                            key_style = f"[bold bright_yellow]{indent} {key}[/bold bright_yellow]"
                        else:
                            key_style = f"[bold white]{indent} {key}[/bold white]"
                        rows.append([key_style, f"[dim italic]{len(value)} fields[/dim italic]", "section_header"])
                        rows.extend(flatten_for_display(value, depth + 1))
                        if depth == 0:
                            rows.append(["", "", "separator"])
                    elif isinstance(value, list) and value and isinstance(value[0], dict):
                        indent = "  " * depth
                        if depth == 0:
                            key_style = f"[bold bright_cyan on grey15]{indent} {key.upper()}[/bold bright_cyan on grey15]"
                        elif depth == 1:
                            key_style = f"[bold bright_yellow]{indent} {key}[/bold bright_yellow]"
                        else:
                            key_style = f"[bold white]{indent} {key}[/bold white]"
                        rows.append([key_style, f"[dim italic]{len(value)} items[/dim italic]", "section_header"])
                        # Show ALL items, fully expanded
                        for idx, item in enumerate(value):
                            sub_indent = "  " * (depth + 1)
                            rows.append([f"[magenta]{sub_indent}[{idx}][/magenta]", "", "list_item"])
                            rows.extend(flatten_for_display(item, depth + 2))
                        if depth == 0:
                            rows.append(["", "", "separator"])
                    else:
                        indent = "  " * depth
                        if depth == 0:
                            key_style = f"[bold bright_white]{indent}{key}[/bold bright_white]"
                        elif depth == 1:
                            key_style = f"[yellow]{indent}  {key}[/yellow]"
                        else:
                            key_style = f"[dim white]{indent}    {key}[/dim white]"
                        rows.append([key_style, format_value(value, 0), "value"])
            return rows
        
        # Create table with visual gridlines for sections
        table = RichTable(
            show_header=True,
            header_style="bold bright_cyan on grey15",
            box=self.box.HEAVY,  # Heavy box for better visual separation
            expand=True,  # Allow table to fill available space
            show_lines=True,  # Show lines between rows for grouping
            padding=(0, 1),
            row_styles=["", "on grey3"],  # Alternate row backgrounds
            width=None  # Let it auto-size to fill panel
        )
        
        # Two columns: Field and Value with ratio-based widths that respect expand=True
        # Field gets 35%, Value gets 65% of available width
        table.add_column("Field", style="bright_white", ratio=35, no_wrap=False, overflow="fold")
        table.add_column("Value", style="bright_white", ratio=65, no_wrap=False, overflow="fold")
        
        # Add rows for this result with visual grouping and scrolling
        all_rows = flatten_for_display(result)
        total_rows = len(all_rows)
        
        # Calculate visible window based on actual terminal height
        # Cap at 25 rows max for better scrolling UX even on large terminals
        max_visible = min(25, self._get_terminal_height())
        start_row = min(self.result_row_scroll, max(0, total_rows - max_visible))
        end_row = min(start_row + max_visible, total_rows)
        
        if self.debug_mode:
            logging.debug(f"TUI_DEBUG: Grid windowing - total_rows={total_rows}, max_visible={max_visible}, "
                         f"scroll_offset={self.result_row_scroll}, start_row={start_row}, end_row={end_row}, "
                         f"will_show={end_row - start_row}_rows")
        
        # Add only visible rows to table
        for field, value, row_type in all_rows[start_row:end_row]:
            if row_type == "separator":
                # Visual separator between major sections
                table.add_row("[dim]" + "" * 40 + "[/dim]", "[dim]" + "" * 60 + "[/dim]")
            else:
                table.add_row(field, value)
        
        # Get metadata
        total = self.last_parsed_data.get('total', len(results))
        actual_limit = self.function_params.get('limit', 1000)  # Default to 1000 if not set
        distinct = self.last_parsed_data.get('distinct', 'N/A')
        
        # Navigation info with row scroll indicator
        nav_info = f"Result {current_result_idx + 1} of {len(results)}"
        result_pct = int(((current_result_idx + 1) / len(results)) * 100) if len(results) > 0 else 100
        
        # Row scroll info with scroll indicators
        if total_rows > max_visible:
            can_scroll_up = start_row > 0
            can_scroll_down = end_row < total_rows
            scroll_indicator = ""
            if can_scroll_up and can_scroll_down:
                scroll_indicator = " [bright_yellow][/bright_yellow]"
            elif can_scroll_up:
                scroll_indicator = " [bright_yellow][/bright_yellow]"
            elif can_scroll_down:
                scroll_indicator = " [bright_yellow][/bright_yellow]"
            row_info = f" | Rows {start_row + 1}-{end_row} of {total_rows}{scroll_indicator}"
        else:
            row_info = f" | All {total_rows} rows visible"
        
        title = (f"[bold bright_yellow]{nav_info}[/bold bright_yellow] "
                f"| Total: {total} | Limit: {actual_limit} | Distinct: {distinct} "
                f"{row_info} | {result_pct}%")
        
        # Wrap in panel - it will auto-size based on table content
        # The max_visible calculation already accounts for the panel borders and help text
        panel = self.Panel(
            table,
            title=title,
            border_style="bright_yellow",
            box=self.box.DOUBLE,
            expand=True
        )
        
        return panel
    
    def _save_debug_result(self, func_name, raw_result, parsed_data):
        """Save API result to file when debug mode is enabled.
        
        Saves the complete raw result as JSON without any parsing or transformation,
        preserving all attributes and structure from the mistapi response.
        """
        import json
        from datetime import datetime
        
        try:
            # Create debug output directory
            debug_dir = os.path.join("data", "tui_debug_results")
            os.makedirs(debug_dir, exist_ok=True)
            
            # Create timestamped filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{func_name}_{timestamp}.json"
            filepath = os.path.join(debug_dir, filename)
            
            # Helper function to convert object to serializable dict
            def make_serializable(obj):
                """Convert any object to JSON-serializable format, preserving all attributes."""
                # Handle None, primitives
                if obj is None or isinstance(obj, (str, int, float, bool)):
                    return obj
                
                # Handle dict
                if isinstance(obj, dict):
                    return {k: make_serializable(v) for k, v in obj.items()}
                
                # Handle list/tuple
                if isinstance(obj, (list, tuple)):
                    return [make_serializable(item) for item in obj]
                
                # Handle objects with __dict__ (like APIResponse)
                if hasattr(obj, '__dict__'):
                    result = {"__type__": type(obj).__name__}
                    for attr_name in dir(obj):
                        # Skip private/magic methods and callables
                        if attr_name.startswith('_') or callable(getattr(obj, attr_name, None)):
                            continue
                        try:
                            attr_value = getattr(obj, attr_name)
                            result[attr_name] = make_serializable(attr_value)
                        except Exception:
                            # Skip attributes that can't be accessed
                            continue
                    return result
                
                # Fallback to string representation
                return str(obj)
            
            # Prepare debug data with complete raw response
            debug_output = {
                "function": func_name,
                "timestamp": timestamp,
                "parameters": {},
                "raw_response": make_serializable(raw_result),
                "parsed_data": parsed_data
            }
            
            # Copy and redact sensitive parameters
            for key, value in self.function_params.items():
                if any(x in key.lower() for x in ['pass', 'token', 'key', 'secret']):
                    debug_output["parameters"][key] = "***REDACTED***"
                else:
                    # Also serialize parameter values properly
                    debug_output["parameters"][key] = make_serializable(value)
            
            # Write to file
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(debug_output, f, indent=2, default=str)
            
            logging.debug(f"TUI_DEBUG: Raw result saved to {filepath}")
            
        except Exception as error:
            logging.error(f"TUI_DEBUG: Failed to save debug result: {error}", exc_info=True)
    
    def _format_result_output(self, parsed_data, func_name, raw_result=None):
        """Format API result for display in output panel with hierarchical structure."""
        output = []
        output.append(f"[SUCCESS] {func_name} completed")
        output.append("")
        
        if self.debug_mode and raw_result:
            output.append(f"[dim]Debug: Result saved to data/tui_debug_results/{func_name}_*.json[/dim]")
            output.append("")
        
        # Format hierarchically based on structure
        self._format_value_hierarchical(parsed_data, output, indent=0, key_name="results")
        
        # Add hint for viewing full data
        if isinstance(parsed_data, (list, dict)) and len(str(parsed_data)) > 500:
            output.append("")
            output.append("[dim]Tip: Full data available in debug log (run with --debug)[/dim]")
        
        return output
    
    def _format_value_hierarchical(self, value, output, indent=0, key_name=None, max_items=5):
        """Recursively format a value with hierarchical indentation."""
        indent_str = "  " * indent
        
        # Handle None
        if value is None:
            if key_name:
                output.append(f"{indent_str}{key_name}: None")
            else:
                output.append(f"{indent_str}None")
            return
        
        # Handle dictionaries
        if isinstance(value, dict):
            if key_name:
                output.append(f"{indent_str}{key_name}: dict ({len(value)} keys)")
            else:
                output.append(f"{indent_str}dict ({len(value)} keys)")
            
            # Show each key-value pair
            for idx, (k, v) in enumerate(value.items()):
                if isinstance(v, (dict, list)):
                    # Nested structure - recurse
                    self._format_value_hierarchical(v, output, indent + 1, key_name=k)
                else:
                    # Simple value - display inline
                    value_str = str(v)
                    if len(value_str) > 60:
                        value_str = value_str[:60] + "..."
                    output.append(f"{indent_str}  {k}: {value_str}")
            return
        
        # Handle lists/tuples
        if isinstance(value, (list, tuple)):
            item_count = len(value)
            type_name = "list" if isinstance(value, list) else "tuple"
            
            if key_name:
                output.append(f"{indent_str}{key_name}: {type_name} ({item_count} items)")
            else:
                output.append(f"{indent_str}{type_name} ({item_count} items)")
            
            if item_count == 0:
                output.append(f"{indent_str}  (empty)")
                return
            
            # Determine if items are dicts, and show structure
            display_count = min(max_items, item_count)
            for idx in range(display_count):
                item = value[idx]
                if isinstance(item, dict):
                    # Show dict item with its keys
                    output.append(f"{indent_str}  [{idx}]: dict ({len(item)} keys)")
                    # Show first few key-value pairs
                    for k, v in list(item.items())[:3]:
                        v_str = str(v)
                        if len(v_str) > 50:
                            v_str = v_str[:50] + "..."
                        output.append(f"{indent_str}    {k}: {v_str}")
                    if len(item) > 3:
                        output.append(f"{indent_str}    ... {len(item) - 3} more keys")
                elif isinstance(item, (list, tuple)):
                    # Nested list - recurse
                    self._format_value_hierarchical(item, output, indent + 1, key_name=f"[{idx}]", max_items=3)
                else:
                    # Simple item
                    item_str = str(item)
                    if len(item_str) > 60:
                        item_str = item_str[:60] + "..."
                    output.append(f"{indent_str}  [{idx}]: {item_str}")
            
            if item_count > display_count:
                output.append(f"{indent_str}  ... {item_count - display_count} more items")
            return
        
        # Handle simple types (string, int, float, bool, etc.)
        value_str = str(value)
        if len(value_str) > 200:
            value_str = value_str[:200] + "..."
        
        if key_name:
            output.append(f"{indent_str}{key_name}: {value_str}")
        else:
            output.append(f"{indent_str}{value_str}")
    
    def execute_current_item(self):
        """Execute the currently selected API function with parameter prompting.
        
        This method:
        1. Gets the selected function and its signature
        2. Prompts user for required parameters
        3. Executes the API call with collected parameters
        4. Displays results or errors
        """
        import inspect
        
        if not (0 <= self.current_selection < len(self.current_items)):
            return
        
        selected = self.current_items[self.current_selection]
        if selected.get('type') != 'function':
            return
        
        func = selected.get('object')
        func_name = selected.get('name')
        
        if not func or not callable(func):
            self.last_error = "Selected item is not callable"
            return
        
        if self.debug_mode:
            logging.debug(f"TUI_DEBUG: Preparing to execute function: {func_name}")
        
        # Clear last results
        self.last_result = None
        self.last_error = None
        
        # Restore terminal for input prompts
        if not self.IS_WINDOWS:
            self.termios.tcsetattr(sys.stdin, self.termios.TCSADRAIN, self.old_terminal_settings)
        
        try:
            # Get function signature
            sig = inspect.signature(func)
            params = {}
            
            if self.debug_mode:
                logging.debug(f"TUI_DEBUG: Function signature: {func_name}{sig}")
            
            # Display function info (screen already cleared by exiting Live() context)
            print(f"\n[Executing] {func_name}")
            print(f"Signature: {func_name}{sig}\n")
            
            # Prompt for parameters
            for param_name, param in sig.parameters.items():
                # Skip self parameter if present
                if param_name == 'self':
                    continue
                
                # Check if mist_session or apisession is available globally
                if param_name in ('mist_session', 'apisession'):
                    if hasattr(self, 'apisession') and self.apisession is not None:
                        params[param_name] = self.apisession
                        continue
                    else:
                        print("[ERROR] API session not available")
                        self.last_error = "API session not initialized"
                        return
                
                # Check if parameter has a default value
                has_default = param.default != inspect.Parameter.empty
                default_str = f" (default: {param.default})" if has_default else ""
                required_str = "" if has_default else " [required]"
                
                # Check if parameter value is available from environment variables
                # Common parameters: org_id, site_id, device_id, etc.
                env_value = os.getenv(param_name)
                if env_value:
                    # Use environment variable value without prompting
                    params[param_name] = env_value
                    print(f"  {param_name}: [from .env] {env_value}")
                    if self.debug_mode:
                        logging.debug(f"TUI_DEBUG: Using environment variable for {param_name}: {env_value}")
                    continue
                
                # Prompt user for parameter value
                prompt = f"  {param_name}{default_str}{required_str}: "
                value = input(prompt).strip()
                
                if self.debug_mode:
                    # Redact sensitive values in logs
                    display_value = "***REDACTED***" if any(x in param_name.lower() for x in ['pass', 'token', 'key', 'secret']) else value
                    logging.debug(f"TUI_DEBUG: User input for {param_name}: {display_value}")
                
                # Use default if no value provided
                if not value and has_default:
                    continue
                
                # Validate required parameters
                if not value and not has_default:
                    print(f"[ERROR] {param_name} is required")
                    self.last_error = f"Missing required parameter: {param_name}"
                    if self.debug_mode:
                        logging.debug(f"TUI_DEBUG: Execution aborted - missing required parameter: {param_name}")
                    return
                
                # Store parameter (basic type conversion)
                params[param_name] = value
            
            if self.debug_mode:
                param_summary = {k: "***REDACTED***" if any(x in k.lower() for x in ['pass', 'token', 'key', 'secret']) else v for k, v in params.items()}
                logging.debug(f"TUI_DEBUG: Calling {func_name} with parameters: {param_summary}")
            
            # Execute the API call
            print(f"\nExecuting API call...")
            result = func(**params)
            
            # Store result (but create smart preview to avoid OOM)
            self.last_result = result
            print(f"\n[SUCCESS]")
            
            # Smart result preview to prevent OOM errors with large responses
            result_type = type(result).__name__
            
            # Build safe preview without converting entire result to string
            if result is None:
                preview = "None"
            elif isinstance(result, (list, tuple)):
                item_count = len(result)
                if item_count == 0:
                    preview = f"{result_type}: [] (empty)"
                elif item_count <= 3:
                    # Small list, safe to show
                    preview = f"{result_type} with {item_count} items:\n"
                    for idx, item in enumerate(result):
                        item_preview = repr(item)[:100]
                        preview += f"  [{idx}]: {item_preview}\n"
                else:
                    # Large list, show summary + first few items
                    preview = f"{result_type} with {item_count} items (showing first 3):\n"
                    for idx in range(3):
                        item_preview = repr(result[idx])[:100]
                        if len(repr(result[idx])) > 100:
                            item_preview += "..."
                        preview += f"  [{idx}]: {item_preview}\n"
                    preview += f"  ... and {item_count - 3} more items"
            elif isinstance(result, dict):
                key_count = len(result)
                if key_count == 0:
                    preview = f"{result_type}: {{}} (empty)"
                elif key_count <= 5:
                    # Small dict, show all keys
                    preview = f"{result_type} with {key_count} keys: {list(result.keys())}"
                else:
                    # Large dict, show first few keys
                    first_keys = list(result.keys())[:5]
                    preview = f"{result_type} with {key_count} keys (first 5): {first_keys}..."
            elif isinstance(result, str):
                if len(result) <= 200:
                    preview = f"String: {result}"
                else:
                    preview = f"String ({len(result)} chars): {result[:200]}..."
            elif isinstance(result, (int, float, bool)):
                preview = f"{result_type}: {result}"
            else:
                # Unknown type, use safe repr with limit
                preview = f"{result_type}: {repr(result)[:200]}"
                if len(repr(result)) > 200:
                    preview += "..."
            
            print(f"\n{preview}")
            
            # Offer to save large results
            if isinstance(result, (list, tuple, dict)) and len(result) > 10:
                print(f"\n[TIP] Result has {len(result)} items. Consider using main menu options to save full data to CSV/SQLite.")
            
            logging.info(f"TUI: Successfully executed {func_name}")
            
            if self.debug_mode:
                result_len = len(result) if hasattr(result, '__len__') else 'N/A'
                logging.debug(f"TUI_DEBUG: Execution successful - result type: {result_type}, length: {result_len}")
            
        except KeyboardInterrupt:
            print("\n[CANCELLED] Execution cancelled by user")
            logging.info(f"TUI: Execution of {func_name} cancelled by user")
            if self.debug_mode:
                logging.debug(f"TUI_DEBUG: KeyboardInterrupt during {func_name} execution")
        except Exception as error:
            self.last_error = str(error)
            print(f"\n[ERROR] {error}")
            logging.error(f"TUI: Execution of {func_name} failed - {error}", exc_info=True)
            if self.debug_mode:
                logging.debug(f"TUI_DEBUG: Exception during {func_name} execution: {type(error).__name__}: {error}")
        finally:
            # Wait for user acknowledgment before returning to TUI
            print("\nPress any key to return to explorer...")
            if self.IS_WINDOWS:
                self.msvcrt.getch()
            else:
                # Wait for keypress with normal terminal settings
                sys.stdin.read(1)
            
            # Restore raw mode for TUI navigation on Unix systems
            if not self.IS_WINDOWS:
                self.tty.setcbreak(sys.stdin.fileno())
            
            if self.debug_mode:
                logging.debug(f"TUI_DEBUG: Returning to API explorer from {func_name} execution, terminal mode restored")
    
    def run(self):
        """Main TUI loop with hierarchical API navigation."""
        logging.info("TUI: Starting hierarchical API explorer")
        
        if self.debug_mode:
            logging.debug("TUI_DEBUG: run() method started - initializing terminal settings")
        
        # Set terminal to raw mode for Unix systems
        if not self.IS_WINDOWS:
            if self.debug_mode:
                logging.debug("TUI_DEBUG: Unix platform detected - setting terminal to raw mode")
            self.old_terminal_settings = self.termios.tcgetattr(sys.stdin)
            self.tty.setcbreak(sys.stdin.fileno())
            if self.debug_mode:
                logging.debug("TUI_DEBUG: Terminal set to raw mode successfully")
        else:
            if self.debug_mode:
                logging.debug("TUI_DEBUG: Windows platform detected - skipping terminal mode setup")
        
        try:
            # Initial discovery at root level
            if self.debug_mode:
                logging.debug("TUI_DEBUG: Starting initial discovery at root level")
            self._discover_current_level()
            if self.debug_mode:
                logging.debug(f"TUI_DEBUG: Initial discovery complete - found {len(self.current_items)} items")
            
            # Use higher refresh rate for more responsive feel
            # screen=True prevents content from scrolling off-screen
            if self.debug_mode:
                logging.debug("TUI_DEBUG: Entering Live() context for TUI rendering")
            
            loop_iteration = 0
            # Higher refresh rate for responsive input - 20/sec for smooth scrolling
            # screen=True keeps content from scrolling
            with self.Live(self.create_layout(), console=self.console, refresh_per_second=20, screen=True) as live:
                if self.debug_mode:
                    logging.debug("TUI_DEBUG: Live() context entered successfully - starting main loop")
                
                while self.running:
                    loop_iteration += 1
                    
                    if self.debug_mode and loop_iteration % 100 == 0:  # Log every 100 iterations
                        logging.debug(f"TUI_DEBUG: Main loop iteration {loop_iteration} - running={self.running}")
                    
                    # Check for keyboard input
                    key = self.check_keyboard_input()
                    if key:
                        if self.debug_mode:
                            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]
                            logging.debug(f"TUI_DEBUG: [{timestamp}] Keyboard input detected in main loop: {repr(key)}")
                        
                        self.handle_input(key)
                        
                        if self.debug_mode:
                            logging.debug(f"TUI_DEBUG: Input handled - running flag now: {self.running}")
                        
                        # Explicit check after handling input for immediate exit
                        if not self.running:
                            if self.debug_mode:
                                logging.debug("TUI_DEBUG: Running flag is False - breaking main loop")
                            break
                        
                        # Update layout - let Live() handle refresh timing automatically
                        if self.debug_mode:
                            logging.debug("TUI_DEBUG: Updating Live() display with new layout")
                        new_layout = self.create_layout()
                        live.update(new_layout)  # Remove refresh=True to let Live() manage refresh
                        if self.debug_mode:
                            logging.debug("TUI_DEBUG: Live() display updated")
                    
                    # Minimal sleep - just yield to prevent CPU spin (10ms for responsive input)
                    time.sleep(0.01)
                
                if self.debug_mode:
                    logging.debug(f"TUI_DEBUG: Main loop exited after {loop_iteration} iterations - exiting Live() context")
        
        except Exception as error:
            logging.error(f"TUI: Critical error in run() method: {error}", exc_info=True)
            if self.debug_mode:
                logging.debug(f"TUI_DEBUG: Exception caught in run() try block: {type(error).__name__}: {error}")
            raise
        
        finally:
            if self.debug_mode:
                logging.debug("TUI_DEBUG: Entered finally block - restoring terminal settings")
            
            # Restore terminal settings on Unix
            if not self.IS_WINDOWS:
                try:
                    if self.debug_mode:
                        logging.debug("TUI_DEBUG: Restoring terminal settings on Unix platform")
                    self.termios.tcsetattr(sys.stdin, self.termios.TCSADRAIN, self.old_terminal_settings)
                    if self.debug_mode:
                        logging.debug("TUI_DEBUG: Terminal settings restored successfully")
                except Exception as term_error:
                    logging.error(f"TUI: Error restoring terminal settings: {term_error}", exc_info=True)
                    if self.debug_mode:
                        logging.debug(f"TUI_DEBUG: Terminal restoration failed: {type(term_error).__name__}: {term_error}")
            else:
                if self.debug_mode:
                    logging.debug("TUI_DEBUG: Windows platform - no terminal restoration needed")
            
            if self.debug_mode:
                timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]
                logging.debug(f"TUI_DEBUG: [{timestamp}] Explorer exiting - run() finally block executing")
            
            logging.info("TUI: Explorer exited cleanly")
            if self.debug_mode:
                timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]
                logging.debug(f"TUI_DEBUG: [{timestamp}] run() method ending - about to print exit message")
            print("\n[EXIT] MistHelper TUI - Hierarchical API Explorer closed")
            if self.debug_mode:
                timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]
                logging.debug(f"TUI_DEBUG: [{timestamp}] Exit message printed - run() method complete")


# ============================================================================
# GATEWAY TEMPLATE CONFIGURATION EXTRACTION AND APPLICATION (Menu 105-106)
# ============================================================================

def extract_gateway_template_configuration():
    """
    Menu Option 105: Extract specific configuration sections from a gateway template.
    
    This function:
    1. Lists all gateway templates with indexed selection
    2. Retrieves the selected template's full configuration
    3. Extracts "Traffic Steering" (path_preferences) - specifically "DIA_Pico"
    4. Extracts "Application Policies" (service_policies) - specifically "Picocell"
    5. Saves both sections to a JSON file named after the template
    
    Use Case: Extract configuration patterns for replication to other templates
    via Menu Option 106.
    
    SECURITY: Read-only operation, no modifications to templates.
    """
    print("\n  Extract Gateway Template Configuration (Menu 105)")
    print("=" * 70)
    logging.info("Menu #105: Starting gateway template configuration extraction")
    
    # Step 1: Get organization ID
    org_id = get_cached_or_prompted_org_id()
    
    # Step 2: Fetch all gateway templates
    print("\n  Fetching gateway templates...")
    try:
        response = mistapi.api.v1.orgs.gatewaytemplates.listOrgGatewayTemplates(
            apisession, 
            org_id, 
            limit=1000
        )
        templates = mistapi.get_all(response=response, mist_session=apisession)
        
        if not templates:
            print("  No gateway templates found for this organization.")
            logging.warning("Menu #105: No gateway templates found")
            return
            
    except Exception as error:
        print(f"  Error fetching gateway templates: {error}")
        logging.error(f"Menu #105: Failed to fetch templates: {error}")
        return
    
    # Step 3: Display indexed list of templates
    templates_sorted = sorted(templates, key=lambda t: t.get("name", "Unnamed Template").lower())
    
    print(f"\n  Available Gateway Templates ({len(templates_sorted)} found):")
    print("-" * 70)
    
    index_to_template = {}
    for index, template in enumerate(templates_sorted):
        template_name = template.get("name", "Unnamed Template")
        template_id = template.get("id", "No ID")
        template_type = template.get("type", "standalone")
        print(f"  [{index}] {template_name:40} Type: {template_type:10} ID: {template_id}")
        index_to_template[index] = template
    
    # Step 4: Get user selection
    print()
    try:
        user_input = safe_input(
            f"Enter template index to extract [0-{len(templates_sorted)-1}]: ",
            context="menu_105_template_selection"
        ).strip()
    except (EOFError, KeyboardInterrupt):
        print("\n  Operation cancelled.")
        logging.info("Menu #105: User cancelled template selection")
        return
    
    if not user_input.isdigit():
        print("  Invalid input. Please enter a numeric index.")
        logging.warning(f"Menu #105: Invalid input: {user_input}")
        return
    
    selected_index = int(user_input)
    if selected_index not in index_to_template:
        print(f"  Invalid index. Please select between 0 and {len(templates)-1}.")
        logging.warning(f"Menu #105: Index out of range: {selected_index}")
        return
    
    selected_template_summary = index_to_template[selected_index]
    template_id = selected_template_summary.get("id")
    template_name = selected_template_summary.get("name", "Unnamed Template")
    
    print(f"\n  Selected Template: {template_name}")
    print(f"  Template ID: {template_id}")
    
    # Step 5: Fetch full template configuration
    print("\n  Fetching full template configuration...")
    try:
        template_response = mistapi.api.v1.orgs.gatewaytemplates.getOrgGatewayTemplate(
            apisession,
            org_id,
            template_id
        )
        template_config = template_response.data if hasattr(template_response, 'data') else {}
        
        if not isinstance(template_config, dict):
            print("  Error: Template configuration is not in expected format.")
            logging.error(f"Menu #105: Invalid template config format for {template_name}")
            return
            
    except Exception as error:
        print(f"  Error fetching template configuration: {error}")
        logging.error(f"Menu #105: Failed to fetch template {template_name}: {error}")
        return
    
    # Step 6: Extract Traffic Steering (path_preferences) - specifically "DIA_Pico"
    print("\n  Extracting Traffic Steering configuration...")
    path_preferences = template_config.get("path_preferences", {})
    
    dia_pico_config = None
    if isinstance(path_preferences, dict):
        dia_pico_config = path_preferences.get("DIA_Pico")
        if dia_pico_config:
            print(f"  -> Found 'DIA_Pico' in Traffic Steering (path_preferences)")
            logging.info(f"Menu #105: Found DIA_Pico in template {template_name}")
        else:
            print(f"  -> 'DIA_Pico' not found in Traffic Steering")
            logging.warning(f"Menu #105: DIA_Pico not found in template {template_name}")
    else:
        print("  -> No path_preferences found in template")
        logging.warning(f"Menu #105: No path_preferences in template {template_name}")
    
    # Step 7: Extract Application Policies (service_policies) - specifically "Picocell"
    print("\n  Extracting Application Policies configuration...")
    service_policies = template_config.get("service_policies", [])
    
    picocell_policy = None
    if isinstance(service_policies, list):
        # Search through service_policies array for an item with name "Picocell"
        for policy in service_policies:
            if isinstance(policy, dict) and policy.get("name") == "Picocell":
                picocell_policy = policy
                print(f"  -> Found 'Picocell' in Application Policies (service_policies)")
                logging.info(f"Menu #105: Found Picocell policy in template {template_name}")
                break
        
        if not picocell_policy:
            print(f"  -> 'Picocell' not found in Application Policies")
            logging.warning(f"Menu #105: Picocell policy not found in template {template_name}")
    else:
        print("  -> No service_policies found in template")
        logging.warning(f"Menu #105: No service_policies in template {template_name}")
    
    # Step 8: Check if we found anything to extract
    if not dia_pico_config and not picocell_policy:
        print("\n  Warning: Neither 'DIA_Pico' nor 'Picocell' configurations were found.")
        print("  No extraction file will be created.")
        logging.warning(f"Menu #105: No target configs found in template {template_name}")
        return
    
    # Step 9: Prepare extraction data
    extraction_data = {
        "source_template_name": template_name,
        "source_template_id": template_id,
        "extraction_timestamp": datetime.now(timezone.utc).isoformat(),
        "extracted_by": "MistHelper Menu #105",
        "configurations": {
            "traffic_steering": {
                "DIA_Pico": dia_pico_config
            },
            "application_policies": {
                "Picocell": picocell_policy
            }
        }
    }
    
    # Step 10: Save to JSON file in data/ directory
    # Sanitize template name for filename
    safe_filename = EnhancedSSHRunner.sanitize_filename(template_name)
    json_filename = f"{safe_filename}_extracted_config.json"
    json_filepath = get_csv_file_path(json_filename)  # Uses data/ directory
    
    try:
        with open(json_filepath, 'w', encoding='utf-8') as json_file:
            json.dump(extraction_data, json_file, indent=2, ensure_ascii=False)
        
        print(f"\n  Success! Configuration extracted and saved to:")
        print(f"  -> {json_filepath}")
        print(f"\n  Extracted Components:")
        if dia_pico_config:
            print(f"     [+] Traffic Steering: DIA_Pico")
        else:
            print(f"     [ ] Traffic Steering: DIA_Pico (not found)")
        if picocell_policy:
            print(f"     [+] Application Policies: Picocell")
        else:
            print(f"     [ ] Application Policies: Picocell (not found)")
        
        print(f"\n  Use Menu Option 106 to apply this configuration to other templates.")
        logging.info(f"Menu #105: Successfully extracted config to {json_filepath}")
        
    except Exception as error:
        print(f"\n  Error saving extraction file: {error}")
        logging.error(f"Menu #105: Failed to save JSON file: {error}")
        return


def apply_gateway_template_configuration():
    """
    Menu Option 106: Apply extracted configuration to gateway template(s).
    
    This function:
    1. Lists available extracted configuration JSON files
    2. Lists all gateway templates for destination selection
    3. Shows preview of what will be applied
    4. Requires uppercase "APPLY" confirmation
    5. Applies the configuration to selected template(s)
    
    Use Case: Replicate configuration patterns extracted via Menu Option 105
    to other gateway templates for consistency.
    
    SECURITY: DESTRUCTIVE operation - modifies gateway templates.
    Requires explicit uppercase confirmation.
    """
    print("\n  Apply Gateway Template Configuration (Menu 106)")
    print("=" * 70)
    print("  !? DESTRUCTIVE: This operation modifies gateway templates")
    print("=" * 70)
    logging.info("Menu #106: Starting gateway template configuration application")
    
    # Step 1: Get organization ID
    org_id = get_cached_or_prompted_org_id()
    
    # Step 2: Find available extraction JSON files in data/ directory
    data_dir = "data"
    if not os.path.exists(data_dir):
        print(f"\n  Error: Data directory '{data_dir}' not found.")
        logging.error("Menu #106: Data directory not found")
        return
    
    # Look for files matching pattern: *_extracted_config.json
    extraction_files = []
    try:
        for filename in os.listdir(data_dir):
            if filename.endswith("_extracted_config.json"):
                filepath = os.path.join(data_dir, filename)
                extraction_files.append({
                    "filename": filename,
                    "filepath": filepath
                })
    except Exception as error:
        print(f"\n  Error scanning for extraction files: {error}")
        logging.error(f"Menu #106: Error scanning data directory: {error}")
        return
    
    if not extraction_files:
        print("\n  No extracted configuration files found.")
        print("  Use Menu Option 105 to extract configurations first.")
        logging.warning("Menu #106: No extraction files found")
        return
    
    # Step 3: Display available extraction files
    print(f"\n  Available Extracted Configurations ({len(extraction_files)} found):")
    print("-" * 70)
    
    index_to_file = {}
    for index, file_info in enumerate(extraction_files):
        filename = file_info["filename"]
        filepath = file_info["filepath"]
        
        # Try to read metadata from file
        try:
            with open(filepath, 'r', encoding='utf-8') as json_file:
                data = json.load(json_file)
                source_template = data.get("source_template_name", "Unknown")
                timestamp = data.get("extraction_timestamp", "Unknown")
                
                # Parse and format timestamp
                try:
                    dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                    formatted_time = dt.strftime('%Y-%m-%d %H:%M UTC')
                except:
                    formatted_time = timestamp
                
                print(f"  [{index}] Source: {source_template:30} Extracted: {formatted_time}")
                print(f"       File: {filename}")
        except Exception as error:
            print(f"  [{index}] File: {filename} (Error reading metadata: {error})")
            logging.warning(f"Menu #106: Could not read metadata from {filename}: {error}")
        
        index_to_file[index] = file_info
    
    # Step 4: Get user selection for source configuration
    print()
    try:
        config_input = safe_input(
            f"Enter configuration file index to use [0-{len(extraction_files)-1}]: ",
            context="menu_106_config_selection"
        ).strip()
    except (EOFError, KeyboardInterrupt):
        print("\n  Operation cancelled.")
        logging.info("Menu #106: User cancelled configuration selection")
        return
    
    if not config_input.isdigit():
        print("  Invalid input. Please enter a numeric index.")
        logging.warning(f"Menu #106: Invalid config input: {config_input}")
        return
    
    config_index = int(config_input)
    if config_index not in index_to_file:
        print(f"  Invalid index. Please select between 0 and {len(extraction_files)-1}.")
        logging.warning(f"Menu #106: Config index out of range: {config_index}")
        return
    
    selected_file = index_to_file[config_index]
    config_filepath = selected_file["filepath"]
    
    # Step 5: Load the extraction data
    print(f"\n  Loading configuration from: {selected_file['filename']}")
    try:
        with open(config_filepath, 'r', encoding='utf-8') as json_file:
            extraction_data = json.load(json_file)
    except Exception as error:
        print(f"  Error loading configuration file: {error}")
        logging.error(f"Menu #106: Failed to load {config_filepath}: {error}")
        return
    
    # Validate extraction data structure
    if not isinstance(extraction_data, dict) or "configurations" not in extraction_data:
        print("  Error: Invalid configuration file format.")
        logging.error(f"Menu #106: Invalid format in {config_filepath}")
        return
    
    configs = extraction_data.get("configurations", {})
    dia_pico_config = configs.get("traffic_steering", {}).get("DIA_Pico")
    picocell_policy = configs.get("application_policies", {}).get("Picocell")
    
    if not dia_pico_config and not picocell_policy:
        print("  Error: No valid configurations found in file.")
        logging.error(f"Menu #106: No configs in {config_filepath}")
        return
    
    source_template_name = extraction_data.get("source_template_name", "Unknown")
    print(f"  Source Template: {source_template_name}")
    
    # Step 6: Fetch all gateway templates for destination selection
    print("\n  Fetching available gateway templates...")
    try:
        response = mistapi.api.v1.orgs.gatewaytemplates.listOrgGatewayTemplates(
            apisession, 
            org_id, 
            limit=1000
        )
        templates = mistapi.get_all(response=response, mist_session=apisession)
        
        if not templates:
            print("  No gateway templates found for this organization.")
            logging.warning("Menu #106: No gateway templates found")
            return
            
    except Exception as error:
        print(f"  Error fetching gateway templates: {error}")
        logging.error(f"Menu #106: Failed to fetch templates: {error}")
        return
    
    # Step 7: Display indexed list of templates
    print(f"\n  Available Destination Templates ({len(templates)} found):")
    print("-" * 70)
    
    index_to_template = {}
    for index, template in enumerate(templates):
        template_name = template.get("name", "Unnamed Template")
        template_id = template.get("id", "No ID")
        template_type = template.get("type", "standalone")
        
        # Mark source template
        marker = " (SOURCE)" if template_name == source_template_name else ""
        print(f"  [{index}] {template_name:40} Type: {template_type:10}{marker}")
        index_to_template[index] = template
    
    # Step 8: Get user selection for destination template(s)
    print()
    print("  Enter destination template index (or comma-separated list for multiple)")
    try:
        dest_input = safe_input(
            f"Destination template(s) [0-{len(templates)-1}]: ",
            context="menu_106_destination_selection"
        ).strip()
    except (EOFError, KeyboardInterrupt):
        print("\n  Operation cancelled.")
        logging.info("Menu #106: User cancelled destination selection")
        return
    
    # Parse destination input (supports comma-separated indices)
    destination_templates = []
    try:
        indices = [int(idx.strip()) for idx in dest_input.split(',')]
        for idx in indices:
            if idx in index_to_template:
                destination_templates.append(index_to_template[idx])
            else:
                print(f"  Warning: Invalid index {idx}, skipping.")
                logging.warning(f"Menu #106: Invalid destination index: {idx}")
    except ValueError:
        print("  Invalid input format. Please enter numeric indices separated by commas.")
        logging.warning(f"Menu #106: Invalid destination input: {dest_input}")
        return
    
    if not destination_templates:
        print("  No valid destination templates selected.")
        logging.warning("Menu #106: No valid destinations")
        return
    
    # Step 9: Show preview of what will be applied
    print(f"\n  Configuration Preview:")
    print("=" * 70)
    print(f"  Source: {source_template_name}")
    print(f"  Destination(s): {len(destination_templates)} template(s)")
    for template in destination_templates:
        print(f"    -> {template.get('name', 'Unnamed')}")
    
    print(f"\n  Configuration to Apply:")
    if dia_pico_config:
        print(f"    [+] Traffic Steering (path_preferences): DIA_Pico")
        # Show summary of configuration
        if isinstance(dia_pico_config, dict):
            for key, value in list(dia_pico_config.items())[:3]:  # Show first 3 keys
                print(f"        {key}: {str(value)[:50]}...")
            if len(dia_pico_config) > 3:
                print(f"        ... and {len(dia_pico_config) - 3} more fields")
    
    if picocell_policy:
        print(f"    [+] Application Policies (service_policies): Picocell")
        # Show summary of policy
        if isinstance(picocell_policy, dict):
            policy_name = picocell_policy.get("name", "Unnamed")
            print(f"        Policy Name: {policy_name}")
            # Show key fields
            for key in ["action", "services", "tenants", "idp"]:
                if key in picocell_policy:
                    value = picocell_policy[key]
                    print(f"        {key}: {str(value)[:50]}...")
    
    # Step 10: Confirmation prompt
    print(f"\n  {'=' * 70}")
    print(f"  !? CRITICAL: This will modify {len(destination_templates)} template(s)")
    print(f"  !? The configurations will be merged/updated (existing data preserved)")
    print(f"  !? Type 'APPLY' (all caps) to proceed or anything else to cancel")
    print(f"  {'=' * 70}")
    
    try:
        confirmation = safe_input("\n  Confirmation: ", context="menu_106_confirmation").strip()
    except (EOFError, KeyboardInterrupt):
        print("\n  Operation cancelled.")
        logging.info("Menu #106: User cancelled at confirmation")
        return
    
    if confirmation != "APPLY":
        print("  Operation cancelled.")
        logging.info("Menu #106: User did not confirm with APPLY")
        return
    
    # Step 11: Apply configuration to destination template(s)
    print("\n  Applying configuration to destination templates...")
    results = []
    
    for template_summary in tqdm(destination_templates, desc="Updating templates", unit="template"):
        template_id = template_summary.get("id")
        template_name = template_summary.get("name", "Unnamed Template")
        
        result = {
            "template_name": template_name,
            "template_id": template_id,
            "status": "",
            "changes_made": [],
            "error": ""
        }
        
        try:
            # Fetch current template configuration
            template_response = mistapi.api.v1.orgs.gatewaytemplates.getOrgGatewayTemplate(
                apisession,
                org_id,
                template_id
            )
            current_config = template_response.data if hasattr(template_response, 'data') else {}
            
            if not isinstance(current_config, dict):
                result["status"] = "FAILED"
                result["error"] = "Invalid template configuration format"
                results.append(result)
                logging.error(f"Menu #106: Invalid config format for {template_name}")
                continue
            
            # Apply Traffic Steering (path_preferences) if present
            if dia_pico_config:
                if "path_preferences" not in current_config:
                    current_config["path_preferences"] = {}
                
                current_config["path_preferences"]["DIA_Pico"] = dia_pico_config
                result["changes_made"].append("Added/Updated DIA_Pico in path_preferences")
                logging.info(f"Menu #106: Added DIA_Pico to {template_name}")
            
            # Apply Application Policies (service_policies) if present
            if picocell_policy:
                if "service_policies" not in current_config:
                    current_config["service_policies"] = []
                
                # Check if Picocell policy already exists
                existing_index = None
                for idx, policy in enumerate(current_config["service_policies"]):
                    if isinstance(policy, dict) and policy.get("name") == "Picocell":
                        existing_index = idx
                        break
                
                if existing_index is not None:
                    # Update existing policy
                    current_config["service_policies"][existing_index] = picocell_policy
                    result["changes_made"].append("Updated existing Picocell in service_policies")
                    logging.info(f"Menu #106: Updated Picocell policy in {template_name}")
                else:
                    # Add new policy - check if we need to insert at position 14
                    policy_count = len(current_config["service_policies"])
                    
                    if policy_count >= 14:
                        # Insert at position 14 (0-indexed position 13), pushing existing policies back
                        current_config["service_policies"].insert(13, picocell_policy)
                        result["changes_made"].append(f"Inserted Picocell at position 14 (pushed {policy_count - 13} policies back)")
                        logging.info(f"Menu #106: Inserted Picocell at position 14 in {template_name}, pushed {policy_count - 13} policies back")
                    else:
                        # Not enough policies, append to end
                        current_config["service_policies"].append(picocell_policy)
                        result["changes_made"].append(f"Added new Picocell to service_policies (position {policy_count + 1})")
                        logging.info(f"Menu #106: Added new Picocell policy to {template_name} at position {policy_count + 1}")
            
            # Push update to API
            update_response = mistapi.api.v1.orgs.gatewaytemplates.updateOrgGatewayTemplate(
                apisession,
                org_id,
                template_id,
                body=current_config
            )
            
            if update_response.status_code == 200:
                result["status"] = "SUCCESS"
                logging.info(f"Menu #106: Successfully updated template {template_name}")
            else:
                result["status"] = "FAILED"
                result["error"] = f"API returned status {update_response.status_code}"
                logging.error(f"Menu #106: API error updating {template_name}: {update_response.status_code}")
        
        except Exception as error:
            result["status"] = "FAILED"
            result["error"] = str(error)
            logging.error(f"Menu #106: Exception updating {template_name}: {error}")
        
        results.append(result)
    
    # Step 12: Generate audit report
    output_file = "GatewayTemplate_Config_Application_Audit.csv"
    
    # Prepare results for CSV export
    csv_results = []
    for result in results:
        csv_results.append({
            "template_name": result["template_name"],
            "template_id": result["template_id"],
            "status": result["status"],
            "changes_made": "; ".join(result["changes_made"]) if result["changes_made"] else "",
            "error": result["error"]
        })
    
    DataExporter.save_data_to_output(csv_results, output_file)
    
    # Step 13: Print summary
    success_count = sum(1 for r in results if r["status"] == "SUCCESS")
    failure_count = len(results) - success_count
    
    print(f"\n  Configuration Application Complete!")
    print(f"=" * 70)
    print(f"  Templates Processed: {len(results)}")
    print(f"  Successfully Updated: {success_count}")
    print(f"  Failed: {failure_count}")
    print(f"\n  Audit report saved to: {output_file}")
    print(f"=" * 70)
    
    if success_count > 0:
        print(f"\n  !? {success_count} template(s) now have the applied configurations")
        print(f"  !? Verify configurations in Mist portal before deploying to production")
    
    if failure_count > 0:
        print(f"\n  !? {failure_count} template(s) failed to update - check audit report")
    
    logging.warning(f"Menu #106 DESTRUCTIVE operation complete: {success_count} templates updated, {failure_count} failed")


def clone_gateway_templates_by_state_and_country():
    """
    Menu #111: Clone Gateway Templates by State and Country (DESTRUCTIVE)
    
    Clones a selected gateway template for each state (or country if no state)
    present in the organization's sites, then assigns sites to matching templates.
    
    Workflow:
    1. Load site list with state and country information
    2. User selects source gateway template to clone
    3. For each unique state found in sites:
       - Create template named "{SourceTemplate}_{State}" (e.g., "Branch_CA")
       - If template exists, skip creation
    4. For sites without state but with country:
       - Create template named "{SourceTemplate}_{Country}" (e.g., "Branch_USA")
       - If template exists, skip creation
    5. Assign each site to its corresponding template:
       - Sites with state -> {SourceTemplate}_{State}
       - Sites without state -> {SourceTemplate}_{Country}
       - Skip if site already assigned to correct template
    6. Generate audit report
    
    SECURITY: DESTRUCTIVE operation requiring explicit confirmation.
    This creates new gateway templates and modifies site configurations.
    """
    print("\n  DESTRUCTIVE: Clone Gateway Templates by State and Country")
    print("=" * 70)
    print("  !? WARNING: This operation creates new gateway templates")
    print("  !? WARNING: This operation modifies site template assignments")
    print("  !? Ensure source template is properly configured before cloning")
    print("=" * 70)
    
    logging.warning("Menu #111 DESTRUCTIVE: Clone Gateway Templates by State/Country operation started")
    
    org_id = get_cached_or_prompted_org_id()
    
    # Step 1: Load site data with state and country information
    print("\n  Step 1: Loading site data...")
    check_and_generate_csv("SiteList.csv", export_all_sites_to_csv)
    
    sites_path = get_csv_file_path("SiteList.csv")
    with open(sites_path, encoding="utf-8") as f:
        all_sites = list(csv.DictReader(f))
    
    if not all_sites:
        print(" No sites found in organization.")
        logging.error("No sites available for template cloning")
        return
    
    # Filter sites with valid state or country
    sites_with_location = []
    for site in all_sites:
        address = site.get("address", "").strip()
        country = site.get("country_code", "").strip()
        site_name = site.get("name", "").strip()
        site_id = site.get("id", "").strip()
        
        # Parse state/province from address field (format varies by country)
        # US example: "27925 New Lancaster Rd, Louisburg, KS 66053, USA"
        # CA example: "456 Oak Ave, Toronto, ON M5H 2N2, Canada"
        # MX example: "Cancun Quintana Roo 77500" (no commas, full state name)
        # CR example: "La Fortuna Alajuela 21007" (full province name)
        # Small countries: "Kingston Jamaica" (no state, just use country)
        state = ""
        if address and country:
            import re
            
            # Skip state extraction for small island nations without meaningful subdivisions
            # These will group by country only
            small_countries = {'BS', 'BZ', 'CU', 'HT', 'JM', 'DO'}  # Bahamas, Belize, Cuba, Haiti, Jamaica, Dominican Republic
            
            if country in small_countries:
                # No state extraction - will create country-level templates only
                state = ""
            elif "," in address:
                # Handle US/CA comma-separated format
                parts = [p.strip() for p in address.split(",")]
                if len(parts) >= 3:
                    # Check the component that should contain "STATE ZIP" or "PROVINCE POSTAL"
                    for part in parts:
                        # Match US pattern: 2-letter state code followed by space and 5 digits (e.g., "KS 66053")
                        state_match_us = re.search(r'\b([A-Z]{2})\s+\d{5}', part)
                        if state_match_us:
                            state = state_match_us.group(1)
                            break
                        # Match CA pattern: 2-letter province code followed by space and postal code (e.g., "ON M5H 2N2")
                        # Look for 2-letter code at START of part, not embedded in postal code
                        state_match_ca = re.search(r'^([A-Z]{2})\s+[A-Z]\d[A-Z]', part)
                        if state_match_ca:
                            state = state_match_ca.group(1)
                            break
                        # Also check for standalone 2-letter codes
                        state_alone = re.search(r'^([A-Z]{2})$', part)
                        if state_alone:
                            state = state_alone.group(1)
                            break
            else:
                # Handle space-separated formats (no commas)
                # CA without commas: "Vancouver BC V6G 1Z4" or "Banff AB T1L 1K2"
                # MX: "Cancun Quintana Roo 77500"
                # CR: "La Fortuna Alajuela 21007"
                parts = address.split()
                
                if country == 'CA' and len(parts) >= 3:
                    # Canadian addresses without commas: look for 2-letter province code followed by postal pattern
                    # Postal code format: A1A 1A1 (letter-digit-letter space digit-letter-digit)
                    for i, part in enumerate(parts):
                        # Check if this looks like a 2-letter province code
                        if len(part) == 2 and part.isupper():
                            # Verify next part looks like start of Canadian postal code (A1A format)
                            if i + 1 < len(parts) and re.match(r'^[A-Z]\d[A-Z]$', parts[i + 1]):
                                state = part
                                break
                elif len(parts) >= 2:
                    # Handle MX/Central America space-separated format: "City StateName PostalCode"
                    # Extract last word before postal code (if present) as state/province
                    # Note: Multi-word states like "Quintana Roo" will only capture last word ("Roo")
                    # but this provides better accuracy for multi-word cities
                    
                    # Special case: Check for multi-word state/territory names
                    # Puerto Rico (US territory): "City Puerto Rico 00901"
                    # Bay Islands (Honduras): "City Bay Islands postal"
                    address_lower = address.lower()
                    if 'puerto rico' in address_lower:
                        state = 'Puerto Rico'
                    elif 'bay islands' in address_lower:
                        state = 'Bay Islands'
                    else:
                        # Look for pattern: word(s) followed by postal code (starts with digit)
                        postal_index = -1
                        for i, part in enumerate(parts):
                            if re.match(r'^\d', part):  # Starts with digit (likely postal code)
                                postal_index = i
                                break
                        
                        # State/province is word immediately before postal code
                        if postal_index > 1:
                            state = parts[postal_index - 1]
                        elif len(parts) >= 2 and postal_index == -1:
                            # No postal code found - check for special patterns
                            # Panama addresses often end with city name repeated: "Panama City Panama"
                            if country == 'PA' and len(parts) >= 3 and parts[-1] == 'Panama':
                                # Skip - this is city name, not province
                                state = ""
                            elif len(parts) == 2 and country not in {'MX', 'CR', 'PA', 'HN', 'GT'}:
                                # Likely "City Country" format for small nations
                                state = ""
                            else:
                                # Take last word as state/province
                                state = parts[-1]
        
        if state or country:
            sites_with_location.append({
                "id": site_id,
                "name": site_name,
                "state": state,
                "country": country,
                "current_template_id": site.get("gatewaytemplate_id", "").strip()
            })
    
    if not sites_with_location:
        print(" No sites found with state or country information.")
        logging.error("No sites have state or country data")
        return
    
    print(f"  Found {len(sites_with_location)} sites with location data")
    
    # Collect unique states and countries
    unique_states = set()
    unique_countries = set()
    
    for site in sites_with_location:
        if site["state"]:
            unique_states.add(site["state"])
        elif site["country"]:
            unique_countries.add(site["country"])
    
    print(f"  Unique states found: {len(unique_states)}")
    print(f"  Unique countries (for sites without state): {len(unique_countries)}")
    
    # Step 2: Load and display available gateway templates
    print("\n  Step 2: Loading gateway templates...")
    check_and_generate_csv("OrgGatewayTemplates.csv", export_gateway_templates_to_csv)
    
    templates_path = get_csv_file_path("OrgGatewayTemplates.csv")
    with open(templates_path, encoding="utf-8") as f:
        template_rows = list(csv.DictReader(f))
    
    if not template_rows:
        print(" No gateway templates found.")
        logging.error("No gateway templates available for cloning")
        return
    
    template_rows_sorted = sorted(template_rows, key=lambda t: t.get("name", "Unnamed Template").lower())
    
    print(f"\n  Available Gateway Templates ({len(template_rows_sorted)}):")
    for idx, template in enumerate(template_rows_sorted, start=1):
        template_name = template.get("name", "Unnamed Template")
        template_id = template.get("id", "")
        print(f"   [{idx}] {template_name}")
    
    # Step 3: Template selection
    print("\n  Select source template to clone:")
    selection_input = input("  Template number (or 'cancel'): ").strip().lower()
    
    if selection_input == "cancel":
        print(" Operation cancelled.")
        logging.info("Menu #111 cancelled by user at template selection")
        return
    
    try:
        selected_idx = int(selection_input) - 1
        if selected_idx < 0 or selected_idx >= len(template_rows_sorted):
            print(" Invalid selection.")
            return
        source_template = template_rows_sorted[selected_idx]
        source_template_id = source_template.get("id", "")
        source_template_name = source_template.get("name", "")
    except ValueError:
        print(" Invalid input.")
        return
    
    print(f"\n  Selected source template: {source_template_name}")
    
    # Step 4: Fetch source template configuration
    print("\n  Step 3: Fetching source template configuration...")
    try:
        template_resp = mistapi.api.v1.orgs.gatewaytemplates.getOrgGatewayTemplate(
            apisession,
            org_id,
            source_template_id
        )
        source_config = template_resp.data if hasattr(template_resp, 'data') else {}
        
        if not isinstance(source_config, dict):
            print(" Error: Invalid template configuration retrieved.")
            logging.error(f"Invalid config for template {source_template_name}")
            return
    except Exception as e:
        print(f" Error fetching source template: {e}")
        logging.error(f"Failed to fetch template {source_template_name}: {e}")
        return
    
    # Step 5: Calculate templates to create
    templates_to_create = []
    
    # Templates for states
    for state in sorted(unique_states):
        new_template_name = f"{source_template_name}_{state}"
        templates_to_create.append({
            "name": new_template_name,
            "location_type": "state",
            "location_value": state
        })
    
    # Templates for countries (sites without state)
    for country in sorted(unique_countries):
        new_template_name = f"{source_template_name}_{country}"
        templates_to_create.append({
            "name": new_template_name,
            "location_type": "country",
            "location_value": country
        })
    
    print(f"\n  Step 4: Preview - {len(templates_to_create)} templates will be created:")
    for template_info in templates_to_create:
        print(f"   - {template_info['name']} (for {template_info['location_type']}: {template_info['location_value']})")
    
    # Count site assignments
    site_assignments = []
    for site in sites_with_location:
        if site["state"]:
            target_template_name = f"{source_template_name}_{site['state']}"
        elif site["country"]:
            target_template_name = f"{source_template_name}_{site['country']}"
        else:
            continue
        
        site_assignments.append({
            "site_id": site["id"],
            "site_name": site["name"],
            "target_template_name": target_template_name,
            "current_template_id": site["current_template_id"]
        })
    
    print(f"\n  {len(site_assignments)} sites will be assigned to templates")
    
    # Confirmation
    print(f"\n  {'=' * 70}")
    print(f"  !? CRITICAL: This will create {len(templates_to_create)} new templates")
    print(f"  !? and modify {len(site_assignments)} site template assignments")
    print(f"  !? Type 'CLONE' (all caps) to proceed or anything else to cancel")
    print(f"  {'=' * 70}")
    
    confirmation = input("\n  Confirmation: ").strip()
    if confirmation != "CLONE":
        print(" Operation cancelled.")
        logging.info("Menu #111 cancelled by user at final confirmation")
        return
    
    # Step 6: Create templates (check if exists first)
    print("\n  Step 5: Creating templates...")
    template_creation_results = []
    created_template_map = {}  # name -> id mapping
    
    # First, get existing templates to check for duplicates
    try:
        existing_templates_resp = mistapi.api.v1.orgs.gatewaytemplates.listOrgGatewayTemplates(
            apisession,
            org_id,
            limit=1000
        )
        existing_templates = mistapi.get_all(response=existing_templates_resp, mist_session=apisession)
        existing_template_names = {t.get("name"): t.get("id") for t in existing_templates if t.get("name")}
    except Exception as e:
        print(f" Error fetching existing templates: {e}")
        logging.error(f"Failed to fetch existing templates: {e}")
        return
    
    for template_info in tqdm(templates_to_create, desc="Creating templates", unit="template"):
        new_template_name = template_info["name"]
        
        result = {
            "template_name": new_template_name,
            "location_type": template_info["location_type"],
            "location_value": template_info["location_value"],
            "status": "",
            "template_id": "",
            "error": ""
        }
        
        # Check if template already exists
        if new_template_name in existing_template_names:
            result["status"] = "SKIPPED"
            result["template_id"] = existing_template_names[new_template_name]
            result["error"] = "Template already exists"
            created_template_map[new_template_name] = existing_template_names[new_template_name]
            logging.info(f"Template {new_template_name} already exists, skipping")
        else:
            try:
                # Create new template config (copy from source, update name)
                new_config = dict(source_config)
                new_config["name"] = new_template_name
                
                # Remove fields that shouldn't be copied
                for field in ["id", "org_id", "created_time", "modified_time"]:
                    new_config.pop(field, None)
                
                # Create template
                create_resp = mistapi.api.v1.orgs.gatewaytemplates.createOrgGatewayTemplate(
                    apisession,
                    org_id,
                    body=new_config
                )
                
                if create_resp.status_code == 200:
                    new_template_id = create_resp.data.get("id") if hasattr(create_resp, 'data') else ""
                    result["status"] = "CREATED"
                    result["template_id"] = new_template_id
                    created_template_map[new_template_name] = new_template_id
                    logging.info(f"Created template {new_template_name} (ID: {new_template_id})")
                else:
                    result["status"] = "FAILED"
                    result["error"] = f"API returned status {create_resp.status_code}"
                    logging.error(f"Failed to create template {new_template_name}: status {create_resp.status_code}")
            except Exception as e:
                result["status"] = "ERROR"
                result["error"] = str(e)
                logging.error(f"Error creating template {new_template_name}: {e}")
        
        template_creation_results.append(result)
    
    # Step 7: Assign sites to templates
    print("\n  Step 6: Assigning sites to templates...")
    site_assignment_results = []
    
    for assignment in tqdm(site_assignments, desc="Assigning sites", unit="site"):
        site_id = assignment["site_id"]
        site_name = assignment["site_name"]
        target_template_name = assignment["target_template_name"]
        current_template_id = assignment["current_template_id"]
        
        result = {
            "site_name": site_name,
            "site_id": site_id,
            "target_template_name": target_template_name,
            "status": "",
            "error": ""
        }
        
        # Get target template ID
        target_template_id = created_template_map.get(target_template_name)
        
        if not target_template_id:
            result["status"] = "SKIPPED"
            result["error"] = "Target template not found or failed to create"
            logging.warning(f"Cannot assign site {site_name}: template {target_template_name} not available")
        elif current_template_id == target_template_id:
            result["status"] = "SKIPPED"
            result["error"] = "Site already assigned to this template"
            logging.info(f"Site {site_name} already assigned to {target_template_name}")
        else:
            try:
                # Update site to use new template
                site_update = {
                    "gatewaytemplate_id": target_template_id
                }
                
                update_resp = mistapi.api.v1.sites.sites.updateSiteInfo(
                    apisession,
                    site_id,
                    body=site_update
                )
                
                if update_resp.status_code == 200:
                    result["status"] = "ASSIGNED"
                    logging.info(f"Assigned site {site_name} to template {target_template_name}")
                else:
                    result["status"] = "FAILED"
                    result["error"] = f"API returned status {update_resp.status_code}"
                    logging.error(f"Failed to assign site {site_name}: status {update_resp.status_code}")
            except Exception as e:
                result["status"] = "ERROR"
                result["error"] = str(e)
                logging.error(f"Error assigning site {site_name}: {e}")
        
        site_assignment_results.append(result)
    
    # Step 8: Generate audit reports
    print("\n  Step 7: Generating audit reports...")
    
    template_output_file = "GatewayTemplate_Clone_By_State_Country_Audit.csv"
    DataExporter.save_data_to_output(template_creation_results, template_output_file)
    
    site_output_file = "Site_Template_Assignment_By_State_Country_Audit.csv"
    DataExporter.save_data_to_output(site_assignment_results, site_output_file)
    
    # Summary
    templates_created = sum(1 for r in template_creation_results if r["status"] == "CREATED")
    templates_skipped = sum(1 for r in template_creation_results if r["status"] == "SKIPPED")
    templates_failed = sum(1 for r in template_creation_results if r["status"] in ["FAILED", "ERROR"])
    
    sites_assigned = sum(1 for r in site_assignment_results if r["status"] == "ASSIGNED")
    sites_skipped = sum(1 for r in site_assignment_results if r["status"] == "SKIPPED")
    sites_failed = sum(1 for r in site_assignment_results if r["status"] in ["FAILED", "ERROR"])
    
    print(f"\n  Gateway Template Cloning by State/Country Complete!")
    print(f"=" * 70)
    print(f"  TEMPLATE CREATION:")
    print(f"    Created: {templates_created}")
    print(f"    Skipped (already exist): {templates_skipped}")
    print(f"    Failed: {templates_failed}")
    print(f"\n  SITE ASSIGNMENTS:")
    print(f"    Assigned: {sites_assigned}")
    print(f"    Skipped (already assigned): {sites_skipped}")
    print(f"    Failed: {sites_failed}")
    print(f"\n  AUDIT REPORTS:")
    print(f"    Template creation: {template_output_file}")
    print(f"    Site assignments: {site_output_file}")
    print(f"=" * 70)
    
    if templates_created > 0 or sites_assigned > 0:
        print(f"\n  !? {templates_created} new templates created from {source_template_name}")
        print(f"  !? {sites_assigned} sites assigned to state/country-specific templates")
        print(f"  !? Verify configurations in Mist portal before proceeding")
    
    if templates_failed > 0 or sites_failed > 0:
        print(f"\n  !? WARNING: {templates_failed} template creations and {sites_failed} site assignments failed")
        print(f"  !? Check audit reports for details")
    
    logging.warning(f"Menu #111 DESTRUCTIVE operation complete: {templates_created} templates created, {sites_assigned} sites assigned")


menu_actions = {
    # ==============================
    # SYSTEM OPERATIONS
    # ==============================
    "0": (lambda: sys.exit(0), "Exit MistHelper"),
    
    # ==============================
    # READ-ONLY OPERATIONS
    # ==============================
    
    # > Setup & Core Logs
    "1": (export_open_org_alarms_to_csv, "Export all organization alarms from the past day"),
    "2": (export_recent_device_events_to_csv, "Export all device events from the past 24 hours"),
    "3": (lambda: export_audit_logs_to_csv(full_history=False), "Export audit logs for the organization (last 24 hours)"),
    "4": (lambda fast=False: export_gateway_management_ips_to_csv(fast=fast), "Export gateway management overlay IPs grouped by template association"),
    
    # > WebSocket Device Commands
    "5": (WebSocketCommands.show_mac_table, "Show MAC table on switch device via WebSocket (Layer 2 switching table)"),
    "6": (WebSocketCommands.show_forwarding_table, "Show forwarding table on gateway device via WebSocket (Layer 3 routing table)"),
    "7": (WebSocketCommands.show_routing_table, "Show routing table on switches via WebSocket (Switch L3 routing - BGP/OSPF/Static)"),
    "8": (WebSocketCommands.show_ssr_routes, "Show SSR/SRX routing table via dedicated API (128T/SRX gateways - Advanced BGP analysis)"),

    # > Packet Capture Operations
    "9": (lambda: PacketCaptureManager(apisession, get_cached_or_prompted_org_id()).start_site_packet_capture(), "Start Site Packet Capture - Wireless/Wired/Gateway/Scan captures with WebSocket streaming"),
    "10": (lambda: PacketCaptureManager(apisession, get_cached_or_prompted_org_id()).start_org_packet_capture(), "Start Organization Packet Capture - MxEdge captures for org-level Mist Edges only"),

    # Organization-Level Exports
    "11": (export_all_sites_to_csv, "Export a list of all sites in the organization"),
    "12": (export_device_inventory_to_csv, "Export the full inventory of devices in the organization"),
    "13": (export_device_stats_to_csv, "Export statistics for all devices in the organization"),
    "14": (export_device_port_stats_to_csv, "Export port-level statistics for switches and gateways"),
    "15": (export_vpn_peer_stats_to_csv, "Export VPN peer path statistics for the organization"),

    # Gateway & Site-Wide Exports
    # Direct reference (removed lambda) so systematic test harness can introspect 'fast' parameter
    "16": (export_gateway_synthetic_tests_to_csv, "Export synthetic test results for all gateways"),
    "17": (export_all_devices_to_csv, "Export a list of all devices in the organization"),
    "18": (export_site_settings_to_csv, "Export configuration settings for all sites"),
    "19": (export_gateway_test_results_by_site_to_csv, "Export all synthetic test results (including speed tests) for gateways"),

    # > Location-Enriched Exports
    "20": (export_sites_with_location_to_csv, "Export a list of sites with location and timezone info"),
    "21": (export_gateways_with_site_info_to_csv, "Export a list of gateways with associated site and address info"),
    "22": (export_devices_with_site_info_to_csv, "Export a list of all devices with associated site and address info"),
    "23": (lambda: (export_current_guest_users_to_csv(), export_historical_guest_users_to_csv()),"Export all current guest users and last 7 days of historical guests to CSV"),
    "24": (export_switch_vc_stats_to_csv, "Export all switch virtual chassis (VC/stacking) stats to CSV"),
    "25": (export_combined_inventory_with_site_info, "Export combined inventory with site and address info by calendar week"),
    "26": (export_gateway_templates_to_csv, "Export gateway templates from the organization"),
    "27": (export_all_sites_list_to_csv, "Export all sites using the 'list' sites API endpoint (to SiteList_ListAPI.csv, only if not already present)"),
    "28": (lambda fast=False: export_gateways_with_wan_overrides_to_csv(fast=fast), "Find gateway ports overridden from template (outliers for compliance correction)"),
    
    # Site-Specific Data Exports
    "29": (export_site_port_stats_to_csv, "Export port statistics for a selected site"),
    "30": (export_site_clients_to_csv, "Export client statistics for a selected site"),
    "31": (export_site_devices_to_csv, "Export device list for a selected site"),
    "32": (export_site_device_stats_to_csv, "Export device statistics for a selected site"),
    "33": (export_site_device_virtual_chassis_to_csv, "Export virtual chassis information for a selected switch device"),
    "34": (export_site_wifi_clients_to_csv, "Export currently connected WiFi clients and session data for a selected site to SiteWiFiClients.CSV"),
    
    # Organization Template Exports
    "35": (export_organization_templates_to_csv, "Export all organization templates (gateway, network, RF, site, AP)"),
    "36": (export_org_network_templates_to_csv, "Export network template information for the organization"),
    "37": (export_org_rf_templates_to_csv, "Export RF template information for the organization"),
    "38": (export_org_ap_templates_to_csv, "Export AP template information for the organization"),
    "39": (export_org_switch_templates_to_csv, "Export switch template information for the organization"),
    
    # Organization Statistics & Analytics  
    "40": (export_org_wireless_clients_to_csv, "Export wireless client statistics for the organization"),
    "41": (export_org_wired_clients_to_csv, "Export wired client statistics for the organization"),
    
    # Security & Monitoring
    "42": (export_org_security_events_to_csv, "Export security events for the organization"),
    "43": (export_org_rogue_clients_to_csv, "Export rogue client detections for the organization"),
    "44": (export_org_rogue_aps_to_csv, "Export rogue AP detections for the organization"),
    
    # Configuration & Management (Read-Only)
    "45": (export_org_licenses_to_csv, "Export license information for the organization"),
    "46": (export_org_psks_to_csv, "Export PSK (Pre-Shared Key) information for the organization"),
    "47": (export_org_webhooks_to_csv, "Export webhook configuration for the organization"),
    "48": (export_org_wlans_to_csv, "Export WLAN configuration for the organization"),
    "49": (export_site_wlans_to_csv, "Export WLAN configuration for a selected site"),
    "50": (export_site_beacons_to_csv, "Export beacon information for a selected site"),
    "51": (export_site_maps_to_csv, "Export map information for a selected site"),
    "52": (export_site_zones_to_csv, "Export zone information for a selected site"),
    "53": (export_site_insights_to_csv, "Export SLE (Service Level Experience) metrics insights for a selected site"),
    
    # ==============================
    # GATEWAY TEMPLATE VARIABLE OPERATIONS
    # ==============================
    "103": (set_wan2_interface_site_variable, "Set WAN2 Interface Site Variable - Configure 'wan2_interface' site variable for template-based WAN migration (Reports sites with ge-0/0/1 overrides)"),
    "104": (lambda fast=False, dry_run=False: update_gateway_templates_wan2_variable(fast=fast, dry_run=dry_run), " DESTRUCTIVE: Update Gateway Templates to Use WAN2 Variable - Replace hardcoded 'ge-0/0/1' references with {{wan2_interface}} variable (Requires uppercase 'MIGRATE' confirmation, supports --dry-run)"),
    "105": (extract_gateway_template_configuration, "Extract Gateway Template Configuration (DIA_Pico, Picocell) - Save specific configs to JSON for replication"),
    "106": (apply_gateway_template_configuration, " DESTRUCTIVE: Apply Gateway Template Configuration - Replicate extracted configs to other templates (Requires uppercase 'APPLY' confirmation)"),
    
    "102": (manage_wlan_radius_auth_timers, "Manage WLAN RADIUS Authentication Timers - Configure auth_servers_timeout, auth_servers_retries, auth_server_selection, and fast_dot1x_timers for site or template WLANs"),
    
    # Organization Management (Read-Only)
    "54": (export_org_api_tokens_to_csv, "Export API token information for the organization"),
    "55": (export_org_admins_to_csv, "Export administrator information for the organization"),
    "56": (export_org_msp_to_csv, "Export MSP (Managed Service Provider) information for the organization"),
    "57": (export_org_sso_to_csv, "Export SSO (Single Sign-On) information for the organization"),
    "58": (export_org_usage_to_csv, "Export license usage information for the organization"),
    "59": (export_org_mx_edges_to_csv, "Export MX Edge information for the organization"),
    
    # Status & Monitoring
    "60": (check_firmware_upgrade_status_direct, "Check current firmware upgrade status across organization with detailed progress monitoring and export to CSV"),
    "61": (lambda fast=False, address_check=False, debug=False, skip_ssl_verify=False: compare_inventory_with_csv(fast=fast, address_check=address_check, debug=debug, skip_ssl_verify=skip_ssl_verify), "Compare inventory data with external CSV file using configurable address similarity threshold (ADDRESS_MATCH_THRESHOLD in .env)"),
    "62": (poll_marvis_actions, "Interactive Marvis (VNA) AI troubleshooting - guided client, device, and network analysis"),
    
    # Work In Progress Features (Read-Only)
    "63": (export_all_org_device_events_52w_to_csv, "WIP Export all org device events from the last 52 weeks"),
    "64": (lambda: export_audit_logs_to_csv(full_history=True, duration="52w"), "WIP Export ALL audit logs for the organization (last 52 weeks)"),
    "65": (export_gateway_device_configs_to_csv, "WIP Export configuration details for all gateway devices across all sites"),
    
    
    # ==============================
    # UNSAFE/INTERACTIVE OPERATIONS
    # ==============================
    
    # > Site Selection & Interactive Tools
    "70": (prompt_and_log_site_selection, "Select a site (used by other functions)"),
    "71": (interactive_display_site_inventory, "View device inventory for a selected site"),
    "72": (interactive_display_device_stats, "View statistics for a selected device at a site"),
    "73": (interactive_display_device_tests, "View synthetic test stats for a selected gateway device"),
    "74": (interactive_display_device_config, "View configuration details for a selected device"),
    
    # > Continuous Operations & Monitoring
    "75": (lambda debug=False: loop_refresh_core_datasets(delay=None, debug=debug), "Loop refresh of core datasets (site list, inventory, stats, ports, VPN) Stop with CTRL+C or create 'stop_loop.txt'"),
    "76": (continuous_data_collection_loop, "Run continuous data collection loop (5 core API calls with rate limiting)"),
    
    # > File Processing & Support Operations
    "77": (SFPTransceiverDataProcessor.merge_transceiver_data, "Process and merge CSV files of SFP Module locations into a single CSV file"),
    "78": (generate_support_package, "Generate support package for each site"),
    
    # > CLI & WebSocket Operations
    "79": (launch_cli_shell, "Interactively execute a CLI command on a gateway or switch (exit with ~)"),
    "80": (run_arp_via_websocket, "Run ARP command on an AP and receive output via WebSocket"),

    # ! DESTRUCTIVE OPERATIONS - USE WITH EXTREME CAUTION
    "90": (bulk_upgrade_ap_firmware_by_site, " DESTRUCTIVE: Advanced AP firmware upgrade with mode selection - upgrade by site list/selection or by Gateway Template assignment"),
    "91": (reboot_devices_by_gateway_template_list, " DESTRUCTIVE: Reboot all devices associated with templates listed in GatewayTemplateRebootList.CSV and log results"),
    "92": (convert_virtual_chassis_to_virtual_mac, " DESTRUCTIVE: Convert a virtual chassis switch to virtual MAC (interactive selection)(WIP)"),
    "93": (convert_virtual_chassis_by_site_list, " DESTRUCTIVE: Convert all virtual chassis switches in sites listed in VCConvert.CSV (bulk operation)"),
    "94": (check_virtual_chassis_conversion_status, "Check virtual chassis to virtual MAC conversion status for all switches"),
    "95": (lambda fast=False: export_gateway_device_stats_to_csv_with_freshness_check(fast=fast), "Export detailed device statistics for all gateways (with freshness check)"),
    "96": (export_gateways_with_wan_port_conflicts_to_csv, "Check and export gateways with duplicate WAN port IP addresses (0/0/0, 0/0/1, 0/0/2)"),
    "97": (ssh_runner_interactive, "Enhanced SSH Command Runner - Execute commands on remote network devices via SSH"),
    "98": (ssh_runner_by_gateway_template, "SSH Runner - Target gateways by template name (online gateways with management IPs only)"),

    # ==============================
    # INSIGHTS API OPERATIONS - Organization & Site Analytics
    # ==============================
    "66": (export_org_sle_metrics_to_csv, "Export Organization SLE Metrics (Service Level Experience)"),
    "67": (export_org_sites_sle_summary_to_csv, "Export SLE summary metrics for all sites in the organization"),
    "68": (export_site_insight_metrics_to_csv, "Export general insight metrics for a selected site"),
    "69": (export_site_client_insights_to_csv, "Export client-specific insight metrics for a selected site"),
    "81": (export_site_device_insights_to_csv, "Export device-specific insight metrics for a selected site"),
    "82": (export_all_const_definitions_to_csv, "Export all available const definitions from the Mist API (comprehensive endpoint coverage)"),
    "83": (export_org_insight_metrics_to_csv, "Export Organization Insight Metrics (comprehensive operational insights)"),
    "84": (export_site_anomaly_metrics_to_csv, "Export Site Anomaly Events (dynamic discovery of all anomaly-related metrics from Mist API)"),
    "85": (export_site_device_anomaly_to_csv, "Export Site Device Anomaly Events (device-specific anomaly detection)"),
    "86": (export_site_client_anomaly_to_csv, "Export Site Client Anomaly Events (client-specific anomaly detection: connectivity, roaming, throughput)"),
    "87": (WebSocketCommands.ping_device, "WebSocket Device Ping - Execute ping command on device via WebSocket stream (real-time output)"),
    "88": (WebSocketCommands.arp_device, "WebSocket Device ARP - Execute ARP command on device via WebSocket stream (real-time output)"),
    "89": (WebSocketCommands.service_ping_device, "WebSocket Service Ping - Execute service-specific ping on SSR gateways via WebSocket stream (real-time output)"),

    # ==============================
    # POST API OPERATIONS - Device Commands (Starting at 100)
    # ==============================
    
    # Device Network Operations removed (options 100, 101)
    
    # ==============================
    # SWITCH FIRMWARE OPERATIONS
    # ==============================
    "99": (bulk_upgrade_switch_firmware_by_site, " DESTRUCTIVE: Advanced Switch firmware upgrade with mode selection - upgrade by site list/selection or by Gateway Template assignment"),
    
    # ==============================
    # SSR FIRMWARE OPERATIONS
    # ==============================
    "100": (lambda: FirmwareManager(apisession, get_cached_or_prompted_org_id()).execute_ssr_firmware_upgrade_with_mode_selection(), " DESTRUCTIVE: Advanced SSR firmware upgrade with mode selection - upgrade by site list/selection or by Gateway Template assignment"),
    
    # ==============================
    # TERMINAL USER INTERFACE MODE
    # ==============================
    "101": (lambda: _launch_tui_from_menu(), "Launch Terminal User Interface (TUI) mode - Visual navigation of Mist API library with interactive exploration"),
    
    # ==============================
    # TEST DATA GENERATION
    # ==============================
    "107": (create_test_sites_from_csv, " DESTRUCTIVE: Create 137 test sites from NorthAmericanTestSites.csv - Real landmarks across 13 North American countries (Requires uppercase 'CREATE' confirmation)"),
    "108": (create_country_rf_templates_and_assign, " DESTRUCTIVE: Create country-specific RF templates and assign sites to matching templates (Requires uppercase 'CREATE' confirmation)"),
    "109": (create_ap_model_device_profiles, " DESTRUCTIVE: Scan org for AP models and create Device Profile per model with inherit/auto settings (Requires uppercase 'CREATE' confirmation)"),
    "110": (assign_aps_to_matching_device_profiles, " DESTRUCTIVE: Assign APs to Device Profiles matching their model type (AP-{model}) - Skips APs without matching profiles (Requires uppercase 'ASSIGN' confirmation)"),
    "111": (clone_gateway_templates_by_state_and_country, " DESTRUCTIVE: Clone Gateway Template by State and Country - Create state/country-specific templates and assign sites (Requires uppercase 'CLONE' confirmation)"),
    
    # ==============================
    # MAPS MANAGER
    # ==============================
    "112": (lambda: MapsManager(apisession, get_cached_or_prompted_org_id()).run_interactive_menu(), "Maps Manager - Interactive site floorplan and map operations (sub-menu)"),
}

def _launch_tui_from_menu():
    """Launch Terminal User Interface mode from interactive menu.
    
    This function replicates the --tui CLI flag behavior but returns to menu
    instead of exiting. Provides an interactive, keyboard-driven API browser.
    
    SECURITY: Read-only browser mode with safe API exploration
    """
    logging.info("TUI_MODE: Starting Terminal User Interface mode from menu")
    print("\n>> Terminal User Interface mode activated")
    print(">> Use arrow keys to navigate, Enter to select, Q to quit")
    
    # Initialize Mist API session for TUI mode if needed
    global apisession
    if not apisession:
        print(">> Initializing Mist API session...")
        if not initialize_mist_session():
            print("[ERROR] Failed to initialize Mist API session")
            logging.error("TUI_MODE: Could not initialize API session")
            return
        print(">> API session initialized successfully")
    
    # Remove console handler during TUI mode to prevent log messages from interfering with Rich display
    root_logger = logging.getLogger()
    console_handlers = [h for h in root_logger.handlers if isinstance(h, logging.StreamHandler) and not isinstance(h, logging.FileHandler)]
    for handler in console_handlers:
        root_logger.removeHandler(handler)
        logging.debug("TUI_MODE: Removed console handler to prevent interference with Rich TUI")
    
    try:
        # Get debug mode from global args if available
        debug_mode = globals().get('args', type('obj', (), {'debug': False})()).debug if hasattr(globals().get('args', type('obj', (), {'debug': False})()), 'debug') else False
        tui = MistHelperTUI(debug_mode=debug_mode)
        # Pass the global apisession to TUI for API call execution
        tui.apisession = apisession
        if debug_mode:
            logging.debug("TUI_MODE: Debug mode is ACTIVE - enhanced logging enabled")
        tui.run()
    except KeyboardInterrupt:
        logging.info("TUI_MODE: User interrupted with Ctrl+C")
        print("\n[EXIT] TUI mode stopped by user")
    except Exception as error:
        logging.error(f"TUI_MODE: Fatal error - {error}", exc_info=True)
        print(f"\n[ERROR] TUI mode crashed: {error}")
    finally:
        # Restore console handler after TUI mode exits
        for handler in console_handlers:
            root_logger.addHandler(handler)
        logging.debug("TUI_MODE: Restored console handler after TUI exit")
    
    # Get debug mode from global args if available for final timestamp
    debug_mode = globals().get('args', type('obj', (), {'debug': False})()).debug if hasattr(globals().get('args', type('obj', (), {'debug': False})()), 'debug') else False
    if debug_mode:
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]
        logging.debug(f"TUI_DEBUG: [{timestamp}] TUI_MODE function completed - returning to caller")
    
    logging.info("TUI_MODE: TUI mode completed successfully")
    print("\n>> Returned from TUI mode to main menu")

def run_systematic_test():
    """
    Run systematic test of all safe menu options.
    
    This function cycles through all menu options that are safe for automated testing:
    - Only GET operations (no POST/PUT/DELETE)
    - No interactive functions requiring user input
    - No websocket operations
    - No reboot or destructive operations
    - No continuous loops
    
    Unsafe operations are skipped with explanatory messages.
    
    Returns:
        bool: True if all tests passed, False if any failed
    """
    start_time = time.time()
    print(" Starting systematic test of MistHelper menu options...")
    print("  Note: This will skip interactive, websocket, POST, and destructive operations")
    print(f"! Test started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    # Define unsafe menu options that should be skipped during testing
    unsafe_options = {
        # Control operations that should not be tested
        "0": "Exit option - would terminate the test prematurely",
        
        # Resource-intensive operations that consistently fail or take excessive time
        "14": "Port-level statistics - extremely resource intensive (8+ hours, often fails)",
        "18": "Site configurations - hits API rate limits after 7+ hours",
        
        # WebSocket operations requiring interactive site and device selection
        "5": "WebSocket ping - requires interactive site and device selection",
        "6": "WebSocket traceroute - requires interactive site and device selection",
        "7": "WebSocket release DHCP - requires interactive site and device selection",
        "8": "WebSocket cable test - requires interactive site and device selection",
        "87": "WebSocket bounce port - requires interactive site and device selection",
        "88": "WebSocket ARP - requires interactive site and device selection",
        "89": "WebSocket service ping - requires interactive site and device selection",
        
        # Interactive operations requiring user input
        "9": "Packet capture - requires interactive configuration and site selection",
        "10": "Packet capture - requires interactive configuration and MxEdge ID",
        "60": "Firmware upgrade status - requires interactive scope selection",
        "61": "CSV comparison - requires interactive file selection",
        "62": "Marvis troubleshooting - requires interactive option selection",
        "101": "Interactive TUI API browser - keyboard navigation required",
        "102": "WLAN RADIUS timer management - requires interactive site selection",
        
        # Site-specific operations requiring site selection
        "29": "Requires site selection",
        "30": "Requires site selection", 
        "31": "Requires site selection",
        "32": "Requires site selection",
        "33": "Requires site and device selection",
        "34": "Requires site selection",
        "49": "Requires site selection",
        "50": "Requires site selection", 
        "51": "Requires site selection",
        "52": "Requires site selection",
        "53": "Requires site selection",
        
        # WIP (Work in Progress) - may be unstable
        "63": "WIP (Work in Progress) - may be unstable", 
        "64": "WIP (Work in Progress) - may be unstable",
        "65": "WIP (Work in Progress) - may be unstable",
        
        # Site-specific operations requiring site selection
        "68": "Requires site selection",
        "69": "Requires site selection",
        "84": "Requires site selection",
        "85": "Requires site and device selection", 
        "86": "Requires site and client selection", 
        
        # Interactive operations
        "70": "Interactive site selection",
        "71": "Interactive site inventory browser",
        "72": "Interactive device stats viewer", 
        "73": "Interactive device tests viewer",
        "74": "Interactive device config viewer",
        
        # Continuous operations
        "75": "Continuous loop operation",
        "76": "Continuous data collection loop",
        
        # File processing and support operations
        "77": "File processing operation - potentially resource intensive",
        "78": "Support package generation - potentially resource intensive",
        
        # CLI and WebSocket operations
        "79": "Interactive CLI shell session",
        "80": "WebSocket operation",
        "81": "Shell command execution via WebSocket",
        
        # SSH operations requiring interactive host/command input
        "97": "Enhanced SSH Command Runner - requires interactive host and command input",
        "98": "SSH Runner by gateway template - requires interactive template and command input",
        
        # DESTRUCTIVE operations - absolutely skip
        "90": "DESTRUCTIVE: AP firmware upgrade operation",
        "91": "DESTRUCTIVE: Device reboot operation", 
        "92": "DESTRUCTIVE: Virtual chassis conversion - WIP",
        "93": "DESTRUCTIVE: Virtual chassis conversion - bulk operation",
        "99": "DESTRUCTIVE: Switch firmware upgrade operation",
        "100": "DESTRUCTIVE: SSR firmware upgrade operation",
        
        # Gateway template configuration operations
        "103": "Requires interactive site selection for WAN2 variable configuration",
        "104": "DESTRUCTIVE: Updates gateway templates with WAN2 variable - requires uppercase confirmation",
        "105": "Requires interactive template selection for configuration extraction",
        "106": "DESTRUCTIVE: Applies gateway template configuration - requires uppercase confirmation",
        
        # Test data generation operations - all DESTRUCTIVE
        "107": "DESTRUCTIVE: Creates 137 test sites from CSV - requires uppercase confirmation",
        "108": "DESTRUCTIVE: Creates country-specific RF templates - requires uppercase confirmation",
        "109": "DESTRUCTIVE: Creates device profiles for AP models - requires uppercase confirmation",
        "110": "DESTRUCTIVE: Assigns APs to device profiles - requires uppercase confirmation",
        "111": "DESTRUCTIVE: Clones gateway templates by state/country - requires uppercase confirmation"
    }
    
    # Get all available menu options
    all_options = sorted(menu_actions.keys(), key=lambda x: float(x.replace('a', '.1')))
    
    # Define optimized test order based on execution time analysis (shortest to longest)
    # This ordering minimizes total test time by running quick tests first
    optimized_test_order = [
        # Fast tests (~0.6-3.5 seconds)
        "3",   # Audit Logs (~0.6s)
        "17",  # All Devices List (~3s)
        "11",  # All Sites List (~3.5s)
        
        # Medium tests (~18-30 seconds)
        "12",  # Device Inventory (~18s)
        "66",  # Organization SLE Metrics (new)
        "67",  # Organization Sites SLE Summary (new)
        "1",   # Organization Alarms (~30s)
        
        # Slower tests (~1-5 minutes)
        "13",  # Device Stats (~97s)
        "15",  # VPN Peer Stats (~257s)
        
        # Slow tests (~8+ minutes)
        "2",   # Device Events (~485s)
        "16",  # Gateway Synthetic Tests (~1115s)
        
        # Note: Options 14 (Port-level Statistics) and 18 (Site Configurations) 
        # have been moved to unsafe_options due to excessive resource consumption
    ]
    
    # Create optimized safe options list, preserving any additional options not in the predefined order
    safe_options_set = set(opt for opt in all_options if opt not in unsafe_options)
    safe_options = []
    
    # Add options in optimized order first
    for opt in optimized_test_order:
        if opt in safe_options_set:
            safe_options.append(opt)
            safe_options_set.remove(opt)
    
    # Add any remaining safe options at the end (for future additions)
    safe_options.extend(sorted(safe_options_set, key=lambda x: float(x.replace('a', '.1'))))
    
    print(f"! Found {len(all_options)} total menu options")
    print(f"! {len(safe_options)} safe options will be tested")
    print(f"!  {len(unsafe_options)} unsafe options will be skipped")
    print()
    
    # Show which options will be skipped and why
    print(" Skipping unsafe operations:")
    for opt in sorted(unsafe_options.keys(), key=lambda x: float(x.replace('a', '.1'))):
        if opt in menu_actions:
            _, description = menu_actions[opt]
            reason = unsafe_options[opt]
            print(f"   {opt:2}: {description[:60]}... (Reason: {reason})")
    print()
    
    # Test safe options
    print(" Testing safe operations:")
    success_count = 0
    error_count = 0
    
    global org_id
    if not org_id:
        org_id = get_cached_or_prompted_org_id()
    
    for i, option in enumerate(safe_options, 1):
        func, description = menu_actions[option]
        print(f"   [{i:2}/{len(safe_options)}] Testing option {option:2}: {description[:60]}...")
        # Determine if fast mode is globally enabled and if function supports it
        fast_enabled = False
        try:
            fast_enabled = bool(globals().get('FAST_MODE_ENABLED', False))
        except Exception:
            fast_enabled = False

        # Defensive fallback: if global not set but original CLI args indicate fast, force enable
        if not fast_enabled:
            cli_args = globals().get('args') if 'args' in globals() else None
            try:
                if cli_args and getattr(cli_args, 'fast', False):
                    fast_enabled = True
                    logging.debug(f"SYSTEMATIC_TEST: Forcing fast_enabled=True for option {option} based on CLI args.fast")
            except Exception:
                pass

        # Introspect signature to see if 'fast' is accepted
        supports_fast = False
        try:
            sig = inspect.signature(func)
            supports_fast = 'fast' in sig.parameters
        except Exception:
            supports_fast = False

        # Log harness invocation detail
        logging.info(
            f"SYSTEMATIC_TEST: INVOKE option={option} fast_supported={supports_fast} fast_enabled={fast_enabled} test_mode=True description='{description}'"
        )

        # Build kwargs dynamically
        invoke_kwargs = {}
        if supports_fast and fast_enabled:
            invoke_kwargs['fast'] = True
        try:
            logging.info(f"SYSTEMATIC_TEST: Starting test of menu option {option} (fast_applied={invoke_kwargs.get('fast', False)})")
            func(**invoke_kwargs)
            print(f"   [SUCCESS] Option {option} completed successfully")
            success_count += 1
            logging.info(f"SYSTEMATIC_TEST: Successfully completed menu option {option}")
        except Exception as e:
            print(f"   [FAILED]  Option {option} failed: {str(e)[:100]}...")
            error_count += 1
            logging.error(f"SYSTEMATIC_TEST: Failed menu option {option}: {e}")
            
        # Small delay between tests to be respectful to the API
        time.sleep(1)
    
    # Summary
    total_time = time.time() - start_time
    print()
    print("=" * 80)
    print(" Systematic Test Summary:")
    print(f"   Successful operations: {success_count}")
    print(f"   Failed operations: {error_count}")
    print(f"   Skipped unsafe operations: {len(unsafe_options)}")
    print(f"   Total coverage: {success_count}/{len(all_options)} ({success_count/len(all_options)*100:.1f}%)")
    print(f"    Total execution time: {total_time:.2f} seconds")
    print(f"   Detailed logs in: script.log")
    
    if error_count == 0:
        print("   All tested operations completed successfully!")
        logging.info(f"SYSTEMATIC_TEST: All {success_count} tested operations completed successfully in {total_time:.2f}s")
        return True
    else:
        print(f"    {error_count} operations failed - check logs for details")
        logging.warning(f"SYSTEMATIC_TEST: {error_count} operations failed out of {len(safe_options)} tested")
        return False


def run_interactive_test():
    """
    Run systematic test of read-only interactive menu options.
    
    This function tests menu options that are:
    - Read-only (GET operations only)
    - Require interactive site/device/client selection
    - Safe for automated testing (no destructive operations)
    
    This complements run_systematic_test() by covering interactive operations
    that require user input for selection purposes.
    
    Returns:
        bool: True if all tests passed, False if any failed
    """
    start_time = time.time()
    print(" Starting interactive test of MistHelper menu options...")
    print("  Note: This tests read-only operations requiring site/device/client selection")
    print(f"! Test started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    # Define interactive read-only options that should be tested
    # These require user input but are safe (no destructive operations)
    interactive_read_only_options = {
        "29": "Export port statistics for a selected site",
        "30": "Export client statistics for a selected site",
        "31": "Export device list for a selected site",
        "32": "Export device statistics for a selected site",
        "33": "Export virtual chassis information for a selected switch device",
        "34": "Export currently connected WiFi clients for a selected site",
        "49": "Export WLAN configuration for a selected site",
        "50": "Export beacon information for a selected site",
        "51": "Export map information for a selected site",
        "52": "Export zone information for a selected site",
        "53": "Export SLE metrics insights for a selected site",
        "68": "Export general insight metrics for a selected site",
        "69": "Export client-specific insight metrics for a selected site",
        "70": "Select a site (used by other functions)",
        "71": "View device inventory for a selected site",
        "72": "View statistics for a selected device at a site",
        "73": "View synthetic test stats for a selected gateway device",
        "74": "View configuration details for a selected device",
        "81": "Export device-specific insight metrics for a selected site",
        "84": "Export Site Anomaly Events for a selected site",
        "85": "Export Site Device Anomaly Events for selected site and device",
        "86": "Export Site Client Anomaly Events for selected site and client",
    }
    
    # Define operations that are interactive but should be skipped
    # (destructive, websocket, continuous, or WIP)
    skip_interactive_options = {
        "5": "WebSocket operation - not suitable for automated testing",
        "6": "WebSocket operation - not suitable for automated testing",
        "7": "WebSocket operation - not suitable for automated testing",
        "8": "WebSocket operation - not suitable for automated testing",
        "9": "Packet capture - requires complex interactive configuration",
        "10": "Packet capture - requires complex interactive configuration",
        "60": "Firmware upgrade status - complex interactive scope selection",
        "62": "Marvis troubleshooting - requires interactive option selection",
        "75": "Continuous loop operation - not suitable for automated testing",
        "76": "Continuous data collection loop - not suitable for automated testing",
        "79": "Interactive CLI shell session - not suitable for automated testing",
        "80": "WebSocket operation - not suitable for automated testing",
        "87": "WebSocket operation - not suitable for automated testing",
        "88": "WebSocket operation - not suitable for automated testing",
        "89": "WebSocket operation - not suitable for automated testing",
        "97": "SSH Command Runner - requires interactive host and command input",
        "98": "SSH Runner - requires interactive template and command input",
        "101": "TUI mode - keyboard navigation required, not suitable for automated testing",
        "102": "WLAN RADIUS timer management - requires interactive site selection and configuration",
        "103": "WAN2 variable configuration - requires interactive site selection",
        "104": "DESTRUCTIVE: Updates gateway templates",
        "105": "Requires interactive template selection",
        "106": "DESTRUCTIVE: Applies gateway template configuration",
        "107": "DESTRUCTIVE: Creates test sites",
        "108": "DESTRUCTIVE: Creates RF templates",
        "109": "DESTRUCTIVE: Creates device profiles",
        "110": "DESTRUCTIVE: Assigns APs to device profiles",
        "111": "DESTRUCTIVE: Clones gateway templates by state/country",
        "90": "DESTRUCTIVE: AP firmware upgrade",
        "91": "DESTRUCTIVE: Device reboot",
        "92": "DESTRUCTIVE: Virtual chassis conversion",
        "93": "DESTRUCTIVE: Virtual chassis conversion bulk",
        "99": "DESTRUCTIVE: Switch firmware upgrade",
        "100": "DESTRUCTIVE: SSR firmware upgrade",
    }
    
    print(f"! Found {len(interactive_read_only_options)} interactive read-only options to test")
    print(f"! {len(skip_interactive_options)} interactive options will be skipped (destructive/websocket/continuous)")
    print()
    
    # Show which interactive options will be tested
    print(" Testing interactive read-only operations:")
    for opt in sorted(interactive_read_only_options.keys(), key=lambda x: int(x)):
        description = interactive_read_only_options[opt]
        print(f"   {opt:2}: {description}")
    print()
    
    # Show which interactive options will be skipped
    print(" Skipping unsafe/unsuitable interactive operations:")
    for opt in sorted(skip_interactive_options.keys(), key=lambda x: int(x)):
        reason = skip_interactive_options[opt]
        print(f"   {opt:2}: {reason}")
    print()
    
    # Test interactive options
    success_count = 0
    error_count = 0
    
    global org_id
    if not org_id:
        org_id = get_cached_or_prompted_org_id()
    
    # Get first available site_id for testing
    test_site_id = None
    try:
        print("   Fetching test site for interactive operations...")
        sites_response = mistapi.api.v1.orgs.sites.listOrgSites(apisession, org_id, limit=1)
        if sites_response.data and len(sites_response.data) > 0:
            test_site_id = sites_response.data[0]['id']
            test_site_name = sites_response.data[0].get('name', 'Unknown')
            print(f"   Using test site: {test_site_name} ({test_site_id})")
            logging.info(f"INTERACTIVE_TEST: Using test site_id={test_site_id} name={test_site_name}")
        else:
            print("[ERROR] No sites found in organization - cannot run interactive tests")
            logging.error("INTERACTIVE_TEST: No sites available for testing")
            return False
    except Exception as error:
        print(f"[ERROR] Failed to fetch test site: {error}")
        logging.error(f"INTERACTIVE_TEST: Failed to fetch test site: {error}")
        return False
    
    print()
    
    for i, option in enumerate(sorted(interactive_read_only_options.keys(), key=lambda x: int(x)), 1):
        func, description = menu_actions[option]
        print(f"   [{i:2}/{len(interactive_read_only_options)}] Testing option {option:2}: {description[:60]}...")
        
        logging.info(f"INTERACTIVE_TEST: Starting test of menu option {option} description='{description}'")
        
        try:
            # Most interactive functions accept site_id parameter
            # We'll use introspection to determine if the function accepts parameters
            sig = inspect.signature(func)
            invoke_kwargs = {}
            
            # Check what parameters the function accepts
            if 'site_id' in sig.parameters:
                invoke_kwargs['site_id'] = test_site_id
            
            # Invoke the function with appropriate parameters
            func(**invoke_kwargs)
            
            print(f"   [SUCCESS] Option {option} completed successfully")
            success_count += 1
            logging.info(f"INTERACTIVE_TEST: Successfully completed menu option {option}")
        except Exception as error:
            print(f"   [FAILED]  Option {option} failed: {str(error)[:100]}...")
            error_count += 1
            logging.error(f"INTERACTIVE_TEST: Failed menu option {option}: {error}")
        
        # Small delay between tests to be respectful to the API
        time.sleep(1)
    
    # Summary
    total_time = time.time() - start_time
    print()
    print("=" * 80)
    print(" Interactive Test Summary:")
    print(f"   Successful operations: {success_count}")
    print(f"   Failed operations: {error_count}")
    print(f"   Skipped operations: {len(skip_interactive_options)}")
    print(f"   Total interactive read-only coverage: {success_count}/{len(interactive_read_only_options)} ({success_count/len(interactive_read_only_options)*100:.1f}%)")
    print(f"   Total execution time: {total_time:.2f} seconds")
    print(f"   Detailed logs in: script.log")
    
    if error_count == 0:
        print("   All tested interactive operations completed successfully!")
        logging.info(f"INTERACTIVE_TEST: All {success_count} tested operations completed successfully in {total_time:.2f}s")
        return True
    else:
        print(f"   {error_count} operations failed - check logs for details")
        logging.warning(f"INTERACTIVE_TEST: {error_count} operations failed out of {len(interactive_read_only_options)} tested")
        return False


class EnhancedSSHRunner:
    """Advanced SSH connection and command execution handler with comprehensive validation"""
    
    def __init__(self, timeout: int = 30, logger: logging.Logger = None):
        """
        Initialize SSH runner
        
        Args:
            timeout: Connection timeout in seconds
            logger: Logger instance
        """
        self.timeout = timeout
        self.client = None
        self.logger = logger or logging.getLogger('ssh_runner_v2')
        self.logger.debug(f"EnhancedSSHRunner initialized with timeout={timeout}")
    
    @staticmethod
    def validate_hostname(hostname: str) -> bool:
        """
        Validate hostname or IP address format
        
        Args:
            hostname: Hostname or IP address to validate
            
        Returns:
            bool: True if valid, False otherwise
        """
        if not hostname or not isinstance(hostname, str):
            return False
        
        # Check length limits
        if len(hostname) > 253:  # RFC 1035 limit
            return False
        
        # Try to parse as IP address first
        try:
            ipaddress.ip_address(hostname)
            return True
        except ValueError:
            pass
        
        # Validate as hostname (RFC 1123 compliant)
        if len(hostname) > 253:
            return False
        
        # Remove trailing dot if present
        hostname = hostname.rstrip('.')
        
        # Check overall format
        hostname_pattern = re.compile(
            r'^([a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?\.)*[a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?$'
        )
        
        return bool(hostname_pattern.match(hostname))
    
    @staticmethod
    def validate_port(port: int) -> bool:
        """
        Validate port number is in valid range
        
        Args:
            port: Port number to validate
            
        Returns:
            bool: True if valid (1-65535), False otherwise
        """
        return isinstance(port, int) and 1 <= port <= 65535
    
    @staticmethod
    def validate_timeout(timeout: int) -> bool:
        """
        Validate timeout value is reasonable
        
        Args:
            timeout: Timeout in seconds
            
        Returns:
            bool: True if valid (1-3600), False otherwise
        """
        return isinstance(timeout, int) and 1 <= timeout <= 3600
    
    @staticmethod
    def validate_username(username: str) -> bool:
        """
        Validate SSH username format
        
        Args:
            username: Username to validate
            
        Returns:
            bool: True if valid, False otherwise
        """
        if not username or not isinstance(username, str):
            return False
        
        # Length check (typical Unix limit is 32 chars)
        if len(username) > 32 or len(username) < 1:
            return False
        
        # Basic character validation (alphanumeric, underscore, hyphen, dot)
        username_pattern = re.compile(r'^[a-zA-Z0-9._-]+$')
        return bool(username_pattern.match(username))
    
    @staticmethod
    def sanitize_filename(filename: str) -> str:
        """
        Sanitize filename to prevent directory traversal and invalid characters
        
        Args:
            filename: Original filename
            
        Returns:
            str: Sanitized filename safe for filesystem use
        """
        if not filename:
            return "unknown"
        
        # Remove or replace dangerous characters
        # Keep only alphanumeric, underscore, hyphen, and dot
        sanitized = re.sub(r'[^\w\-_\.]', '_', filename)
        
        # Remove leading/trailing dots and dashes
        sanitized = sanitized.strip('.-')
        
        # Ensure filename isn't empty after sanitization
        if not sanitized:
            sanitized = "sanitized_host"
        
        # Limit length to prevent filesystem issues
        if len(sanitized) > 100:
            sanitized = sanitized[:100]
        
        # Prevent reserved filenames on Windows
        reserved_names = ['CON', 'PRN', 'AUX', 'NUL'] + [f'COM{port_num}' for port_num in range(1, 10)] + [f'LPT{port_num}' for port_num in range(1, 10)]
        if sanitized.upper() in reserved_names:
            sanitized = f"host_{sanitized}"
        
        return sanitized
    
    @staticmethod
    def validate_command(command: str) -> bool:
        """
        Basic validation for SSH commands
        
        Args:
            command: Command to validate
            
        Returns:
            bool: True if valid, False otherwise
        """
        if not command or not isinstance(command, str):
            return False
        
        # Length check (reasonable command length limit)
        if len(command) > 1000:
            return False
        
        # Check for null bytes (can cause issues in some contexts)
        if '\x00' in command:
            return False
        
        return True
    
    @staticmethod
    def validate_thread_count(thread_count: int, max_hosts: int) -> int:
        """
        Validate and adjust thread count to reasonable limits
        
        Args:
            thread_count: Requested thread count
            max_hosts: Maximum number of hosts
            
        Returns:
            int: Validated thread count
        """
        if not isinstance(thread_count, int) or thread_count <= 0:
            return min(max_hosts, multiprocessing.cpu_count())
        
        # Limit to reasonable maximum (don't overwhelm system)
        max_reasonable_threads = min(50, max_hosts * 2)
        return min(thread_count, max_reasonable_threads, max_hosts)
    
    @staticmethod
    def parse_host_list(hosts_str: str) -> list:
        """
        Parse comma-separated host list from .env file with validation
        
        Args:
            hosts_str: String containing comma-separated hosts (e.g., '192.168.1.1,192.168.1.2')
            
        Returns:
            list: List of validated hostnames/IPs
        """
        if not hosts_str or not isinstance(hosts_str, str):
            return []
        
        # Length check to prevent DoS
        if len(hosts_str) > 10000:  # Reasonable limit for host list
            print("[WARNING] Host list too long, truncating to first 10000 characters")
            hosts_str = hosts_str[:10000]
        
        # Split by comma and validate each host
        hosts = []
        invalid_hosts = []
        
        for host in hosts_str.split(','):
            host = host.strip()
            if not host:  # Skip empty entries
                continue
                
            # Validate hostname/IP format
            if EnhancedSSHRunner.validate_hostname(host):
                hosts.append(host)
            else:
                invalid_hosts.append(host)
        
        # Warn about invalid hosts
        if invalid_hosts:
            print(f"[WARNING] Skipping {len(invalid_hosts)} invalid hosts: {', '.join(invalid_hosts[:5])}")
            if len(invalid_hosts) > 5:
                print(f"    ... and {len(invalid_hosts) - 5} more")
        
        # Limit total number of hosts to prevent resource exhaustion
        max_hosts = 100  # Reasonable limit
        if len(hosts) > max_hosts:
            print(f"[WARNING] Too many hosts ({len(hosts)}), limiting to first {max_hosts}")
            hosts = hosts[:max_hosts]
        
        return hosts
    
    @staticmethod
    def parse_command_list(commands_str: str) -> list:
        """
        Parse comma-separated command list from .env file with validation
        
        Args:
            commands_str: String containing comma-separated commands (e.g., 'show ver,show route' or '"show ver","show route"')
            
        Returns:
            list: List of validated commands
        """
        if not commands_str or not isinstance(commands_str, str):
            return []
        
        # Length check to prevent DoS
        if len(commands_str) > 50000:  # Reasonable limit for command string
            print("[WARNING] Command list too long, truncating to first 50000 characters")
            commands_str = commands_str[:50000]
        
        # Remove outer quotes if present
        commands_str = commands_str.strip('\'"')
        
        # Split by comma and validate each command
        commands = []
        invalid_commands = []
        
        for cmd in commands_str.split(','):
            # Remove quotes and whitespace
            clean_cmd = cmd.strip().strip('\'"').strip()
            
            if not clean_cmd:  # Skip empty commands
                continue
            
            # Validate command
            if EnhancedSSHRunner.validate_command(clean_cmd):
                commands.append(clean_cmd)
            else:
                invalid_commands.append(clean_cmd[:50] + "..." if len(clean_cmd) > 50 else clean_cmd)
        
        # Warn about invalid commands
        if invalid_commands:
            print(f"[WARNING] Skipping {len(invalid_commands)} invalid commands: {', '.join(invalid_commands[:3])}")
            if len(invalid_commands) > 3:
                print(f"    ... and {len(invalid_commands) - 3} more")
        
        # Limit total number of commands to prevent resource exhaustion
        max_commands = 50  # Reasonable limit
        if len(commands) > max_commands:
            print(f"[WARNING] Too many commands ({len(commands)}), limiting to first {max_commands}")
            commands = commands[:max_commands]
        
        return commands
    
    @staticmethod
    def load_commands_from_csv(csv_file_path: str = "data/SSH_COMMANDS.CSV") -> list:
        """
        Load SSH commands from a CSV file as fallback when .env has no commands.
        
        Expected CSV format:
        - First column: command
        - Optional second column: description/comment (ignored)
        - Lines starting with # are treated as comments and ignored
        - Empty lines are ignored
        
        Example CSV content:
        # Network device commands
        show version
        show interfaces,Interface status
        show route,Routing table
        
        Args:
            csv_file_path (str): Path to the CSV file (default: data/SSH_COMMANDS.CSV)
            
        Returns:
            list: List of validated commands loaded from the CSV file
        """
        import csv
        
        commands = []
        
        if not os.path.exists(csv_file_path):
            # Legacy fallback: check previous root location if default data path missing
            if csv_file_path.startswith("data/"):
                legacy_path = csv_file_path.replace("data/", "")
                if os.path.exists(legacy_path):
                    try:
                        print(f"X  Using legacy SSH commands file at {legacy_path}; move it to data/ for consistency.")
                        csv_file_path = legacy_path
                    except Exception:
                        return commands
                else:
                    return commands
            else:
                return commands
            
        try:
            with open(csv_file_path, 'r', newline='', encoding='utf-8') as csvfile:
                # Use simple comma delimiter instead of trying to detect dialect
                # This is more reliable for simple CSV files with comments
                reader = csv.reader(csvfile, delimiter=',')
                invalid_commands = []
                
                for row_num, row in enumerate(reader, 1):
                    if not row:  # Skip empty rows
                        continue
                        
                    # Skip comment lines (lines starting with #)
                    first_cell = str(row[0]).strip()
                    if first_cell.startswith('#') or not first_cell:
                        continue
                    
                    # Get the command (first column)
                    command = first_cell
                    
                    # Validate the command
                    if EnhancedSSHRunner.validate_command(command):
                        commands.append(command)
                    else:
                        invalid_cmd = command[:50] + "..." if len(command) > 50 else command
                        invalid_commands.append(f"line {row_num}: {invalid_cmd}")
                
                # Warn about invalid commands
                if invalid_commands:
                    print(f"[WARNING] Skipping {len(invalid_commands)} invalid commands from {csv_file_path}:")
                    for invalid_cmd in invalid_commands[:3]:  # Show first 3
                        print(f"    {invalid_cmd}")
                    if len(invalid_commands) > 3:
                        print(f"    ... and {len(invalid_commands) - 3} more")
                
                # Limit total number of commands to prevent resource exhaustion
                max_commands = 50  # Reasonable limit
                if len(commands) > max_commands:
                    print(f"[WARNING] Too many commands in {csv_file_path} ({len(commands)}), limiting to first {max_commands}")
                    commands = commands[:max_commands]
                    
        except Exception as e:
            print(f"[WARNING] Warning: Could not read {csv_file_path}: {e}")
            return []
            
        return commands
    
    def create_secure_log_file(self, hostname: str) -> tuple:
        """
        Create a secure per-host log file with proper sanitization
        
        Args:
            hostname: Original hostname
            
        Returns:
            tuple: (log_file_path, write_function)
        """
        # Create per-host log file in subfolder with proper sanitization
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        safe_hostname = self.sanitize_filename(hostname)
        
        # Ensure per-host-logs directory exists and is secure (in data folder)
        # SECURITY: Use proper data directory path to avoid permission issues
        data_dir = os.path.dirname(get_csv_file_path("dummy.csv"))  # Get data directory path
        log_dir = os.path.join(data_dir, "per-host-logs")
        try:
            os.makedirs(log_dir, exist_ok=True)
            # Set secure permissions on directory (owner read/write/execute only)
            if hasattr(os, 'chmod'):
                os.chmod(log_dir, 0o700)
        except OSError as e:
            self.logger.error(f"Failed to create log directory {log_dir}: {e}")
            # Fallback to data directory
            log_dir = data_dir
            safe_hostname = f"fallback_{safe_hostname}"
        
        host_log_file = os.path.join(log_dir, f"ssh_output_{safe_hostname}_{timestamp}.log")
        
        def write_to_host_log(message: str):
            """Write message to host-specific log file only (not console)"""
            if not message:
                return
            
            try:
                # Sanitize message to prevent log injection
                safe_message = message.replace('\x00', '').replace('\r\n', '\n')
                
                with open(host_log_file, 'a', encoding='utf-8') as f:
                    f.write(f"{safe_message}\n")
                    f.flush()  # Ensure data is written immediately
            except IOError as e:
                self.logger.error(f"IO error writing to host log {host_log_file}: {e}")
            except UnicodeEncodeError as e:
                self.logger.error(f"Unicode encoding error writing to host log {host_log_file}: {e}")
                # Try writing a sanitized version
                try:
                    safe_message = message.encode('ascii', errors='replace').decode('ascii')
                    with open(host_log_file, 'a', encoding='utf-8') as f:
                        f.write(f"{safe_message}\n")
                        f.flush()
                except Exception:
                    self.logger.error(f"Failed to write sanitized message to host log")
            except Exception as e:
                self.logger.error(f"Unexpected error writing to host log {host_log_file}: {e}")
        
        return host_log_file, write_to_host_log
    
    def connect(self, hostname: str, username: str, password: str, port: int = 22) -> bool:
        """
        Establish SSH connection to remote host with input validation
        
        Args:
            hostname: IP address or hostname
            username: SSH username
            password: SSH password
            port: SSH port (default 22)
            
        Returns:
            bool: True if connection successful, False otherwise
        """
        # Validate inputs before attempting connection
        if not self.validate_hostname(hostname):
            error_msg = f"Invalid hostname format: {hostname}"
            self.logger.error(error_msg)
            print(f"[ERROR] {error_msg}")
            return False
        
        if not self.validate_username(username):
            error_msg = f"Invalid username format: {username}"
            self.logger.error(error_msg)
            print(f"[ERROR] {error_msg}")
            return False
        
        if not self.validate_port(port):
            error_msg = f"Invalid port number: {port} (must be 1-65535)"
            self.logger.error(error_msg)
            print(f"[ERROR] {error_msg}")
            return False
        
        if not password:
            error_msg = "Password cannot be empty"
            self.logger.error(error_msg)
            print(f"[ERROR] {error_msg}")
            return False
        
        # Check if paramiko is available
        if SSHClient is None or paramiko is None:
            error_msg = "SSH functionality unavailable: paramiko module not installed"
            self.logger.error(error_msg)
            print(f"[ERROR] {error_msg}")
            print("Install paramiko with: pip install paramiko")
            return False
        
        try:
            self.logger.info(f"Attempting SSH connection to {hostname}:{port} as {username}")
            print(f">> Connecting to {hostname}:{port} as {username}...")
            
            # Create SSH client
            self.client = SSHClient()
            # Load existing host keys if available
            self.client.load_system_host_keys()  
            try:
                self.client.load_host_keys(os.path.expanduser('~/.ssh/known_hosts'))
            except FileNotFoundError:
                # known_hosts file doesn't exist yet - that's fine
                pass
            
            # For internal networks: Auto-accept new host keys
            # NOTE: Only use this for trusted internal networks, not internet-facing connections
            self.client.set_missing_host_key_policy(AutoAddPolicy())
            self.logger.debug("SSH client created with AutoAddPolicy for internal network use")
            
            # Attempt connection
            connection_start = time.time()
            self.logger.debug(f"Initiating SSH connection with timeout={self.timeout}s")
            self.client.connect(
                hostname=hostname,
                port=port,
                username=username,
                password=password,
                timeout=self.timeout,
                allow_agent=False,
                look_for_keys=False
            )
            connection_time = time.time() - connection_start
            self.logger.debug(f"SSH connection established in {connection_time:.2f} seconds")
            
            self.logger.info(f"Successfully connected to {hostname} in {connection_time:.2f} seconds")
            print(f"[OK] Successfully connected to {hostname}")
            return True
            
        except socket.gaierror as e:
            error_msg = f"DNS Resolution Error for {hostname}: {e}"
            self.logger.error(error_msg)
            print(f"[ERROR] DNS Resolution Error: {e}")
            return False
        except socket.timeout:
            error_msg = f"Connection timeout to {hostname}:{port} after {self.timeout} seconds"
            self.logger.error(error_msg)
            print(f"[ERROR] Connection timeout after {self.timeout} seconds")
            return False
        except paramiko.AuthenticationException as e:
            error_msg = f"Authentication failed for {username}@{hostname}: {e}"
            self.logger.error(error_msg)
            print("[ERROR] Authentication failed - check username and password")
            return False
        except paramiko.SSHException as e:
            error_msg = f"SSH Error connecting to {hostname}: {e}"
            self.logger.error(error_msg)
            print(f"[ERROR] SSH Error: {e}")
            return False
        except Exception as e:
            error_msg = f"Unexpected error connecting to {hostname}: {type(e).__name__}: {e}"
            self.logger.error(error_msg, exc_info=True)
            print(f"[ERROR] Unexpected error: {e}")
            return False
    
    def execute_command(self, command: str, use_shell: bool = False, hostname: str = "unknown") -> Tuple[bool, str, str]:
        """
        Execute command on remote host
        
        Args:
            command: Command to execute
            use_shell: Use interactive shell instead of exec_command (better for network devices)
            hostname: Hostname for display purposes
            
        Returns:
            Tuple of (success, stdout, stderr)
        """
        if not self.client:
            error_msg = "No active SSH connection"
            self.logger.error(error_msg)
            return False, "", error_msg
        
        try:
            self.logger.debug(f"Executing command: '{command}' (shell_mode={use_shell})")
            self.logger.debug(f"Command execution method: {'shell' if use_shell else 'direct'}")
            
            command_start = time.time()
            
            if use_shell:
                # Use interactive shell for network devices
                self.logger.debug("Using shell-based execution for network device compatibility")
                return self._execute_with_shell(command, command_start, hostname)
            else:
                # Use direct exec_command (try with PTY first for network devices)
                self.logger.debug("Using direct exec_command execution")
                return self._execute_direct(command, command_start, hostname)
                
        except socket.timeout:
            error_msg = f"Command execution timeout after {self.timeout} seconds"
            self.logger.error(error_msg)
            return False, "", error_msg
        except Exception as e:
            error_msg = f"Execution error: {type(e).__name__}: {e}"
            self.logger.error(error_msg, exc_info=True)
            return False, "", error_msg
    
    def _execute_direct(self, command: str, start_time: float, hostname: str = 'unknown') -> Tuple[bool, str, str]:
        """Execute command using exec_command with PTY support"""
        try:
            # Try with PTY first (better for network devices)
            self.logger.debug("Attempting exec_command with get_pty=True")
            stdin, stdout, stderr = self.client.exec_command(
                command, 
                timeout=self.timeout, 
                get_pty=True
            )
            
            # Get output
            stdout_output = stdout.read().decode('utf-8', errors='ignore')
            stderr_output = stderr.read().decode('utf-8', errors='ignore')
            exit_status = stdout.channel.recv_exit_status()
            command_time = time.time() - start_time
            
            self.logger.debug(f"Command completed in {command_time:.2f} seconds with exit status: {exit_status}")
            # Escape newlines and special characters for clean logging
            stdout_sample = stdout_output[:200].replace('\n', '\\n').replace('\r', '\\r').replace('\t', '\\t')
            self.logger.debug(f"STDOUT ({len(stdout_output)} chars): {stdout_sample}{'...' if len(stdout_output) > 200 else ''}")
            
            if stderr_output:
                stderr_sample = stderr_output[:200].replace('\n', '\\n').replace('\r', '\\r').replace('\t', '\\t')
                self.logger.warning(f"STDERR ({len(stderr_output)} chars): {stderr_sample}{'...' if len(stderr_output) > 200 else ''}")
            
            print(f"- [{hostname}] Command completed with exit status: {exit_status}")
            return exit_status == 0, stdout_output, stderr_output
            
        except Exception as e:
            # If PTY fails, try without PTY
            self.logger.warning(f"exec_command with PTY failed: {e}, trying without PTY")
            try:
                stdin, stdout, stderr = self.client.exec_command(command, timeout=self.timeout)
                stdout_output = stdout.read().decode('utf-8', errors='ignore')
                stderr_output = stderr.read().decode('utf-8', errors='ignore')
                exit_status = stdout.channel.recv_exit_status()
                command_time = time.time() - start_time
                
                self.logger.debug(f"Command completed (no PTY) in {command_time:.2f} seconds with exit status: {exit_status}")
                print(f"- [{hostname}] Command completed with exit status: {exit_status}")
                return exit_status == 0, stdout_output, stderr_output
            except Exception as e2:
                self.logger.error(f"Both PTY and non-PTY exec_command failed: {e2}")
                raise e2
    
    def _execute_with_shell(self, command: str, start_time: float, hostname: str = 'unknown') -> Tuple[bool, str, str]:
        """Execute command using interactive shell with device type detection"""
        try:
            self.logger.debug("Using interactive shell mode")
            
            # Start interactive shell
            shell = self.client.invoke_shell(term='vt100', width=120, height=24)
            shell.settimeout(self.timeout)
            
            # Wait for initial prompt
            max_wait = 3  # Maximum wait time
            wait_increment = 0.2
            total_wait = 0
            initial_sample = "(no initial data)"
            
            while total_wait < max_wait:
                time.sleep(wait_increment)
                total_wait += wait_increment
                if shell.recv_ready():
                    initial_output = shell.recv(4096).decode('utf-8', errors='ignore')
                    # Escape newlines and special characters for clean logging
                    initial_sample = initial_output[:100].replace('\n', '\\n').replace('\r', '\\r').replace('\t', '\\t')
                    self.logger.debug(f"Initial shell output: {initial_sample}...")
                    break
            
            # Send command with improved buffering
            try:
                command_with_newline = command + '\n'
                shell.send(command_with_newline)
                time.sleep(0.1)  # Small delay to ensure command is sent completely
                self.logger.debug(f"Sent command to shell: {command}")
            except Exception as e:
                self.logger.warning(f"Error sending command: {e}")
                return False, "", f"Failed to send command: {e}"
            
            # Wait for command execution with adaptive timing
            max_cmd_wait = 6  # Increased maximum command wait time
            cmd_wait = 0
            
            while cmd_wait < max_cmd_wait:
                time.sleep(wait_increment)
                cmd_wait += wait_increment
                if shell.recv_ready():
                    break            # Collect output with universal timing-based approach
            output = ""
            last_data_time = time.time()
            no_data_timeout = 3.0  # Universal timeout - wait 3 seconds after no new data
            max_total_wait = 120  # Universal maximum wait time (2 minutes) for any command
            
            max_output_size = 100 * 1024 * 1024  # 100MB limit - higher since we now drain properly
            chunk_count = 0
            
            try:
                while (time.time() - start_time) < max_total_wait:
                    current_duration = time.time() - start_time
                    
                    # Hard timeout detection - if we've been running too long, force completion
                    if current_duration > 90:  # 90 second hard timeout
                        print(f"[TIMEOUT] [{hostname}] HANG DETECTED: Command running for {current_duration:.0f}s, forcing completion")
                        self.logger.warning(f"Command hang detected after {current_duration:.0f}s, forcing completion: {command}")
                        output += f"\n\n[COMMAND TIMEOUT - Forced completion after {current_duration:.0f}s]\n"
                        break
                    
                    # Progress messages for long-running commands
                    if current_duration > 30:  # Show progress after 30 seconds
                        if chunk_count % 150 == 0:  # Every 150 chunks after 30 seconds
                            print(f"- [{hostname}] Long-running command... {current_duration:.0f}s elapsed (Ctrl+C to interrupt)")
                    
                    if shell.recv_ready():
                        chunk = shell.recv(131072).decode('utf-8', errors='ignore')  # Even larger buffer (128KB) for efficiency
                        output += chunk
                        last_data_time = time.time()  # Reset timer when we get data
                        chunk_count += 1
                        
                        # Log progress every 100 chunks for very large outputs
                        if chunk_count % 100 == 0:
                            output_mb = len(output) / (1024 * 1024)
                            self.logger.debug(f"Receiving data... {chunk_count} chunks, {output_mb:.1f}MB")
                            # Print progress for user feedback on large outputs
                            if output_mb > 5:
                                print(f"- [{hostname}] Receiving large output... {output_mb:.1f}MB (Press Ctrl+C to interrupt)")
                        
                        # Check output size limit - but keep draining to prevent blocking
                        if len(output) > max_output_size:
                            self.logger.warning(f"Output size limit ({max_output_size // (1024*1024)}MB) reached, draining remaining data...")
                            output += f"\n\n[OUTPUT TRUNCATED - Size limit of {max_output_size // (1024*1024)}MB reached]\n"
                            print(f"!? [{hostname}] Output truncated at {max_output_size // (1024*1024)}MB, draining remaining data...")
                            
                            # Continue draining data without storing it to prevent device blocking
                            drain_start = time.time()
                            max_drain_time = 30  # Maximum 30 seconds to drain
                            drained_chunks = 0
                            
                            while (time.time() - drain_start) < max_drain_time:
                                if shell.recv_ready():
                                    shell.recv(262144)  # Large drain buffer (256KB) for maximum efficiency
                                    drained_chunks += 1
                                    last_data_time = time.time()  # Reset timeout
                                    
                                    # Show drain progress
                                    if drained_chunks % 100 == 0:
                                        drain_duration = time.time() - drain_start
                                        print(f"X  [{hostname}] Draining excess data... {drain_duration:.0f}s ({drained_chunks} chunks discarded)")
                                        
                                else:
                                    # Check if we've waited long enough since last data
                                    if (time.time() - last_data_time) >= no_data_timeout:
                                        break  # No new data, device finished
                                    time.sleep(0.05)
                            
                            drain_duration = time.time() - drain_start
                            print(f"[OK] [{hostname}] Data drain completed in {drain_duration:.1f}s ({drained_chunks} chunks discarded)")
                            break
                        
                        time.sleep(0.01)  # Very small delay for maximum throughput
                    else:
                        # Check if we've waited long enough since last data
                        if (time.time() - last_data_time) >= no_data_timeout:
                            break  # No new data for timeout period, command likely complete
                        time.sleep(0.05)  # Small sleep when no data available
                    
            except KeyboardInterrupt:
                print(f"\nX  [{hostname}] Ctrl+C detected! Interrupting command: {command}")
                self.logger.warning(f"Command interrupted by user: {command}")
                output += f"\n\n[COMMAND INTERRUPTED BY USER - Ctrl+C pressed during data collection]\n"
                # Don't return here, continue with cleanup and return what we have
            
            # Log command completion status
            command_duration = time.time() - start_time
            output_size_mb = len(output) / (1024 * 1024)
            if output_size_mb > 1:
                self.logger.info(f"Command data collection completed after {command_duration:.2f}s, output size: {output_size_mb:.2f}MB ({chunk_count} chunks)")
            else:
                self.logger.debug(f"Command data collection completed after {command_duration:.2f}s, output size: {len(output)} bytes ({chunk_count} chunks)")
            
            # Fast cleanup - especially important after truncation
            cleanup_start = time.time()
            max_cleanup_time = 2.0  # Maximum 2 seconds for cleanup to prevent hangs
            
            try:
                shell.send('exit\n')
                shell.send('\n')  # Extra newline to ensure command completion
                
                # Quick cleanup collection with timeout
                cleanup_timeout = time.time() + max_cleanup_time
                while time.time() < cleanup_timeout:
                    if shell.recv_ready():
                        try:
                            shell.recv(4096)  # Drain any remaining output quickly
                            time.sleep(0.1)
                        except:
                            break
                    else:
                        time.sleep(0.1)
                        break  # No more data, exit quickly
                        
            except KeyboardInterrupt:
                print(f"X  [{hostname}] Ctrl+C during cleanup - forcing shell close")
                self.logger.warning("Command cleanup interrupted by user")
            except Exception as e:
                self.logger.debug(f"Warning during cleanup: {e}")
            
            cleanup_duration = time.time() - cleanup_start
            if cleanup_duration > 1.0:
                self.logger.debug(f"Cleanup took {cleanup_duration:.2f}s")
            
            # Force close shell to prevent hangs
            try:
                shell.close()
            except Exception as e:
                self.logger.debug(f"Warning during shell close: {e}")
            command_time = time.time() - start_time
            
            # Enhanced output cleaning to remove shell artifacts and prompts
            lines = output.split('\n')
            cleaned_lines = []
            skip_command = False
            command_found = False
            
            # Common shell prompts and artifacts to filter out
            shell_artifacts = [
                'exit', 'logout', 'Connection to', 'Last login:',
                'Welcome to', 'Match except:', '---(more)---',
                'No next tag', 'press RETURN', 'Invalid command:', 'xit',
                'vyos@vyos:~$', 'Connection closed'
            ]
            
            # Shell prompt patterns (more comprehensive)
            shell_prompt_patterns = [
                r'.*[$#>]\s*$',  # Basic prompts ending with $, #, or >
                r'vyos@.*[$#>]\s*$',  # VyOS prompts
                r'.*@.*:.*[$#>]\s*$',  # Standard user@host:path$ prompts
                r'{master:\d+}',  # Juniper master mode prompts
                r'^\s*$',  # Empty lines (remove excessive whitespace)
                r':+.*\[.*\d+;\d+.*H.*',  # ANSI cursor positioning sequences
                r'^:.*press RETURN.*',  # Pager "press RETURN" prompts
                r'^>vyos@.*\$ xit$',  # VyOS shell prompt with truncated exit
                r'^vyos@.*:~\$.*xit$',  # VyOS shell cleanup with xit
                r'^Invalid command: \[xit\]$',  # VyOS invalid xit command error
                r'^.*Connection to .* closed\.$',  # Connection closed messages
                r'^\s*xit\s*$'  # Standalone truncated exit commands
            ]
            
            import re
            
            for line in lines:
                original_line = line
                line = line.strip()
                
                # Skip empty lines
                if not line:
                    continue
                
                # Skip command echo (first occurrence of the command)
                if not command_found and command.strip() in line:
                    command_found = True
                    continue
                
                # Skip shell artifacts
                should_skip = False
                for artifact in shell_artifacts:
                    if artifact.lower() in line.lower():
                        should_skip = True
                        break
                
                if should_skip:
                    continue
                
                # Skip shell prompts using regex patterns
                is_prompt = False
                for pattern in shell_prompt_patterns:
                    if re.match(pattern, line):
                        is_prompt = True
                        break
                
                if is_prompt:
                    continue
                
                # Enhanced cleaning for terminal control sequences and VyOS artifacts
                clean_line = re.sub(r'\x1b\[[0-9;]*[mK]', '', line)  # ANSI escape codes
                clean_line = re.sub(r'\x1b\[\?[0-9]+[hl]', '', clean_line)  # ANSI mode changes
                clean_line = re.sub(r'\x1b\[[0-9]+;[0-9]+H', '', clean_line)  # ANSI cursor positioning
                clean_line = re.sub(r':\s*$', '', clean_line)  # Remove trailing colons from pager prompts
                clean_line = clean_line.replace('\r', '').replace('\x08', '').strip()  # Remove carriage returns and backspaces
                
                # Skip VyOS-specific shell artifacts
                vyos_artifacts = [
                    r'^\s*xit\s*$',
                    r'^Invalid command: \[xit\]$',
                    r'^vyos@.*:~\$',
                    r'^Connection.*closed\.$'
                ]
                
                skip_vyos_artifact = False
                for artifact_pattern in vyos_artifacts:
                    if re.match(artifact_pattern, clean_line):
                        skip_vyos_artifact = True
                        break
                
                # Only add non-empty cleaned lines that aren't VyOS artifacts
                if clean_line and not skip_vyos_artifact:
                    cleaned_lines.append(clean_line)
            
            cleaned_output = '\n'.join(cleaned_lines).strip()
            
            self.logger.debug(f"Shell command completed in {command_time:.2f} seconds")
            # Only log output sample for smaller outputs to avoid log spam
            if len(cleaned_output) < 10000:  # Only log sample for outputs under 10KB
                output_sample = cleaned_output[:200].replace('\n', '\\n').replace('\r', '\\r').replace('\t', '\\t')
                self.logger.debug(f"Shell output ({len(cleaned_output)} chars): {output_sample}{'...' if len(cleaned_output) > 200 else ''}")
            else:
                self.logger.debug(f"Shell output: {len(cleaned_output)} characters (large output, sample not logged)")
            
            # Universal success detection - simple and reliable
            command_success = len(cleaned_output) > 0
            
            # More intelligent error detection - only flag real command errors
            # Skip error detection for shell cleanup artifacts
            error_patterns = [
                "command not found", "syntax error",
                "permission denied", "authentication failed",
                "connection refused", "host unreachable", "network unreachable",
                "no such file or directory"
            ]
            
            # Exclude patterns that are likely shell cleanup artifacts
            shell_cleanup_indicators = [
                "invalid command: [xit]",
                "unknown command: xit",
                "invalid command: exit",
                "connection to .* closed"
            ]
            
            output_lower = cleaned_output.lower()
            
            # Check for shell cleanup indicators first - if found, don't treat as error
            is_shell_cleanup = False
            for cleanup_pattern in shell_cleanup_indicators:
                if cleanup_pattern in output_lower:
                    is_shell_cleanup = True
                    self.logger.debug(f"Shell cleanup artifact detected, ignoring: {cleanup_pattern}")
                    break
            
            # Only check for real errors if this isn't shell cleanup
            if not is_shell_cleanup:
                for pattern in error_patterns:
                    if pattern in output_lower:
                        command_success = False
                        self.logger.warning(f"Command error detected: {pattern}")
                        break
            
            self.logger.debug(f"Command success determination: success={command_success}, output_length={len(cleaned_output)}")
            print(f"[STATUS] [{hostname}] Command completed in {command_time:.2f} seconds")
            return command_success, cleaned_output, ""
            
        except Exception as e:
            error_msg = f"Shell execution error: {type(e).__name__}: {e}"
            self.logger.error(error_msg, exc_info=True)
            return False, "", error_msg
    
    def disconnect(self):
        """Close SSH connection"""
        if self.client:
            self.logger.debug("Closing SSH connection")
            self.client.close()
            self.client = None
            print(">> SSH connection closed")
        else:
            self.logger.debug("No SSH connection to close")
    
    @staticmethod
    def load_ssh_config_from_env(env_file: str = ".env") -> dict:
        """
        Load SSH configuration from .env file with comprehensive validation
        
        Args:
            env_file: Path to the .env file (default: ".env")
            
        Returns:
            dict: SSH configuration with keys: hosts, username, password, commands
        """
        config = {
            'hosts': [],
            'username': None, 
            'password': None,
            'commands': []
        }
        
        # Validate env_file path to prevent directory traversal
        if not env_file or '..' in env_file or env_file.startswith('/') or '\\' in env_file:
            print(f"[WARNING] Invalid .env file path: {env_file}")
            return config
        
        if not os.path.exists(env_file):
            return config
        
        # Check file size to prevent DoS
        try:
            file_size = os.path.getsize(env_file)
            if file_size > 1024 * 1024:  # 1MB limit
                print(f"[WARNING] .env file too large ({file_size} bytes), skipping")
                return config
        except OSError as e:
            print(f"[WARNING] Cannot access .env file: {e}")
            return config
        
        if DOTENV_AVAILABLE:
            # Use python-dotenv for proper parsing
            try:
                load_dotenv(env_file)
                ssh_host = os.getenv('SSH_HOST')
                if ssh_host:
                    config['hosts'] = EnhancedSSHRunner.parse_host_list(ssh_host)
                
                # Validate username
                username = os.getenv('SSH_USER')
                if username and EnhancedSSHRunner.validate_username(username):
                    config['username'] = username
                elif username:
                    print(f"[WARNING] Invalid username format in .env file: {username}")
                
                config['password'] = os.getenv('SSH_PASSWORD')
                
                # Parse SSH_COMMANDS
                ssh_commands = os.getenv('SSH_COMMANDS')
                if ssh_commands:
                    config['commands'] = EnhancedSSHRunner.parse_command_list(ssh_commands)
            except Exception as e:
                print(f"[WARNING] Error loading .env with python-dotenv: {e}")
        else:
            # Basic manual parsing for .env files with enhanced validation
            try:
                with open(env_file, 'r', encoding='utf-8', errors='ignore') as f:
                    line_count = 0
                    for line in f:
                        line_count += 1
                        
                        # Prevent processing too many lines
                        if line_count > 1000:
                            print("[WARNING] .env file has too many lines, stopping at 1000")
                            break
                        
                        line = line.strip()
                        
                        # Skip empty lines and comments
                        if not line or line.startswith('#'):
                            continue
                        
                        # Skip lines without equals sign
                        if '=' not in line:
                            continue
                        
                        # Handle multiple = signs correctly
                        parts = line.split('=', 1)
                        if len(parts) != 2:
                            continue
                        
                        key = parts[0].strip()
                        value = parts[1].strip()
                        
                        # Remove quotes if present
                        if value.startswith('"') and value.endswith('"'):
                            value = value[1:-1]
                        elif value.startswith("'") and value.endswith("'"):
                            value = value[1:-1]
                        
                        # Process known keys with validation
                        if key == 'SSH_HOST':
                            config['hosts'] = EnhancedSSHRunner.parse_host_list(value)
                        elif key == 'SSH_USER':
                            if EnhancedSSHRunner.validate_username(value):
                                config['username'] = value
                            else:
                                print(f"[WARNING] Invalid username format in .env file: {value}")
                        elif key == 'SSH_PASSWORD':
                            config['password'] = value
                        elif key == 'SSH_COMMANDS':
                            config['commands'] = EnhancedSSHRunner.parse_command_list(value)
                            
            except UnicodeDecodeError as e:
                print(f"[WARNING] .env file encoding error: {e}")
            except IOError as e:
                print(f"[WARNING] Error reading {env_file}: {e}")
            except Exception as e:
                print(f"[WARNING] Unexpected error reading {env_file}: {e}")
        
        return config
    
    @staticmethod
    def setup_logging(log_level: str = 'INFO') -> logging.Logger:
        """
        Setup comprehensive logging configuration with syslog-style levels
        
        Args:
            log_level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
            
        Returns:
            logging.Logger: Configured logger instance
        """
        # Unified logging: use root handlers (script.log + console) only
        logger = logging.getLogger('ssh_runner_v2')
        logger.setLevel(getattr(logging, log_level.upper(), logging.INFO))
        # Remove any prior dedicated handlers so we don't duplicate output
        for h in list(logger.handlers):
            logger.removeHandler(h)
        # Ensure messages bubble to root configuration
        logger.propagate = True
        # Emit initialization message (will land in script.log)
        if log_level.upper() == 'DEBUG':
            logger.debug("Enhanced SSH Runner v2 logging initialized (root handlers)")
        else:
            logger.info("Enhanced SSH Runner v2 logging initialized (root handlers)")
        return logger
    
    @staticmethod
    def run_multiple_ssh_commands_interactive(hostname: str, username: str, password: str, commands: list, 
                                            port: int = 22, timeout: int = 30, use_shell: bool = True) -> bool:
        """
        Connect via SSH and execute multiple commands with interactive prompt support
        
        Handles password prompts and interactive sequences like:
        1. su -> Password: -> (send password) -> root prompt
        2. pcli -> PCLI mode
        3. show commands work in PCLI
        
        Args:
            hostname: IP address or hostname
            username: SSH username
            password: SSH password
            commands: List of commands/responses to execute
            port: SSH port (default 22)
            timeout: Connection timeout
            use_shell: Use interactive shell mode (required for interactive prompts)
            
        Returns:
            bool: True if all commands successful, False otherwise
        """
        # Get the already-configured logger
        logger = logging.getLogger('ssh_runner_v2')
        logger.debug(f"Starting SSH interactive multi-command execution: {hostname}:{port} - {len(commands)} commands")
        logger.debug(f"Interactive commands to execute: {commands}")
        
        # Create per-host log file in subfolder with proper sanitization
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        safe_hostname = EnhancedSSHRunner.sanitize_filename(hostname)
        
        # Ensure per-host-logs directory exists and is secure (in data folder)
        # SECURITY: Use proper data directory path to avoid permission issues
        data_dir = os.path.dirname(get_csv_file_path("dummy.csv"))  # Get data directory path
        log_dir = os.path.join(data_dir, "per-host-logs")
        try:
            os.makedirs(log_dir, exist_ok=True)
            # Set secure permissions on directory (owner read/write/execute only)
            if hasattr(os, 'chmod'):
                os.chmod(log_dir, 0o700)
        except OSError as e:
            logger.error(f"Failed to create log directory {log_dir}: {e}")
            # Fallback to data directory
            log_dir = data_dir
            safe_hostname = f"fallback_{safe_hostname}"
        
        host_log_file = os.path.join(log_dir, f"ssh_output_{safe_hostname}_{timestamp}.log")
        print(f"** [{hostname}] Logging to: {host_log_file}")
        
        def write_to_host_log(message: str):
            """Write message to host-specific log file only (not console)"""
            if not message:
                return
            
            try:
                # Clean ANSI escape sequences and terminal control codes for readable logs
                import re
                clean_message = message
                
                # Remove ANSI escape sequences (colors, cursor positioning, etc.)
                ansi_escape = re.compile(r'\x1b\[[0-9;]*[mGKHfABCDsuJ]')
                clean_message = ansi_escape.sub('', clean_message)
                
                # Remove other common terminal control sequences
                control_sequences = [
                    r'\x1b\[\?[0-9]+[lh]',  # DEC private mode sequences
                    r'\x1b\[[0-9]+[ABCDGK]',  # Cursor movement
                    r'\x1b\[[0-9]+;[0-9]+[Hf]',  # Cursor positioning
                    r'\x1b\[[0-9]*[J]',  # Erase sequences
                    r'\x1b\[6n',  # Cursor position request
                    r'\x1b\[[0-9]+D',  # Cursor backward
                    r'\x1b\[\?2004[hl]',  # Bracketed paste mode
                    r'\x1b\[\?25[lh]',  # Cursor visibility
                    r'\x1b\[\?7[lh]',   # Line wrap mode
                    r'\x1b\[\?12[lh]',  # Start/stop blinking cursor
                ]
                
                for pattern in control_sequences:
                    clean_message = re.sub(pattern, '', clean_message)
                
                # Remove excessive whitespace and clean up line breaks
                clean_message = re.sub(r'\n\s*\n\s*\n', '\n\n', clean_message)  # Max 2 consecutive newlines
                clean_message = re.sub(r'[ \t]+\n', '\n', clean_message)  # Remove trailing spaces
                
                # Sanitize message to prevent log injection
                safe_message = clean_message.replace('\x00', '').replace('\r\n', '\n')
                
                with open(host_log_file, 'a', encoding='utf-8') as f:
                    f.write(f"{safe_message}\n")
                    f.flush()
                    
                # Set secure permissions on log file (owner read/write only)
                if hasattr(os, 'chmod'):
                    os.chmod(host_log_file, 0o600)
            except UnicodeEncodeError:
                # Try writing a sanitized version
                try:
                    safe_message = message.encode('ascii', errors='replace').decode('ascii')
                    with open(host_log_file, 'a', encoding='utf-8') as f:
                        f.write(f"{safe_message}\n")
                        f.flush()
                except Exception:
                    logger.error(f"Failed to write sanitized message to host log")
            except Exception as e:
                logger.error(f"Unexpected error writing to host log {host_log_file}: {e}")
        
        runner = EnhancedSSHRunner(timeout=timeout, logger=logger)
        overall_success = True
        
        # Initialize host log with header
        header = f"""
{'='*80}
SSH Interactive Session Log for Host: {hostname}
Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Commands/responses to execute: {len(commands)}
{'='*80}"""
        write_to_host_log(header)
        
        try:
            # Connect once for all commands
            if not runner.connect(hostname, username, password, port):
                error_msg = f"Failed to connect to {hostname}"
                logger.error(f"SSH connection failed: {hostname}:{port}")
                write_to_host_log(f"[ERROR] {error_msg}")
                return False
            
            logger.debug(f"SSH connected to {hostname}, starting interactive session")
            connection_msg = f"\n>> Starting interactive session with {len(commands)} steps..."
            write_to_host_log(connection_msg)
            
            # Create persistent shell for interactive session
            if not use_shell:
                logger.warning(f"Interactive mode requires shell=True, enabling shell mode")
                use_shell = True
            
            # Start interactive shell
            shell = runner.client.invoke_shell(term='vt100', width=120, height=24)
            shell.settimeout(timeout)
            
            # Wait for initial prompt
            time.sleep(1)
            if shell.recv_ready():
                initial_output = shell.recv(4096).decode('utf-8', errors='ignore')
                write_to_host_log(f"[OUTPUT] INITIAL PROMPT:\n{initial_output}")
                logger.debug(f"Initial shell prompt received")
            
            # Process each command/response in sequence
            command_index = 0
            while command_index < len(commands):
                current_item = commands[command_index].strip()
                
                # Skip empty commands
                if not current_item:
                    command_index += 1
                    continue
                
                step_num = command_index + 1
                separator = f"\n{'='*60}"
                step_header = f"[STEP] Step {step_num}/{len(commands)}: {current_item}"
                separator_line = '='*60
                
                write_to_host_log(separator)
                write_to_host_log(step_header)
                write_to_host_log(separator_line)
                
                # SECURITY: Redact potential passwords in console output
                display_item = current_item
                if any(pwd_hint in current_item.lower() for pwd_hint in ['password', 'pass', 'pwd']) and len(current_item) > 5:
                    display_item = "*" * len(current_item)  # Redact password
                
                print(f"* [{hostname}] Executing step {step_num}: {display_item}")
                logger.debug(f"[{hostname}] Sending: {current_item}")
                
                try:
                    # Send command/response
                    shell.send(current_item + '\n')
                    time.sleep(0.2)  # Brief pause to let command register
                    
                    # Wait for and collect response
                    max_wait_time = 10  # Maximum wait for response
                    wait_increment = 0.1
                    total_wait = 0
                    response_output = ""
                    last_data_time = time.time()
                    no_data_timeout = 3.0  # Wait 3 seconds after no new data
                    
                    while total_wait < max_wait_time:
                        if shell.recv_ready():
                            chunk = shell.recv(4096).decode('utf-8', errors='ignore')
                            response_output += chunk
                            last_data_time = time.time()
                            
                            # Check if we got a password prompt
                            if any(prompt in response_output.lower() for prompt in ['password:', 'password ', 'passwd:']):
                                logger.debug(f"[{hostname}] Password prompt detected")
                                break
                                
                            # Check if we got a shell prompt (command completed)
                            prompt_patterns = ['$', '#', '>', 'pcli']
                            if any(pattern in response_output[-50:] for pattern in prompt_patterns):
                                if (time.time() - last_data_time) > 1.0:  # No new data for 1 second
                                    break
                        else:
                            # Check if we've waited long enough since last data
                            if (time.time() - last_data_time) >= no_data_timeout:
                                break  # No new data, likely command completed
                                
                        time.sleep(wait_increment)
                        total_wait += wait_increment
                    
                    # Log the response
                    if response_output.strip():
                        write_to_host_log("[OUTPUT] RESPONSE:")
                        write_to_host_log(response_output)
                        logger.debug(f"[{hostname}] Response received: {len(response_output)} chars")
                    else:
                        write_to_host_log("[STATUS] No response output")
                        logger.debug(f"[{hostname}] No response output received")
                    
                    # Check for success indicators
                    step_success = True
                    if "command not found" in response_output.lower():
                        step_success = False
                        logger.warning(f"[{hostname}] Step {step_num} failed: command not found")
                    elif "permission denied" in response_output.lower():
                        step_success = False
                        logger.warning(f"[{hostname}] Step {step_num} failed: permission denied")
                    elif "authentication failed" in response_output.lower():
                        step_success = False
                        logger.warning(f"[{hostname}] Step {step_num} failed: authentication failed")
                    
                    if step_success:
                        success_msg = f"[OK] Step {step_num} completed successfully"
                        write_to_host_log(success_msg)
                        logger.debug(f"[{hostname}] Step {step_num} completed successfully")
                    else:
                        failure_msg = f"[ERROR] Step {step_num} failed"
                        write_to_host_log(failure_msg)
                        overall_success = False
                    
                    command_index += 1
                    
                    # Brief pause between commands for stability
                    if command_index < len(commands):
                        time.sleep(0.5)
                        
                except KeyboardInterrupt:
                    print(f"\n[INTERRUPT] [{hostname}] Ctrl+C detected! Stopping interactive session...")
                    logger.warning(f"Interactive session interrupted by user at step {step_num}")
                    write_to_host_log(f"\n[INTERRUPT] Session interrupted by user at step {step_num}")
                    overall_success = False
                    break
                except Exception as step_e:
                    logger.error(f"[{hostname}] Error at step {step_num}: {type(step_e).__name__}: {step_e}")
                    error_msg = f"[ERROR] Step {step_num} error: {step_e}"
                    write_to_host_log(error_msg)
                    overall_success = False
                    break
            
            # Cleanup - close shell gracefully
            try:
                shell.send('exit\n')
                time.sleep(0.5)
                shell.close()
                logger.debug(f"[{hostname}] Interactive shell closed gracefully")
            except Exception as cleanup_e:
                logger.debug(f"[{hostname}] Shell cleanup warning: {cleanup_e}")
            
            # Final status
            if overall_success:
                logger.info(f"[{hostname}] All {len(commands)} interactive steps completed successfully")
                final_msg = "[OK] All interactive steps completed successfully"
                write_to_host_log(final_msg)
            else:
                logger.warning(f"[{hostname}] Some interactive steps failed")
                final_msg = "[WARNING] Some interactive steps failed - check output above"
                write_to_host_log(final_msg)
            
            return overall_success
            
        except Exception as e:
            logger.error(f"[{hostname}] Unexpected error during interactive session: {type(e).__name__}: {e}", exc_info=True)
            error_msg = f"[ERROR] Unexpected error: {e}"
            write_to_host_log(error_msg)
            return False
        finally:
            runner.disconnect()
            logger.debug(f"[{hostname}] SSH interactive session completed")
            
            # Write session footer to host log with safer success check
            try:
                # Ensure we have a valid overall_success value
                final_success = locals().get('overall_success', False)
                if not isinstance(final_success, bool):
                    logger.warning(f"Overall success value is not boolean: {type(final_success)} = {final_success}")
                    final_success = False
                    
                footer = f"""
{'='*80}
SSH Interactive Session Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Status: {'SUCCESS' if final_success else 'FAILED'}
Log file: {host_log_file}
{'='*80}"""
                write_to_host_log(footer)
            except Exception as e:
                logger.error(f"Error in interactive session footer generation: {type(e).__name__}: {e}")
                # Write minimal footer
                try:
                    simple_footer = f"Session completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
                    write_to_host_log(simple_footer)
                except Exception as e2:
                    logger.error(f"Even simple interactive footer failed: {e2}")

    @staticmethod
    def run_multiple_ssh_commands(hostname: str, username: str, password: str, commands: list, 
                                 port: int = 22, timeout: int = 30, use_shell: bool = False) -> bool:
        """
        Connect via SSH and execute multiple commands sequentially
        
        Args:
            hostname: IP address or hostname
            username: SSH username
            password: SSH password
            commands: List of commands to execute
            port: SSH port (default 22)
            timeout: Connection timeout
            use_shell: Use interactive shell mode (better for network devices)
            
        Returns:
            bool: True if all commands successful, False otherwise
        """
        # Get the already-configured logger
        logger = logging.getLogger('ssh_runner_v2')
        logger.debug(f"Starting SSH multi-command execution: {hostname}:{port} - {len(commands)} commands (shell={use_shell})")
        logger.debug(f"Commands to execute: {commands}")
        
        # Create per-host log file in subfolder with proper sanitization
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        safe_hostname = EnhancedSSHRunner.sanitize_filename(hostname)
        
        # Ensure per-host-logs directory exists and is secure (in data folder)
        # SECURITY: Use proper data directory path to avoid permission issues
        data_dir = os.path.dirname(get_csv_file_path("dummy.csv"))  # Get data directory path
        log_dir = os.path.join(data_dir, "per-host-logs")
        try:
            os.makedirs(log_dir, exist_ok=True)
            # Set secure permissions on directory (owner read/write/execute only)
            if hasattr(os, 'chmod'):
                os.chmod(log_dir, 0o700)
        except OSError as e:
            logger.error(f"Failed to create log directory {log_dir}: {e}")
            # Fallback to data directory
            log_dir = data_dir
            safe_hostname = f"fallback_{safe_hostname}"
        
        host_log_file = os.path.join(log_dir, f"ssh_output_{safe_hostname}_{timestamp}.log")
        print(f"- [{hostname}] Logging to: {host_log_file}")
        
        def write_to_host_log(message: str):
            """Write message to host-specific log file only (not console)"""
            if not message:
                return
            
            try:
                # Sanitize message to prevent log injection
                safe_message = message.replace('\x00', '').replace('\r\n', '\n')
                
                with open(host_log_file, 'a', encoding='utf-8') as f:
                    f.write(f"{safe_message}\n")
                    f.flush()  # Ensure data is written immediately
            except IOError as e:
                logger.error(f"IO error writing to host log {host_log_file}: {e}")
            except UnicodeEncodeError as e:
                logger.error(f"Unicode encoding error writing to host log {host_log_file}: {e}")
                # Try writing a sanitized version
                try:
                    safe_message = message.encode('ascii', errors='replace').decode('ascii')
                    with open(host_log_file, 'a', encoding='utf-8') as f:
                        f.write(f"{safe_message}\n")
                        f.flush()
                except Exception:
                    logger.error(f"Failed to write sanitized message to host log")
            except Exception as e:
                logger.error(f"Unexpected error writing to host log {host_log_file}: {e}")
        
        runner = EnhancedSSHRunner(timeout=timeout, logger=logger)
        overall_success = True
        
        # Initialize host log with header
        header = f"""
{'='*80}
SSH Session Log for Host: {hostname}
Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Commands to execute: {len(commands)}
{'='*80}"""
        write_to_host_log(header)
        
        try:
            # Connect once for all commands
            if not runner.connect(hostname, username, password, port):
                error_msg = f"Failed to connect to {hostname}"
                logger.error(f"SSH connection failed: {hostname}:{port}")
                write_to_host_log(f"X  {error_msg}")
                return False
            
            logger.debug(f"SSH connected to {hostname}, executing {len(commands)} commands")
            connection_msg = f"\n>> Executing {len(commands)} commands sequentially..."
            write_to_host_log(connection_msg)
            
            # Execute each command with keyboard interrupt handling
            for i, command in enumerate(commands, 1):
                try:
                    separator = f"\n{'='*60}"
                    command_header = f"X  Command {i}/{len(commands)}: {command}"
                    separator_line = '='*60
                    
                    write_to_host_log(separator)
                    write_to_host_log(command_header)
                    write_to_host_log(separator_line)
                    
                    print(f"!? [{hostname}] Executing command: {command}")
                    success, stdout, stderr = runner.execute_command(command, use_shell=use_shell, hostname=hostname)
                    
                    if stdout:
                        write_to_host_log("-> OUTPUT:")
                        write_to_host_log(stdout)
                    
                    if stderr:
                        write_to_host_log("-> ERRORS:")
                        write_to_host_log(stderr)
                    
                    if success:
                        logger.debug(f"[{hostname}] Command {i}/{len(commands)} completed: {command}")
                        success_msg = f"[OK] Command {i} executed successfully"
                        write_to_host_log(success_msg)
                    else:
                        logger.warning(f"[{hostname}] Command {i}/{len(commands)} failed: {command[:50]}...")
                        failure_msg = f"[ERROR] Command {i} failed"
                        write_to_host_log(failure_msg)
                        overall_success = False
                    
                    # Small delay between commands for network devices
                    if i < len(commands):
                        time.sleep(0.5)
                        
                except KeyboardInterrupt:
                    print(f"\nX  [{hostname}] Ctrl+C detected! Skipping remaining commands...")
                    interrupt_msg = f"\n[ERROR] Command {i} interrupted by user (Ctrl+C)\n[SKIP] Skipping remaining {len(commands) - i} commands"
                    write_to_host_log(interrupt_msg)
                    logger.warning(f"[{hostname}] Command execution interrupted by user at command {i}/{len(commands)}")
                    overall_success = False
                    break
            
            final_separator = f"\n{'='*60}"
            write_to_host_log(final_separator)
            
            if overall_success:
                logger.info(f"[{hostname}] All {len(commands)} commands completed successfully")
                final_msg = "[OK] All commands executed successfully"
                write_to_host_log(final_msg)
            else:
                logger.warning(f"[{hostname}] Some commands failed during execution")
                final_msg = "[WARNING] Some commands failed - check output above"
                write_to_host_log(final_msg)
            
            return overall_success
            
        except Exception as e:
            logger.error(f"[{hostname}] Unexpected error during multi-command execution: {type(e).__name__}: {e}", exc_info=True)
            error_msg = f"[ERROR] Unexpected error: {e}"
            write_to_host_log(error_msg)
            return False
        finally:
            runner.disconnect()
            logger.debug(f"[{hostname}] SSH multi-command session completed")
            
            # Write session footer to host log with safer success check
            try:
                # Ensure we have a valid overall_success value
                final_success = locals().get('overall_success', False)
                if not isinstance(final_success, bool):
                    logger.warning(f"Overall success value is not boolean: {type(final_success)} = {final_success}")
                    final_success = False
                    
                footer = f"""
{'='*80}
SSH Session Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Status: {'SUCCESS' if final_success else 'FAILED'}
Log file: {host_log_file}
{'='*80}"""
                write_to_host_log(footer)
            except Exception as e:
                logger.error(f"Error in multi-command footer generation: {type(e).__name__}: {e}")
                # Write minimal footer
                try:
                    simple_footer = f"Session completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
                    write_to_host_log(simple_footer)
                except Exception as e2:
                    logger.error(f"Even simple multi-command footer failed: {e2}")
    
    @staticmethod
    def run_ssh_command(hostname: str, username: str, password: str, command: str, 
                       port: int = 22, timeout: int = 30, use_shell: bool = False) -> bool:
        """
        Connect via SSH and execute a command
        
        Args:
            hostname: IP address or hostname
            username: SSH username
            password: SSH password
            command: Command to execute
            port: SSH port (default 22)
            timeout: Connection timeout
            use_shell: Use interactive shell mode (better for network devices)
            
        Returns:
            bool: True if successful, False otherwise
        """
        # Get the already-configured logger
        logger = logging.getLogger('ssh_runner_v2')
        logger.debug(f"Starting SSH command execution: {hostname}:{port} - '{command}' (shell={use_shell})")
        logger.debug(f"Single command details: timeout={timeout}, use_shell={use_shell}")
        
        # Create per-host log file in subfolder with proper sanitization
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        safe_hostname = EnhancedSSHRunner.sanitize_filename(hostname)
        
        # Ensure per-host-logs directory exists and is secure (in data folder)
        # SECURITY: Use proper data directory path to avoid permission issues
        data_dir = os.path.dirname(get_csv_file_path("dummy.csv"))  # Get data directory path
        log_dir = os.path.join(data_dir, "per-host-logs")
        try:
            os.makedirs(log_dir, exist_ok=True)
            # Set secure permissions on directory (owner read/write/execute only)
            if hasattr(os, 'chmod'):
                os.chmod(log_dir, 0o700)
        except OSError as e:
            logger.error(f"Failed to create log directory {log_dir}: {e}")
            # Fallback to data directory
            log_dir = data_dir
            safe_hostname = f"fallback_{safe_hostname}"
        
        host_log_file = os.path.join(log_dir, f"ssh_output_{safe_hostname}_{timestamp}.log")
        print(f"- [{hostname}] Logging to: {host_log_file}")
        
        def write_to_host_log(message: str):
            """Write message to host-specific log file only (not console)"""
            if not message:
                return
            
            try:
                # Sanitize message to prevent log injection
                safe_message = message.replace('\x00', '').replace('\r\n', '\n')
                
                with open(host_log_file, 'a', encoding='utf-8') as f:
                    f.write(f"{safe_message}\n")
                    f.flush()  # Ensure data is written immediately
            except IOError as e:
                logger.error(f"IO error writing to host log {host_log_file}: {e}")
            except UnicodeEncodeError as e:
                logger.error(f"Unicode encoding error writing to host log {host_log_file}: {e}")
                # Try writing a sanitized version
                try:
                    safe_message = message.encode('ascii', errors='replace').decode('ascii')
                    with open(host_log_file, 'a', encoding='utf-8') as f:
                        f.write(f"{safe_message}\n")
                        f.flush()
                except Exception:
                    logger.error(f"Failed to write sanitized message to host log")
            except Exception as e:
                logger.error(f"Unexpected error writing to host log {host_log_file}: {e}")
        
        runner = EnhancedSSHRunner(timeout=timeout, logger=logger)
        
        # Initialize host log with header
        header = f"""
{'='*80}
SSH Single Command Log for Host: {hostname}
Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Command: {command}
{'='*80}"""
        write_to_host_log(header)
        
        try:
            # Connect
            if not runner.connect(hostname, username, password, port):
                error_msg = f"Failed to connect to {hostname}"
                logger.error(f"SSH connection failed: {hostname}:{port}")
                write_to_host_log(f"X  {error_msg}")
                return False
            
            logger.debug(f"SSH connected to {hostname}, executing single command")
            
            # Execute command
            single_cmd_success, stdout, stderr = runner.execute_command(command, use_shell=use_shell, hostname=hostname)
            
            # Display results
            separator = "\n" + "=" * 60
            output_header = "!? COMMAND OUTPUT"
            separator_line = "=" * 60
            
            write_to_host_log(separator)
            write_to_host_log(output_header)
            write_to_host_log(separator_line)
            
            if stdout:
                write_to_host_log("-> STDOUT:")
                write_to_host_log(stdout)
            
            if stderr:
                write_to_host_log("-> STDERR:")
                write_to_host_log(stderr)
            
            if not stdout and not stderr:
                write_to_host_log("X  No output returned")
            
            write_to_host_log(separator_line)
            
            if single_cmd_success:
                logger.info(f"[{hostname}] Command completed successfully")
                success_msg = "[OK] Command executed successfully"
                write_to_host_log(success_msg)
            else:
                logger.warning(f"[{hostname}] Command failed: {command[:50]}...")
                failure_msg = "[ERROR] Command execution failed or returned non-zero exit status"
                write_to_host_log(failure_msg)
                    
            return single_cmd_success
            
        except Exception as e:
            logger.error(f"[{hostname}] Unexpected error during SSH command execution: {type(e).__name__}: {e}", exc_info=True)
            error_msg = f"[ERROR] Unexpected error: {e}"
            write_to_host_log(error_msg)
            return False
        finally:
            runner.disconnect()
            logger.debug(f"[{hostname}] SSH single command session completed")
            
            # Write session footer to host log with safer success check
            try:
                # Ensure we have a valid success value
                final_success = locals().get('single_cmd_success', False)
                if not isinstance(final_success, bool):
                    logger.warning(f"Success value is not boolean: {type(final_success)} = {final_success}")
                    final_success = False
                    
                footer = f"""
{'='*80}
SSH Single Command Session Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Status: {'SUCCESS' if final_success else 'FAILED'}
Log file: {host_log_file}
{'='*80}"""
                write_to_host_log(footer)
            except Exception as e:
                logger.error(f"Error in footer generation: {type(e).__name__}: {e}")
                # Write minimal footer
                try:
                    simple_footer = f"Session completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
                    write_to_host_log(simple_footer)
                except Exception as e2:
                    logger.error(f"Even simple footer failed: {e2}")
    
    @staticmethod
    def run_ssh_command_on_host(hostname: str, username: str, password: str, commands: list, 
                               port: int = 22, timeout: int = 30, use_shell: bool = True) -> tuple:
        """
        Run SSH commands on a single host (for multi-threading)
        
        Args:
            hostname: IP address or hostname
            username: SSH username  
            password: SSH password
            commands: List of commands to execute
            port: SSH port
            timeout: Connection timeout
            use_shell: Whether to use shell mode
            
        Returns:
            tuple: (hostname, success, results_summary)
        """
        # Use the unified SSH runner logger (propagates to script.log)
        logger = logging.getLogger('ssh_runner_v2')

        try:
            logger.debug(f"[{hostname}] Starting SSH session...")

            if len(commands) == 1:
                # Single command
                host_success = EnhancedSSHRunner.run_ssh_command(hostname, username, password, commands[0], port, timeout, use_shell)
                return (hostname, host_success, f"Single command: {commands[0]}")
            else:
                # Multiple commands - check if we need interactive mode
                # Detect interactive sequences (su commands followed by potential passwords)
                needs_interactive = False
                for i, cmd in enumerate(commands):
                    cmd_lower = cmd.strip().lower()
                    # Check for commands that typically require interactive input
                    if cmd_lower in ['su', 'sudo', 'sudo su'] or cmd_lower.startswith('su '):
                        needs_interactive = True
                        logger.debug(f"[{hostname}] Interactive mode needed: detected '{cmd}' command")
                        break
                    # Check for sequences that look like password responses
                    if i > 0 and len(cmd.strip()) > 5:
                        prev_cmd = commands[i-1].strip().lower()
                        if prev_cmd in ['su', 'sudo'] and not cmd.startswith('/') and not cmd.startswith('show'):
                            needs_interactive = True
                            logger.debug(f"[{hostname}] Interactive mode needed: '{cmd}' looks like password response")
                            break
                
                if needs_interactive:
                    logger.info(f"[{hostname}] Using interactive mode for {len(commands)} commands")
                    host_success = EnhancedSSHRunner.run_multiple_ssh_commands_interactive(hostname, username, password, commands, port, timeout, use_shell)
                    return (hostname, host_success, f"{len(commands)} interactive commands executed")
                else:
                    # Standard sequential command execution
                    host_success = EnhancedSSHRunner.run_multiple_ssh_commands(hostname, username, password, commands, port, timeout, use_shell)
                    return (hostname, host_success, f"{len(commands)} commands executed")

        except Exception as e:
            logger.error(f"[{hostname}] Unexpected error: {type(e).__name__}: {e}", exc_info=True)
            return (hostname, False, f"Error: {e}")
    
    @staticmethod
    def run_ssh_commands_multi_host(hosts: list, username: str, password: str, commands: list,
                                   port: int = 22, timeout: int = 30, use_shell: bool = True,
                                   max_threads: int = 5) -> dict:
        """
        Run SSH commands on multiple hosts concurrently using threading

        Args:
            hosts: List of hostnames/IPs
            username: SSH username
            password: SSH password  
            commands: List of commands to execute on each host
            port: SSH port
            timeout: Connection timeout
            use_shell: Whether to use shell mode
            max_threads: Maximum number of concurrent threads

        Returns:
            dict: Results summary with success/failure counts per host
        """
        logger = logging.getLogger('ssh_runner_v2')
        # Debug diagnostic for mysterious dict+float TypeError
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(f"[TRACE] Enter run_ssh_commands_multi_host(hosts={hosts}, username={username}, port={port}, timeout={timeout}, use_shell={use_shell}, max_threads={max_threads})")
            logger.debug(f"[TRACE] Types: hosts={type(hosts)}, username={type(username)}, password={'***' if password else None}, commands={type(commands)}, timeout={type(timeout)}")
        
        print(f"\n>> Starting SSH execution on {len(hosts)} hosts ({max_threads} threads)")
        logger.info(f"Multi-host SSH execution: {len(hosts)} hosts, {len(commands)} commands, {max_threads} threads")
        logger.debug(f"Target hosts: {hosts}")
        logger.debug(f"Commands: {commands}")
        logger.debug(f"Connection parameters: port={port}, timeout={timeout}, use_shell={use_shell}")
        
        ssh_execution_results = {}
        successful_hosts = []
        failed_hosts = []
        
        # Use ThreadPoolExecutor for thread management
        with ThreadPoolExecutor(max_workers=max_threads, thread_name_prefix="SSH") as executor:
            # Submit all host tasks
            future_to_host = {
                executor.submit(EnhancedSSHRunner.run_ssh_command_on_host, host, username, password, commands, 
                               port, timeout, use_shell): host 
                for host in hosts
            }
            
            # Process completed tasks (custom loop to avoid as_completed timeout TypeError)
            try:
                import concurrent.futures as _cf
                pending = set(future_to_host.keys())
                iteration = 0
                while pending:
                    iteration += 1
                    done, pending = _cf.wait(pending, return_when=_cf.FIRST_COMPLETED)
                    for future in done:
                        if logger.isEnabledFor(logging.DEBUG):
                            logger.debug(f"[TRACE] wait loop iteration={iteration} future_done={future.done()} future={future}")
                        try:
                            hostname, host_success, summary = future.result()
                        except Exception as fut_e:
                            logger.error(f"[TRACE] Future exception: {type(fut_e).__name__}: {fut_e}", exc_info=True)
                            hostname = future_to_host.get(future, 'unknown')
                            host_success = False
                            summary = f"Error: {fut_e}"
                        ssh_execution_results[hostname] = {
                            'success': host_success,
                            'summary': summary
                        }
                        if host_success:
                            successful_hosts.append(hostname)
                            logger.debug(f"[{hostname}] Completed successfully: {summary}")
                        else:
                            failed_hosts.append(hostname)
                            logger.error(f"[{hostname}] Failed: {summary}")
            except Exception as loop_e:
                logger.error(f"[TRACE] Multi-host wait loop failure: {type(loop_e).__name__}: {loop_e}", exc_info=True)
                # Fallback: mark any remaining hosts as failed
                for future, host in future_to_host.items():
                    if host not in ssh_execution_results:
                        ssh_execution_results[host] = {'success': False, 'summary': f'Loop failure: {loop_e}'}
                        failed_hosts.append(host)
        
        # Summary report
        print(f"\n{'='*60}")
        print(f"[STATUS] EXECUTION SUMMARY")
        print(f"{'='*60}")
        print(f"Total hosts: {len(hosts)}")
        print(f"Successful: {len(successful_hosts)} [OK]")
        print(f"Failed: {len(failed_hosts)} [ERROR]")
        print(f"Per-host logs: per-host-logs/ssh_output_<hostname>_<timestamp>.log")
        
        if successful_hosts:
            print(f"\n[OK] Successful hosts: {', '.join(successful_hosts)}")
        
        if failed_hosts:
            print(f"\n[ERROR] Failed hosts: {', '.join(failed_hosts)}")
        
        logger.info(f"Multi-host execution completed: {len(successful_hosts)}/{len(hosts)} successful")
        
        return {
            'total': len(hosts),
            'successful': len(successful_hosts),
            'failed': len(failed_hosts),
            'successful_hosts': successful_hosts,
            'failed_hosts': failed_hosts,
            'results': ssh_execution_results
        }
    
    @staticmethod
    def run_application(args):
        """Main application logic - handles all the SSH runner functionality"""
        # Determine logging level (--debug flag overrides --log-level)
        log_level = 'DEBUG' if args.debug else args.log_level
        
        # Setup logging with specified level
        logger = EnhancedSSHRunner.setup_logging(log_level)

        # Optional line-level tracing (only when debug enabled) to capture exact failing line
        tracer_installed = False
        previous_tracer = None
        if logger.isEnabledFor(logging.DEBUG):
            try:
                import sys, inspect
                runner_file = __file__
                # Rough bounds: limit tracing to lines inside this file within the class region to reduce noise
                CLASS_START = 14300  # approximate lower bound (keep generous)
                CLASS_END = 16600    # approximate upper bound
                def _ssh_line_tracer(frame, event, arg):
                    if event == 'line':
                        try:
                            if frame.f_code.co_filename == runner_file and CLASS_START <= frame.f_lineno <= CLASS_END:
                                logger.debug(f"[LINE] {frame.f_code.co_name}:{frame.f_lineno}")
                        except Exception:
                            pass
                    return _ssh_line_tracer
                previous_tracer = sys.gettrace()
                sys.settrace(_ssh_line_tracer)
                tracer_installed = True
                logger.debug("[TRACE] Line-level tracer installed for EnhancedSSHRunner region")
            except Exception as _trace_e:
                logger.debug(f"[TRACE] Failed to install line tracer: {_trace_e}")
        
        # Interactive mode
        if args.interactive:
            return EnhancedSSHRunner.interactive_mode()
        
        # Determine if we should use .env file (default behavior unless --no-env is specified)
        use_env = not args.no_env
        
        # Try to load .env configuration
        env_config = {}
        if use_env:
            logger.info("Loading SSH credentials from .env file (default behavior)")
            env_config = EnhancedSSHRunner.load_ssh_config_from_env()
            if any([env_config.get('hosts'), env_config['username'], env_config['password']]):
                host_count = len(env_config.get('hosts', []))
                hosts_str = ', '.join(env_config.get('hosts', [])) if host_count <= 3 else f"{host_count} hosts"
                logger.info(f"Found .env credentials - Hosts: {hosts_str}, User: {env_config['username']}, Commands: {len(env_config['commands'])}")
        
        # Determine final connection parameters (command line overrides .env)
        final_hosts = []
        if args.hostname:
            final_hosts = [args.hostname]  # Single host from command line
        elif env_config.get('hosts'):
            final_hosts = env_config['hosts']  # Multiple hosts from .env
        
        final_username = args.username or env_config.get('username') 
        final_password = env_config.get('password')  # Only from .env, never from command line
        
        # Handle secure password input if needed
        if not final_password and not args.secure:
            if final_username and final_hosts:
                host_display = final_hosts[0] if len(final_hosts) == 1 else f"{len(final_hosts)} hosts"
                final_password = getpass.getpass(f"!? Enter password for {final_username}@{host_display}: ")
            else:
                print("X  Password required but not provided")
                return False
        elif args.secure and not final_password:
            host_display = final_hosts[0] if len(final_hosts) == 1 else f"{len(final_hosts)} hosts"
            final_password = getpass.getpass(f"!? Enter password for {final_username}@{host_display}: ")
        # SECURITY: Password argument removed - this code block is no longer needed
        
        # Validate final parameters
        validated_hosts = []
        invalid_hosts = []
        
        for host in final_hosts:
            if EnhancedSSHRunner.validate_hostname(host):
                validated_hosts.append(host)
            else:
                invalid_hosts.append(host)
        
        if invalid_hosts:
            print(f"X  Invalid hosts detected: {', '.join(invalid_hosts)}")
            if not validated_hosts:
                print("X  No valid hosts remaining")
                return False
            else:
                print(f"[WARNING] Proceeding with {len(validated_hosts)} valid hosts")
                final_hosts = validated_hosts
        
        # Validate username
        if final_username and not EnhancedSSHRunner.validate_username(final_username):
            print(f"[ERROR] Invalid username format: {final_username}")
            return False
        
        # Check if we have minimum required parameters
        if not all([final_hosts, final_username, final_password]):
            missing = []
            if not final_hosts: missing.append("hostname/SSH_HOST")
            if not final_username: missing.append("username/SSH_USER") 
            if not final_password: missing.append("password/SSH_PASSWORD")
            
            print(f"X  Error: Missing required parameters: {', '.join(missing)}")
            if use_env:
                print("!? Add these to your .env file or provide as command line arguments")
                print("!? Use --no-env flag to disable .env file loading")
            else:
                print("!? Provide as command line arguments or remove --no-env flag to use .env file")
                # Since we can't access the parser here, we'll let the caller handle help display
            return False
        
        # Determine commands to execute
        commands_to_run = []
        
        # Priority 1: Command line argument
        if args.command:
            commands_to_run = [args.command]
            logger.info(f"Using command from command line: {args.command}")
        # Priority 2: SSH_COMMANDS from .env file
        elif use_env and env_config.get('commands'):
            commands_to_run = env_config['commands']
            logger.info(f"Using {len(commands_to_run)} commands from .env file: {commands_to_run}")
    # Priority 3: data/SSH_COMMANDS.CSV file as fallback
        elif not args.command:
            csv_commands = EnhancedSSHRunner.load_commands_from_csv()
            if csv_commands:
                commands_to_run = csv_commands
                logger.info(f"Using {len(commands_to_run)} commands from data/SSH_COMMANDS.CSV: {commands_to_run}")
                print(f"!? Loaded {len(commands_to_run)} commands from data/SSH_COMMANDS.CSV")
        # Priority 4: Interactive input
        else:
            # Check what command sources are available
            env_commands = env_config.get('commands', []) if use_env else []
            csv_commands = EnhancedSSHRunner.load_commands_from_csv() if not commands_to_run else []
            
            if env_commands and csv_commands:
                command = input(f"!? Enter command to execute (or press Enter to use {len(env_commands)} commands from .env, or 'csv' for {len(csv_commands)} commands from CSV): ").strip()
                if not command:
                    commands_to_run = env_commands
                    print(f"!? Using {len(commands_to_run)} commands from .env file: {commands_to_run}")
                elif command.lower() == 'csv':
                    commands_to_run = csv_commands
                    print(f"!? Using {len(commands_to_run)} commands from data/SSH_COMMANDS.CSV: {commands_to_run}")
                else:
                    commands_to_run = [command]
            elif env_commands:
                command = input(f"!? Enter command to execute (or press Enter to use {len(env_commands)} commands from .env): ").strip()
                if not command:
                    commands_to_run = env_commands
                    print(f"!? Using {len(commands_to_run)} commands from .env file: {commands_to_run}")
                else:
                    commands_to_run = [command]
            elif csv_commands:
                command = input(f"!? Enter command to execute (or press Enter to use {len(csv_commands)} commands from data/SSH_COMMANDS.CSV): ").strip()
                if not command:
                    commands_to_run = csv_commands
                    print(f"!? Using {len(commands_to_run)} commands from data/SSH_COMMANDS.CSV: {commands_to_run}")
                else:
                    commands_to_run = [command]
            else:
                command = input("!? Enter command to execute: ").strip()
                if not command:
                    print("X  No commands specified")
                    return False
                commands_to_run = [command]
        
        # Validate commands
        validated_commands = []
        invalid_commands = []
        
        for cmd in commands_to_run:
            if EnhancedSSHRunner.validate_command(cmd):
                validated_commands.append(cmd)
            else:
                invalid_cmd = cmd[:50] + "..." if len(cmd) > 50 else cmd
                invalid_commands.append(invalid_cmd)
        
        if invalid_commands:
            print(f"X  Invalid commands detected: {', '.join(invalid_commands)}")
            if not validated_commands:
                print("X  No valid commands remaining")
                return False
            else:
                print(f"!? Proceeding with {len(validated_commands)} valid commands")
                commands_to_run = validated_commands
        
        if not commands_to_run:
            print("X  No commands to execute")
            return False
        
        # Determine shell mode (default is True unless --no-shell is specified)
        use_shell_mode = args.shell and not args.no_shell
        
        # Execute SSH commands
        try:
            if len(final_hosts) == 1:
                # Single host execution
                hostname = final_hosts[0]
                if len(commands_to_run) == 1:
                    # Single command on single host
                    ssh_success = EnhancedSSHRunner.run_ssh_command(
                        hostname,
                        final_username,
                        final_password,
                        commands_to_run[0],
                        args.port,
                        args.timeout,
                        use_shell_mode
                    )
                else:
                    # Multiple commands on single host
                    ssh_success = EnhancedSSHRunner.run_multiple_ssh_commands(
                        hostname,
                        final_username,
                        final_password,
                        commands_to_run,
                        args.port,
                        args.timeout,
                        use_shell_mode
                    )
                
                return ssh_success
                
            else:
                # Multiple host execution (multi-threaded)
                default_threads = multiprocessing.cpu_count()
                requested_threads = args.max_threads or default_threads
                max_threads = EnhancedSSHRunner.validate_thread_count(requested_threads, len(final_hosts))
                
                if max_threads != requested_threads:
                    print(f"!? Adjusted thread count from {requested_threads} to {max_threads}")
                
                ssh_results = EnhancedSSHRunner.run_ssh_commands_multi_host(
                    final_hosts,
                    final_username,
                    final_password,
                    commands_to_run,
                    args.port,
                    args.timeout,
                    use_shell_mode,
                    max_threads
                )
                
                # Return success if all hosts succeeded
                return ssh_results['failed'] == 0
            
        except KeyboardInterrupt:
            print("\n[INTERRUPT] Operation cancelled by user")
            return False
        except Exception as e:
            # Enhanced diagnostic logging for elusive dict+float TypeError
            logger.error("Fatal error during SSH runner execution", exc_info=True)
            try:
                logger.debug(f"[DIAG] Type of exception object: {type(e)}")
            except Exception:
                pass
            print(f"X  Fatal error: {e}")
            return False
        finally:
            if tracer_installed:
                try:
                    import sys
                    sys.settrace(previous_tracer)
                    logger.debug("[TRACE] Line-level tracer removed")
                except Exception as _trace_cleanup_e:
                    logger.debug(f"[TRACE] Failed to remove line tracer: {_trace_cleanup_e}")

    @staticmethod
    def create_argument_parser():
        """Create and configure the argument parser"""
        parser = argparse.ArgumentParser(
            description="Enhanced SSH Command Runner v2 - Execute commands on remote hosts via SSH",
            formatter_class=argparse.RawDescriptionHelpFormatter,
            epilog="""
Examples:
    # Default: Uses .env file and shell mode (recommended)
    python ssh_runner_v2.py
    
    # Override with specific command (still uses shell mode by default)
    python ssh_runner_v2.py "show version"
    
    # Manual SSH connection (uses secure password prompt)
    python ssh_runner_v2.py 192.168.1.1 vyos --secure "show version"
    
    # Use exec_command mode instead of shell mode  
    python ssh_runner_v2.py --no-shell "ls -la"
    
    # Multi-host with custom thread count
    python ssh_runner_v2.py --max-threads 10
    
    # Interactive mode
    python ssh_runner_v2.py --interactive
    
    # Disable .env loading and use exec_command mode with secure password
    python ssh_runner_v2.py --no-env --no-shell --secure 192.168.1.1 vyos "show version"

.env file format (SECURITY: Keep this file private and out of version control):
    SSH_HOST=192.168.1.1,192.168.1.2,192.168.1.3
    SSH_USER=vyos
    SSH_PASSWORD=your_password
    SSH_COMMANDS=show version,show interfaces,show route
    
SECURITY NOTES:
    - Never commit .env files containing passwords to version control
    - Use secure password prompts (--secure flag) when possible
    - Consider using SSH keys instead of passwords for better security
    - Add .env to your .gitignore file
            """
        )
        
        # Interactive mode
        parser.add_argument("--interactive", "-i", action="store_true",
                           help="Run in interactive mode")
        
        # .env file mode controls
        parser.add_argument("--no-env", action="store_true",
                           help="Disable automatic .env file loading (use manual credentials)")
        
        # Connection parameters
        parser.add_argument("hostname", nargs="?", help="Hostname or IP address (overrides SSH_HOST)")
        parser.add_argument("username", nargs="?", help="SSH username (overrides SSH_USER)") 
        parser.add_argument("password", nargs="?", help="SSH password (overrides SSH_PASSWORD)")
        parser.add_argument("command", nargs="?", help="Command to execute (overrides SSH_COMMANDS)")
        
        # Optional parameters with validation
        def validate_port_arg(value):
            ivalue = int(value)
            if not EnhancedSSHRunner.validate_port(ivalue):
                raise argparse.ArgumentTypeError(f"Port must be between 1 and 65535, got {ivalue}")
            return ivalue
        
        def validate_timeout_arg(value):
            ivalue = int(value)
            if not EnhancedSSHRunner.validate_timeout(ivalue):
                raise argparse.ArgumentTypeError(f"Timeout must be between 1 and 3600 seconds, got {ivalue}")
            return ivalue
        
        def validate_threads_arg(value):
            ivalue = int(value)
            if ivalue <= 0 or ivalue > 100:
                raise argparse.ArgumentTypeError(f"Thread count must be between 1 and 100, got {ivalue}")
            return ivalue
        
        parser.add_argument("--port", "-p", type=validate_port_arg, default=22,
                           help="SSH port (default: 22)")
        parser.add_argument("--timeout", "-t", type=validate_timeout_arg, default=30,
                           help="Connection timeout in seconds (default: 30)")
        parser.add_argument("--secure", "-s", action="store_true",
                           help="Prompt for password securely instead of command line")
        parser.add_argument("--shell", action="store_true", default=True,
                           help="Use interactive shell mode (default, recommended for network devices)")
        parser.add_argument("--no-shell", action="store_true",
                           help="Disable shell mode and use exec_command instead")
        parser.add_argument("--log-level", choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                           default='INFO', help="Set logging level (default: INFO)")
        parser.add_argument("--debug", "-d", action="store_true",
                           help="Enable debug logging (equivalent to --log-level DEBUG)")
        parser.add_argument("--max-threads", type=validate_threads_arg, default=None,
                           help=f"Maximum threads for multi-host execution (default: {multiprocessing.cpu_count()} cores)")
        
        return parser

    @staticmethod
    def interactive_mode():
        """Interactive mode for SSH command execution with input validation"""
        print("- Enhanced SSH Command Runner v2 - Interactive Mode")
        print("=" * 60)
        
        # Get connection details with validation
        while True:
            hostname = input("- Enter hostname or IP address: ").strip()
            if not hostname:
                print("X  Hostname is required")
                continue
            if not EnhancedSSHRunner.validate_hostname(hostname):
                print("X  Invalid hostname or IP address format")
                continue
            break
        
        while True:
            username = input("X  Enter username: ").strip()
            if not username:
                print("X  Username is required")
                continue
            if not EnhancedSSHRunner.validate_username(username):
                print("X  Invalid username format (alphanumeric, underscore, hyphen, dot only)")
                continue
            break
        
        password = getpass.getpass("!? Enter password: ")
        if not password:
            print("X  Password is required")
            return False
        
        # Optional settings with validation
        while True:
            try:
                port_input = input(">> Enter SSH port (default 22): ").strip()
                if not port_input:
                    port = 22
                    break
                port = int(port_input)
                if not EnhancedSSHRunner.validate_port(port):
                    print("X  Port must be between 1 and 65535")
                    continue
                break
            except ValueError:
                print("X  Port must be a valid number")
        
        while True:
            try:
                timeout_input = input("- Enter timeout in seconds (default 30): ").strip()
                if not timeout_input:
                    timeout = 30
                    break
                timeout = int(timeout_input)
                if not EnhancedSSHRunner.validate_timeout(timeout):
                    print("X  Timeout must be between 1 and 3600 seconds")
                    continue
                break
            except ValueError:
                print("X  Timeout must be a valid number")
        
        # Execution mode
        shell_mode = input("X  Use interactive shell mode? (y/N - recommended for network devices): ").strip().lower()
        use_shell = shell_mode in ['y', 'yes', 'true', '1']
        
        # Get command with validation
        while True:
            command = input("!? Enter command to execute: ").strip()
            if not command:
                print("X  Command is required")
                continue
            if not EnhancedSSHRunner.validate_command(command):
                print("X  Invalid command (too long or contains null bytes)")
                continue
            break
        
        print(f"\n>> Starting SSH session (shell_mode={use_shell})...")
        
        # Execute
        return EnhancedSSHRunner.run_ssh_command(hostname, username, password, command, port, timeout, use_shell)



def main():
    """Main entry point for MistHelper CLI application."""
    logging.debug("ENTRY: main()")
    
    # Handle deferred import initialization if needed (only once)
    global success, global_assignments
    if not success and not global_assignments and not hasattr(import_manager, '_deferred_init_done'):
        logging.info("Initializing deferred imports at application start...")
        success, global_assignments = import_manager.initialize_all_imports()
        import_manager._deferred_init_done = True  # Mark as completed
        
        # Apply global assignments to module namespace
        if global_assignments:
            for var_name, var_value in global_assignments.items():
                globals()[var_name] = var_value
                # Special handling for tqdm to ensure it overrides the fallback
                if var_name == 'tqdm' and var_value is not None:
                    logging.info(f"Successfully imported real tqdm in deferred mode: {type(var_value)}")
            logging.debug(f"Applied {len(global_assignments)} global variable assignments")
            
            # Verify tqdm was properly imported
            if 'tqdm' in global_assignments:
                logging.info(f"tqdm is available in global namespace: {type(globals().get('tqdm'))}")
            else:
                logging.warning("tqdm was not found in global assignments - progress bars will not be functional")
        
        if not success:
            logging.warning("Some required imports failed - functionality may be limited")
    elif hasattr(import_manager, '_deferred_init_done'):
        logging.debug("Deferred imports already initialized, skipping duplicate initialization")
    
    # Ensure tqdm is properly available
    ensure_tqdm_available()
    
    # --- CLI Argument Parsing ---
    parser = argparse.ArgumentParser(description="MistHelper CLI Interface")
    parser.add_argument("-O", "--org", help="Organization ID")
    parser.add_argument("-M", "--menu", help="Menu option number to execute")
    parser.add_argument("-S", "--site", help="Human-readable site name")
    parser.add_argument("-D", "--device", help="Human-readable device name")
    parser.add_argument("-P", "--port", help="Port ID")
    parser.add_argument("--debug", action="store_true", help="Enable debug output (includes detailed table data in logs)")
    parser.add_argument("--delay", type=int, help="Fixed delay between loop iterations (in seconds). If omitted, delay is dynamic.")
    parser.add_argument("--fast", action="store_true", help="Enable fast mode with multithreading (bypasses rate limiting)")
    parser.add_argument("--skip-deps", action="store_true", help="Skip dependency check on startup for faster script initialization")
    parser.add_argument("--output-format", choices=["csv", "sqlite"], default="csv", 
                       help="Output format: 'csv' for CSV files (default) or 'sqlite' for hybrid database with natural primary keys")
    parser.add_argument("--test", action="store_true", help="Run systematic test of all safe menu options (GET operations only, no interactive/websocket/POST operations)")
    parser.add_argument("--testinteractive", action="store_true", help="Run systematic test of read-only menu options requiring interactive site/device/client selection (excludes destructive operations)")
    parser.add_argument("--dry-run", action="store_true", help="Enable dry-run mode for destructive operations (show what would be changed without making actual changes)")
    parser.add_argument("--address-check", action="store_true", help="Enable external address validation using Nominatim API for address comparison operations")
    parser.add_argument("--skip-ssl-verify", action="store_true", help="Skip SSL certificate verification for external API calls (use with caution - for corporate networks only)")
    parser.add_argument("--no-env", action="store_true", help="Disable .env file loading for SSH operations (require explicit command line parameters)")
    parser.add_argument("--tui", action="store_true", help="Launch MistHelper in Terminal User Interface (TUI) mode for visual navigation of Mist API library")
    args = parser.parse_args()

    # Store args globally for menu functions to access CLI flags
    globals()['args'] = args

    # ------------------------------------------------------------------------
    # Establish global FAST_MODE_ENABLED flag for systematic test harness
    # The harness inspects globals()['FAST_MODE_ENABLED']; previously this was
    # never set, causing fast mode to be ignored inside run_systematic_test.
    # SECURITY: Read-only flag derived solely from CLI input; no external input.
    # ------------------------------------------------------------------------
    try:
        global FAST_MODE_ENABLED
        FAST_MODE_ENABLED = bool(args.fast)
    except Exception:
        # Fail-safe: ensure symbol exists even if something unexpected happens
        FAST_MODE_ENABLED = False

    # ------------------------------------------------------------------------
    # FAST MODE STARTUP BANNER (Feature A)
    # Enumerate functions that currently accept fast= so operators know scope.
    # This is intentionally static (no reflection over globals()) for safety & clarity.
    # ------------------------------------------------------------------------
    if args.fast:
        fast_capable = [
            "export_gateway_synthetic_tests_to_csv",
            "get_gateway_devices_with_sites",
            "export_gateway_device_stats_to_csv_with_freshness_check",
            "export_gateway_device_stats_to_csv",
            "export_gateway_test_results_by_site_to_csv",
            "export_devices_with_site_info_to_csv",
            "export_gateway_device_configs_to_csv",
            "fetch_gateway_device_configs_from_api",
            "compare_inventory_with_csv",
            "export_gateways_with_wan_overrides_to_csv",
            # Newly added fast-capable stats exporters:
            "export_device_stats_to_csv",
            "export_device_port_stats_to_csv",
            "export_vpn_peer_stats_to_csv",
        ]
        logging.info("FAST MODE ACTIVE: Enabling caching/concurrency shortcuts for: " + ", ".join(fast_capable))
        print("* Fast mode active (caching/concurrency). Functions optimized:")
        for name in fast_capable:
            print(f"  - {name}")
    
    # ============================================================================
    # DEPENDENCY MANAGEMENT - Initialize imports if not already done
    # ============================================================================
    if not _initialize_imports_now and not hasattr(import_manager, '_deferred_init_done'):
        # If imports were deferred, initialize them now with proper skip behavior
        if not args.skip_deps:
            logging.info("Initializing deferred dependencies with full checking...")
            success, global_assignments = import_manager.initialize_all_imports(skip_deps=False)
            import_manager._deferred_init_done = True  # Mark as completed
            
            # Apply global assignments
            if global_assignments:
                for var_name, var_value in global_assignments.items():
                    globals()[var_name] = var_value
                logging.debug(f"Applied {len(global_assignments)} global variable assignments")
            
            if not success and not args.test:
                logging.error("Critical dependencies missing. Exiting.")
                print("!! Critical dependencies missing. Use --skip-deps to bypass or install missing packages.")
                sys.exit(1)
        else:
            logging.info("Dependency initialization skipped due to --skip-deps flag")
            # Still need to initialize the basic imports for core functionality
            success, global_assignments = import_manager.initialize_all_imports(skip_deps=True)
            import_manager._deferred_init_done = True  # Mark as completed
            
            # Apply global assignments even in skip mode
            if global_assignments:
                for var_name, var_value in global_assignments.items():
                    globals()[var_name] = var_value
                logging.debug(f"Applied {len(global_assignments)} global variable assignments in skip mode")
    elif hasattr(import_manager, '_deferred_init_done'):
        logging.debug("Dependencies already initialized, skipping duplicate initialization")
    
    # Initialize Mist API session after dependencies are available
    if not initialize_mist_session():
        logging.error("Failed to initialize Mist API session")
        print(" Failed to initialize Mist API session. Check your credentials.")
        sys.exit(1)
    
    # Set global output format based on CLI argument
    global OUTPUT_FORMAT
    OUTPUT_FORMAT = args.output_format
    timestamp = datetime.now(timezone.utc).isoformat()
    logging.info(f"Output format set to: {OUTPUT_FORMAT} at {timestamp}")
    
    # Enable debug logging if --debug flag is provided
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
        # Set ONLY file handlers to DEBUG level, keep console at INFO
        for handler in logging.getLogger().handlers:
            if isinstance(handler, logging.FileHandler):
                handler.setLevel(logging.DEBUG)
            elif isinstance(handler, logging.StreamHandler):
                handler.setLevel(logging.INFO)
        logging.debug("Debug logging enabled via --debug flag")
        logging.debug(f"Command line arguments: {' '.join(sys.argv)}")
        logging.debug("Performance monitoring will trigger circuit breakers for infinite loops")
    
    # Handle systematic testing mode
    if args.test:
        logging.info("SYSTEMATIC_TEST: Starting systematic test mode")
        print(">> Systematic test mode activated")
        if args.skip_deps:
            print(">> Dependency checks skipped due to --skip-deps flag")
        else:
            print(">> Running systematic test with full dependency verification")
        success = run_systematic_test()
        logging.info(f"SYSTEMATIC_TEST: Test mode completed with success={success}")
        sys.exit(0 if success else 1)
    
    # Handle interactive testing mode
    if args.testinteractive:
        logging.info("INTERACTIVE_TEST: Starting interactive test mode")
        print(">> Interactive test mode activated")
        if args.skip_deps:
            print(">> Dependency checks skipped due to --skip-deps flag")
        else:
            print(">> Running interactive test with full dependency verification")
        success = run_interactive_test()
        logging.info(f"INTERACTIVE_TEST: Test mode completed with success={success}")
        sys.exit(0 if success else 1)
    
    # Handle TUI mode
    if args.tui:
        logging.info("TUI_MODE: Starting Terminal User Interface mode")
        print(">> Terminal User Interface mode activated")
        
        # Initialize Mist API session for TUI mode
        if not apisession:
            print(">> Initializing Mist API session...")
            if not initialize_mist_session():
                print("[ERROR] Failed to initialize Mist API session")
                logging.error("TUI_MODE: Could not initialize API session")
                sys.exit(1)
            print(">> API session initialized successfully")
        
        # Remove console handler during TUI mode to prevent log messages from interfering with Rich display
        root_logger = logging.getLogger()
        console_handlers = [h for h in root_logger.handlers if isinstance(h, logging.StreamHandler) and not isinstance(h, logging.FileHandler)]
        for handler in console_handlers:
            root_logger.removeHandler(handler)
            logging.debug("TUI_MODE: Removed console handler to prevent interference with Rich TUI")
        
        try:
            tui = MistHelperTUI(debug_mode=args.debug)
            # Pass the global apisession to TUI for API call execution
            tui.apisession = apisession
            if args.debug:
                logging.debug("TUI_MODE: Debug mode is ACTIVE - enhanced logging enabled")
            tui.run()
        except KeyboardInterrupt:
            if args.debug:
                timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]
                logging.debug(f"TUI_DEBUG: [{timestamp}] KeyboardInterrupt caught - user pressed Ctrl+C")
            logging.info("TUI_MODE: User interrupted with Ctrl+C")
            print("\n[EXIT] TUI mode stopped by user")
        except Exception as error:
            if args.debug:
                timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]
                logging.debug(f"TUI_DEBUG: [{timestamp}] Exception caught in TUI mode: {type(error).__name__}: {error}")
            logging.error(f"TUI_MODE: Fatal error - {error}", exc_info=True)
            print(f"\n[ERROR] TUI mode crashed: {error}")
            sys.exit(1)
        
        if args.debug:
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]
            logging.debug(f"TUI_DEBUG: [{timestamp}] TUI mode completed successfully - about to exit")
        logging.info("TUI_MODE: TUI mode completed successfully")
        sys.exit(0)
    
    logging.debug(f"Parsed CLI arguments: org={args.org}, menu={args.menu}, site={args.site}, device={args.device}, port={args.port}, debug={args.debug}, delay={args.delay}, fast={args.fast}, skip_deps={args.skip_deps}, output_format={args.output_format}, test={args.test}, address_check={args.address_check}, tui={args.tui}")

    global org_id
    # Check if meaningful CLI arguments are provided (not just script name or flags-only)
    meaningful_cli_args = args.menu or args.org or args.site or args.device or args.port or args.test
    if meaningful_cli_args:
        logging.info("CLI arguments detected, running in non-interactive mode.")
        if args.org:
            org_id = args.org
            logging.info(f"Using org_id from CLI argument: {org_id}")
        else:
            org_id = get_cached_or_prompted_org_id()

        site_id = None
        if args.site:
            logging.info(f"Resolving site name '{args.site}' to site_id using unified pagination limit {DEFAULT_API_PAGE_LIMIT}...")
            sites = fetch_all_sites_with_limit(org_id)
            site_lookup = {site.get("name"): site.get("id") for site in sites if site.get("name") and site.get("id")}
            site_id = site_lookup.get(args.site)
            if not site_id:
                logging.error(f"! Site name '{args.site}' not found.")
                print(f"! Site name '{args.site}' not found.")
                sys.exit(1)
            else:
                logging.info(f"Resolved site name '{args.site}' to site_id '{site_id}'.")

        device_id = None
        if args.device and site_id:
            logging.info(f"Resolving device name '{args.device}' at site_id '{site_id}'...")
            response = mistapi.api.v1.sites.devices.listSiteDevices(apisession, site_id, type='all')
            devices = mistapi.get_all(response=response, mist_session=apisession)
            device_lookup = {dev["name"]: dev["id"] for dev in devices}
            device_id = device_lookup.get(args.device)
            if not device_id:
                logging.error(f"! Device name '{args.device}' not found at site '{args.site}'.")
                print(f"! Device name '{args.device}' not found at site '{args.site}'.")
                sys.exit(1)
            else:
                logging.info(f"Resolved device name '{args.device}' to device_id '{device_id}'.")

        if args.menu in menu_actions:
            func, _ = menu_actions[args.menu]
            logging.info(f"Executing menu action '{args.menu}'.")
            func_args = {
                "site_id": site_id,
                "device_id": device_id,
                "port": args.port,
                "org_id": org_id,
                "debug": args.debug,
                "delay": args.delay,
                "fast": args.fast,
                "dry_run": args.dry_run,
                "address_check": args.address_check,
                "skip_ssl_verify": args.skip_ssl_verify
            }
            sig = inspect.signature(func)
            accepted_args = {k: v for k, v in func_args.items() if k in sig.parameters and v is not None}
            func(**accepted_args)
        else:
            logging.error(f"! Invalid menu option: {args.menu}")
            print(f"! Invalid menu option: {args.menu}")
            sys.exit(1)

        logging.info("CLI execution complete. Exiting.")
        logging.debug("EXIT: main() - CLI success")
        sys.exit(0)

    # --- Interactive Menu Fallback ---
    logging.info("No CLI arguments detected, running in interactive menu mode.")
    
    # Initialize org_id for interactive mode
    org_id = get_cached_or_prompted_org_id()
    logging.info(f"Organization ID initialized for interactive mode: {org_id}")
    
    # Check if running in container for different behavior
    container_mode = is_running_in_container()
    if container_mode:
        logging.info("Container mode detected - enabling continuous menu loop")
        print("[CONTAINER MODE] MistHelper will return to menu after each operation")
        print("                 Use option 0 to exit the container")
    
    # Container mode: continuous menu loop, Direct mode: single execution
    while True:
        print("\nAvailable Options:")
        # Sort menu options numerically for proper presentation order
        sorted_menu_keys = sorted(menu_actions.keys(), key=lambda x: float(x.replace('a', '.1')))
        for key in sorted_menu_keys:
            func, description = menu_actions[key]
            print(f"{key}: {description}")
        
        try:
            iwant = input("\nEnter your selection number now: ").strip()
        except EOFError:
            # Handle EOF condition (Ctrl+D, broken pipe, SSH disconnection)
            print("\n[EOF] Input stream closed. Exiting gracefully...")
            logging.info("EOF encountered on input - user disconnected or input stream closed")
            if container_mode:
                print("[CONTAINER MODE] SSH session ended. Terminating MistHelper.")
            break
        except KeyboardInterrupt:
            # Handle Ctrl+C
            print("\n[INTERRUPT] User interrupted. Exiting...")
            logging.info("KeyboardInterrupt encountered - user pressed Ctrl+C")
            break
            
        # Graceful handling of empty input: simply redisplay menu without logging an error
        if iwant == "":
            if container_mode:
                print("[CONTAINER MODE] No selection entered. Redisplaying menu...")
                print("=" * 60)
                continue
            else:
                # In direct (non-container) interactive mode, just prompt again for clarity
                print("No selection entered. Please enter a menu number.")
                continue
        selected = menu_actions.get(iwant)
        
        if selected:
            func, _ = selected
            logging.info(f"User selected menu option '{iwant}'. Executing associated function.")
            
            try:
                # Special handling for exit option
                if iwant == "0":
                    logging.info("Exit option selected by user.")
                    logging.debug("EXIT: main() - user requested exit")
                    sys.exit(0)
                
                func()
                logging.info(f"Menu option '{iwant}' execution complete.")
                
                # In container mode, return to menu. In direct mode, exit.
                if not container_mode:
                    logging.debug("EXIT: main() - interactive success (direct mode)")
                    sys.exit(0)
                else:
                    logging.debug(f"Container mode: option '{iwant}' completed successfully, returning to menu")
                    print(f"\n[CONTAINER MODE] Operation '{iwant}' completed. Returning to menu...")
                    print("=" * 60)
                    
            except KeyboardInterrupt:
                logging.info("Operation interrupted by user (Ctrl+C)")
                if container_mode:
                    logging.debug(f"Container mode: option '{iwant}' interrupted, returning to menu")
                    print("\n[CONTAINER MODE] Operation interrupted. Returning to menu...")
                    print("=" * 60)
                    continue
                else:
                    logging.debug("EXIT: main() - user interrupt")
                    sys.exit(130)
                    
            except Exception as e:
                logging.error(f"Error executing menu option '{iwant}': {e}")
                if container_mode:
                    logging.debug(f"Container mode: option '{iwant}' failed with error, returning to menu")
                    print(f"\n[CONTAINER MODE] Error in operation '{iwant}': {e}")
                    print("Returning to menu...")
                    print("=" * 60)
                    continue
                else:
                    logging.debug("EXIT: main() - interactive error (direct mode)")
                    sys.exit(1)
        else:
            # Invalid (non-empty) entry
            logging.error(f"Invalid selection '{iwant}' entered by user.")
            print("Invalid selection. Please try again.")
            if not container_mode:
                logging.debug("EXIT: main() - invalid selection (direct mode)")
                sys.exit(1)
            else:
                logging.debug(f"Container mode: invalid selection '{iwant}', redisplaying menu")
            # In container mode, continue loop for another attempt

if __name__ == "__main__":
    try:
        logging.info("=== MistHelper application starting ===")
        # Single explicit banner for test mode to clarify reduced lookbacks
        try:
            if IS_TEST_MODE:
                logging.info("TEST MODE ACTIVE: Reducing default 24h lookback windows to 1h for eligible exports.")
        except NameError:
            # IS_TEST_MODE may not yet be defined if refactor order changes; ignore safely
            pass
        # Install a global exception hook early so we capture full tracebacks for unexpected issues
        def _global_excepthook(exc_type, exc_value, exc_traceback):
            try:
                import traceback as _tb
                if issubclass(exc_type, KeyboardInterrupt):
                    # Defer to default behavior for Ctrl+C
                    sys.__excepthook__(exc_type, exc_value, exc_traceback)
                    return
                formatted = ''.join(_tb.format_exception(exc_type, exc_value, exc_traceback))
                logging.error("UNHANDLED TOP-LEVEL EXCEPTION TRACEBACK FOLLOWS")
                for line in formatted.rstrip().splitlines():
                    logging.error(line)
            except Exception as hook_err:
                logging.error(f"Exception in global excepthook: {hook_err}")
        try:
            import sys as _sys_mod
            _sys_mod.excepthook = _global_excepthook  # type: ignore[attr-defined]
        except Exception as hook_setup_err:
            logging.warning(f"Failed to install global excepthook: {hook_setup_err}")
        main()
    except KeyboardInterrupt:
        logging.info("Application interrupted by user (Ctrl+C)")
        logging.debug("EXIT: __main__ - user interrupt")
        sys.exit(130)  # Standard exit code for SIGINT
    except Exception as e:
        logging.error(f"Unhandled exception in main application: {e}")
        try:
            import traceback
            traceback_details = ''.join(traceback.format_exception(type(e), e, e.__traceback__))
            for line in traceback_details.rstrip().splitlines():
                logging.error(line)
        except Exception as trace_err:
            logging.error(f"Failed to log exception traceback: {trace_err}")
        logging.debug("EXIT: __main__ - unhandled exception")
        sys.exit(1)
    finally:
        logging.info("=== MistHelper application ending ===")
#hi